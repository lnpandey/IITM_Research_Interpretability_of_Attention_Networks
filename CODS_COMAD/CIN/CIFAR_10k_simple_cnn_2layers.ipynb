{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Exp_4_Attention_models_on_9_datasets_made_from_10k_mosaic_on_cnn_net_lr0_01_epoch100.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d6ec75b894c44439a2caf44c91fb8b5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c031f1795196474ea26ea0df2e6f788b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_1ee467ef16fe4b10b58d4eb913dcdced",
              "IPY_MODEL_2f23ba08bce54d9d88b55ee98829862f"
            ]
          }
        },
        "c031f1795196474ea26ea0df2e6f788b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1ee467ef16fe4b10b58d4eb913dcdced": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_184dddba7a5b465693df2b3df17b0efd",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 170498071,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 170498071,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3924784a90fa470ea7c2451c7bf47bab"
          }
        },
        "2f23ba08bce54d9d88b55ee98829862f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9d23722a1bb245e7a6b74a7b03835da1",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170499072/? [00:06&lt;00:00, 25830752.08it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_161a6dab0f9749f88ec679469d3380b5"
          }
        },
        "184dddba7a5b465693df2b3df17b0efd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3924784a90fa470ea7c2451c7bf47bab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9d23722a1bb245e7a6b74a7b03835da1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "161a6dab0f9749f88ec679469d3380b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JSjG64ra4aFu",
        "outputId": "3c49ea00-dcb9-41ca-91e7-8e801101de9b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!pip install torch"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.9.0+cu102)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8-7SARDZErK"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "import torch.optim as optim\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import copy\n",
        "import pickle\n",
        "# Ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124,
          "referenced_widgets": [
            "d6ec75b894c44439a2caf44c91fb8b5e",
            "c031f1795196474ea26ea0df2e6f788b",
            "1ee467ef16fe4b10b58d4eb913dcdced",
            "2f23ba08bce54d9d88b55ee98829862f",
            "184dddba7a5b465693df2b3df17b0efd",
            "3924784a90fa470ea7c2451c7bf47bab",
            "9d23722a1bb245e7a6b74a7b03835da1",
            "161a6dab0f9749f88ec679469d3380b5"
          ]
        },
        "id": "vwJv7Y8Rewez",
        "outputId": "582a4cdb-4ad3-4a8a-c1a3-0495e596e50c"
      },
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=10, shuffle=True)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=10, shuffle=False)\n",
        "\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "foreground_classes = {'plane', 'car', 'bird'}\n",
        "\n",
        "background_classes = {'cat', 'deer', 'dog', 'frog', 'horse','ship', 'truck'}\n",
        "\n",
        "# print(type(foreground_classes))\n",
        "\n",
        "dataiter = iter(trainloader)\n",
        "background_data=[]\n",
        "background_label=[]\n",
        "foreground_data=[]\n",
        "foreground_label=[]\n",
        "batch_size=10\n",
        "\n",
        "for i in range(5000):   #5000*batch_size = 50000 data points\n",
        "  images, labels = dataiter.next()\n",
        "  for j in range(batch_size):\n",
        "    if(classes[labels[j]] in background_classes):\n",
        "      img = images[j].tolist()\n",
        "      background_data.append(img)\n",
        "      background_label.append(labels[j])\n",
        "    else:\n",
        "      img = images[j].tolist()\n",
        "      foreground_data.append(img)\n",
        "      foreground_label.append(labels[j])\n",
        "            \n",
        "foreground_data = torch.tensor(foreground_data)\n",
        "foreground_label = torch.tensor(foreground_label)\n",
        "background_data = torch.tensor(background_data)\n",
        "background_label = torch.tensor(background_label)\n",
        "    \n",
        "  "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d6ec75b894c44439a2caf44c91fb8b5e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=170498071.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nDYhjJse6Qq"
      },
      "source": [
        "def create_mosaic_img(bg_idx,fg_idx,fg): \n",
        "  \"\"\"\n",
        "  bg_idx : list of indexes of background_data[] to be used as background images in mosaic\n",
        "  fg_idx : index of image to be used as foreground image from foreground data\n",
        "  fg : at what position/index foreground image has to be stored out of 0-8\n",
        "  \"\"\"\n",
        "  image_list=[]\n",
        "  j=0\n",
        "  for i in range(9):\n",
        "    if i != fg:\n",
        "      image_list.append(background_data[bg_idx[j]].type(\"torch.DoubleTensor\"))\n",
        "      j+=1\n",
        "    else: \n",
        "      image_list.append(foreground_data[fg_idx].type(\"torch.DoubleTensor\"))\n",
        "      label = foreground_label[fg_idx]  #-7  # minus 7 because our fore ground classes are 7,8,9 but we have to store it as 0,1,2\n",
        "  #image_list = np.concatenate(image_list ,axis=0)\n",
        "  image_list = torch.stack(image_list) \n",
        "  return image_list,label"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aivGVg14e9iZ"
      },
      "source": [
        "desired_num = 20000\n",
        "mosaic_list_of_images =[]      # list of mosaic images, each mosaic image is saved as list of 9 images\n",
        "fore_idx =[]                   # list of indexes at which foreground image is present in a mosaic image i.e from 0 to 9               \n",
        "mosaic_label=[]                # label of mosaic image = foreground class present in that mosaic\n",
        "for i in range(desired_num):\n",
        "  bg_idx = np.random.randint(0,35000,8)\n",
        "  fg_idx = np.random.randint(0,15000)\n",
        "  fg = np.random.randint(0,9)\n",
        "  fore_idx.append(fg)\n",
        "  image_list,label = create_mosaic_img(bg_idx,fg_idx,fg)\n",
        "  mosaic_list_of_images.append(image_list)\n",
        "  mosaic_label.append(label)\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6Jy35SSfBS9"
      },
      "source": [
        "# dict = {\"mosaic_list_of_images\": mosaic_list_of_images, \"mosaic_label\": mosaic_label , \"fore_idx\":fore_idx}\n",
        "# f = open(\"/content/drive/My Drive/Research/Experiments on CIFAR mosaic/Exp_3_Attention_models_on_9_datasets_made_from_10k_mosaic_on_cnn_network/weights/file.pkl\",\"wb\")\n",
        "# pickle.dump(dict,f)\n",
        "# f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIuiboIUfViV"
      },
      "source": [
        "# Load data from saved file "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MbYf6zuBfViX"
      },
      "source": [
        "# with open('/content/drive/My Drive/Research/Experiments on CIFAR mosaic/Exp_4_Attention_models_on_9_datasets_made_from_10k_mosaic_on_cnn_network/weights/file.pkl', 'rb') as f:\n",
        "#   data = pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwZjDB5lfVib"
      },
      "source": [
        "# mosaic_list_of_images = data[\"mosaic_list_of_images\"]\n",
        "# mosaic_label = data[\"mosaic_label\"]\n",
        "# fore_idx = data[\"fore_idx\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cog5VUzGgE5L",
        "outputId": "771d1b1d-d8b8-40f4-d59d-61bc9b186cc0"
      },
      "source": [
        "print(len(mosaic_list_of_images) , len(mosaic_label), len(mosaic_list_of_images[0:10000]))\n",
        "print(len(fore_idx))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20000 20000 10000\n",
            "20000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xX91RwMy-IP4"
      },
      "source": [
        "def create_avg_image_from_mosaic_dataset(mosaic_dataset,labels,foreground_index,dataset_number):\n",
        "  \"\"\"\n",
        "  mosaic_dataset : mosaic_dataset contains 9 images 32 x 32 each as 1 data point\n",
        "  labels : mosaic_dataset labels\n",
        "  foreground_index : contains list of indexes where foreground image is present so that using this we can take weighted average\n",
        "  dataset_number : will help us to tell what ratio of foreground image to be taken. for eg: if it is \"j\" then fg_image_ratio = j/9 , bg_image_ratio = (9-j)/8*9\n",
        "  \"\"\"\n",
        "  avg_image_dataset = []\n",
        "  for i in range(len(mosaic_dataset)):\n",
        "    img = torch.zeros([3, 32,32], dtype=torch.float64)\n",
        "    for j in range(9):\n",
        "      if j == foreground_index[i]:\n",
        "        img = img + mosaic_dataset[i][j]*dataset_number/9\n",
        "      else :\n",
        "        img = img + mosaic_dataset[i][j]*(9-dataset_number)/(8*9)\n",
        "    \n",
        "    avg_image_dataset.append(img)\n",
        "    \n",
        "  return avg_image_dataset , labels , foreground_index\n",
        "        \n",
        "  "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGz8Y88vIZPT"
      },
      "source": [
        "avg_image_dataset_1 , labels_1,  fg_index_1 = create_avg_image_from_mosaic_dataset(mosaic_list_of_images[0:10000], mosaic_label[0:10000], fore_idx[0:10000] , 1)\n",
        "avg_image_dataset_2 , labels_2,  fg_index_2 = create_avg_image_from_mosaic_dataset(mosaic_list_of_images[0:10000], mosaic_label[0:10000], fore_idx[0:10000] , 2)\n",
        "avg_image_dataset_3 , labels_3,  fg_index_3 = create_avg_image_from_mosaic_dataset(mosaic_list_of_images[0:10000], mosaic_label[0:10000], fore_idx[0:10000] , 3)\n",
        "avg_image_dataset_4 , labels_4,  fg_index_4 = create_avg_image_from_mosaic_dataset(mosaic_list_of_images[0:10000], mosaic_label[0:10000], fore_idx[0:10000] , 4)\n",
        "avg_image_dataset_5 , labels_5,  fg_index_5 = create_avg_image_from_mosaic_dataset(mosaic_list_of_images[0:10000], mosaic_label[0:10000], fore_idx[0:10000] , 5)\n",
        "avg_image_dataset_6 , labels_6,  fg_index_6 = create_avg_image_from_mosaic_dataset(mosaic_list_of_images[0:10000], mosaic_label[0:10000], fore_idx[0:10000] , 6)\n",
        "avg_image_dataset_7 , labels_7,  fg_index_7 = create_avg_image_from_mosaic_dataset(mosaic_list_of_images[0:10000], mosaic_label[0:10000], fore_idx[0:10000] , 7)\n",
        "avg_image_dataset_8 , labels_8,  fg_index_8 = create_avg_image_from_mosaic_dataset(mosaic_list_of_images[0:10000], mosaic_label[0:10000], fore_idx[0:10000] , 8)\n",
        "avg_image_dataset_9 , labels_9,  fg_index_9 = create_avg_image_from_mosaic_dataset(mosaic_list_of_images[0:10000], mosaic_label[0:10000], fore_idx[0:10000] , 9)\n",
        "\n",
        "test_dataset_10 , labels_10 , fg_index_10 = create_avg_image_from_mosaic_dataset(mosaic_list_of_images[10000:20000], mosaic_label[10000:20000], fore_idx[10000:20000] , 9)\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSO9SFE25Lrk"
      },
      "source": [
        "class MosaicDataset(Dataset):\n",
        "  \"\"\"MosaicDataset dataset.\"\"\"\n",
        "\n",
        "  def __init__(self, mosaic_list_of_images, mosaic_label):\n",
        "    \"\"\"\n",
        "      Args:\n",
        "        csv_file (string): Path to the csv file with annotations.\n",
        "        root_dir (string): Directory with all the images.\n",
        "        transform (callable, optional): Optional transform to be applied\n",
        "            on a sample.\n",
        "    \"\"\"\n",
        "    self.mosaic = mosaic_list_of_images\n",
        "    self.label = mosaic_label\n",
        "    #self.fore_idx = fore_idx\n",
        "    \n",
        "  def __len__(self):\n",
        "    return len(self.label)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.mosaic[idx] , self.label[idx] #, self.fore_idx[idx]\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obE1xeyRks1Q"
      },
      "source": [
        "batch = 256\n",
        "\n",
        "\n",
        "# training_data = avg_image_dataset_5    #just change this and training_label to desired dataset for training\n",
        "# training_label = labels_5\n",
        "\n",
        "traindata_1 = MosaicDataset(avg_image_dataset_1, labels_1 )\n",
        "trainloader_1 = DataLoader( traindata_1 , batch_size= batch ,shuffle=True)\n",
        "\n",
        "traindata_2 = MosaicDataset(avg_image_dataset_2, labels_2 )\n",
        "trainloader_2 = DataLoader( traindata_2 , batch_size= batch ,shuffle=True)\n",
        "\n",
        "traindata_3 = MosaicDataset(avg_image_dataset_3, labels_3 )\n",
        "trainloader_3 = DataLoader( traindata_3 , batch_size= batch ,shuffle=True)\n",
        "\n",
        "traindata_4 = MosaicDataset(avg_image_dataset_4, labels_4 )\n",
        "trainloader_4 = DataLoader( traindata_4 , batch_size= batch ,shuffle=True)\n",
        "\n",
        "traindata_5 = MosaicDataset(avg_image_dataset_5, labels_5 )\n",
        "trainloader_5 = DataLoader( traindata_5 , batch_size= batch ,shuffle=True)\n",
        "\n",
        "traindata_6 = MosaicDataset(avg_image_dataset_6, labels_6 )\n",
        "trainloader_6 = DataLoader( traindata_6 , batch_size= batch ,shuffle=True)\n",
        "\n",
        "traindata_7 = MosaicDataset(avg_image_dataset_7, labels_7 )\n",
        "trainloader_7 = DataLoader( traindata_7 , batch_size= batch ,shuffle=True)\n",
        "\n",
        "traindata_8 = MosaicDataset(avg_image_dataset_8, labels_8 )\n",
        "trainloader_8 = DataLoader( traindata_8 , batch_size= batch ,shuffle=True)\n",
        "\n",
        "traindata_9 = MosaicDataset(avg_image_dataset_9, labels_9 )\n",
        "trainloader_9 = DataLoader( traindata_9 , batch_size= batch ,shuffle=True)\n",
        "\n",
        "testdata_1 = MosaicDataset(avg_image_dataset_1, labels_1 )\n",
        "testloader_1 = DataLoader( testdata_1 , batch_size= batch ,shuffle=False)\n",
        "\n",
        "testdata_2 = MosaicDataset(avg_image_dataset_2, labels_2 )\n",
        "testloader_2 = DataLoader( testdata_2 , batch_size= batch ,shuffle=False)\n",
        "\n",
        "testdata_3 = MosaicDataset(avg_image_dataset_3, labels_3 )\n",
        "testloader_3 = DataLoader( testdata_3 , batch_size= batch ,shuffle=False)\n",
        "\n",
        "testdata_4 = MosaicDataset(avg_image_dataset_4, labels_4 )\n",
        "testloader_4 = DataLoader( testdata_4 , batch_size= batch ,shuffle=False)\n",
        "\n",
        "testdata_5 = MosaicDataset(avg_image_dataset_5, labels_5 )\n",
        "testloader_5 = DataLoader( testdata_5 , batch_size= batch ,shuffle=False)\n",
        "\n",
        "testdata_6 = MosaicDataset(avg_image_dataset_6, labels_6 )\n",
        "testloader_6 = DataLoader( testdata_6 , batch_size= batch ,shuffle=False)\n",
        "\n",
        "testdata_7 = MosaicDataset(avg_image_dataset_7, labels_7 )\n",
        "testloader_7 = DataLoader( testdata_7 , batch_size= batch ,shuffle=False)\n",
        "\n",
        "testdata_8 = MosaicDataset(avg_image_dataset_8, labels_8 )\n",
        "testloader_8 = DataLoader( testdata_8 , batch_size= batch ,shuffle=False)\n",
        "\n",
        "testdata_9 = MosaicDataset(avg_image_dataset_9, labels_9 )\n",
        "testloader_9 = DataLoader( testdata_9 , batch_size= batch ,shuffle=False)\n",
        "\n",
        "testdata_10 = MosaicDataset(test_dataset_10, labels_10 )\n",
        "testloader_10 = DataLoader( testdata_10 , batch_size= batch ,shuffle=False)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnQDiKT6Mayo"
      },
      "source": [
        "***SIMPLE MODEL***\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUgz-nA7MZmA"
      },
      "source": [
        "class Module2(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Module2, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "    self.pool = nn.MaxPool2d(2, 2)\n",
        "    self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "    self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "    self.fc2 = nn.Linear(120, 84)\n",
        "    self.fc3 = nn.Linear(84, 10)\n",
        "    self.fc4 = nn.Linear(10,3)\n",
        "\n",
        "  def forward(self,z):  #z batch of list of 9 images\n",
        "    # y = torch.zeros([batch,3, 32,32], dtype=torch.float64)\n",
        "    # x = torch.zeros([batch,9],dtype=torch.float64)\n",
        "\n",
        "    # for i in range(9):\n",
        "    #     x[:,i] = self.module1.forward(z[:,i])[:,0]\n",
        "\n",
        "    # x = F.softmax(x,dim=1)\n",
        "\n",
        "    # x1 = x[:,0]\n",
        "    # torch.mul(x1[:,None,None,None],z[:,0])\n",
        "\n",
        "    # for i in range(9):            \n",
        "    #   x1 = x[:,i]          \n",
        "    #   y = y + torch.mul(x1[:,None,None,None],z[:,i])\n",
        "\n",
        "\n",
        "    y1 = self.pool(F.relu(self.conv1(z)))\n",
        "    y1 = self.pool(F.relu(self.conv2(y1)))\n",
        "    y1 = y1.view(-1, 16 * 5 * 5)\n",
        "\n",
        "    y1 = F.relu(self.fc1(y1))\n",
        "    y1 = F.relu(self.fc2(y1))\n",
        "    y1 = F.relu(self.fc3(y1))\n",
        "    y1 = self.fc4(y1)\n",
        "    return y1 "
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOWrnzv1fVjD"
      },
      "source": [
        "def test_all(number, testloader,inc):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    out = []\n",
        "    pred = []\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data\n",
        "            images, labels = images.to(\"cuda\"),labels.to(\"cuda\")\n",
        "            out.append(labels.cpu().numpy())\n",
        "            outputs= inc(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            pred.append(predicted.cpu().numpy())\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print('Accuracy of the network on the 10000 test dataset %d: %d %%' % (number , 100 * correct / total))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFfAJZkcZEsY"
      },
      "source": [
        "def train_all(trainloader, ds_number, testloader_list):\n",
        "    \n",
        "    print(\"--\"*40)\n",
        "    print(\"training on data set  \", ds_number)\n",
        "    \n",
        "    inc = Module2().double()\n",
        "    inc = inc.to(\"cuda\")\n",
        "    \n",
        "    criterion_inception = nn.CrossEntropyLoss()\n",
        "    optimizer_inception = optim.SGD(inc.parameters(), lr=0.01, momentum=0.9)\n",
        "    \n",
        "    acti = []\n",
        "    loss_curi = []\n",
        "    epochs = 200\n",
        "    \n",
        "    for epoch in range(epochs): # loop over the dataset multiple times\n",
        "        ep_lossi = []\n",
        "\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            # get the inputs\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(\"cuda\"),labels.to(\"cuda\")\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer_inception.zero_grad()\n",
        "\n",
        "            # forward + backward + optimize\n",
        "            outputs = inc(inputs)\n",
        "            loss = criterion_inception(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer_inception.step()\n",
        "\n",
        "            # print statistics\n",
        "            running_loss += loss.item()\n",
        "            if i % 10 == 9:    # print every 10 mini-batches\n",
        "                print('[%d, %5d] loss: %.3f' %\n",
        "                      (epoch + 1, i + 1, running_loss / 10))\n",
        "                ep_lossi.append(running_loss/10) # loss per minibatch\n",
        "                running_loss = 0.0\n",
        "                \n",
        "        loss_curi.append(np.mean(ep_lossi))   #loss per epoch\n",
        "        if(np.mean(ep_lossi)<= 0.001):\n",
        "          break\n",
        "    #     if (epoch%5 == 0):\n",
        "    #         _,actis= inc(inputs)\n",
        "    #         acti.append(actis)\n",
        "\n",
        "\n",
        "\n",
        "    print('Finished Training')\n",
        "    # torch.save(inc.state_dict(),\"/content/drive/My Drive/Research/Experiments on CIFAR mosaic/Exp_3_Attention_models_on_9_datasets_made_from_10k_mosaic_on_simple_network/weights/train_dataset_\"+str(ds_number)+\"_\"+str(epochs)+\".pt\")\n",
        "    \n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in trainloader:\n",
        "            images, labels = data\n",
        "            images, labels = images.to(\"cuda\"), labels.to(\"cuda\")\n",
        "            outputs = inc(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print('Accuracy of the network on the 10000 train images: %d %%' % (  100 * correct / total))\n",
        "    \n",
        "    for i, j in enumerate(testloader_list):\n",
        "        test_all(i+1, j,inc)\n",
        "    \n",
        "    print(\"--\"*40)\n",
        "    \n",
        "    return loss_curi\n",
        "    "
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mI-vqhB-fVjJ",
        "outputId": "494e3291-b2cd-4d92-abe6-eae76e0ba872"
      },
      "source": [
        "train_loss_all=[]\n",
        "\n",
        "testloader_list= [ testloader_1, testloader_2, testloader_3, testloader_4, testloader_5, testloader_6,\n",
        "                 testloader_7, testloader_8, testloader_9, testloader_10]\n",
        "\n",
        "train_loss_all.append(train_all(trainloader_1, 1, testloader_list))\n",
        "train_loss_all.append(train_all(trainloader_2, 2, testloader_list))\n",
        "train_loss_all.append(train_all(trainloader_3, 3, testloader_list))\n",
        "train_loss_all.append(train_all(trainloader_4, 4, testloader_list))\n",
        "train_loss_all.append(train_all(trainloader_5, 5, testloader_list))\n",
        "train_loss_all.append(train_all(trainloader_6, 6, testloader_list))\n",
        "train_loss_all.append(train_all(trainloader_7, 7, testloader_list))\n",
        "train_loss_all.append(train_all(trainloader_8, 8, testloader_list))\n",
        "train_loss_all.append(train_all(trainloader_9, 9, testloader_list))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "training on data set   1\n",
            "[1,    10] loss: 1.117\n",
            "[1,    20] loss: 1.111\n",
            "[1,    30] loss: 1.101\n",
            "[1,    40] loss: 1.099\n",
            "[2,    10] loss: 1.099\n",
            "[2,    20] loss: 1.099\n",
            "[2,    30] loss: 1.099\n",
            "[2,    40] loss: 1.099\n",
            "[3,    10] loss: 1.099\n",
            "[3,    20] loss: 1.099\n",
            "[3,    30] loss: 1.098\n",
            "[3,    40] loss: 1.098\n",
            "[4,    10] loss: 1.099\n",
            "[4,    20] loss: 1.099\n",
            "[4,    30] loss: 1.098\n",
            "[4,    40] loss: 1.099\n",
            "[5,    10] loss: 1.098\n",
            "[5,    20] loss: 1.098\n",
            "[5,    30] loss: 1.100\n",
            "[5,    40] loss: 1.099\n",
            "[6,    10] loss: 1.099\n",
            "[6,    20] loss: 1.098\n",
            "[6,    30] loss: 1.098\n",
            "[6,    40] loss: 1.099\n",
            "[7,    10] loss: 1.098\n",
            "[7,    20] loss: 1.099\n",
            "[7,    30] loss: 1.098\n",
            "[7,    40] loss: 1.099\n",
            "[8,    10] loss: 1.099\n",
            "[8,    20] loss: 1.098\n",
            "[8,    30] loss: 1.099\n",
            "[8,    40] loss: 1.098\n",
            "[9,    10] loss: 1.098\n",
            "[9,    20] loss: 1.098\n",
            "[9,    30] loss: 1.100\n",
            "[9,    40] loss: 1.099\n",
            "[10,    10] loss: 1.098\n",
            "[10,    20] loss: 1.098\n",
            "[10,    30] loss: 1.098\n",
            "[10,    40] loss: 1.099\n",
            "[11,    10] loss: 1.098\n",
            "[11,    20] loss: 1.098\n",
            "[11,    30] loss: 1.098\n",
            "[11,    40] loss: 1.098\n",
            "[12,    10] loss: 1.098\n",
            "[12,    20] loss: 1.099\n",
            "[12,    30] loss: 1.098\n",
            "[12,    40] loss: 1.098\n",
            "[13,    10] loss: 1.098\n",
            "[13,    20] loss: 1.098\n",
            "[13,    30] loss: 1.098\n",
            "[13,    40] loss: 1.099\n",
            "[14,    10] loss: 1.098\n",
            "[14,    20] loss: 1.098\n",
            "[14,    30] loss: 1.098\n",
            "[14,    40] loss: 1.098\n",
            "[15,    10] loss: 1.098\n",
            "[15,    20] loss: 1.098\n",
            "[15,    30] loss: 1.098\n",
            "[15,    40] loss: 1.097\n",
            "[16,    10] loss: 1.098\n",
            "[16,    20] loss: 1.098\n",
            "[16,    30] loss: 1.098\n",
            "[16,    40] loss: 1.097\n",
            "[17,    10] loss: 1.098\n",
            "[17,    20] loss: 1.098\n",
            "[17,    30] loss: 1.098\n",
            "[17,    40] loss: 1.098\n",
            "[18,    10] loss: 1.098\n",
            "[18,    20] loss: 1.098\n",
            "[18,    30] loss: 1.098\n",
            "[18,    40] loss: 1.097\n",
            "[19,    10] loss: 1.098\n",
            "[19,    20] loss: 1.099\n",
            "[19,    30] loss: 1.097\n",
            "[19,    40] loss: 1.097\n",
            "[20,    10] loss: 1.098\n",
            "[20,    20] loss: 1.098\n",
            "[20,    30] loss: 1.097\n",
            "[20,    40] loss: 1.097\n",
            "[21,    10] loss: 1.097\n",
            "[21,    20] loss: 1.099\n",
            "[21,    30] loss: 1.097\n",
            "[21,    40] loss: 1.097\n",
            "[22,    10] loss: 1.097\n",
            "[22,    20] loss: 1.097\n",
            "[22,    30] loss: 1.097\n",
            "[22,    40] loss: 1.097\n",
            "[23,    10] loss: 1.097\n",
            "[23,    20] loss: 1.096\n",
            "[23,    30] loss: 1.097\n",
            "[23,    40] loss: 1.096\n",
            "[24,    10] loss: 1.096\n",
            "[24,    20] loss: 1.097\n",
            "[24,    30] loss: 1.095\n",
            "[24,    40] loss: 1.096\n",
            "[25,    10] loss: 1.096\n",
            "[25,    20] loss: 1.095\n",
            "[25,    30] loss: 1.095\n",
            "[25,    40] loss: 1.093\n",
            "[26,    10] loss: 1.093\n",
            "[26,    20] loss: 1.094\n",
            "[26,    30] loss: 1.094\n",
            "[26,    40] loss: 1.092\n",
            "[27,    10] loss: 1.093\n",
            "[27,    20] loss: 1.092\n",
            "[27,    30] loss: 1.092\n",
            "[27,    40] loss: 1.088\n",
            "[28,    10] loss: 1.091\n",
            "[28,    20] loss: 1.090\n",
            "[28,    30] loss: 1.087\n",
            "[28,    40] loss: 1.083\n",
            "[29,    10] loss: 1.084\n",
            "[29,    20] loss: 1.088\n",
            "[29,    30] loss: 1.087\n",
            "[29,    40] loss: 1.081\n",
            "[30,    10] loss: 1.081\n",
            "[30,    20] loss: 1.083\n",
            "[30,    30] loss: 1.078\n",
            "[30,    40] loss: 1.084\n",
            "[31,    10] loss: 1.078\n",
            "[31,    20] loss: 1.075\n",
            "[31,    30] loss: 1.078\n",
            "[31,    40] loss: 1.075\n",
            "[32,    10] loss: 1.070\n",
            "[32,    20] loss: 1.071\n",
            "[32,    30] loss: 1.077\n",
            "[32,    40] loss: 1.078\n",
            "[33,    10] loss: 1.066\n",
            "[33,    20] loss: 1.075\n",
            "[33,    30] loss: 1.073\n",
            "[33,    40] loss: 1.081\n",
            "[34,    10] loss: 1.068\n",
            "[34,    20] loss: 1.070\n",
            "[34,    30] loss: 1.073\n",
            "[34,    40] loss: 1.070\n",
            "[35,    10] loss: 1.066\n",
            "[35,    20] loss: 1.074\n",
            "[35,    30] loss: 1.066\n",
            "[35,    40] loss: 1.079\n",
            "[36,    10] loss: 1.079\n",
            "[36,    20] loss: 1.071\n",
            "[36,    30] loss: 1.075\n",
            "[36,    40] loss: 1.068\n",
            "[37,    10] loss: 1.065\n",
            "[37,    20] loss: 1.068\n",
            "[37,    30] loss: 1.070\n",
            "[37,    40] loss: 1.069\n",
            "[38,    10] loss: 1.069\n",
            "[38,    20] loss: 1.064\n",
            "[38,    30] loss: 1.066\n",
            "[38,    40] loss: 1.073\n",
            "[39,    10] loss: 1.064\n",
            "[39,    20] loss: 1.076\n",
            "[39,    30] loss: 1.066\n",
            "[39,    40] loss: 1.064\n",
            "[40,    10] loss: 1.065\n",
            "[40,    20] loss: 1.066\n",
            "[40,    30] loss: 1.071\n",
            "[40,    40] loss: 1.058\n",
            "[41,    10] loss: 1.066\n",
            "[41,    20] loss: 1.063\n",
            "[41,    30] loss: 1.069\n",
            "[41,    40] loss: 1.070\n",
            "[42,    10] loss: 1.069\n",
            "[42,    20] loss: 1.064\n",
            "[42,    30] loss: 1.060\n",
            "[42,    40] loss: 1.065\n",
            "[43,    10] loss: 1.065\n",
            "[43,    20] loss: 1.063\n",
            "[43,    30] loss: 1.064\n",
            "[43,    40] loss: 1.060\n",
            "[44,    10] loss: 1.061\n",
            "[44,    20] loss: 1.069\n",
            "[44,    30] loss: 1.056\n",
            "[44,    40] loss: 1.065\n",
            "[45,    10] loss: 1.055\n",
            "[45,    20] loss: 1.063\n",
            "[45,    30] loss: 1.062\n",
            "[45,    40] loss: 1.062\n",
            "[46,    10] loss: 1.059\n",
            "[46,    20] loss: 1.054\n",
            "[46,    30] loss: 1.069\n",
            "[46,    40] loss: 1.056\n",
            "[47,    10] loss: 1.066\n",
            "[47,    20] loss: 1.059\n",
            "[47,    30] loss: 1.062\n",
            "[47,    40] loss: 1.054\n",
            "[48,    10] loss: 1.057\n",
            "[48,    20] loss: 1.056\n",
            "[48,    30] loss: 1.059\n",
            "[48,    40] loss: 1.077\n",
            "[49,    10] loss: 1.062\n",
            "[49,    20] loss: 1.061\n",
            "[49,    30] loss: 1.055\n",
            "[49,    40] loss: 1.054\n",
            "[50,    10] loss: 1.054\n",
            "[50,    20] loss: 1.067\n",
            "[50,    30] loss: 1.059\n",
            "[50,    40] loss: 1.062\n",
            "[51,    10] loss: 1.053\n",
            "[51,    20] loss: 1.054\n",
            "[51,    30] loss: 1.063\n",
            "[51,    40] loss: 1.065\n",
            "[52,    10] loss: 1.053\n",
            "[52,    20] loss: 1.059\n",
            "[52,    30] loss: 1.059\n",
            "[52,    40] loss: 1.050\n",
            "[53,    10] loss: 1.064\n",
            "[53,    20] loss: 1.060\n",
            "[53,    30] loss: 1.043\n",
            "[53,    40] loss: 1.059\n",
            "[54,    10] loss: 1.054\n",
            "[54,    20] loss: 1.049\n",
            "[54,    30] loss: 1.057\n",
            "[54,    40] loss: 1.064\n",
            "[55,    10] loss: 1.063\n",
            "[55,    20] loss: 1.055\n",
            "[55,    30] loss: 1.060\n",
            "[55,    40] loss: 1.047\n",
            "[56,    10] loss: 1.066\n",
            "[56,    20] loss: 1.058\n",
            "[56,    30] loss: 1.046\n",
            "[56,    40] loss: 1.051\n",
            "[57,    10] loss: 1.042\n",
            "[57,    20] loss: 1.054\n",
            "[57,    30] loss: 1.053\n",
            "[57,    40] loss: 1.059\n",
            "[58,    10] loss: 1.052\n",
            "[58,    20] loss: 1.049\n",
            "[58,    30] loss: 1.043\n",
            "[58,    40] loss: 1.051\n",
            "[59,    10] loss: 1.052\n",
            "[59,    20] loss: 1.048\n",
            "[59,    30] loss: 1.045\n",
            "[59,    40] loss: 1.057\n",
            "[60,    10] loss: 1.046\n",
            "[60,    20] loss: 1.052\n",
            "[60,    30] loss: 1.037\n",
            "[60,    40] loss: 1.061\n",
            "[61,    10] loss: 1.044\n",
            "[61,    20] loss: 1.039\n",
            "[61,    30] loss: 1.040\n",
            "[61,    40] loss: 1.059\n",
            "[62,    10] loss: 1.046\n",
            "[62,    20] loss: 1.043\n",
            "[62,    30] loss: 1.038\n",
            "[62,    40] loss: 1.056\n",
            "[63,    10] loss: 1.045\n",
            "[63,    20] loss: 1.042\n",
            "[63,    30] loss: 1.049\n",
            "[63,    40] loss: 1.037\n",
            "[64,    10] loss: 1.046\n",
            "[64,    20] loss: 1.040\n",
            "[64,    30] loss: 1.044\n",
            "[64,    40] loss: 1.028\n",
            "[65,    10] loss: 1.039\n",
            "[65,    20] loss: 1.037\n",
            "[65,    30] loss: 1.046\n",
            "[65,    40] loss: 1.028\n",
            "[66,    10] loss: 1.045\n",
            "[66,    20] loss: 1.038\n",
            "[66,    30] loss: 1.033\n",
            "[66,    40] loss: 1.056\n",
            "[67,    10] loss: 1.042\n",
            "[67,    20] loss: 1.034\n",
            "[67,    30] loss: 1.036\n",
            "[67,    40] loss: 1.019\n",
            "[68,    10] loss: 1.027\n",
            "[68,    20] loss: 1.035\n",
            "[68,    30] loss: 1.042\n",
            "[68,    40] loss: 1.043\n",
            "[69,    10] loss: 1.037\n",
            "[69,    20] loss: 1.049\n",
            "[69,    30] loss: 1.027\n",
            "[69,    40] loss: 1.015\n",
            "[70,    10] loss: 1.036\n",
            "[70,    20] loss: 1.027\n",
            "[70,    30] loss: 1.045\n",
            "[70,    40] loss: 1.003\n",
            "[71,    10] loss: 1.045\n",
            "[71,    20] loss: 1.038\n",
            "[71,    30] loss: 1.034\n",
            "[71,    40] loss: 1.032\n",
            "[72,    10] loss: 1.046\n",
            "[72,    20] loss: 1.019\n",
            "[72,    30] loss: 1.036\n",
            "[72,    40] loss: 1.033\n",
            "[73,    10] loss: 1.048\n",
            "[73,    20] loss: 1.036\n",
            "[73,    30] loss: 1.035\n",
            "[73,    40] loss: 1.029\n",
            "[74,    10] loss: 1.034\n",
            "[74,    20] loss: 1.037\n",
            "[74,    30] loss: 1.039\n",
            "[74,    40] loss: 1.019\n",
            "[75,    10] loss: 1.031\n",
            "[75,    20] loss: 1.012\n",
            "[75,    30] loss: 1.044\n",
            "[75,    40] loss: 1.023\n",
            "[76,    10] loss: 1.035\n",
            "[76,    20] loss: 1.033\n",
            "[76,    30] loss: 1.017\n",
            "[76,    40] loss: 1.033\n",
            "[77,    10] loss: 1.042\n",
            "[77,    20] loss: 1.041\n",
            "[77,    30] loss: 1.029\n",
            "[77,    40] loss: 1.030\n",
            "[78,    10] loss: 1.050\n",
            "[78,    20] loss: 1.048\n",
            "[78,    30] loss: 1.024\n",
            "[78,    40] loss: 1.020\n",
            "[79,    10] loss: 1.030\n",
            "[79,    20] loss: 1.021\n",
            "[79,    30] loss: 1.025\n",
            "[79,    40] loss: 1.036\n",
            "[80,    10] loss: 1.030\n",
            "[80,    20] loss: 1.023\n",
            "[80,    30] loss: 1.027\n",
            "[80,    40] loss: 1.011\n",
            "[81,    10] loss: 1.040\n",
            "[81,    20] loss: 1.028\n",
            "[81,    30] loss: 1.037\n",
            "[81,    40] loss: 1.038\n",
            "[82,    10] loss: 1.040\n",
            "[82,    20] loss: 1.023\n",
            "[82,    30] loss: 1.025\n",
            "[82,    40] loss: 1.037\n",
            "[83,    10] loss: 1.015\n",
            "[83,    20] loss: 1.026\n",
            "[83,    30] loss: 1.030\n",
            "[83,    40] loss: 1.024\n",
            "[84,    10] loss: 1.014\n",
            "[84,    20] loss: 1.024\n",
            "[84,    30] loss: 1.021\n",
            "[84,    40] loss: 1.024\n",
            "[85,    10] loss: 1.027\n",
            "[85,    20] loss: 1.019\n",
            "[85,    30] loss: 1.012\n",
            "[85,    40] loss: 1.034\n",
            "[86,    10] loss: 1.030\n",
            "[86,    20] loss: 1.023\n",
            "[86,    30] loss: 1.013\n",
            "[86,    40] loss: 0.995\n",
            "[87,    10] loss: 1.017\n",
            "[87,    20] loss: 1.026\n",
            "[87,    30] loss: 1.011\n",
            "[87,    40] loss: 1.032\n",
            "[88,    10] loss: 1.015\n",
            "[88,    20] loss: 1.022\n",
            "[88,    30] loss: 1.015\n",
            "[88,    40] loss: 1.011\n",
            "[89,    10] loss: 1.011\n",
            "[89,    20] loss: 1.029\n",
            "[89,    30] loss: 1.012\n",
            "[89,    40] loss: 1.020\n",
            "[90,    10] loss: 1.040\n",
            "[90,    20] loss: 1.009\n",
            "[90,    30] loss: 1.017\n",
            "[90,    40] loss: 1.022\n",
            "[91,    10] loss: 1.011\n",
            "[91,    20] loss: 1.008\n",
            "[91,    30] loss: 1.012\n",
            "[91,    40] loss: 1.035\n",
            "[92,    10] loss: 1.014\n",
            "[92,    20] loss: 1.000\n",
            "[92,    30] loss: 1.024\n",
            "[92,    40] loss: 1.016\n",
            "[93,    10] loss: 1.009\n",
            "[93,    20] loss: 1.019\n",
            "[93,    30] loss: 0.998\n",
            "[93,    40] loss: 1.000\n",
            "[94,    10] loss: 1.013\n",
            "[94,    20] loss: 1.001\n",
            "[94,    30] loss: 1.012\n",
            "[94,    40] loss: 1.000\n",
            "[95,    10] loss: 1.002\n",
            "[95,    20] loss: 0.999\n",
            "[95,    30] loss: 1.005\n",
            "[95,    40] loss: 1.011\n",
            "[96,    10] loss: 0.999\n",
            "[96,    20] loss: 0.997\n",
            "[96,    30] loss: 1.007\n",
            "[96,    40] loss: 1.010\n",
            "[97,    10] loss: 0.997\n",
            "[97,    20] loss: 1.013\n",
            "[97,    30] loss: 0.994\n",
            "[97,    40] loss: 1.016\n",
            "[98,    10] loss: 1.011\n",
            "[98,    20] loss: 1.007\n",
            "[98,    30] loss: 0.990\n",
            "[98,    40] loss: 1.006\n",
            "[99,    10] loss: 1.014\n",
            "[99,    20] loss: 0.996\n",
            "[99,    30] loss: 0.994\n",
            "[99,    40] loss: 0.977\n",
            "[100,    10] loss: 0.981\n",
            "[100,    20] loss: 0.993\n",
            "[100,    30] loss: 0.997\n",
            "[100,    40] loss: 0.998\n",
            "[101,    10] loss: 1.001\n",
            "[101,    20] loss: 0.998\n",
            "[101,    30] loss: 0.992\n",
            "[101,    40] loss: 1.005\n",
            "[102,    10] loss: 0.990\n",
            "[102,    20] loss: 0.999\n",
            "[102,    30] loss: 0.988\n",
            "[102,    40] loss: 1.013\n",
            "[103,    10] loss: 0.992\n",
            "[103,    20] loss: 0.986\n",
            "[103,    30] loss: 1.002\n",
            "[103,    40] loss: 0.976\n",
            "[104,    10] loss: 0.994\n",
            "[104,    20] loss: 1.006\n",
            "[104,    30] loss: 0.986\n",
            "[104,    40] loss: 0.970\n",
            "[105,    10] loss: 0.986\n",
            "[105,    20] loss: 0.979\n",
            "[105,    30] loss: 0.987\n",
            "[105,    40] loss: 0.975\n",
            "[106,    10] loss: 1.006\n",
            "[106,    20] loss: 0.991\n",
            "[106,    30] loss: 0.991\n",
            "[106,    40] loss: 0.962\n",
            "[107,    10] loss: 0.968\n",
            "[107,    20] loss: 0.980\n",
            "[107,    30] loss: 0.983\n",
            "[107,    40] loss: 0.993\n",
            "[108,    10] loss: 0.968\n",
            "[108,    20] loss: 0.980\n",
            "[108,    30] loss: 0.968\n",
            "[108,    40] loss: 0.976\n",
            "[109,    10] loss: 0.963\n",
            "[109,    20] loss: 0.966\n",
            "[109,    30] loss: 0.974\n",
            "[109,    40] loss: 0.959\n",
            "[110,    10] loss: 0.965\n",
            "[110,    20] loss: 0.961\n",
            "[110,    30] loss: 0.959\n",
            "[110,    40] loss: 0.977\n",
            "[111,    10] loss: 0.971\n",
            "[111,    20] loss: 0.959\n",
            "[111,    30] loss: 0.947\n",
            "[111,    40] loss: 0.963\n",
            "[112,    10] loss: 0.951\n",
            "[112,    20] loss: 0.951\n",
            "[112,    30] loss: 0.970\n",
            "[112,    40] loss: 0.968\n",
            "[113,    10] loss: 0.965\n",
            "[113,    20] loss: 0.969\n",
            "[113,    30] loss: 0.941\n",
            "[113,    40] loss: 0.947\n",
            "[114,    10] loss: 0.956\n",
            "[114,    20] loss: 0.945\n",
            "[114,    30] loss: 0.973\n",
            "[114,    40] loss: 0.961\n",
            "[115,    10] loss: 0.949\n",
            "[115,    20] loss: 0.968\n",
            "[115,    30] loss: 0.944\n",
            "[115,    40] loss: 0.932\n",
            "[116,    10] loss: 0.957\n",
            "[116,    20] loss: 0.947\n",
            "[116,    30] loss: 0.930\n",
            "[116,    40] loss: 0.919\n",
            "[117,    10] loss: 0.939\n",
            "[117,    20] loss: 0.949\n",
            "[117,    30] loss: 0.936\n",
            "[117,    40] loss: 0.946\n",
            "[118,    10] loss: 0.951\n",
            "[118,    20] loss: 0.924\n",
            "[118,    30] loss: 0.944\n",
            "[118,    40] loss: 0.924\n",
            "[119,    10] loss: 0.921\n",
            "[119,    20] loss: 0.919\n",
            "[119,    30] loss: 0.922\n",
            "[119,    40] loss: 0.914\n",
            "[120,    10] loss: 0.926\n",
            "[120,    20] loss: 0.928\n",
            "[120,    30] loss: 0.915\n",
            "[120,    40] loss: 0.930\n",
            "[121,    10] loss: 0.933\n",
            "[121,    20] loss: 0.910\n",
            "[121,    30] loss: 0.926\n",
            "[121,    40] loss: 0.934\n",
            "[122,    10] loss: 0.996\n",
            "[122,    20] loss: 0.952\n",
            "[122,    30] loss: 0.922\n",
            "[122,    40] loss: 0.913\n",
            "[123,    10] loss: 0.901\n",
            "[123,    20] loss: 0.895\n",
            "[123,    30] loss: 0.904\n",
            "[123,    40] loss: 0.895\n",
            "[124,    10] loss: 0.922\n",
            "[124,    20] loss: 0.904\n",
            "[124,    30] loss: 0.896\n",
            "[124,    40] loss: 0.918\n",
            "[125,    10] loss: 0.888\n",
            "[125,    20] loss: 0.896\n",
            "[125,    30] loss: 0.901\n",
            "[125,    40] loss: 0.900\n",
            "[126,    10] loss: 0.913\n",
            "[126,    20] loss: 0.880\n",
            "[126,    30] loss: 0.879\n",
            "[126,    40] loss: 0.875\n",
            "[127,    10] loss: 0.894\n",
            "[127,    20] loss: 0.852\n",
            "[127,    30] loss: 0.892\n",
            "[127,    40] loss: 0.913\n",
            "[128,    10] loss: 0.935\n",
            "[128,    20] loss: 0.885\n",
            "[128,    30] loss: 0.878\n",
            "[128,    40] loss: 0.879\n",
            "[129,    10] loss: 0.867\n",
            "[129,    20] loss: 0.866\n",
            "[129,    30] loss: 0.855\n",
            "[129,    40] loss: 0.856\n",
            "[130,    10] loss: 0.825\n",
            "[130,    20] loss: 0.857\n",
            "[130,    30] loss: 0.867\n",
            "[130,    40] loss: 0.872\n",
            "[131,    10] loss: 0.870\n",
            "[131,    20] loss: 0.857\n",
            "[131,    30] loss: 0.828\n",
            "[131,    40] loss: 0.868\n",
            "[132,    10] loss: 0.863\n",
            "[132,    20] loss: 0.849\n",
            "[132,    30] loss: 0.821\n",
            "[132,    40] loss: 0.805\n",
            "[133,    10] loss: 0.807\n",
            "[133,    20] loss: 0.816\n",
            "[133,    30] loss: 0.808\n",
            "[133,    40] loss: 0.817\n",
            "[134,    10] loss: 0.784\n",
            "[134,    20] loss: 0.800\n",
            "[134,    30] loss: 0.785\n",
            "[134,    40] loss: 0.814\n",
            "[135,    10] loss: 0.864\n",
            "[135,    20] loss: 0.885\n",
            "[135,    30] loss: 0.886\n",
            "[135,    40] loss: 0.838\n",
            "[136,    10] loss: 0.801\n",
            "[136,    20] loss: 0.766\n",
            "[136,    30] loss: 0.771\n",
            "[136,    40] loss: 0.783\n",
            "[137,    10] loss: 0.790\n",
            "[137,    20] loss: 0.779\n",
            "[137,    30] loss: 0.768\n",
            "[137,    40] loss: 0.763\n",
            "[138,    10] loss: 0.761\n",
            "[138,    20] loss: 0.753\n",
            "[138,    30] loss: 0.757\n",
            "[138,    40] loss: 0.734\n",
            "[139,    10] loss: 0.774\n",
            "[139,    20] loss: 0.766\n",
            "[139,    30] loss: 0.767\n",
            "[139,    40] loss: 0.763\n",
            "[140,    10] loss: 0.712\n",
            "[140,    20] loss: 0.678\n",
            "[140,    30] loss: 0.701\n",
            "[140,    40] loss: 0.700\n",
            "[141,    10] loss: 0.656\n",
            "[141,    20] loss: 0.664\n",
            "[141,    30] loss: 0.701\n",
            "[141,    40] loss: 0.689\n",
            "[142,    10] loss: 0.692\n",
            "[142,    20] loss: 0.685\n",
            "[142,    30] loss: 0.657\n",
            "[142,    40] loss: 0.726\n",
            "[143,    10] loss: 0.709\n",
            "[143,    20] loss: 0.711\n",
            "[143,    30] loss: 0.713\n",
            "[143,    40] loss: 0.681\n",
            "[144,    10] loss: 0.657\n",
            "[144,    20] loss: 0.686\n",
            "[144,    30] loss: 0.647\n",
            "[144,    40] loss: 0.672\n",
            "[145,    10] loss: 0.660\n",
            "[145,    20] loss: 0.631\n",
            "[145,    30] loss: 0.611\n",
            "[145,    40] loss: 0.608\n",
            "[146,    10] loss: 0.633\n",
            "[146,    20] loss: 0.607\n",
            "[146,    30] loss: 0.584\n",
            "[146,    40] loss: 0.596\n",
            "[147,    10] loss: 0.579\n",
            "[147,    20] loss: 0.576\n",
            "[147,    30] loss: 0.624\n",
            "[147,    40] loss: 0.718\n",
            "[148,    10] loss: 0.626\n",
            "[148,    20] loss: 0.579\n",
            "[148,    30] loss: 0.560\n",
            "[148,    40] loss: 0.563\n",
            "[149,    10] loss: 0.535\n",
            "[149,    20] loss: 0.526\n",
            "[149,    30] loss: 0.507\n",
            "[149,    40] loss: 0.533\n",
            "[150,    10] loss: 0.637\n",
            "[150,    20] loss: 0.629\n",
            "[150,    30] loss: 0.583\n",
            "[150,    40] loss: 0.544\n",
            "[151,    10] loss: 0.579\n",
            "[151,    20] loss: 0.513\n",
            "[151,    30] loss: 0.517\n",
            "[151,    40] loss: 0.506\n",
            "[152,    10] loss: 0.490\n",
            "[152,    20] loss: 0.477\n",
            "[152,    30] loss: 0.459\n",
            "[152,    40] loss: 0.457\n",
            "[153,    10] loss: 0.532\n",
            "[153,    20] loss: 0.516\n",
            "[153,    30] loss: 0.453\n",
            "[153,    40] loss: 0.424\n",
            "[154,    10] loss: 0.460\n",
            "[154,    20] loss: 0.442\n",
            "[154,    30] loss: 0.449\n",
            "[154,    40] loss: 0.495\n",
            "[155,    10] loss: 0.547\n",
            "[155,    20] loss: 0.483\n",
            "[155,    30] loss: 0.452\n",
            "[155,    40] loss: 0.435\n",
            "[156,    10] loss: 0.424\n",
            "[156,    20] loss: 0.421\n",
            "[156,    30] loss: 0.398\n",
            "[156,    40] loss: 0.455\n",
            "[157,    10] loss: 0.553\n",
            "[157,    20] loss: 0.554\n",
            "[157,    30] loss: 0.458\n",
            "[157,    40] loss: 0.392\n",
            "[158,    10] loss: 0.327\n",
            "[158,    20] loss: 0.302\n",
            "[158,    30] loss: 0.315\n",
            "[158,    40] loss: 0.336\n",
            "[159,    10] loss: 0.450\n",
            "[159,    20] loss: 0.441\n",
            "[159,    30] loss: 0.375\n",
            "[159,    40] loss: 0.355\n",
            "[160,    10] loss: 0.378\n",
            "[160,    20] loss: 0.329\n",
            "[160,    30] loss: 0.299\n",
            "[160,    40] loss: 0.285\n",
            "[161,    10] loss: 0.246\n",
            "[161,    20] loss: 0.257\n",
            "[161,    30] loss: 0.259\n",
            "[161,    40] loss: 0.261\n",
            "[162,    10] loss: 0.284\n",
            "[162,    20] loss: 0.277\n",
            "[162,    30] loss: 0.295\n",
            "[162,    40] loss: 0.242\n",
            "[163,    10] loss: 0.303\n",
            "[163,    20] loss: 0.310\n",
            "[163,    30] loss: 0.259\n",
            "[163,    40] loss: 0.276\n",
            "[164,    10] loss: 0.444\n",
            "[164,    20] loss: 0.411\n",
            "[164,    30] loss: 0.337\n",
            "[164,    40] loss: 0.319\n",
            "[165,    10] loss: 0.288\n",
            "[165,    20] loss: 0.259\n",
            "[165,    30] loss: 0.263\n",
            "[165,    40] loss: 0.238\n",
            "[166,    10] loss: 0.242\n",
            "[166,    20] loss: 0.201\n",
            "[166,    30] loss: 0.175\n",
            "[166,    40] loss: 0.189\n",
            "[167,    10] loss: 0.157\n",
            "[167,    20] loss: 0.150\n",
            "[167,    30] loss: 0.138\n",
            "[167,    40] loss: 0.125\n",
            "[168,    10] loss: 0.184\n",
            "[168,    20] loss: 0.186\n",
            "[168,    30] loss: 0.160\n",
            "[168,    40] loss: 0.153\n",
            "[169,    10] loss: 0.214\n",
            "[169,    20] loss: 0.224\n",
            "[169,    30] loss: 0.199\n",
            "[169,    40] loss: 0.166\n",
            "[170,    10] loss: 0.339\n",
            "[170,    20] loss: 0.308\n",
            "[170,    30] loss: 0.242\n",
            "[170,    40] loss: 0.191\n",
            "[171,    10] loss: 0.175\n",
            "[171,    20] loss: 0.164\n",
            "[171,    30] loss: 0.138\n",
            "[171,    40] loss: 0.148\n",
            "[172,    10] loss: 0.298\n",
            "[172,    20] loss: 0.298\n",
            "[172,    30] loss: 0.222\n",
            "[172,    40] loss: 0.187\n",
            "[173,    10] loss: 0.291\n",
            "[173,    20] loss: 0.210\n",
            "[173,    30] loss: 0.184\n",
            "[173,    40] loss: 0.166\n",
            "[174,    10] loss: 0.170\n",
            "[174,    20] loss: 0.146\n",
            "[174,    30] loss: 0.126\n",
            "[174,    40] loss: 0.099\n",
            "[175,    10] loss: 0.072\n",
            "[175,    20] loss: 0.071\n",
            "[175,    30] loss: 0.067\n",
            "[175,    40] loss: 0.076\n",
            "[176,    10] loss: 0.069\n",
            "[176,    20] loss: 0.059\n",
            "[176,    30] loss: 0.051\n",
            "[176,    40] loss: 0.047\n",
            "[177,    10] loss: 0.037\n",
            "[177,    20] loss: 0.029\n",
            "[177,    30] loss: 0.032\n",
            "[177,    40] loss: 0.025\n",
            "[178,    10] loss: 0.017\n",
            "[178,    20] loss: 0.019\n",
            "[178,    30] loss: 0.014\n",
            "[178,    40] loss: 0.013\n",
            "[179,    10] loss: 0.008\n",
            "[179,    20] loss: 0.013\n",
            "[179,    30] loss: 0.007\n",
            "[179,    40] loss: 0.006\n",
            "[180,    10] loss: 0.005\n",
            "[180,    20] loss: 0.008\n",
            "[180,    30] loss: 0.004\n",
            "[180,    40] loss: 0.004\n",
            "[181,    10] loss: 0.007\n",
            "[181,    20] loss: 0.003\n",
            "[181,    30] loss: 0.003\n",
            "[181,    40] loss: 0.003\n",
            "[182,    10] loss: 0.003\n",
            "[182,    20] loss: 0.006\n",
            "[182,    30] loss: 0.003\n",
            "[182,    40] loss: 0.003\n",
            "[183,    10] loss: 0.003\n",
            "[183,    20] loss: 0.005\n",
            "[183,    30] loss: 0.003\n",
            "[183,    40] loss: 0.002\n",
            "[184,    10] loss: 0.002\n",
            "[184,    20] loss: 0.002\n",
            "[184,    30] loss: 0.004\n",
            "[184,    40] loss: 0.002\n",
            "[185,    10] loss: 0.002\n",
            "[185,    20] loss: 0.002\n",
            "[185,    30] loss: 0.002\n",
            "[185,    40] loss: 0.002\n",
            "[186,    10] loss: 0.002\n",
            "[186,    20] loss: 0.002\n",
            "[186,    30] loss: 0.002\n",
            "[186,    40] loss: 0.002\n",
            "[187,    10] loss: 0.002\n",
            "[187,    20] loss: 0.002\n",
            "[187,    30] loss: 0.002\n",
            "[187,    40] loss: 0.002\n",
            "[188,    10] loss: 0.001\n",
            "[188,    20] loss: 0.001\n",
            "[188,    30] loss: 0.002\n",
            "[188,    40] loss: 0.001\n",
            "[189,    10] loss: 0.001\n",
            "[189,    20] loss: 0.001\n",
            "[189,    30] loss: 0.001\n",
            "[189,    40] loss: 0.001\n",
            "[190,    10] loss: 0.001\n",
            "[190,    20] loss: 0.001\n",
            "[190,    30] loss: 0.001\n",
            "[190,    40] loss: 0.001\n",
            "[191,    10] loss: 0.001\n",
            "[191,    20] loss: 0.001\n",
            "[191,    30] loss: 0.001\n",
            "[191,    40] loss: 0.001\n",
            "[192,    10] loss: 0.001\n",
            "[192,    20] loss: 0.001\n",
            "[192,    30] loss: 0.001\n",
            "[192,    40] loss: 0.001\n",
            "[193,    10] loss: 0.001\n",
            "[193,    20] loss: 0.001\n",
            "[193,    30] loss: 0.001\n",
            "[193,    40] loss: 0.001\n",
            "[194,    10] loss: 0.001\n",
            "[194,    20] loss: 0.001\n",
            "[194,    30] loss: 0.001\n",
            "[194,    40] loss: 0.001\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 train images: 100 %\n",
            "Accuracy of the network on the 10000 test dataset 1: 100 %\n",
            "Accuracy of the network on the 10000 test dataset 2: 85 %\n",
            "Accuracy of the network on the 10000 test dataset 3: 70 %\n",
            "Accuracy of the network on the 10000 test dataset 4: 63 %\n",
            "Accuracy of the network on the 10000 test dataset 5: 60 %\n",
            "Accuracy of the network on the 10000 test dataset 6: 59 %\n",
            "Accuracy of the network on the 10000 test dataset 7: 58 %\n",
            "Accuracy of the network on the 10000 test dataset 8: 57 %\n",
            "Accuracy of the network on the 10000 test dataset 9: 56 %\n",
            "Accuracy of the network on the 10000 test dataset 10: 55 %\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "training on data set   2\n",
            "[1,    10] loss: 1.105\n",
            "[1,    20] loss: 1.103\n",
            "[1,    30] loss: 1.101\n",
            "[1,    40] loss: 1.100\n",
            "[2,    10] loss: 1.099\n",
            "[2,    20] loss: 1.098\n",
            "[2,    30] loss: 1.098\n",
            "[2,    40] loss: 1.098\n",
            "[3,    10] loss: 1.098\n",
            "[3,    20] loss: 1.098\n",
            "[3,    30] loss: 1.098\n",
            "[3,    40] loss: 1.098\n",
            "[4,    10] loss: 1.098\n",
            "[4,    20] loss: 1.098\n",
            "[4,    30] loss: 1.098\n",
            "[4,    40] loss: 1.099\n",
            "[5,    10] loss: 1.098\n",
            "[5,    20] loss: 1.098\n",
            "[5,    30] loss: 1.098\n",
            "[5,    40] loss: 1.099\n",
            "[6,    10] loss: 1.098\n",
            "[6,    20] loss: 1.098\n",
            "[6,    30] loss: 1.098\n",
            "[6,    40] loss: 1.097\n",
            "[7,    10] loss: 1.097\n",
            "[7,    20] loss: 1.098\n",
            "[7,    30] loss: 1.098\n",
            "[7,    40] loss: 1.097\n",
            "[8,    10] loss: 1.097\n",
            "[8,    20] loss: 1.097\n",
            "[8,    30] loss: 1.097\n",
            "[8,    40] loss: 1.096\n",
            "[9,    10] loss: 1.096\n",
            "[9,    20] loss: 1.096\n",
            "[9,    30] loss: 1.096\n",
            "[9,    40] loss: 1.096\n",
            "[10,    10] loss: 1.095\n",
            "[10,    20] loss: 1.095\n",
            "[10,    30] loss: 1.094\n",
            "[10,    40] loss: 1.094\n",
            "[11,    10] loss: 1.093\n",
            "[11,    20] loss: 1.092\n",
            "[11,    30] loss: 1.092\n",
            "[11,    40] loss: 1.091\n",
            "[12,    10] loss: 1.089\n",
            "[12,    20] loss: 1.088\n",
            "[12,    30] loss: 1.086\n",
            "[12,    40] loss: 1.081\n",
            "[13,    10] loss: 1.081\n",
            "[13,    20] loss: 1.075\n",
            "[13,    30] loss: 1.073\n",
            "[13,    40] loss: 1.070\n",
            "[14,    10] loss: 1.061\n",
            "[14,    20] loss: 1.054\n",
            "[14,    30] loss: 1.055\n",
            "[14,    40] loss: 1.046\n",
            "[15,    10] loss: 1.043\n",
            "[15,    20] loss: 1.023\n",
            "[15,    30] loss: 1.039\n",
            "[15,    40] loss: 1.040\n",
            "[16,    10] loss: 1.026\n",
            "[16,    20] loss: 1.015\n",
            "[16,    30] loss: 1.024\n",
            "[16,    40] loss: 1.022\n",
            "[17,    10] loss: 1.006\n",
            "[17,    20] loss: 1.008\n",
            "[17,    30] loss: 1.021\n",
            "[17,    40] loss: 0.994\n",
            "[18,    10] loss: 0.999\n",
            "[18,    20] loss: 0.988\n",
            "[18,    30] loss: 0.995\n",
            "[18,    40] loss: 1.011\n",
            "[19,    10] loss: 0.994\n",
            "[19,    20] loss: 0.989\n",
            "[19,    30] loss: 0.980\n",
            "[19,    40] loss: 0.975\n",
            "[20,    10] loss: 0.989\n",
            "[20,    20] loss: 0.989\n",
            "[20,    30] loss: 0.979\n",
            "[20,    40] loss: 1.009\n",
            "[21,    10] loss: 0.984\n",
            "[21,    20] loss: 0.962\n",
            "[21,    30] loss: 0.981\n",
            "[21,    40] loss: 0.960\n",
            "[22,    10] loss: 0.973\n",
            "[22,    20] loss: 0.982\n",
            "[22,    30] loss: 0.963\n",
            "[22,    40] loss: 0.947\n",
            "[23,    10] loss: 0.974\n",
            "[23,    20] loss: 0.961\n",
            "[23,    30] loss: 0.960\n",
            "[23,    40] loss: 0.944\n",
            "[24,    10] loss: 0.949\n",
            "[24,    20] loss: 0.945\n",
            "[24,    30] loss: 0.936\n",
            "[24,    40] loss: 0.952\n",
            "[25,    10] loss: 0.940\n",
            "[25,    20] loss: 0.928\n",
            "[25,    30] loss: 0.912\n",
            "[25,    40] loss: 0.913\n",
            "[26,    10] loss: 0.981\n",
            "[26,    20] loss: 0.936\n",
            "[26,    30] loss: 0.928\n",
            "[26,    40] loss: 0.908\n",
            "[27,    10] loss: 0.894\n",
            "[27,    20] loss: 0.895\n",
            "[27,    30] loss: 0.907\n",
            "[27,    40] loss: 0.899\n",
            "[28,    10] loss: 0.897\n",
            "[28,    20] loss: 0.866\n",
            "[28,    30] loss: 0.869\n",
            "[28,    40] loss: 0.868\n",
            "[29,    10] loss: 0.916\n",
            "[29,    20] loss: 0.880\n",
            "[29,    30] loss: 0.835\n",
            "[29,    40] loss: 0.849\n",
            "[30,    10] loss: 0.876\n",
            "[30,    20] loss: 0.858\n",
            "[30,    30] loss: 0.846\n",
            "[30,    40] loss: 0.868\n",
            "[31,    10] loss: 0.953\n",
            "[31,    20] loss: 0.882\n",
            "[31,    30] loss: 0.846\n",
            "[31,    40] loss: 0.866\n",
            "[32,    10] loss: 0.837\n",
            "[32,    20] loss: 0.823\n",
            "[32,    30] loss: 0.836\n",
            "[32,    40] loss: 0.822\n",
            "[33,    10] loss: 0.877\n",
            "[33,    20] loss: 0.799\n",
            "[33,    30] loss: 0.821\n",
            "[33,    40] loss: 0.818\n",
            "[34,    10] loss: 0.804\n",
            "[34,    20] loss: 0.803\n",
            "[34,    30] loss: 0.801\n",
            "[34,    40] loss: 0.811\n",
            "[35,    10] loss: 0.807\n",
            "[35,    20] loss: 0.796\n",
            "[35,    30] loss: 0.792\n",
            "[35,    40] loss: 0.792\n",
            "[36,    10] loss: 0.806\n",
            "[36,    20] loss: 0.781\n",
            "[36,    30] loss: 0.813\n",
            "[36,    40] loss: 0.750\n",
            "[37,    10] loss: 0.781\n",
            "[37,    20] loss: 0.809\n",
            "[37,    30] loss: 0.773\n",
            "[37,    40] loss: 0.760\n",
            "[38,    10] loss: 0.760\n",
            "[38,    20] loss: 0.754\n",
            "[38,    30] loss: 0.791\n",
            "[38,    40] loss: 0.769\n",
            "[39,    10] loss: 0.774\n",
            "[39,    20] loss: 0.781\n",
            "[39,    30] loss: 0.769\n",
            "[39,    40] loss: 0.771\n",
            "[40,    10] loss: 0.770\n",
            "[40,    20] loss: 0.753\n",
            "[40,    30] loss: 0.752\n",
            "[40,    40] loss: 0.773\n",
            "[41,    10] loss: 0.746\n",
            "[41,    20] loss: 0.761\n",
            "[41,    30] loss: 0.746\n",
            "[41,    40] loss: 0.759\n",
            "[42,    10] loss: 0.762\n",
            "[42,    20] loss: 0.763\n",
            "[42,    30] loss: 0.760\n",
            "[42,    40] loss: 0.732\n",
            "[43,    10] loss: 0.749\n",
            "[43,    20] loss: 0.706\n",
            "[43,    30] loss: 0.773\n",
            "[43,    40] loss: 0.771\n",
            "[44,    10] loss: 0.776\n",
            "[44,    20] loss: 0.734\n",
            "[44,    30] loss: 0.756\n",
            "[44,    40] loss: 0.748\n",
            "[45,    10] loss: 0.718\n",
            "[45,    20] loss: 0.721\n",
            "[45,    30] loss: 0.740\n",
            "[45,    40] loss: 0.779\n",
            "[46,    10] loss: 0.790\n",
            "[46,    20] loss: 0.758\n",
            "[46,    30] loss: 0.722\n",
            "[46,    40] loss: 0.708\n",
            "[47,    10] loss: 0.771\n",
            "[47,    20] loss: 0.754\n",
            "[47,    30] loss: 0.722\n",
            "[47,    40] loss: 0.725\n",
            "[48,    10] loss: 0.758\n",
            "[48,    20] loss: 0.715\n",
            "[48,    30] loss: 0.715\n",
            "[48,    40] loss: 0.671\n",
            "[49,    10] loss: 0.752\n",
            "[49,    20] loss: 0.739\n",
            "[49,    30] loss: 0.707\n",
            "[49,    40] loss: 0.695\n",
            "[50,    10] loss: 0.681\n",
            "[50,    20] loss: 0.674\n",
            "[50,    30] loss: 0.713\n",
            "[50,    40] loss: 0.756\n",
            "[51,    10] loss: 0.762\n",
            "[51,    20] loss: 0.728\n",
            "[51,    30] loss: 0.719\n",
            "[51,    40] loss: 0.712\n",
            "[52,    10] loss: 0.696\n",
            "[52,    20] loss: 0.676\n",
            "[52,    30] loss: 0.673\n",
            "[52,    40] loss: 0.702\n",
            "[53,    10] loss: 0.733\n",
            "[53,    20] loss: 0.689\n",
            "[53,    30] loss: 0.696\n",
            "[53,    40] loss: 0.705\n",
            "[54,    10] loss: 0.655\n",
            "[54,    20] loss: 0.670\n",
            "[54,    30] loss: 0.671\n",
            "[54,    40] loss: 0.699\n",
            "[55,    10] loss: 0.772\n",
            "[55,    20] loss: 0.687\n",
            "[55,    30] loss: 0.700\n",
            "[55,    40] loss: 0.667\n",
            "[56,    10] loss: 0.657\n",
            "[56,    20] loss: 0.669\n",
            "[56,    30] loss: 0.639\n",
            "[56,    40] loss: 0.674\n",
            "[57,    10] loss: 0.657\n",
            "[57,    20] loss: 0.669\n",
            "[57,    30] loss: 0.653\n",
            "[57,    40] loss: 0.660\n",
            "[58,    10] loss: 0.625\n",
            "[58,    20] loss: 0.673\n",
            "[58,    30] loss: 0.652\n",
            "[58,    40] loss: 0.617\n",
            "[59,    10] loss: 0.607\n",
            "[59,    20] loss: 0.619\n",
            "[59,    30] loss: 0.652\n",
            "[59,    40] loss: 0.678\n",
            "[60,    10] loss: 0.639\n",
            "[60,    20] loss: 0.611\n",
            "[60,    30] loss: 0.622\n",
            "[60,    40] loss: 0.668\n",
            "[61,    10] loss: 0.630\n",
            "[61,    20] loss: 0.647\n",
            "[61,    30] loss: 0.608\n",
            "[61,    40] loss: 0.597\n",
            "[62,    10] loss: 0.656\n",
            "[62,    20] loss: 0.662\n",
            "[62,    30] loss: 0.622\n",
            "[62,    40] loss: 0.650\n",
            "[63,    10] loss: 0.660\n",
            "[63,    20] loss: 0.606\n",
            "[63,    30] loss: 0.620\n",
            "[63,    40] loss: 0.596\n",
            "[64,    10] loss: 0.641\n",
            "[64,    20] loss: 0.607\n",
            "[64,    30] loss: 0.626\n",
            "[64,    40] loss: 0.630\n",
            "[65,    10] loss: 0.636\n",
            "[65,    20] loss: 0.596\n",
            "[65,    30] loss: 0.594\n",
            "[65,    40] loss: 0.589\n",
            "[66,    10] loss: 0.564\n",
            "[66,    20] loss: 0.587\n",
            "[66,    30] loss: 0.564\n",
            "[66,    40] loss: 0.562\n",
            "[67,    10] loss: 0.547\n",
            "[67,    20] loss: 0.549\n",
            "[67,    30] loss: 0.587\n",
            "[67,    40] loss: 0.543\n",
            "[68,    10] loss: 0.552\n",
            "[68,    20] loss: 0.537\n",
            "[68,    30] loss: 0.542\n",
            "[68,    40] loss: 0.552\n",
            "[69,    10] loss: 0.590\n",
            "[69,    20] loss: 0.569\n",
            "[69,    30] loss: 0.535\n",
            "[69,    40] loss: 0.534\n",
            "[70,    10] loss: 0.508\n",
            "[70,    20] loss: 0.514\n",
            "[70,    30] loss: 0.524\n",
            "[70,    40] loss: 0.542\n",
            "[71,    10] loss: 0.517\n",
            "[71,    20] loss: 0.518\n",
            "[71,    30] loss: 0.489\n",
            "[71,    40] loss: 0.526\n",
            "[72,    10] loss: 0.636\n",
            "[72,    20] loss: 0.555\n",
            "[72,    30] loss: 0.530\n",
            "[72,    40] loss: 0.509\n",
            "[73,    10] loss: 0.488\n",
            "[73,    20] loss: 0.473\n",
            "[73,    30] loss: 0.497\n",
            "[73,    40] loss: 0.483\n",
            "[74,    10] loss: 0.500\n",
            "[74,    20] loss: 0.458\n",
            "[74,    30] loss: 0.481\n",
            "[74,    40] loss: 0.498\n",
            "[75,    10] loss: 0.469\n",
            "[75,    20] loss: 0.487\n",
            "[75,    30] loss: 0.446\n",
            "[75,    40] loss: 0.457\n",
            "[76,    10] loss: 0.450\n",
            "[76,    20] loss: 0.501\n",
            "[76,    30] loss: 0.441\n",
            "[76,    40] loss: 0.475\n",
            "[77,    10] loss: 0.637\n",
            "[77,    20] loss: 0.541\n",
            "[77,    30] loss: 0.509\n",
            "[77,    40] loss: 0.471\n",
            "[78,    10] loss: 0.406\n",
            "[78,    20] loss: 0.445\n",
            "[78,    30] loss: 0.432\n",
            "[78,    40] loss: 0.436\n",
            "[79,    10] loss: 0.414\n",
            "[79,    20] loss: 0.403\n",
            "[79,    30] loss: 0.413\n",
            "[79,    40] loss: 0.416\n",
            "[80,    10] loss: 0.426\n",
            "[80,    20] loss: 0.427\n",
            "[80,    30] loss: 0.413\n",
            "[80,    40] loss: 0.414\n",
            "[81,    10] loss: 0.501\n",
            "[81,    20] loss: 0.449\n",
            "[81,    30] loss: 0.391\n",
            "[81,    40] loss: 0.382\n",
            "[82,    10] loss: 0.386\n",
            "[82,    20] loss: 0.369\n",
            "[82,    30] loss: 0.350\n",
            "[82,    40] loss: 0.363\n",
            "[83,    10] loss: 0.360\n",
            "[83,    20] loss: 0.349\n",
            "[83,    30] loss: 0.331\n",
            "[83,    40] loss: 0.313\n",
            "[84,    10] loss: 0.355\n",
            "[84,    20] loss: 0.359\n",
            "[84,    30] loss: 0.302\n",
            "[84,    40] loss: 0.344\n",
            "[85,    10] loss: 0.428\n",
            "[85,    20] loss: 0.359\n",
            "[85,    30] loss: 0.381\n",
            "[85,    40] loss: 0.349\n",
            "[86,    10] loss: 0.397\n",
            "[86,    20] loss: 0.353\n",
            "[86,    30] loss: 0.327\n",
            "[86,    40] loss: 0.365\n",
            "[87,    10] loss: 0.343\n",
            "[87,    20] loss: 0.294\n",
            "[87,    30] loss: 0.285\n",
            "[87,    40] loss: 0.288\n",
            "[88,    10] loss: 0.327\n",
            "[88,    20] loss: 0.323\n",
            "[88,    30] loss: 0.321\n",
            "[88,    40] loss: 0.291\n",
            "[89,    10] loss: 0.288\n",
            "[89,    20] loss: 0.253\n",
            "[89,    30] loss: 0.237\n",
            "[89,    40] loss: 0.215\n",
            "[90,    10] loss: 0.204\n",
            "[90,    20] loss: 0.205\n",
            "[90,    30] loss: 0.236\n",
            "[90,    40] loss: 0.214\n",
            "[91,    10] loss: 0.231\n",
            "[91,    20] loss: 0.211\n",
            "[91,    30] loss: 0.171\n",
            "[91,    40] loss: 0.165\n",
            "[92,    10] loss: 0.151\n",
            "[92,    20] loss: 0.165\n",
            "[92,    30] loss: 0.139\n",
            "[92,    40] loss: 0.134\n",
            "[93,    10] loss: 0.169\n",
            "[93,    20] loss: 0.140\n",
            "[93,    30] loss: 0.129\n",
            "[93,    40] loss: 0.193\n",
            "[94,    10] loss: 0.421\n",
            "[94,    20] loss: 0.327\n",
            "[94,    30] loss: 0.252\n",
            "[94,    40] loss: 0.225\n",
            "[95,    10] loss: 0.195\n",
            "[95,    20] loss: 0.199\n",
            "[95,    30] loss: 0.146\n",
            "[95,    40] loss: 0.160\n",
            "[96,    10] loss: 0.142\n",
            "[96,    20] loss: 0.125\n",
            "[96,    30] loss: 0.110\n",
            "[96,    40] loss: 0.139\n",
            "[97,    10] loss: 0.102\n",
            "[97,    20] loss: 0.089\n",
            "[97,    30] loss: 0.075\n",
            "[97,    40] loss: 0.084\n",
            "[98,    10] loss: 0.338\n",
            "[98,    20] loss: 0.250\n",
            "[98,    30] loss: 0.185\n",
            "[98,    40] loss: 0.138\n",
            "[99,    10] loss: 0.125\n",
            "[99,    20] loss: 0.106\n",
            "[99,    30] loss: 0.127\n",
            "[99,    40] loss: 0.087\n",
            "[100,    10] loss: 0.052\n",
            "[100,    20] loss: 0.049\n",
            "[100,    30] loss: 0.045\n",
            "[100,    40] loss: 0.044\n",
            "[101,    10] loss: 0.044\n",
            "[101,    20] loss: 0.039\n",
            "[101,    30] loss: 0.045\n",
            "[101,    40] loss: 0.042\n",
            "[102,    10] loss: 0.034\n",
            "[102,    20] loss: 0.025\n",
            "[102,    30] loss: 0.027\n",
            "[102,    40] loss: 0.019\n",
            "[103,    10] loss: 0.020\n",
            "[103,    20] loss: 0.014\n",
            "[103,    30] loss: 0.014\n",
            "[103,    40] loss: 0.014\n",
            "[104,    10] loss: 0.013\n",
            "[104,    20] loss: 0.010\n",
            "[104,    30] loss: 0.010\n",
            "[104,    40] loss: 0.009\n",
            "[105,    10] loss: 0.006\n",
            "[105,    20] loss: 0.005\n",
            "[105,    30] loss: 0.006\n",
            "[105,    40] loss: 0.005\n",
            "[106,    10] loss: 0.004\n",
            "[106,    20] loss: 0.004\n",
            "[106,    30] loss: 0.004\n",
            "[106,    40] loss: 0.004\n",
            "[107,    10] loss: 0.003\n",
            "[107,    20] loss: 0.004\n",
            "[107,    30] loss: 0.003\n",
            "[107,    40] loss: 0.003\n",
            "[108,    10] loss: 0.003\n",
            "[108,    20] loss: 0.003\n",
            "[108,    30] loss: 0.003\n",
            "[108,    40] loss: 0.003\n",
            "[109,    10] loss: 0.003\n",
            "[109,    20] loss: 0.003\n",
            "[109,    30] loss: 0.003\n",
            "[109,    40] loss: 0.003\n",
            "[110,    10] loss: 0.002\n",
            "[110,    20] loss: 0.002\n",
            "[110,    30] loss: 0.002\n",
            "[110,    40] loss: 0.002\n",
            "[111,    10] loss: 0.002\n",
            "[111,    20] loss: 0.002\n",
            "[111,    30] loss: 0.002\n",
            "[111,    40] loss: 0.002\n",
            "[112,    10] loss: 0.002\n",
            "[112,    20] loss: 0.002\n",
            "[112,    30] loss: 0.002\n",
            "[112,    40] loss: 0.002\n",
            "[113,    10] loss: 0.002\n",
            "[113,    20] loss: 0.002\n",
            "[113,    30] loss: 0.002\n",
            "[113,    40] loss: 0.002\n",
            "[114,    10] loss: 0.002\n",
            "[114,    20] loss: 0.001\n",
            "[114,    30] loss: 0.002\n",
            "[114,    40] loss: 0.002\n",
            "[115,    10] loss: 0.001\n",
            "[115,    20] loss: 0.001\n",
            "[115,    30] loss: 0.002\n",
            "[115,    40] loss: 0.001\n",
            "[116,    10] loss: 0.001\n",
            "[116,    20] loss: 0.001\n",
            "[116,    30] loss: 0.001\n",
            "[116,    40] loss: 0.001\n",
            "[117,    10] loss: 0.001\n",
            "[117,    20] loss: 0.001\n",
            "[117,    30] loss: 0.001\n",
            "[117,    40] loss: 0.001\n",
            "[118,    10] loss: 0.001\n",
            "[118,    20] loss: 0.001\n",
            "[118,    30] loss: 0.001\n",
            "[118,    40] loss: 0.001\n",
            "[119,    10] loss: 0.001\n",
            "[119,    20] loss: 0.001\n",
            "[119,    30] loss: 0.001\n",
            "[119,    40] loss: 0.001\n",
            "[120,    10] loss: 0.001\n",
            "[120,    20] loss: 0.001\n",
            "[120,    30] loss: 0.001\n",
            "[120,    40] loss: 0.001\n",
            "[121,    10] loss: 0.001\n",
            "[121,    20] loss: 0.001\n",
            "[121,    30] loss: 0.001\n",
            "[121,    40] loss: 0.001\n",
            "[122,    10] loss: 0.001\n",
            "[122,    20] loss: 0.001\n",
            "[122,    30] loss: 0.001\n",
            "[122,    40] loss: 0.001\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 train images: 100 %\n",
            "Accuracy of the network on the 10000 test dataset 1: 65 %\n",
            "Accuracy of the network on the 10000 test dataset 2: 100 %\n",
            "Accuracy of the network on the 10000 test dataset 3: 94 %\n",
            "Accuracy of the network on the 10000 test dataset 4: 86 %\n",
            "Accuracy of the network on the 10000 test dataset 5: 81 %\n",
            "Accuracy of the network on the 10000 test dataset 6: 77 %\n",
            "Accuracy of the network on the 10000 test dataset 7: 74 %\n",
            "Accuracy of the network on the 10000 test dataset 8: 72 %\n",
            "Accuracy of the network on the 10000 test dataset 9: 70 %\n",
            "Accuracy of the network on the 10000 test dataset 10: 68 %\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "training on data set   3\n",
            "[1,    10] loss: 1.107\n",
            "[1,    20] loss: 1.102\n",
            "[1,    30] loss: 1.100\n",
            "[1,    40] loss: 1.098\n",
            "[2,    10] loss: 1.099\n",
            "[2,    20] loss: 1.098\n",
            "[2,    30] loss: 1.099\n",
            "[2,    40] loss: 1.098\n",
            "[3,    10] loss: 1.098\n",
            "[3,    20] loss: 1.098\n",
            "[3,    30] loss: 1.098\n",
            "[3,    40] loss: 1.098\n",
            "[4,    10] loss: 1.098\n",
            "[4,    20] loss: 1.099\n",
            "[4,    30] loss: 1.098\n",
            "[4,    40] loss: 1.098\n",
            "[5,    10] loss: 1.098\n",
            "[5,    20] loss: 1.098\n",
            "[5,    30] loss: 1.097\n",
            "[5,    40] loss: 1.097\n",
            "[6,    10] loss: 1.097\n",
            "[6,    20] loss: 1.097\n",
            "[6,    30] loss: 1.097\n",
            "[6,    40] loss: 1.096\n",
            "[7,    10] loss: 1.096\n",
            "[7,    20] loss: 1.096\n",
            "[7,    30] loss: 1.096\n",
            "[7,    40] loss: 1.095\n",
            "[8,    10] loss: 1.094\n",
            "[8,    20] loss: 1.094\n",
            "[8,    30] loss: 1.092\n",
            "[8,    40] loss: 1.090\n",
            "[9,    10] loss: 1.089\n",
            "[9,    20] loss: 1.086\n",
            "[9,    30] loss: 1.083\n",
            "[9,    40] loss: 1.077\n",
            "[10,    10] loss: 1.071\n",
            "[10,    20] loss: 1.066\n",
            "[10,    30] loss: 1.047\n",
            "[10,    40] loss: 1.053\n",
            "[11,    10] loss: 1.040\n",
            "[11,    20] loss: 1.030\n",
            "[11,    30] loss: 1.017\n",
            "[11,    40] loss: 1.025\n",
            "[12,    10] loss: 1.020\n",
            "[12,    20] loss: 1.015\n",
            "[12,    30] loss: 1.005\n",
            "[12,    40] loss: 1.016\n",
            "[13,    10] loss: 0.997\n",
            "[13,    20] loss: 1.021\n",
            "[13,    30] loss: 0.995\n",
            "[13,    40] loss: 1.008\n",
            "[14,    10] loss: 1.003\n",
            "[14,    20] loss: 1.007\n",
            "[14,    30] loss: 1.007\n",
            "[14,    40] loss: 0.983\n",
            "[15,    10] loss: 0.993\n",
            "[15,    20] loss: 0.987\n",
            "[15,    30] loss: 0.987\n",
            "[15,    40] loss: 1.008\n",
            "[16,    10] loss: 1.003\n",
            "[16,    20] loss: 0.988\n",
            "[16,    30] loss: 0.983\n",
            "[16,    40] loss: 0.983\n",
            "[17,    10] loss: 0.984\n",
            "[17,    20] loss: 0.986\n",
            "[17,    30] loss: 0.974\n",
            "[17,    40] loss: 0.966\n",
            "[18,    10] loss: 0.981\n",
            "[18,    20] loss: 0.953\n",
            "[18,    30] loss: 0.973\n",
            "[18,    40] loss: 0.932\n",
            "[19,    10] loss: 0.956\n",
            "[19,    20] loss: 0.949\n",
            "[19,    30] loss: 0.920\n",
            "[19,    40] loss: 0.911\n",
            "[20,    10] loss: 0.933\n",
            "[20,    20] loss: 0.883\n",
            "[20,    30] loss: 0.884\n",
            "[20,    40] loss: 0.854\n",
            "[21,    10] loss: 0.874\n",
            "[21,    20] loss: 0.831\n",
            "[21,    30] loss: 0.824\n",
            "[21,    40] loss: 0.779\n",
            "[22,    10] loss: 0.803\n",
            "[22,    20] loss: 0.801\n",
            "[22,    30] loss: 0.767\n",
            "[22,    40] loss: 0.750\n",
            "[23,    10] loss: 0.761\n",
            "[23,    20] loss: 0.722\n",
            "[23,    30] loss: 0.711\n",
            "[23,    40] loss: 0.748\n",
            "[24,    10] loss: 0.739\n",
            "[24,    20] loss: 0.685\n",
            "[24,    30] loss: 0.726\n",
            "[24,    40] loss: 0.690\n",
            "[25,    10] loss: 0.676\n",
            "[25,    20] loss: 0.672\n",
            "[25,    30] loss: 0.717\n",
            "[25,    40] loss: 0.682\n",
            "[26,    10] loss: 0.728\n",
            "[26,    20] loss: 0.707\n",
            "[26,    30] loss: 0.651\n",
            "[26,    40] loss: 0.662\n",
            "[27,    10] loss: 0.703\n",
            "[27,    20] loss: 0.704\n",
            "[27,    30] loss: 0.670\n",
            "[27,    40] loss: 0.633\n",
            "[28,    10] loss: 0.633\n",
            "[28,    20] loss: 0.676\n",
            "[28,    30] loss: 0.665\n",
            "[28,    40] loss: 0.634\n",
            "[29,    10] loss: 0.626\n",
            "[29,    20] loss: 0.633\n",
            "[29,    30] loss: 0.645\n",
            "[29,    40] loss: 0.597\n",
            "[30,    10] loss: 0.592\n",
            "[30,    20] loss: 0.620\n",
            "[30,    30] loss: 0.633\n",
            "[30,    40] loss: 0.643\n",
            "[31,    10] loss: 0.647\n",
            "[31,    20] loss: 0.602\n",
            "[31,    30] loss: 0.661\n",
            "[31,    40] loss: 0.577\n",
            "[32,    10] loss: 0.644\n",
            "[32,    20] loss: 0.609\n",
            "[32,    30] loss: 0.629\n",
            "[32,    40] loss: 0.570\n",
            "[33,    10] loss: 0.594\n",
            "[33,    20] loss: 0.582\n",
            "[33,    30] loss: 0.586\n",
            "[33,    40] loss: 0.574\n",
            "[34,    10] loss: 0.584\n",
            "[34,    20] loss: 0.557\n",
            "[34,    30] loss: 0.564\n",
            "[34,    40] loss: 0.588\n",
            "[35,    10] loss: 0.688\n",
            "[35,    20] loss: 0.626\n",
            "[35,    30] loss: 0.596\n",
            "[35,    40] loss: 0.588\n",
            "[36,    10] loss: 0.577\n",
            "[36,    20] loss: 0.567\n",
            "[36,    30] loss: 0.534\n",
            "[36,    40] loss: 0.527\n",
            "[37,    10] loss: 0.548\n",
            "[37,    20] loss: 0.541\n",
            "[37,    30] loss: 0.530\n",
            "[37,    40] loss: 0.579\n",
            "[38,    10] loss: 0.524\n",
            "[38,    20] loss: 0.542\n",
            "[38,    30] loss: 0.538\n",
            "[38,    40] loss: 0.537\n",
            "[39,    10] loss: 0.550\n",
            "[39,    20] loss: 0.522\n",
            "[39,    30] loss: 0.556\n",
            "[39,    40] loss: 0.507\n",
            "[40,    10] loss: 0.534\n",
            "[40,    20] loss: 0.511\n",
            "[40,    30] loss: 0.489\n",
            "[40,    40] loss: 0.500\n",
            "[41,    10] loss: 0.493\n",
            "[41,    20] loss: 0.519\n",
            "[41,    30] loss: 0.503\n",
            "[41,    40] loss: 0.486\n",
            "[42,    10] loss: 0.489\n",
            "[42,    20] loss: 0.523\n",
            "[42,    30] loss: 0.512\n",
            "[42,    40] loss: 0.482\n",
            "[43,    10] loss: 0.496\n",
            "[43,    20] loss: 0.475\n",
            "[43,    30] loss: 0.499\n",
            "[43,    40] loss: 0.468\n",
            "[44,    10] loss: 0.497\n",
            "[44,    20] loss: 0.465\n",
            "[44,    30] loss: 0.496\n",
            "[44,    40] loss: 0.509\n",
            "[45,    10] loss: 0.464\n",
            "[45,    20] loss: 0.447\n",
            "[45,    30] loss: 0.482\n",
            "[45,    40] loss: 0.486\n",
            "[46,    10] loss: 0.510\n",
            "[46,    20] loss: 0.489\n",
            "[46,    30] loss: 0.480\n",
            "[46,    40] loss: 0.470\n",
            "[47,    10] loss: 0.503\n",
            "[47,    20] loss: 0.451\n",
            "[47,    30] loss: 0.447\n",
            "[47,    40] loss: 0.438\n",
            "[48,    10] loss: 0.464\n",
            "[48,    20] loss: 0.468\n",
            "[48,    30] loss: 0.454\n",
            "[48,    40] loss: 0.448\n",
            "[49,    10] loss: 0.486\n",
            "[49,    20] loss: 0.435\n",
            "[49,    30] loss: 0.428\n",
            "[49,    40] loss: 0.458\n",
            "[50,    10] loss: 0.444\n",
            "[50,    20] loss: 0.409\n",
            "[50,    30] loss: 0.440\n",
            "[50,    40] loss: 0.382\n",
            "[51,    10] loss: 0.412\n",
            "[51,    20] loss: 0.414\n",
            "[51,    30] loss: 0.419\n",
            "[51,    40] loss: 0.384\n",
            "[52,    10] loss: 0.463\n",
            "[52,    20] loss: 0.427\n",
            "[52,    30] loss: 0.412\n",
            "[52,    40] loss: 0.364\n",
            "[53,    10] loss: 0.361\n",
            "[53,    20] loss: 0.361\n",
            "[53,    30] loss: 0.381\n",
            "[53,    40] loss: 0.360\n",
            "[54,    10] loss: 0.362\n",
            "[54,    20] loss: 0.361\n",
            "[54,    30] loss: 0.380\n",
            "[54,    40] loss: 0.385\n",
            "[55,    10] loss: 0.362\n",
            "[55,    20] loss: 0.385\n",
            "[55,    30] loss: 0.359\n",
            "[55,    40] loss: 0.388\n",
            "[56,    10] loss: 0.345\n",
            "[56,    20] loss: 0.351\n",
            "[56,    30] loss: 0.352\n",
            "[56,    40] loss: 0.360\n",
            "[57,    10] loss: 0.355\n",
            "[57,    20] loss: 0.319\n",
            "[57,    30] loss: 0.315\n",
            "[57,    40] loss: 0.381\n",
            "[58,    10] loss: 0.392\n",
            "[58,    20] loss: 0.348\n",
            "[58,    30] loss: 0.329\n",
            "[58,    40] loss: 0.317\n",
            "[59,    10] loss: 0.291\n",
            "[59,    20] loss: 0.302\n",
            "[59,    30] loss: 0.267\n",
            "[59,    40] loss: 0.279\n",
            "[60,    10] loss: 0.264\n",
            "[60,    20] loss: 0.271\n",
            "[60,    30] loss: 0.277\n",
            "[60,    40] loss: 0.305\n",
            "[61,    10] loss: 0.414\n",
            "[61,    20] loss: 0.368\n",
            "[61,    30] loss: 0.317\n",
            "[61,    40] loss: 0.289\n",
            "[62,    10] loss: 0.300\n",
            "[62,    20] loss: 0.264\n",
            "[62,    30] loss: 0.256\n",
            "[62,    40] loss: 0.257\n",
            "[63,    10] loss: 0.344\n",
            "[63,    20] loss: 0.310\n",
            "[63,    30] loss: 0.258\n",
            "[63,    40] loss: 0.277\n",
            "[64,    10] loss: 0.304\n",
            "[64,    20] loss: 0.255\n",
            "[64,    30] loss: 0.271\n",
            "[64,    40] loss: 0.238\n",
            "[65,    10] loss: 0.258\n",
            "[65,    20] loss: 0.222\n",
            "[65,    30] loss: 0.240\n",
            "[65,    40] loss: 0.219\n",
            "[66,    10] loss: 0.202\n",
            "[66,    20] loss: 0.192\n",
            "[66,    30] loss: 0.195\n",
            "[66,    40] loss: 0.221\n",
            "[67,    10] loss: 0.241\n",
            "[67,    20] loss: 0.205\n",
            "[67,    30] loss: 0.201\n",
            "[67,    40] loss: 0.225\n",
            "[68,    10] loss: 0.201\n",
            "[68,    20] loss: 0.195\n",
            "[68,    30] loss: 0.202\n",
            "[68,    40] loss: 0.198\n",
            "[69,    10] loss: 0.175\n",
            "[69,    20] loss: 0.173\n",
            "[69,    30] loss: 0.178\n",
            "[69,    40] loss: 0.182\n",
            "[70,    10] loss: 0.185\n",
            "[70,    20] loss: 0.138\n",
            "[70,    30] loss: 0.139\n",
            "[70,    40] loss: 0.188\n",
            "[71,    10] loss: 0.385\n",
            "[71,    20] loss: 0.350\n",
            "[71,    30] loss: 0.287\n",
            "[71,    40] loss: 0.219\n",
            "[72,    10] loss: 0.159\n",
            "[72,    20] loss: 0.144\n",
            "[72,    30] loss: 0.153\n",
            "[72,    40] loss: 0.149\n",
            "[73,    10] loss: 0.139\n",
            "[73,    20] loss: 0.133\n",
            "[73,    30] loss: 0.155\n",
            "[73,    40] loss: 0.166\n",
            "[74,    10] loss: 0.183\n",
            "[74,    20] loss: 0.148\n",
            "[74,    30] loss: 0.134\n",
            "[74,    40] loss: 0.129\n",
            "[75,    10] loss: 0.133\n",
            "[75,    20] loss: 0.124\n",
            "[75,    30] loss: 0.106\n",
            "[75,    40] loss: 0.125\n",
            "[76,    10] loss: 0.131\n",
            "[76,    20] loss: 0.105\n",
            "[76,    30] loss: 0.114\n",
            "[76,    40] loss: 0.096\n",
            "[77,    10] loss: 0.100\n",
            "[77,    20] loss: 0.093\n",
            "[77,    30] loss: 0.088\n",
            "[77,    40] loss: 0.081\n",
            "[78,    10] loss: 0.086\n",
            "[78,    20] loss: 0.073\n",
            "[78,    30] loss: 0.077\n",
            "[78,    40] loss: 0.082\n",
            "[79,    10] loss: 0.091\n",
            "[79,    20] loss: 0.077\n",
            "[79,    30] loss: 0.067\n",
            "[79,    40] loss: 0.080\n",
            "[80,    10] loss: 0.174\n",
            "[80,    20] loss: 0.131\n",
            "[80,    30] loss: 0.099\n",
            "[80,    40] loss: 0.092\n",
            "[81,    10] loss: 0.131\n",
            "[81,    20] loss: 0.122\n",
            "[81,    30] loss: 0.109\n",
            "[81,    40] loss: 0.083\n",
            "[82,    10] loss: 0.058\n",
            "[82,    20] loss: 0.054\n",
            "[82,    30] loss: 0.048\n",
            "[82,    40] loss: 0.055\n",
            "[83,    10] loss: 0.066\n",
            "[83,    20] loss: 0.061\n",
            "[83,    30] loss: 0.052\n",
            "[83,    40] loss: 0.040\n",
            "[84,    10] loss: 0.032\n",
            "[84,    20] loss: 0.032\n",
            "[84,    30] loss: 0.027\n",
            "[84,    40] loss: 0.032\n",
            "[85,    10] loss: 0.025\n",
            "[85,    20] loss: 0.023\n",
            "[85,    30] loss: 0.024\n",
            "[85,    40] loss: 0.022\n",
            "[86,    10] loss: 0.015\n",
            "[86,    20] loss: 0.020\n",
            "[86,    30] loss: 0.022\n",
            "[86,    40] loss: 0.013\n",
            "[87,    10] loss: 0.013\n",
            "[87,    20] loss: 0.011\n",
            "[87,    30] loss: 0.011\n",
            "[87,    40] loss: 0.013\n",
            "[88,    10] loss: 0.016\n",
            "[88,    20] loss: 0.012\n",
            "[88,    30] loss: 0.009\n",
            "[88,    40] loss: 0.010\n",
            "[89,    10] loss: 0.028\n",
            "[89,    20] loss: 0.027\n",
            "[89,    30] loss: 0.022\n",
            "[89,    40] loss: 0.023\n",
            "[90,    10] loss: 0.023\n",
            "[90,    20] loss: 0.017\n",
            "[90,    30] loss: 0.022\n",
            "[90,    40] loss: 0.019\n",
            "[91,    10] loss: 0.078\n",
            "[91,    20] loss: 0.063\n",
            "[91,    30] loss: 0.069\n",
            "[91,    40] loss: 0.049\n",
            "[92,    10] loss: 0.037\n",
            "[92,    20] loss: 0.030\n",
            "[92,    30] loss: 0.021\n",
            "[92,    40] loss: 0.031\n",
            "[93,    10] loss: 0.034\n",
            "[93,    20] loss: 0.025\n",
            "[93,    30] loss: 0.027\n",
            "[93,    40] loss: 0.028\n",
            "[94,    10] loss: 0.023\n",
            "[94,    20] loss: 0.021\n",
            "[94,    30] loss: 0.014\n",
            "[94,    40] loss: 0.013\n",
            "[95,    10] loss: 0.022\n",
            "[95,    20] loss: 0.012\n",
            "[95,    30] loss: 0.010\n",
            "[95,    40] loss: 0.007\n",
            "[96,    10] loss: 0.007\n",
            "[96,    20] loss: 0.004\n",
            "[96,    30] loss: 0.008\n",
            "[96,    40] loss: 0.005\n",
            "[97,    10] loss: 0.003\n",
            "[97,    20] loss: 0.004\n",
            "[97,    30] loss: 0.007\n",
            "[97,    40] loss: 0.005\n",
            "[98,    10] loss: 0.008\n",
            "[98,    20] loss: 0.003\n",
            "[98,    30] loss: 0.006\n",
            "[98,    40] loss: 0.002\n",
            "[99,    10] loss: 0.005\n",
            "[99,    20] loss: 0.001\n",
            "[99,    30] loss: 0.003\n",
            "[99,    40] loss: 0.005\n",
            "[100,    10] loss: 0.005\n",
            "[100,    20] loss: 0.003\n",
            "[100,    30] loss: 0.002\n",
            "[100,    40] loss: 0.003\n",
            "[101,    10] loss: 0.001\n",
            "[101,    20] loss: 0.002\n",
            "[101,    30] loss: 0.007\n",
            "[101,    40] loss: 0.002\n",
            "[102,    10] loss: 0.002\n",
            "[102,    20] loss: 0.003\n",
            "[102,    30] loss: 0.001\n",
            "[102,    40] loss: 0.003\n",
            "[103,    10] loss: 0.001\n",
            "[103,    20] loss: 0.002\n",
            "[103,    30] loss: 0.001\n",
            "[103,    40] loss: 0.003\n",
            "[104,    10] loss: 0.002\n",
            "[104,    20] loss: 0.001\n",
            "[104,    30] loss: 0.001\n",
            "[104,    40] loss: 0.002\n",
            "[105,    10] loss: 0.001\n",
            "[105,    20] loss: 0.001\n",
            "[105,    30] loss: 0.001\n",
            "[105,    40] loss: 0.001\n",
            "[106,    10] loss: 0.001\n",
            "[106,    20] loss: 0.001\n",
            "[106,    30] loss: 0.001\n",
            "[106,    40] loss: 0.001\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 train images: 100 %\n",
            "Accuracy of the network on the 10000 test dataset 1: 46 %\n",
            "Accuracy of the network on the 10000 test dataset 2: 79 %\n",
            "Accuracy of the network on the 10000 test dataset 3: 100 %\n",
            "Accuracy of the network on the 10000 test dataset 4: 97 %\n",
            "Accuracy of the network on the 10000 test dataset 5: 92 %\n",
            "Accuracy of the network on the 10000 test dataset 6: 87 %\n",
            "Accuracy of the network on the 10000 test dataset 7: 84 %\n",
            "Accuracy of the network on the 10000 test dataset 8: 81 %\n",
            "Accuracy of the network on the 10000 test dataset 9: 78 %\n",
            "Accuracy of the network on the 10000 test dataset 10: 74 %\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "training on data set   4\n",
            "[1,    10] loss: 1.102\n",
            "[1,    20] loss: 1.102\n",
            "[1,    30] loss: 1.100\n",
            "[1,    40] loss: 1.099\n",
            "[2,    10] loss: 1.099\n",
            "[2,    20] loss: 1.098\n",
            "[2,    30] loss: 1.098\n",
            "[2,    40] loss: 1.098\n",
            "[3,    10] loss: 1.098\n",
            "[3,    20] loss: 1.098\n",
            "[3,    30] loss: 1.099\n",
            "[3,    40] loss: 1.100\n",
            "[4,    10] loss: 1.098\n",
            "[4,    20] loss: 1.098\n",
            "[4,    30] loss: 1.098\n",
            "[4,    40] loss: 1.098\n",
            "[5,    10] loss: 1.098\n",
            "[5,    20] loss: 1.098\n",
            "[5,    30] loss: 1.098\n",
            "[5,    40] loss: 1.097\n",
            "[6,    10] loss: 1.099\n",
            "[6,    20] loss: 1.098\n",
            "[6,    30] loss: 1.098\n",
            "[6,    40] loss: 1.098\n",
            "[7,    10] loss: 1.098\n",
            "[7,    20] loss: 1.098\n",
            "[7,    30] loss: 1.098\n",
            "[7,    40] loss: 1.098\n",
            "[8,    10] loss: 1.098\n",
            "[8,    20] loss: 1.098\n",
            "[8,    30] loss: 1.098\n",
            "[8,    40] loss: 1.097\n",
            "[9,    10] loss: 1.097\n",
            "[9,    20] loss: 1.097\n",
            "[9,    30] loss: 1.097\n",
            "[9,    40] loss: 1.097\n",
            "[10,    10] loss: 1.096\n",
            "[10,    20] loss: 1.096\n",
            "[10,    30] loss: 1.096\n",
            "[10,    40] loss: 1.096\n",
            "[11,    10] loss: 1.095\n",
            "[11,    20] loss: 1.095\n",
            "[11,    30] loss: 1.094\n",
            "[11,    40] loss: 1.094\n",
            "[12,    10] loss: 1.093\n",
            "[12,    20] loss: 1.092\n",
            "[12,    30] loss: 1.091\n",
            "[12,    40] loss: 1.089\n",
            "[13,    10] loss: 1.088\n",
            "[13,    20] loss: 1.087\n",
            "[13,    30] loss: 1.083\n",
            "[13,    40] loss: 1.080\n",
            "[14,    10] loss: 1.074\n",
            "[14,    20] loss: 1.067\n",
            "[14,    30] loss: 1.057\n",
            "[14,    40] loss: 1.041\n",
            "[15,    10] loss: 1.009\n",
            "[15,    20] loss: 0.978\n",
            "[15,    30] loss: 0.951\n",
            "[15,    40] loss: 0.893\n",
            "[16,    10] loss: 0.884\n",
            "[16,    20] loss: 0.848\n",
            "[16,    30] loss: 0.827\n",
            "[16,    40] loss: 0.820\n",
            "[17,    10] loss: 0.786\n",
            "[17,    20] loss: 0.767\n",
            "[17,    30] loss: 0.742\n",
            "[17,    40] loss: 0.725\n",
            "[18,    10] loss: 0.743\n",
            "[18,    20] loss: 0.726\n",
            "[18,    30] loss: 0.687\n",
            "[18,    40] loss: 0.668\n",
            "[19,    10] loss: 0.711\n",
            "[19,    20] loss: 0.647\n",
            "[19,    30] loss: 0.646\n",
            "[19,    40] loss: 0.654\n",
            "[20,    10] loss: 0.646\n",
            "[20,    20] loss: 0.624\n",
            "[20,    30] loss: 0.635\n",
            "[20,    40] loss: 0.596\n",
            "[21,    10] loss: 0.636\n",
            "[21,    20] loss: 0.633\n",
            "[21,    30] loss: 0.597\n",
            "[21,    40] loss: 0.546\n",
            "[22,    10] loss: 0.571\n",
            "[22,    20] loss: 0.573\n",
            "[22,    30] loss: 0.588\n",
            "[22,    40] loss: 0.588\n",
            "[23,    10] loss: 0.579\n",
            "[23,    20] loss: 0.572\n",
            "[23,    30] loss: 0.560\n",
            "[23,    40] loss: 0.570\n",
            "[24,    10] loss: 0.556\n",
            "[24,    20] loss: 0.576\n",
            "[24,    30] loss: 0.525\n",
            "[24,    40] loss: 0.565\n",
            "[25,    10] loss: 0.558\n",
            "[25,    20] loss: 0.519\n",
            "[25,    30] loss: 0.530\n",
            "[25,    40] loss: 0.511\n",
            "[26,    10] loss: 0.525\n",
            "[26,    20] loss: 0.514\n",
            "[26,    30] loss: 0.503\n",
            "[26,    40] loss: 0.498\n",
            "[27,    10] loss: 0.526\n",
            "[27,    20] loss: 0.523\n",
            "[27,    30] loss: 0.526\n",
            "[27,    40] loss: 0.501\n",
            "[28,    10] loss: 0.508\n",
            "[28,    20] loss: 0.468\n",
            "[28,    30] loss: 0.516\n",
            "[28,    40] loss: 0.477\n",
            "[29,    10] loss: 0.491\n",
            "[29,    20] loss: 0.498\n",
            "[29,    30] loss: 0.492\n",
            "[29,    40] loss: 0.494\n",
            "[30,    10] loss: 0.488\n",
            "[30,    20] loss: 0.486\n",
            "[30,    30] loss: 0.469\n",
            "[30,    40] loss: 0.478\n",
            "[31,    10] loss: 0.465\n",
            "[31,    20] loss: 0.485\n",
            "[31,    30] loss: 0.443\n",
            "[31,    40] loss: 0.457\n",
            "[32,    10] loss: 0.487\n",
            "[32,    20] loss: 0.486\n",
            "[32,    30] loss: 0.464\n",
            "[32,    40] loss: 0.462\n",
            "[33,    10] loss: 0.464\n",
            "[33,    20] loss: 0.444\n",
            "[33,    30] loss: 0.435\n",
            "[33,    40] loss: 0.423\n",
            "[34,    10] loss: 0.508\n",
            "[34,    20] loss: 0.458\n",
            "[34,    30] loss: 0.421\n",
            "[34,    40] loss: 0.459\n",
            "[35,    10] loss: 0.480\n",
            "[35,    20] loss: 0.454\n",
            "[35,    30] loss: 0.419\n",
            "[35,    40] loss: 0.388\n",
            "[36,    10] loss: 0.412\n",
            "[36,    20] loss: 0.390\n",
            "[36,    30] loss: 0.398\n",
            "[36,    40] loss: 0.412\n",
            "[37,    10] loss: 0.402\n",
            "[37,    20] loss: 0.394\n",
            "[37,    30] loss: 0.392\n",
            "[37,    40] loss: 0.455\n",
            "[38,    10] loss: 0.479\n",
            "[38,    20] loss: 0.435\n",
            "[38,    30] loss: 0.394\n",
            "[38,    40] loss: 0.389\n",
            "[39,    10] loss: 0.390\n",
            "[39,    20] loss: 0.391\n",
            "[39,    30] loss: 0.383\n",
            "[39,    40] loss: 0.374\n",
            "[40,    10] loss: 0.349\n",
            "[40,    20] loss: 0.391\n",
            "[40,    30] loss: 0.374\n",
            "[40,    40] loss: 0.360\n",
            "[41,    10] loss: 0.379\n",
            "[41,    20] loss: 0.368\n",
            "[41,    30] loss: 0.379\n",
            "[41,    40] loss: 0.390\n",
            "[42,    10] loss: 0.410\n",
            "[42,    20] loss: 0.355\n",
            "[42,    30] loss: 0.340\n",
            "[42,    40] loss: 0.346\n",
            "[43,    10] loss: 0.348\n",
            "[43,    20] loss: 0.339\n",
            "[43,    30] loss: 0.309\n",
            "[43,    40] loss: 0.357\n",
            "[44,    10] loss: 0.312\n",
            "[44,    20] loss: 0.327\n",
            "[44,    30] loss: 0.349\n",
            "[44,    40] loss: 0.340\n",
            "[45,    10] loss: 0.352\n",
            "[45,    20] loss: 0.322\n",
            "[45,    30] loss: 0.326\n",
            "[45,    40] loss: 0.356\n",
            "[46,    10] loss: 0.368\n",
            "[46,    20] loss: 0.359\n",
            "[46,    30] loss: 0.319\n",
            "[46,    40] loss: 0.318\n",
            "[47,    10] loss: 0.310\n",
            "[47,    20] loss: 0.297\n",
            "[47,    30] loss: 0.256\n",
            "[47,    40] loss: 0.285\n",
            "[48,    10] loss: 0.282\n",
            "[48,    20] loss: 0.306\n",
            "[48,    30] loss: 0.301\n",
            "[48,    40] loss: 0.286\n",
            "[49,    10] loss: 0.288\n",
            "[49,    20] loss: 0.278\n",
            "[49,    30] loss: 0.277\n",
            "[49,    40] loss: 0.267\n",
            "[50,    10] loss: 0.272\n",
            "[50,    20] loss: 0.252\n",
            "[50,    30] loss: 0.268\n",
            "[50,    40] loss: 0.293\n",
            "[51,    10] loss: 0.247\n",
            "[51,    20] loss: 0.243\n",
            "[51,    30] loss: 0.234\n",
            "[51,    40] loss: 0.244\n",
            "[52,    10] loss: 0.300\n",
            "[52,    20] loss: 0.246\n",
            "[52,    30] loss: 0.233\n",
            "[52,    40] loss: 0.238\n",
            "[53,    10] loss: 0.316\n",
            "[53,    20] loss: 0.291\n",
            "[53,    30] loss: 0.270\n",
            "[53,    40] loss: 0.211\n",
            "[54,    10] loss: 0.201\n",
            "[54,    20] loss: 0.186\n",
            "[54,    30] loss: 0.215\n",
            "[54,    40] loss: 0.217\n",
            "[55,    10] loss: 0.291\n",
            "[55,    20] loss: 0.260\n",
            "[55,    30] loss: 0.227\n",
            "[55,    40] loss: 0.194\n",
            "[56,    10] loss: 0.190\n",
            "[56,    20] loss: 0.187\n",
            "[56,    30] loss: 0.183\n",
            "[56,    40] loss: 0.173\n",
            "[57,    10] loss: 0.173\n",
            "[57,    20] loss: 0.163\n",
            "[57,    30] loss: 0.159\n",
            "[57,    40] loss: 0.176\n",
            "[58,    10] loss: 0.148\n",
            "[58,    20] loss: 0.143\n",
            "[58,    30] loss: 0.153\n",
            "[58,    40] loss: 0.135\n",
            "[59,    10] loss: 0.146\n",
            "[59,    20] loss: 0.140\n",
            "[59,    30] loss: 0.146\n",
            "[59,    40] loss: 0.143\n",
            "[60,    10] loss: 0.119\n",
            "[60,    20] loss: 0.143\n",
            "[60,    30] loss: 0.152\n",
            "[60,    40] loss: 0.114\n",
            "[61,    10] loss: 0.128\n",
            "[61,    20] loss: 0.116\n",
            "[61,    30] loss: 0.123\n",
            "[61,    40] loss: 0.114\n",
            "[62,    10] loss: 0.110\n",
            "[62,    20] loss: 0.101\n",
            "[62,    30] loss: 0.105\n",
            "[62,    40] loss: 0.093\n",
            "[63,    10] loss: 0.102\n",
            "[63,    20] loss: 0.093\n",
            "[63,    30] loss: 0.121\n",
            "[63,    40] loss: 0.110\n",
            "[64,    10] loss: 0.093\n",
            "[64,    20] loss: 0.079\n",
            "[64,    30] loss: 0.082\n",
            "[64,    40] loss: 0.078\n",
            "[65,    10] loss: 0.085\n",
            "[65,    20] loss: 0.080\n",
            "[65,    30] loss: 0.090\n",
            "[65,    40] loss: 0.086\n",
            "[66,    10] loss: 0.078\n",
            "[66,    20] loss: 0.086\n",
            "[66,    30] loss: 0.131\n",
            "[66,    40] loss: 0.099\n",
            "[67,    10] loss: 0.084\n",
            "[67,    20] loss: 0.088\n",
            "[67,    30] loss: 0.079\n",
            "[67,    40] loss: 0.093\n",
            "[68,    10] loss: 0.199\n",
            "[68,    20] loss: 0.141\n",
            "[68,    30] loss: 0.108\n",
            "[68,    40] loss: 0.101\n",
            "[69,    10] loss: 0.079\n",
            "[69,    20] loss: 0.068\n",
            "[69,    30] loss: 0.069\n",
            "[69,    40] loss: 0.054\n",
            "[70,    10] loss: 0.064\n",
            "[70,    20] loss: 0.058\n",
            "[70,    30] loss: 0.088\n",
            "[70,    40] loss: 0.109\n",
            "[71,    10] loss: 0.157\n",
            "[71,    20] loss: 0.116\n",
            "[71,    30] loss: 0.085\n",
            "[71,    40] loss: 0.081\n",
            "[72,    10] loss: 0.086\n",
            "[72,    20] loss: 0.061\n",
            "[72,    30] loss: 0.053\n",
            "[72,    40] loss: 0.061\n",
            "[73,    10] loss: 0.117\n",
            "[73,    20] loss: 0.113\n",
            "[73,    30] loss: 0.088\n",
            "[73,    40] loss: 0.080\n",
            "[74,    10] loss: 0.048\n",
            "[74,    20] loss: 0.033\n",
            "[74,    30] loss: 0.033\n",
            "[74,    40] loss: 0.028\n",
            "[75,    10] loss: 0.020\n",
            "[75,    20] loss: 0.019\n",
            "[75,    30] loss: 0.020\n",
            "[75,    40] loss: 0.024\n",
            "[76,    10] loss: 0.024\n",
            "[76,    20] loss: 0.021\n",
            "[76,    30] loss: 0.023\n",
            "[76,    40] loss: 0.017\n",
            "[77,    10] loss: 0.014\n",
            "[77,    20] loss: 0.013\n",
            "[77,    30] loss: 0.014\n",
            "[77,    40] loss: 0.011\n",
            "[78,    10] loss: 0.011\n",
            "[78,    20] loss: 0.012\n",
            "[78,    30] loss: 0.009\n",
            "[78,    40] loss: 0.010\n",
            "[79,    10] loss: 0.007\n",
            "[79,    20] loss: 0.008\n",
            "[79,    30] loss: 0.007\n",
            "[79,    40] loss: 0.004\n",
            "[80,    10] loss: 0.006\n",
            "[80,    20] loss: 0.005\n",
            "[80,    30] loss: 0.004\n",
            "[80,    40] loss: 0.005\n",
            "[81,    10] loss: 0.003\n",
            "[81,    20] loss: 0.004\n",
            "[81,    30] loss: 0.004\n",
            "[81,    40] loss: 0.004\n",
            "[82,    10] loss: 0.003\n",
            "[82,    20] loss: 0.002\n",
            "[82,    30] loss: 0.004\n",
            "[82,    40] loss: 0.003\n",
            "[83,    10] loss: 0.003\n",
            "[83,    20] loss: 0.002\n",
            "[83,    30] loss: 0.003\n",
            "[83,    40] loss: 0.002\n",
            "[84,    10] loss: 0.003\n",
            "[84,    20] loss: 0.002\n",
            "[84,    30] loss: 0.003\n",
            "[84,    40] loss: 0.002\n",
            "[85,    10] loss: 0.002\n",
            "[85,    20] loss: 0.002\n",
            "[85,    30] loss: 0.002\n",
            "[85,    40] loss: 0.002\n",
            "[86,    10] loss: 0.002\n",
            "[86,    20] loss: 0.002\n",
            "[86,    30] loss: 0.002\n",
            "[86,    40] loss: 0.002\n",
            "[87,    10] loss: 0.001\n",
            "[87,    20] loss: 0.001\n",
            "[87,    30] loss: 0.002\n",
            "[87,    40] loss: 0.001\n",
            "[88,    10] loss: 0.002\n",
            "[88,    20] loss: 0.001\n",
            "[88,    30] loss: 0.001\n",
            "[88,    40] loss: 0.001\n",
            "[89,    10] loss: 0.001\n",
            "[89,    20] loss: 0.001\n",
            "[89,    30] loss: 0.001\n",
            "[89,    40] loss: 0.001\n",
            "[90,    10] loss: 0.001\n",
            "[90,    20] loss: 0.001\n",
            "[90,    30] loss: 0.001\n",
            "[90,    40] loss: 0.001\n",
            "[91,    10] loss: 0.001\n",
            "[91,    20] loss: 0.001\n",
            "[91,    30] loss: 0.001\n",
            "[91,    40] loss: 0.001\n",
            "[92,    10] loss: 0.001\n",
            "[92,    20] loss: 0.001\n",
            "[92,    30] loss: 0.001\n",
            "[92,    40] loss: 0.001\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 train images: 100 %\n",
            "Accuracy of the network on the 10000 test dataset 1: 42 %\n",
            "Accuracy of the network on the 10000 test dataset 2: 64 %\n",
            "Accuracy of the network on the 10000 test dataset 3: 91 %\n",
            "Accuracy of the network on the 10000 test dataset 4: 100 %\n",
            "Accuracy of the network on the 10000 test dataset 5: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 6: 95 %\n",
            "Accuracy of the network on the 10000 test dataset 7: 92 %\n",
            "Accuracy of the network on the 10000 test dataset 8: 89 %\n",
            "Accuracy of the network on the 10000 test dataset 9: 86 %\n",
            "Accuracy of the network on the 10000 test dataset 10: 80 %\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "training on data set   5\n",
            "[1,    10] loss: 1.110\n",
            "[1,    20] loss: 1.105\n",
            "[1,    30] loss: 1.105\n",
            "[1,    40] loss: 1.100\n",
            "[2,    10] loss: 1.099\n",
            "[2,    20] loss: 1.099\n",
            "[2,    30] loss: 1.099\n",
            "[2,    40] loss: 1.099\n",
            "[3,    10] loss: 1.098\n",
            "[3,    20] loss: 1.098\n",
            "[3,    30] loss: 1.099\n",
            "[3,    40] loss: 1.097\n",
            "[4,    10] loss: 1.097\n",
            "[4,    20] loss: 1.099\n",
            "[4,    30] loss: 1.098\n",
            "[4,    40] loss: 1.098\n",
            "[5,    10] loss: 1.097\n",
            "[5,    20] loss: 1.097\n",
            "[5,    30] loss: 1.096\n",
            "[5,    40] loss: 1.096\n",
            "[6,    10] loss: 1.095\n",
            "[6,    20] loss: 1.094\n",
            "[6,    30] loss: 1.094\n",
            "[6,    40] loss: 1.092\n",
            "[7,    10] loss: 1.091\n",
            "[7,    20] loss: 1.089\n",
            "[7,    30] loss: 1.087\n",
            "[7,    40] loss: 1.081\n",
            "[8,    10] loss: 1.079\n",
            "[8,    20] loss: 1.072\n",
            "[8,    30] loss: 1.063\n",
            "[8,    40] loss: 1.051\n",
            "[9,    10] loss: 1.034\n",
            "[9,    20] loss: 1.020\n",
            "[9,    30] loss: 0.992\n",
            "[9,    40] loss: 0.984\n",
            "[10,    10] loss: 0.958\n",
            "[10,    20] loss: 0.956\n",
            "[10,    30] loss: 0.943\n",
            "[10,    40] loss: 0.926\n",
            "[11,    10] loss: 0.893\n",
            "[11,    20] loss: 0.899\n",
            "[11,    30] loss: 0.875\n",
            "[11,    40] loss: 0.865\n",
            "[12,    10] loss: 0.862\n",
            "[12,    20] loss: 0.814\n",
            "[12,    30] loss: 0.799\n",
            "[12,    40] loss: 0.762\n",
            "[13,    10] loss: 0.766\n",
            "[13,    20] loss: 0.723\n",
            "[13,    30] loss: 0.702\n",
            "[13,    40] loss: 0.678\n",
            "[14,    10] loss: 0.731\n",
            "[14,    20] loss: 0.639\n",
            "[14,    30] loss: 0.644\n",
            "[14,    40] loss: 0.622\n",
            "[15,    10] loss: 0.702\n",
            "[15,    20] loss: 0.637\n",
            "[15,    30] loss: 0.616\n",
            "[15,    40] loss: 0.614\n",
            "[16,    10] loss: 0.584\n",
            "[16,    20] loss: 0.559\n",
            "[16,    30] loss: 0.562\n",
            "[16,    40] loss: 0.554\n",
            "[17,    10] loss: 0.537\n",
            "[17,    20] loss: 0.508\n",
            "[17,    30] loss: 0.562\n",
            "[17,    40] loss: 0.531\n",
            "[18,    10] loss: 0.547\n",
            "[18,    20] loss: 0.521\n",
            "[18,    30] loss: 0.536\n",
            "[18,    40] loss: 0.503\n",
            "[19,    10] loss: 0.532\n",
            "[19,    20] loss: 0.496\n",
            "[19,    30] loss: 0.511\n",
            "[19,    40] loss: 0.483\n",
            "[20,    10] loss: 0.476\n",
            "[20,    20] loss: 0.551\n",
            "[20,    30] loss: 0.505\n",
            "[20,    40] loss: 0.520\n",
            "[21,    10] loss: 0.529\n",
            "[21,    20] loss: 0.485\n",
            "[21,    30] loss: 0.481\n",
            "[21,    40] loss: 0.480\n",
            "[22,    10] loss: 0.486\n",
            "[22,    20] loss: 0.456\n",
            "[22,    30] loss: 0.448\n",
            "[22,    40] loss: 0.448\n",
            "[23,    10] loss: 0.452\n",
            "[23,    20] loss: 0.449\n",
            "[23,    30] loss: 0.461\n",
            "[23,    40] loss: 0.479\n",
            "[24,    10] loss: 0.620\n",
            "[24,    20] loss: 0.527\n",
            "[24,    30] loss: 0.475\n",
            "[24,    40] loss: 0.429\n",
            "[25,    10] loss: 0.451\n",
            "[25,    20] loss: 0.423\n",
            "[25,    30] loss: 0.422\n",
            "[25,    40] loss: 0.398\n",
            "[26,    10] loss: 0.439\n",
            "[26,    20] loss: 0.405\n",
            "[26,    30] loss: 0.411\n",
            "[26,    40] loss: 0.386\n",
            "[27,    10] loss: 0.417\n",
            "[27,    20] loss: 0.397\n",
            "[27,    30] loss: 0.401\n",
            "[27,    40] loss: 0.365\n",
            "[28,    10] loss: 0.433\n",
            "[28,    20] loss: 0.421\n",
            "[28,    30] loss: 0.381\n",
            "[28,    40] loss: 0.407\n",
            "[29,    10] loss: 0.398\n",
            "[29,    20] loss: 0.392\n",
            "[29,    30] loss: 0.380\n",
            "[29,    40] loss: 0.349\n",
            "[30,    10] loss: 0.364\n",
            "[30,    20] loss: 0.391\n",
            "[30,    30] loss: 0.354\n",
            "[30,    40] loss: 0.397\n",
            "[31,    10] loss: 0.383\n",
            "[31,    20] loss: 0.379\n",
            "[31,    30] loss: 0.356\n",
            "[31,    40] loss: 0.418\n",
            "[32,    10] loss: 0.376\n",
            "[32,    20] loss: 0.326\n",
            "[32,    30] loss: 0.340\n",
            "[32,    40] loss: 0.300\n",
            "[33,    10] loss: 0.327\n",
            "[33,    20] loss: 0.334\n",
            "[33,    30] loss: 0.341\n",
            "[33,    40] loss: 0.296\n",
            "[34,    10] loss: 0.307\n",
            "[34,    20] loss: 0.324\n",
            "[34,    30] loss: 0.305\n",
            "[34,    40] loss: 0.330\n",
            "[35,    10] loss: 0.320\n",
            "[35,    20] loss: 0.323\n",
            "[35,    30] loss: 0.304\n",
            "[35,    40] loss: 0.331\n",
            "[36,    10] loss: 0.319\n",
            "[36,    20] loss: 0.295\n",
            "[36,    30] loss: 0.298\n",
            "[36,    40] loss: 0.305\n",
            "[37,    10] loss: 0.311\n",
            "[37,    20] loss: 0.283\n",
            "[37,    30] loss: 0.284\n",
            "[37,    40] loss: 0.271\n",
            "[38,    10] loss: 0.274\n",
            "[38,    20] loss: 0.259\n",
            "[38,    30] loss: 0.266\n",
            "[38,    40] loss: 0.244\n",
            "[39,    10] loss: 0.237\n",
            "[39,    20] loss: 0.232\n",
            "[39,    30] loss: 0.247\n",
            "[39,    40] loss: 0.258\n",
            "[40,    10] loss: 0.260\n",
            "[40,    20] loss: 0.250\n",
            "[40,    30] loss: 0.242\n",
            "[40,    40] loss: 0.231\n",
            "[41,    10] loss: 0.247\n",
            "[41,    20] loss: 0.258\n",
            "[41,    30] loss: 0.222\n",
            "[41,    40] loss: 0.198\n",
            "[42,    10] loss: 0.207\n",
            "[42,    20] loss: 0.198\n",
            "[42,    30] loss: 0.201\n",
            "[42,    40] loss: 0.192\n",
            "[43,    10] loss: 0.207\n",
            "[43,    20] loss: 0.187\n",
            "[43,    30] loss: 0.207\n",
            "[43,    40] loss: 0.219\n",
            "[44,    10] loss: 0.232\n",
            "[44,    20] loss: 0.250\n",
            "[44,    30] loss: 0.232\n",
            "[44,    40] loss: 0.202\n",
            "[45,    10] loss: 0.174\n",
            "[45,    20] loss: 0.204\n",
            "[45,    30] loss: 0.179\n",
            "[45,    40] loss: 0.156\n",
            "[46,    10] loss: 0.135\n",
            "[46,    20] loss: 0.168\n",
            "[46,    30] loss: 0.173\n",
            "[46,    40] loss: 0.161\n",
            "[47,    10] loss: 0.176\n",
            "[47,    20] loss: 0.153\n",
            "[47,    30] loss: 0.157\n",
            "[47,    40] loss: 0.153\n",
            "[48,    10] loss: 0.133\n",
            "[48,    20] loss: 0.144\n",
            "[48,    30] loss: 0.134\n",
            "[48,    40] loss: 0.135\n",
            "[49,    10] loss: 0.129\n",
            "[49,    20] loss: 0.118\n",
            "[49,    30] loss: 0.111\n",
            "[49,    40] loss: 0.145\n",
            "[50,    10] loss: 0.156\n",
            "[50,    20] loss: 0.161\n",
            "[50,    30] loss: 0.149\n",
            "[50,    40] loss: 0.134\n",
            "[51,    10] loss: 0.106\n",
            "[51,    20] loss: 0.114\n",
            "[51,    30] loss: 0.105\n",
            "[51,    40] loss: 0.109\n",
            "[52,    10] loss: 0.111\n",
            "[52,    20] loss: 0.115\n",
            "[52,    30] loss: 0.107\n",
            "[52,    40] loss: 0.111\n",
            "[53,    10] loss: 0.104\n",
            "[53,    20] loss: 0.123\n",
            "[53,    30] loss: 0.100\n",
            "[53,    40] loss: 0.084\n",
            "[54,    10] loss: 0.115\n",
            "[54,    20] loss: 0.114\n",
            "[54,    30] loss: 0.092\n",
            "[54,    40] loss: 0.103\n",
            "[55,    10] loss: 0.142\n",
            "[55,    20] loss: 0.108\n",
            "[55,    30] loss: 0.100\n",
            "[55,    40] loss: 0.075\n",
            "[56,    10] loss: 0.108\n",
            "[56,    20] loss: 0.098\n",
            "[56,    30] loss: 0.078\n",
            "[56,    40] loss: 0.069\n",
            "[57,    10] loss: 0.062\n",
            "[57,    20] loss: 0.055\n",
            "[57,    30] loss: 0.059\n",
            "[57,    40] loss: 0.053\n",
            "[58,    10] loss: 0.040\n",
            "[58,    20] loss: 0.047\n",
            "[58,    30] loss: 0.038\n",
            "[58,    40] loss: 0.043\n",
            "[59,    10] loss: 0.046\n",
            "[59,    20] loss: 0.031\n",
            "[59,    30] loss: 0.034\n",
            "[59,    40] loss: 0.030\n",
            "[60,    10] loss: 0.031\n",
            "[60,    20] loss: 0.029\n",
            "[60,    30] loss: 0.027\n",
            "[60,    40] loss: 0.024\n",
            "[61,    10] loss: 0.021\n",
            "[61,    20] loss: 0.026\n",
            "[61,    30] loss: 0.027\n",
            "[61,    40] loss: 0.038\n",
            "[62,    10] loss: 0.045\n",
            "[62,    20] loss: 0.043\n",
            "[62,    30] loss: 0.032\n",
            "[62,    40] loss: 0.027\n",
            "[63,    10] loss: 0.016\n",
            "[63,    20] loss: 0.014\n",
            "[63,    30] loss: 0.017\n",
            "[63,    40] loss: 0.022\n",
            "[64,    10] loss: 0.061\n",
            "[64,    20] loss: 0.078\n",
            "[64,    30] loss: 0.043\n",
            "[64,    40] loss: 0.036\n",
            "[65,    10] loss: 0.039\n",
            "[65,    20] loss: 0.030\n",
            "[65,    30] loss: 0.032\n",
            "[65,    40] loss: 0.032\n",
            "[66,    10] loss: 0.033\n",
            "[66,    20] loss: 0.021\n",
            "[66,    30] loss: 0.021\n",
            "[66,    40] loss: 0.022\n",
            "[67,    10] loss: 0.035\n",
            "[67,    20] loss: 0.027\n",
            "[67,    30] loss: 0.015\n",
            "[67,    40] loss: 0.029\n",
            "[68,    10] loss: 0.088\n",
            "[68,    20] loss: 0.109\n",
            "[68,    30] loss: 0.079\n",
            "[68,    40] loss: 0.089\n",
            "[69,    10] loss: 0.159\n",
            "[69,    20] loss: 0.147\n",
            "[69,    30] loss: 0.104\n",
            "[69,    40] loss: 0.079\n",
            "[70,    10] loss: 0.041\n",
            "[70,    20] loss: 0.031\n",
            "[70,    30] loss: 0.029\n",
            "[70,    40] loss: 0.029\n",
            "[71,    10] loss: 0.061\n",
            "[71,    20] loss: 0.037\n",
            "[71,    30] loss: 0.027\n",
            "[71,    40] loss: 0.023\n",
            "[72,    10] loss: 0.019\n",
            "[72,    20] loss: 0.015\n",
            "[72,    30] loss: 0.014\n",
            "[72,    40] loss: 0.009\n",
            "[73,    10] loss: 0.007\n",
            "[73,    20] loss: 0.007\n",
            "[73,    30] loss: 0.010\n",
            "[73,    40] loss: 0.008\n",
            "[74,    10] loss: 0.011\n",
            "[74,    20] loss: 0.011\n",
            "[74,    30] loss: 0.006\n",
            "[74,    40] loss: 0.004\n",
            "[75,    10] loss: 0.003\n",
            "[75,    20] loss: 0.003\n",
            "[75,    30] loss: 0.005\n",
            "[75,    40] loss: 0.004\n",
            "[76,    10] loss: 0.002\n",
            "[76,    20] loss: 0.002\n",
            "[76,    30] loss: 0.002\n",
            "[76,    40] loss: 0.005\n",
            "[77,    10] loss: 0.005\n",
            "[77,    20] loss: 0.004\n",
            "[77,    30] loss: 0.002\n",
            "[77,    40] loss: 0.002\n",
            "[78,    10] loss: 0.002\n",
            "[78,    20] loss: 0.002\n",
            "[78,    30] loss: 0.003\n",
            "[78,    40] loss: 0.002\n",
            "[79,    10] loss: 0.001\n",
            "[79,    20] loss: 0.002\n",
            "[79,    30] loss: 0.002\n",
            "[79,    40] loss: 0.003\n",
            "[80,    10] loss: 0.001\n",
            "[80,    20] loss: 0.001\n",
            "[80,    30] loss: 0.003\n",
            "[80,    40] loss: 0.001\n",
            "[81,    10] loss: 0.001\n",
            "[81,    20] loss: 0.001\n",
            "[81,    30] loss: 0.003\n",
            "[81,    40] loss: 0.001\n",
            "[82,    10] loss: 0.003\n",
            "[82,    20] loss: 0.001\n",
            "[82,    30] loss: 0.001\n",
            "[82,    40] loss: 0.001\n",
            "[83,    10] loss: 0.001\n",
            "[83,    20] loss: 0.002\n",
            "[83,    30] loss: 0.001\n",
            "[83,    40] loss: 0.001\n",
            "[84,    10] loss: 0.001\n",
            "[84,    20] loss: 0.001\n",
            "[84,    30] loss: 0.002\n",
            "[84,    40] loss: 0.001\n",
            "[85,    10] loss: 0.001\n",
            "[85,    20] loss: 0.001\n",
            "[85,    30] loss: 0.001\n",
            "[85,    40] loss: 0.002\n",
            "[86,    10] loss: 0.001\n",
            "[86,    20] loss: 0.002\n",
            "[86,    30] loss: 0.001\n",
            "[86,    40] loss: 0.001\n",
            "[87,    10] loss: 0.002\n",
            "[87,    20] loss: 0.001\n",
            "[87,    30] loss: 0.001\n",
            "[87,    40] loss: 0.001\n",
            "[88,    10] loss: 0.001\n",
            "[88,    20] loss: 0.001\n",
            "[88,    30] loss: 0.002\n",
            "[88,    40] loss: 0.001\n",
            "[89,    10] loss: 0.001\n",
            "[89,    20] loss: 0.001\n",
            "[89,    30] loss: 0.001\n",
            "[89,    40] loss: 0.001\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 train images: 100 %\n",
            "Accuracy of the network on the 10000 test dataset 1: 38 %\n",
            "Accuracy of the network on the 10000 test dataset 2: 52 %\n",
            "Accuracy of the network on the 10000 test dataset 3: 76 %\n",
            "Accuracy of the network on the 10000 test dataset 4: 95 %\n",
            "Accuracy of the network on the 10000 test dataset 5: 100 %\n",
            "Accuracy of the network on the 10000 test dataset 6: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 7: 97 %\n",
            "Accuracy of the network on the 10000 test dataset 8: 94 %\n",
            "Accuracy of the network on the 10000 test dataset 9: 90 %\n",
            "Accuracy of the network on the 10000 test dataset 10: 84 %\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "training on data set   6\n",
            "[1,    10] loss: 1.100\n",
            "[1,    20] loss: 1.098\n",
            "[1,    30] loss: 1.099\n",
            "[1,    40] loss: 1.098\n",
            "[2,    10] loss: 1.098\n",
            "[2,    20] loss: 1.097\n",
            "[2,    30] loss: 1.098\n",
            "[2,    40] loss: 1.097\n",
            "[3,    10] loss: 1.096\n",
            "[3,    20] loss: 1.096\n",
            "[3,    30] loss: 1.096\n",
            "[3,    40] loss: 1.095\n",
            "[4,    10] loss: 1.094\n",
            "[4,    20] loss: 1.094\n",
            "[4,    30] loss: 1.093\n",
            "[4,    40] loss: 1.091\n",
            "[5,    10] loss: 1.090\n",
            "[5,    20] loss: 1.090\n",
            "[5,    30] loss: 1.087\n",
            "[5,    40] loss: 1.083\n",
            "[6,    10] loss: 1.080\n",
            "[6,    20] loss: 1.076\n",
            "[6,    30] loss: 1.068\n",
            "[6,    40] loss: 1.059\n",
            "[7,    10] loss: 1.047\n",
            "[7,    20] loss: 1.029\n",
            "[7,    30] loss: 0.991\n",
            "[7,    40] loss: 0.946\n",
            "[8,    10] loss: 0.911\n",
            "[8,    20] loss: 0.917\n",
            "[8,    30] loss: 0.888\n",
            "[8,    40] loss: 0.867\n",
            "[9,    10] loss: 0.856\n",
            "[9,    20] loss: 0.837\n",
            "[9,    30] loss: 0.816\n",
            "[9,    40] loss: 0.772\n",
            "[10,    10] loss: 0.775\n",
            "[10,    20] loss: 0.697\n",
            "[10,    30] loss: 0.701\n",
            "[10,    40] loss: 0.706\n",
            "[11,    10] loss: 0.655\n",
            "[11,    20] loss: 0.644\n",
            "[11,    30] loss: 0.650\n",
            "[11,    40] loss: 0.620\n",
            "[12,    10] loss: 0.594\n",
            "[12,    20] loss: 0.581\n",
            "[12,    30] loss: 0.550\n",
            "[12,    40] loss: 0.568\n",
            "[13,    10] loss: 0.566\n",
            "[13,    20] loss: 0.559\n",
            "[13,    30] loss: 0.522\n",
            "[13,    40] loss: 0.510\n",
            "[14,    10] loss: 0.557\n",
            "[14,    20] loss: 0.520\n",
            "[14,    30] loss: 0.496\n",
            "[14,    40] loss: 0.520\n",
            "[15,    10] loss: 0.645\n",
            "[15,    20] loss: 0.565\n",
            "[15,    30] loss: 0.535\n",
            "[15,    40] loss: 0.528\n",
            "[16,    10] loss: 0.515\n",
            "[16,    20] loss: 0.462\n",
            "[16,    30] loss: 0.475\n",
            "[16,    40] loss: 0.472\n",
            "[17,    10] loss: 0.494\n",
            "[17,    20] loss: 0.431\n",
            "[17,    30] loss: 0.476\n",
            "[17,    40] loss: 0.437\n",
            "[18,    10] loss: 0.458\n",
            "[18,    20] loss: 0.457\n",
            "[18,    30] loss: 0.444\n",
            "[18,    40] loss: 0.426\n",
            "[19,    10] loss: 0.451\n",
            "[19,    20] loss: 0.439\n",
            "[19,    30] loss: 0.424\n",
            "[19,    40] loss: 0.434\n",
            "[20,    10] loss: 0.419\n",
            "[20,    20] loss: 0.441\n",
            "[20,    30] loss: 0.423\n",
            "[20,    40] loss: 0.407\n",
            "[21,    10] loss: 0.380\n",
            "[21,    20] loss: 0.387\n",
            "[21,    30] loss: 0.396\n",
            "[21,    40] loss: 0.445\n",
            "[22,    10] loss: 0.398\n",
            "[22,    20] loss: 0.393\n",
            "[22,    30] loss: 0.400\n",
            "[22,    40] loss: 0.389\n",
            "[23,    10] loss: 0.394\n",
            "[23,    20] loss: 0.361\n",
            "[23,    30] loss: 0.376\n",
            "[23,    40] loss: 0.343\n",
            "[24,    10] loss: 0.342\n",
            "[24,    20] loss: 0.357\n",
            "[24,    30] loss: 0.343\n",
            "[24,    40] loss: 0.346\n",
            "[25,    10] loss: 0.326\n",
            "[25,    20] loss: 0.366\n",
            "[25,    30] loss: 0.315\n",
            "[25,    40] loss: 0.317\n",
            "[26,    10] loss: 0.310\n",
            "[26,    20] loss: 0.307\n",
            "[26,    30] loss: 0.344\n",
            "[26,    40] loss: 0.297\n",
            "[27,    10] loss: 0.315\n",
            "[27,    20] loss: 0.321\n",
            "[27,    30] loss: 0.295\n",
            "[27,    40] loss: 0.313\n",
            "[28,    10] loss: 0.301\n",
            "[28,    20] loss: 0.313\n",
            "[28,    30] loss: 0.271\n",
            "[28,    40] loss: 0.277\n",
            "[29,    10] loss: 0.317\n",
            "[29,    20] loss: 0.276\n",
            "[29,    30] loss: 0.267\n",
            "[29,    40] loss: 0.295\n",
            "[30,    10] loss: 0.280\n",
            "[30,    20] loss: 0.261\n",
            "[30,    30] loss: 0.270\n",
            "[30,    40] loss: 0.228\n",
            "[31,    10] loss: 0.249\n",
            "[31,    20] loss: 0.249\n",
            "[31,    30] loss: 0.250\n",
            "[31,    40] loss: 0.230\n",
            "[32,    10] loss: 0.223\n",
            "[32,    20] loss: 0.216\n",
            "[32,    30] loss: 0.227\n",
            "[32,    40] loss: 0.246\n",
            "[33,    10] loss: 0.350\n",
            "[33,    20] loss: 0.285\n",
            "[33,    30] loss: 0.305\n",
            "[33,    40] loss: 0.231\n",
            "[34,    10] loss: 0.217\n",
            "[34,    20] loss: 0.202\n",
            "[34,    30] loss: 0.215\n",
            "[34,    40] loss: 0.202\n",
            "[35,    10] loss: 0.203\n",
            "[35,    20] loss: 0.176\n",
            "[35,    30] loss: 0.203\n",
            "[35,    40] loss: 0.166\n",
            "[36,    10] loss: 0.190\n",
            "[36,    20] loss: 0.199\n",
            "[36,    30] loss: 0.206\n",
            "[36,    40] loss: 0.163\n",
            "[37,    10] loss: 0.151\n",
            "[37,    20] loss: 0.159\n",
            "[37,    30] loss: 0.159\n",
            "[37,    40] loss: 0.178\n",
            "[38,    10] loss: 0.157\n",
            "[38,    20] loss: 0.158\n",
            "[38,    30] loss: 0.153\n",
            "[38,    40] loss: 0.148\n",
            "[39,    10] loss: 0.206\n",
            "[39,    20] loss: 0.220\n",
            "[39,    30] loss: 0.183\n",
            "[39,    40] loss: 0.139\n",
            "[40,    10] loss: 0.134\n",
            "[40,    20] loss: 0.126\n",
            "[40,    30] loss: 0.139\n",
            "[40,    40] loss: 0.123\n",
            "[41,    10] loss: 0.111\n",
            "[41,    20] loss: 0.104\n",
            "[41,    30] loss: 0.103\n",
            "[41,    40] loss: 0.098\n",
            "[42,    10] loss: 0.083\n",
            "[42,    20] loss: 0.104\n",
            "[42,    30] loss: 0.095\n",
            "[42,    40] loss: 0.101\n",
            "[43,    10] loss: 0.132\n",
            "[43,    20] loss: 0.104\n",
            "[43,    30] loss: 0.105\n",
            "[43,    40] loss: 0.091\n",
            "[44,    10] loss: 0.132\n",
            "[44,    20] loss: 0.093\n",
            "[44,    30] loss: 0.090\n",
            "[44,    40] loss: 0.076\n",
            "[45,    10] loss: 0.076\n",
            "[45,    20] loss: 0.058\n",
            "[45,    30] loss: 0.073\n",
            "[45,    40] loss: 0.062\n",
            "[46,    10] loss: 0.053\n",
            "[46,    20] loss: 0.053\n",
            "[46,    30] loss: 0.051\n",
            "[46,    40] loss: 0.063\n",
            "[47,    10] loss: 0.071\n",
            "[47,    20] loss: 0.101\n",
            "[47,    30] loss: 0.120\n",
            "[47,    40] loss: 0.092\n",
            "[48,    10] loss: 0.066\n",
            "[48,    20] loss: 0.058\n",
            "[48,    30] loss: 0.052\n",
            "[48,    40] loss: 0.052\n",
            "[49,    10] loss: 0.049\n",
            "[49,    20] loss: 0.043\n",
            "[49,    30] loss: 0.042\n",
            "[49,    40] loss: 0.043\n",
            "[50,    10] loss: 0.035\n",
            "[50,    20] loss: 0.038\n",
            "[50,    30] loss: 0.034\n",
            "[50,    40] loss: 0.029\n",
            "[51,    10] loss: 0.045\n",
            "[51,    20] loss: 0.036\n",
            "[51,    30] loss: 0.036\n",
            "[51,    40] loss: 0.034\n",
            "[52,    10] loss: 0.030\n",
            "[52,    20] loss: 0.024\n",
            "[52,    30] loss: 0.018\n",
            "[52,    40] loss: 0.028\n",
            "[53,    10] loss: 0.016\n",
            "[53,    20] loss: 0.025\n",
            "[53,    30] loss: 0.026\n",
            "[53,    40] loss: 0.018\n",
            "[54,    10] loss: 0.022\n",
            "[54,    20] loss: 0.013\n",
            "[54,    30] loss: 0.013\n",
            "[54,    40] loss: 0.009\n",
            "[55,    10] loss: 0.008\n",
            "[55,    20] loss: 0.007\n",
            "[55,    30] loss: 0.008\n",
            "[55,    40] loss: 0.010\n",
            "[56,    10] loss: 0.006\n",
            "[56,    20] loss: 0.006\n",
            "[56,    30] loss: 0.006\n",
            "[56,    40] loss: 0.012\n",
            "[57,    10] loss: 0.009\n",
            "[57,    20] loss: 0.008\n",
            "[57,    30] loss: 0.009\n",
            "[57,    40] loss: 0.005\n",
            "[58,    10] loss: 0.004\n",
            "[58,    20] loss: 0.004\n",
            "[58,    30] loss: 0.005\n",
            "[58,    40] loss: 0.004\n",
            "[59,    10] loss: 0.003\n",
            "[59,    20] loss: 0.003\n",
            "[59,    30] loss: 0.002\n",
            "[59,    40] loss: 0.003\n",
            "[60,    10] loss: 0.002\n",
            "[60,    20] loss: 0.002\n",
            "[60,    30] loss: 0.002\n",
            "[60,    40] loss: 0.002\n",
            "[61,    10] loss: 0.002\n",
            "[61,    20] loss: 0.002\n",
            "[61,    30] loss: 0.002\n",
            "[61,    40] loss: 0.002\n",
            "[62,    10] loss: 0.002\n",
            "[62,    20] loss: 0.002\n",
            "[62,    30] loss: 0.001\n",
            "[62,    40] loss: 0.002\n",
            "[63,    10] loss: 0.002\n",
            "[63,    20] loss: 0.002\n",
            "[63,    30] loss: 0.001\n",
            "[63,    40] loss: 0.004\n",
            "[64,    10] loss: 0.003\n",
            "[64,    20] loss: 0.002\n",
            "[64,    30] loss: 0.003\n",
            "[64,    40] loss: 0.003\n",
            "[65,    10] loss: 0.002\n",
            "[65,    20] loss: 0.002\n",
            "[65,    30] loss: 0.001\n",
            "[65,    40] loss: 0.002\n",
            "[66,    10] loss: 0.001\n",
            "[66,    20] loss: 0.001\n",
            "[66,    30] loss: 0.001\n",
            "[66,    40] loss: 0.001\n",
            "[67,    10] loss: 0.001\n",
            "[67,    20] loss: 0.001\n",
            "[67,    30] loss: 0.001\n",
            "[67,    40] loss: 0.001\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 train images: 100 %\n",
            "Accuracy of the network on the 10000 test dataset 1: 37 %\n",
            "Accuracy of the network on the 10000 test dataset 2: 49 %\n",
            "Accuracy of the network on the 10000 test dataset 3: 69 %\n",
            "Accuracy of the network on the 10000 test dataset 4: 87 %\n",
            "Accuracy of the network on the 10000 test dataset 5: 98 %\n",
            "Accuracy of the network on the 10000 test dataset 6: 100 %\n",
            "Accuracy of the network on the 10000 test dataset 7: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 8: 98 %\n",
            "Accuracy of the network on the 10000 test dataset 9: 96 %\n",
            "Accuracy of the network on the 10000 test dataset 10: 87 %\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "training on data set   7\n",
            "[1,    10] loss: 1.100\n",
            "[1,    20] loss: 1.098\n",
            "[1,    30] loss: 1.098\n",
            "[1,    40] loss: 1.099\n",
            "[2,    10] loss: 1.098\n",
            "[2,    20] loss: 1.098\n",
            "[2,    30] loss: 1.097\n",
            "[2,    40] loss: 1.097\n",
            "[3,    10] loss: 1.097\n",
            "[3,    20] loss: 1.096\n",
            "[3,    30] loss: 1.096\n",
            "[3,    40] loss: 1.094\n",
            "[4,    10] loss: 1.094\n",
            "[4,    20] loss: 1.092\n",
            "[4,    30] loss: 1.090\n",
            "[4,    40] loss: 1.087\n",
            "[5,    10] loss: 1.085\n",
            "[5,    20] loss: 1.078\n",
            "[5,    30] loss: 1.066\n",
            "[5,    40] loss: 1.063\n",
            "[6,    10] loss: 1.024\n",
            "[6,    20] loss: 1.002\n",
            "[6,    30] loss: 0.983\n",
            "[6,    40] loss: 0.949\n",
            "[7,    10] loss: 0.936\n",
            "[7,    20] loss: 0.922\n",
            "[7,    30] loss: 0.922\n",
            "[7,    40] loss: 0.926\n",
            "[8,    10] loss: 0.880\n",
            "[8,    20] loss: 0.877\n",
            "[8,    30] loss: 0.857\n",
            "[8,    40] loss: 0.813\n",
            "[9,    10] loss: 0.822\n",
            "[9,    20] loss: 0.778\n",
            "[9,    30] loss: 0.729\n",
            "[9,    40] loss: 0.679\n",
            "[10,    10] loss: 0.689\n",
            "[10,    20] loss: 0.671\n",
            "[10,    30] loss: 0.628\n",
            "[10,    40] loss: 0.563\n",
            "[11,    10] loss: 0.633\n",
            "[11,    20] loss: 0.580\n",
            "[11,    30] loss: 0.585\n",
            "[11,    40] loss: 0.546\n",
            "[12,    10] loss: 0.559\n",
            "[12,    20] loss: 0.506\n",
            "[12,    30] loss: 0.531\n",
            "[12,    40] loss: 0.504\n",
            "[13,    10] loss: 0.529\n",
            "[13,    20] loss: 0.487\n",
            "[13,    30] loss: 0.497\n",
            "[13,    40] loss: 0.485\n",
            "[14,    10] loss: 0.457\n",
            "[14,    20] loss: 0.488\n",
            "[14,    30] loss: 0.464\n",
            "[14,    40] loss: 0.440\n",
            "[15,    10] loss: 0.454\n",
            "[15,    20] loss: 0.438\n",
            "[15,    30] loss: 0.428\n",
            "[15,    40] loss: 0.458\n",
            "[16,    10] loss: 0.462\n",
            "[16,    20] loss: 0.446\n",
            "[16,    30] loss: 0.431\n",
            "[16,    40] loss: 0.378\n",
            "[17,    10] loss: 0.483\n",
            "[17,    20] loss: 0.448\n",
            "[17,    30] loss: 0.399\n",
            "[17,    40] loss: 0.419\n",
            "[18,    10] loss: 0.460\n",
            "[18,    20] loss: 0.425\n",
            "[18,    30] loss: 0.421\n",
            "[18,    40] loss: 0.393\n",
            "[19,    10] loss: 0.404\n",
            "[19,    20] loss: 0.367\n",
            "[19,    30] loss: 0.394\n",
            "[19,    40] loss: 0.371\n",
            "[20,    10] loss: 0.393\n",
            "[20,    20] loss: 0.369\n",
            "[20,    30] loss: 0.368\n",
            "[20,    40] loss: 0.335\n",
            "[21,    10] loss: 0.364\n",
            "[21,    20] loss: 0.376\n",
            "[21,    30] loss: 0.346\n",
            "[21,    40] loss: 0.353\n",
            "[22,    10] loss: 0.474\n",
            "[22,    20] loss: 0.412\n",
            "[22,    30] loss: 0.376\n",
            "[22,    40] loss: 0.361\n",
            "[23,    10] loss: 0.388\n",
            "[23,    20] loss: 0.387\n",
            "[23,    30] loss: 0.339\n",
            "[23,    40] loss: 0.339\n",
            "[24,    10] loss: 0.318\n",
            "[24,    20] loss: 0.314\n",
            "[24,    30] loss: 0.334\n",
            "[24,    40] loss: 0.320\n",
            "[25,    10] loss: 0.306\n",
            "[25,    20] loss: 0.304\n",
            "[25,    30] loss: 0.308\n",
            "[25,    40] loss: 0.317\n",
            "[26,    10] loss: 0.302\n",
            "[26,    20] loss: 0.345\n",
            "[26,    30] loss: 0.294\n",
            "[26,    40] loss: 0.300\n",
            "[27,    10] loss: 0.279\n",
            "[27,    20] loss: 0.302\n",
            "[27,    30] loss: 0.282\n",
            "[27,    40] loss: 0.268\n",
            "[28,    10] loss: 0.245\n",
            "[28,    20] loss: 0.258\n",
            "[28,    30] loss: 0.257\n",
            "[28,    40] loss: 0.231\n",
            "[29,    10] loss: 0.236\n",
            "[29,    20] loss: 0.255\n",
            "[29,    30] loss: 0.239\n",
            "[29,    40] loss: 0.234\n",
            "[30,    10] loss: 0.241\n",
            "[30,    20] loss: 0.222\n",
            "[30,    30] loss: 0.208\n",
            "[30,    40] loss: 0.245\n",
            "[31,    10] loss: 0.256\n",
            "[31,    20] loss: 0.248\n",
            "[31,    30] loss: 0.222\n",
            "[31,    40] loss: 0.228\n",
            "[32,    10] loss: 0.194\n",
            "[32,    20] loss: 0.218\n",
            "[32,    30] loss: 0.209\n",
            "[32,    40] loss: 0.222\n",
            "[33,    10] loss: 0.185\n",
            "[33,    20] loss: 0.181\n",
            "[33,    30] loss: 0.204\n",
            "[33,    40] loss: 0.180\n",
            "[34,    10] loss: 0.256\n",
            "[34,    20] loss: 0.246\n",
            "[34,    30] loss: 0.224\n",
            "[34,    40] loss: 0.213\n",
            "[35,    10] loss: 0.248\n",
            "[35,    20] loss: 0.178\n",
            "[35,    30] loss: 0.173\n",
            "[35,    40] loss: 0.154\n",
            "[36,    10] loss: 0.162\n",
            "[36,    20] loss: 0.155\n",
            "[36,    30] loss: 0.151\n",
            "[36,    40] loss: 0.154\n",
            "[37,    10] loss: 0.115\n",
            "[37,    20] loss: 0.139\n",
            "[37,    30] loss: 0.125\n",
            "[37,    40] loss: 0.134\n",
            "[38,    10] loss: 0.267\n",
            "[38,    20] loss: 0.230\n",
            "[38,    30] loss: 0.204\n",
            "[38,    40] loss: 0.191\n",
            "[39,    10] loss: 0.283\n",
            "[39,    20] loss: 0.175\n",
            "[39,    30] loss: 0.163\n",
            "[39,    40] loss: 0.132\n",
            "[40,    10] loss: 0.109\n",
            "[40,    20] loss: 0.111\n",
            "[40,    30] loss: 0.121\n",
            "[40,    40] loss: 0.135\n",
            "[41,    10] loss: 0.122\n",
            "[41,    20] loss: 0.107\n",
            "[41,    30] loss: 0.102\n",
            "[41,    40] loss: 0.104\n",
            "[42,    10] loss: 0.089\n",
            "[42,    20] loss: 0.074\n",
            "[42,    30] loss: 0.085\n",
            "[42,    40] loss: 0.094\n",
            "[43,    10] loss: 0.103\n",
            "[43,    20] loss: 0.084\n",
            "[43,    30] loss: 0.075\n",
            "[43,    40] loss: 0.070\n",
            "[44,    10] loss: 0.063\n",
            "[44,    20] loss: 0.063\n",
            "[44,    30] loss: 0.063\n",
            "[44,    40] loss: 0.063\n",
            "[45,    10] loss: 0.098\n",
            "[45,    20] loss: 0.065\n",
            "[45,    30] loss: 0.075\n",
            "[45,    40] loss: 0.080\n",
            "[46,    10] loss: 0.125\n",
            "[46,    20] loss: 0.123\n",
            "[46,    30] loss: 0.086\n",
            "[46,    40] loss: 0.057\n",
            "[47,    10] loss: 0.052\n",
            "[47,    20] loss: 0.061\n",
            "[47,    30] loss: 0.051\n",
            "[47,    40] loss: 0.042\n",
            "[48,    10] loss: 0.047\n",
            "[48,    20] loss: 0.049\n",
            "[48,    30] loss: 0.043\n",
            "[48,    40] loss: 0.036\n",
            "[49,    10] loss: 0.032\n",
            "[49,    20] loss: 0.033\n",
            "[49,    30] loss: 0.033\n",
            "[49,    40] loss: 0.033\n",
            "[50,    10] loss: 0.046\n",
            "[50,    20] loss: 0.056\n",
            "[50,    30] loss: 0.033\n",
            "[50,    40] loss: 0.041\n",
            "[51,    10] loss: 0.027\n",
            "[51,    20] loss: 0.021\n",
            "[51,    30] loss: 0.020\n",
            "[51,    40] loss: 0.020\n",
            "[52,    10] loss: 0.023\n",
            "[52,    20] loss: 0.017\n",
            "[52,    30] loss: 0.018\n",
            "[52,    40] loss: 0.019\n",
            "[53,    10] loss: 0.016\n",
            "[53,    20] loss: 0.012\n",
            "[53,    30] loss: 0.014\n",
            "[53,    40] loss: 0.013\n",
            "[54,    10] loss: 0.013\n",
            "[54,    20] loss: 0.007\n",
            "[54,    30] loss: 0.007\n",
            "[54,    40] loss: 0.011\n",
            "[55,    10] loss: 0.010\n",
            "[55,    20] loss: 0.009\n",
            "[55,    30] loss: 0.011\n",
            "[55,    40] loss: 0.008\n",
            "[56,    10] loss: 0.005\n",
            "[56,    20] loss: 0.007\n",
            "[56,    30] loss: 0.007\n",
            "[56,    40] loss: 0.007\n",
            "[57,    10] loss: 0.006\n",
            "[57,    20] loss: 0.004\n",
            "[57,    30] loss: 0.006\n",
            "[57,    40] loss: 0.007\n",
            "[58,    10] loss: 0.011\n",
            "[58,    20] loss: 0.011\n",
            "[58,    30] loss: 0.013\n",
            "[58,    40] loss: 0.010\n",
            "[59,    10] loss: 0.008\n",
            "[59,    20] loss: 0.008\n",
            "[59,    30] loss: 0.008\n",
            "[59,    40] loss: 0.005\n",
            "[60,    10] loss: 0.004\n",
            "[60,    20] loss: 0.003\n",
            "[60,    30] loss: 0.004\n",
            "[60,    40] loss: 0.004\n",
            "[61,    10] loss: 0.002\n",
            "[61,    20] loss: 0.003\n",
            "[61,    30] loss: 0.003\n",
            "[61,    40] loss: 0.002\n",
            "[62,    10] loss: 0.002\n",
            "[62,    20] loss: 0.002\n",
            "[62,    30] loss: 0.002\n",
            "[62,    40] loss: 0.002\n",
            "[63,    10] loss: 0.002\n",
            "[63,    20] loss: 0.002\n",
            "[63,    30] loss: 0.002\n",
            "[63,    40] loss: 0.002\n",
            "[64,    10] loss: 0.001\n",
            "[64,    20] loss: 0.001\n",
            "[64,    30] loss: 0.001\n",
            "[64,    40] loss: 0.001\n",
            "[65,    10] loss: 0.001\n",
            "[65,    20] loss: 0.001\n",
            "[65,    30] loss: 0.001\n",
            "[65,    40] loss: 0.001\n",
            "[66,    10] loss: 0.001\n",
            "[66,    20] loss: 0.001\n",
            "[66,    30] loss: 0.001\n",
            "[66,    40] loss: 0.001\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 train images: 100 %\n",
            "Accuracy of the network on the 10000 test dataset 1: 36 %\n",
            "Accuracy of the network on the 10000 test dataset 2: 43 %\n",
            "Accuracy of the network on the 10000 test dataset 3: 58 %\n",
            "Accuracy of the network on the 10000 test dataset 4: 77 %\n",
            "Accuracy of the network on the 10000 test dataset 5: 91 %\n",
            "Accuracy of the network on the 10000 test dataset 6: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 7: 100 %\n",
            "Accuracy of the network on the 10000 test dataset 8: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 9: 98 %\n",
            "Accuracy of the network on the 10000 test dataset 10: 91 %\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "training on data set   8\n",
            "[1,    10] loss: 1.099\n",
            "[1,    20] loss: 1.100\n",
            "[1,    30] loss: 1.099\n",
            "[1,    40] loss: 1.098\n",
            "[2,    10] loss: 1.098\n",
            "[2,    20] loss: 1.097\n",
            "[2,    30] loss: 1.098\n",
            "[2,    40] loss: 1.098\n",
            "[3,    10] loss: 1.096\n",
            "[3,    20] loss: 1.096\n",
            "[3,    30] loss: 1.095\n",
            "[3,    40] loss: 1.095\n",
            "[4,    10] loss: 1.094\n",
            "[4,    20] loss: 1.093\n",
            "[4,    30] loss: 1.092\n",
            "[4,    40] loss: 1.090\n",
            "[5,    10] loss: 1.087\n",
            "[5,    20] loss: 1.083\n",
            "[5,    30] loss: 1.076\n",
            "[5,    40] loss: 1.072\n",
            "[6,    10] loss: 1.057\n",
            "[6,    20] loss: 1.037\n",
            "[6,    30] loss: 1.009\n",
            "[6,    40] loss: 0.989\n",
            "[7,    10] loss: 0.941\n",
            "[7,    20] loss: 0.912\n",
            "[7,    30] loss: 0.889\n",
            "[7,    40] loss: 0.871\n",
            "[8,    10] loss: 0.858\n",
            "[8,    20] loss: 0.792\n",
            "[8,    30] loss: 0.766\n",
            "[8,    40] loss: 0.732\n",
            "[9,    10] loss: 0.726\n",
            "[9,    20] loss: 0.688\n",
            "[9,    30] loss: 0.640\n",
            "[9,    40] loss: 0.638\n",
            "[10,    10] loss: 0.599\n",
            "[10,    20] loss: 0.581\n",
            "[10,    30] loss: 0.560\n",
            "[10,    40] loss: 0.598\n",
            "[11,    10] loss: 0.583\n",
            "[11,    20] loss: 0.555\n",
            "[11,    30] loss: 0.566\n",
            "[11,    40] loss: 0.507\n",
            "[12,    10] loss: 0.605\n",
            "[12,    20] loss: 0.517\n",
            "[12,    30] loss: 0.533\n",
            "[12,    40] loss: 0.525\n",
            "[13,    10] loss: 0.521\n",
            "[13,    20] loss: 0.510\n",
            "[13,    30] loss: 0.484\n",
            "[13,    40] loss: 0.485\n",
            "[14,    10] loss: 0.476\n",
            "[14,    20] loss: 0.456\n",
            "[14,    30] loss: 0.460\n",
            "[14,    40] loss: 0.485\n",
            "[15,    10] loss: 0.510\n",
            "[15,    20] loss: 0.481\n",
            "[15,    30] loss: 0.461\n",
            "[15,    40] loss: 0.439\n",
            "[16,    10] loss: 0.452\n",
            "[16,    20] loss: 0.419\n",
            "[16,    30] loss: 0.465\n",
            "[16,    40] loss: 0.416\n",
            "[17,    10] loss: 0.468\n",
            "[17,    20] loss: 0.458\n",
            "[17,    30] loss: 0.423\n",
            "[17,    40] loss: 0.454\n",
            "[18,    10] loss: 0.411\n",
            "[18,    20] loss: 0.429\n",
            "[18,    30] loss: 0.393\n",
            "[18,    40] loss: 0.434\n",
            "[19,    10] loss: 0.455\n",
            "[19,    20] loss: 0.416\n",
            "[19,    30] loss: 0.416\n",
            "[19,    40] loss: 0.385\n",
            "[20,    10] loss: 0.376\n",
            "[20,    20] loss: 0.357\n",
            "[20,    30] loss: 0.376\n",
            "[20,    40] loss: 0.400\n",
            "[21,    10] loss: 0.366\n",
            "[21,    20] loss: 0.365\n",
            "[21,    30] loss: 0.363\n",
            "[21,    40] loss: 0.395\n",
            "[22,    10] loss: 0.366\n",
            "[22,    20] loss: 0.335\n",
            "[22,    30] loss: 0.332\n",
            "[22,    40] loss: 0.380\n",
            "[23,    10] loss: 0.397\n",
            "[23,    20] loss: 0.345\n",
            "[23,    30] loss: 0.354\n",
            "[23,    40] loss: 0.337\n",
            "[24,    10] loss: 0.400\n",
            "[24,    20] loss: 0.337\n",
            "[24,    30] loss: 0.334\n",
            "[24,    40] loss: 0.313\n",
            "[25,    10] loss: 0.360\n",
            "[25,    20] loss: 0.310\n",
            "[25,    30] loss: 0.307\n",
            "[25,    40] loss: 0.285\n",
            "[26,    10] loss: 0.288\n",
            "[26,    20] loss: 0.267\n",
            "[26,    30] loss: 0.316\n",
            "[26,    40] loss: 0.285\n",
            "[27,    10] loss: 0.268\n",
            "[27,    20] loss: 0.272\n",
            "[27,    30] loss: 0.256\n",
            "[27,    40] loss: 0.282\n",
            "[28,    10] loss: 0.335\n",
            "[28,    20] loss: 0.298\n",
            "[28,    30] loss: 0.285\n",
            "[28,    40] loss: 0.257\n",
            "[29,    10] loss: 0.254\n",
            "[29,    20] loss: 0.225\n",
            "[29,    30] loss: 0.249\n",
            "[29,    40] loss: 0.272\n",
            "[30,    10] loss: 0.241\n",
            "[30,    20] loss: 0.237\n",
            "[30,    30] loss: 0.227\n",
            "[30,    40] loss: 0.230\n",
            "[31,    10] loss: 0.273\n",
            "[31,    20] loss: 0.218\n",
            "[31,    30] loss: 0.243\n",
            "[31,    40] loss: 0.206\n",
            "[32,    10] loss: 0.219\n",
            "[32,    20] loss: 0.192\n",
            "[32,    30] loss: 0.206\n",
            "[32,    40] loss: 0.202\n",
            "[33,    10] loss: 0.298\n",
            "[33,    20] loss: 0.236\n",
            "[33,    30] loss: 0.208\n",
            "[33,    40] loss: 0.185\n",
            "[34,    10] loss: 0.164\n",
            "[34,    20] loss: 0.176\n",
            "[34,    30] loss: 0.166\n",
            "[34,    40] loss: 0.158\n",
            "[35,    10] loss: 0.185\n",
            "[35,    20] loss: 0.183\n",
            "[35,    30] loss: 0.149\n",
            "[35,    40] loss: 0.170\n",
            "[36,    10] loss: 0.187\n",
            "[36,    20] loss: 0.185\n",
            "[36,    30] loss: 0.163\n",
            "[36,    40] loss: 0.155\n",
            "[37,    10] loss: 0.155\n",
            "[37,    20] loss: 0.123\n",
            "[37,    30] loss: 0.134\n",
            "[37,    40] loss: 0.139\n",
            "[38,    10] loss: 0.138\n",
            "[38,    20] loss: 0.150\n",
            "[38,    30] loss: 0.136\n",
            "[38,    40] loss: 0.120\n",
            "[39,    10] loss: 0.118\n",
            "[39,    20] loss: 0.112\n",
            "[39,    30] loss: 0.124\n",
            "[39,    40] loss: 0.113\n",
            "[40,    10] loss: 0.139\n",
            "[40,    20] loss: 0.130\n",
            "[40,    30] loss: 0.092\n",
            "[40,    40] loss: 0.083\n",
            "[41,    10] loss: 0.072\n",
            "[41,    20] loss: 0.073\n",
            "[41,    30] loss: 0.073\n",
            "[41,    40] loss: 0.106\n",
            "[42,    10] loss: 0.095\n",
            "[42,    20] loss: 0.081\n",
            "[42,    30] loss: 0.079\n",
            "[42,    40] loss: 0.068\n",
            "[43,    10] loss: 0.055\n",
            "[43,    20] loss: 0.058\n",
            "[43,    30] loss: 0.058\n",
            "[43,    40] loss: 0.063\n",
            "[44,    10] loss: 0.148\n",
            "[44,    20] loss: 0.115\n",
            "[44,    30] loss: 0.100\n",
            "[44,    40] loss: 0.063\n",
            "[45,    10] loss: 0.054\n",
            "[45,    20] loss: 0.051\n",
            "[45,    30] loss: 0.044\n",
            "[45,    40] loss: 0.048\n",
            "[46,    10] loss: 0.042\n",
            "[46,    20] loss: 0.032\n",
            "[46,    30] loss: 0.033\n",
            "[46,    40] loss: 0.027\n",
            "[47,    10] loss: 0.025\n",
            "[47,    20] loss: 0.025\n",
            "[47,    30] loss: 0.034\n",
            "[47,    40] loss: 0.031\n",
            "[48,    10] loss: 0.048\n",
            "[48,    20] loss: 0.047\n",
            "[48,    30] loss: 0.046\n",
            "[48,    40] loss: 0.047\n",
            "[49,    10] loss: 0.026\n",
            "[49,    20] loss: 0.032\n",
            "[49,    30] loss: 0.028\n",
            "[49,    40] loss: 0.020\n",
            "[50,    10] loss: 0.015\n",
            "[50,    20] loss: 0.019\n",
            "[50,    30] loss: 0.014\n",
            "[50,    40] loss: 0.010\n",
            "[51,    10] loss: 0.008\n",
            "[51,    20] loss: 0.008\n",
            "[51,    30] loss: 0.011\n",
            "[51,    40] loss: 0.007\n",
            "[52,    10] loss: 0.006\n",
            "[52,    20] loss: 0.005\n",
            "[52,    30] loss: 0.006\n",
            "[52,    40] loss: 0.005\n",
            "[53,    10] loss: 0.004\n",
            "[53,    20] loss: 0.004\n",
            "[53,    30] loss: 0.004\n",
            "[53,    40] loss: 0.005\n",
            "[54,    10] loss: 0.003\n",
            "[54,    20] loss: 0.003\n",
            "[54,    30] loss: 0.003\n",
            "[54,    40] loss: 0.003\n",
            "[55,    10] loss: 0.002\n",
            "[55,    20] loss: 0.002\n",
            "[55,    30] loss: 0.003\n",
            "[55,    40] loss: 0.004\n",
            "[56,    10] loss: 0.003\n",
            "[56,    20] loss: 0.003\n",
            "[56,    30] loss: 0.003\n",
            "[56,    40] loss: 0.004\n",
            "[57,    10] loss: 0.003\n",
            "[57,    20] loss: 0.002\n",
            "[57,    30] loss: 0.002\n",
            "[57,    40] loss: 0.002\n",
            "[58,    10] loss: 0.002\n",
            "[58,    20] loss: 0.002\n",
            "[58,    30] loss: 0.002\n",
            "[58,    40] loss: 0.002\n",
            "[59,    10] loss: 0.001\n",
            "[59,    20] loss: 0.001\n",
            "[59,    30] loss: 0.001\n",
            "[59,    40] loss: 0.001\n",
            "[60,    10] loss: 0.001\n",
            "[60,    20] loss: 0.001\n",
            "[60,    30] loss: 0.001\n",
            "[60,    40] loss: 0.001\n",
            "[61,    10] loss: 0.001\n",
            "[61,    20] loss: 0.001\n",
            "[61,    30] loss: 0.001\n",
            "[61,    40] loss: 0.001\n",
            "[62,    10] loss: 0.001\n",
            "[62,    20] loss: 0.001\n",
            "[62,    30] loss: 0.001\n",
            "[62,    40] loss: 0.001\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 train images: 100 %\n",
            "Accuracy of the network on the 10000 test dataset 1: 34 %\n",
            "Accuracy of the network on the 10000 test dataset 2: 39 %\n",
            "Accuracy of the network on the 10000 test dataset 3: 51 %\n",
            "Accuracy of the network on the 10000 test dataset 4: 69 %\n",
            "Accuracy of the network on the 10000 test dataset 5: 84 %\n",
            "Accuracy of the network on the 10000 test dataset 6: 95 %\n",
            "Accuracy of the network on the 10000 test dataset 7: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 8: 100 %\n",
            "Accuracy of the network on the 10000 test dataset 9: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 10: 92 %\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "training on data set   9\n",
            "[1,    10] loss: 1.116\n",
            "[1,    20] loss: 1.108\n",
            "[1,    30] loss: 1.103\n",
            "[1,    40] loss: 1.101\n",
            "[2,    10] loss: 1.099\n",
            "[2,    20] loss: 1.098\n",
            "[2,    30] loss: 1.099\n",
            "[2,    40] loss: 1.099\n",
            "[3,    10] loss: 1.097\n",
            "[3,    20] loss: 1.097\n",
            "[3,    30] loss: 1.097\n",
            "[3,    40] loss: 1.096\n",
            "[4,    10] loss: 1.095\n",
            "[4,    20] loss: 1.094\n",
            "[4,    30] loss: 1.093\n",
            "[4,    40] loss: 1.091\n",
            "[5,    10] loss: 1.089\n",
            "[5,    20] loss: 1.087\n",
            "[5,    30] loss: 1.084\n",
            "[5,    40] loss: 1.081\n",
            "[6,    10] loss: 1.069\n",
            "[6,    20] loss: 1.058\n",
            "[6,    30] loss: 1.041\n",
            "[6,    40] loss: 1.011\n",
            "[7,    10] loss: 0.996\n",
            "[7,    20] loss: 0.971\n",
            "[7,    30] loss: 0.964\n",
            "[7,    40] loss: 0.970\n",
            "[8,    10] loss: 0.951\n",
            "[8,    20] loss: 0.927\n",
            "[8,    30] loss: 0.884\n",
            "[8,    40] loss: 0.881\n",
            "[9,    10] loss: 0.838\n",
            "[9,    20] loss: 0.805\n",
            "[9,    30] loss: 0.753\n",
            "[9,    40] loss: 0.728\n",
            "[10,    10] loss: 0.733\n",
            "[10,    20] loss: 0.678\n",
            "[10,    30] loss: 0.655\n",
            "[10,    40] loss: 0.641\n",
            "[11,    10] loss: 0.608\n",
            "[11,    20] loss: 0.596\n",
            "[11,    30] loss: 0.597\n",
            "[11,    40] loss: 0.607\n",
            "[12,    10] loss: 0.595\n",
            "[12,    20] loss: 0.574\n",
            "[12,    30] loss: 0.592\n",
            "[12,    40] loss: 0.512\n",
            "[13,    10] loss: 0.518\n",
            "[13,    20] loss: 0.527\n",
            "[13,    30] loss: 0.526\n",
            "[13,    40] loss: 0.506\n",
            "[14,    10] loss: 0.609\n",
            "[14,    20] loss: 0.555\n",
            "[14,    30] loss: 0.507\n",
            "[14,    40] loss: 0.477\n",
            "[15,    10] loss: 0.479\n",
            "[15,    20] loss: 0.480\n",
            "[15,    30] loss: 0.515\n",
            "[15,    40] loss: 0.478\n",
            "[16,    10] loss: 0.453\n",
            "[16,    20] loss: 0.466\n",
            "[16,    30] loss: 0.448\n",
            "[16,    40] loss: 0.467\n",
            "[17,    10] loss: 0.497\n",
            "[17,    20] loss: 0.484\n",
            "[17,    30] loss: 0.427\n",
            "[17,    40] loss: 0.439\n",
            "[18,    10] loss: 0.457\n",
            "[18,    20] loss: 0.429\n",
            "[18,    30] loss: 0.418\n",
            "[18,    40] loss: 0.425\n",
            "[19,    10] loss: 0.408\n",
            "[19,    20] loss: 0.395\n",
            "[19,    30] loss: 0.400\n",
            "[19,    40] loss: 0.386\n",
            "[20,    10] loss: 0.436\n",
            "[20,    20] loss: 0.402\n",
            "[20,    30] loss: 0.361\n",
            "[20,    40] loss: 0.371\n",
            "[21,    10] loss: 0.349\n",
            "[21,    20] loss: 0.344\n",
            "[21,    30] loss: 0.372\n",
            "[21,    40] loss: 0.382\n",
            "[22,    10] loss: 0.410\n",
            "[22,    20] loss: 0.377\n",
            "[22,    30] loss: 0.367\n",
            "[22,    40] loss: 0.393\n",
            "[23,    10] loss: 0.375\n",
            "[23,    20] loss: 0.312\n",
            "[23,    30] loss: 0.343\n",
            "[23,    40] loss: 0.375\n",
            "[24,    10] loss: 0.331\n",
            "[24,    20] loss: 0.332\n",
            "[24,    30] loss: 0.305\n",
            "[24,    40] loss: 0.309\n",
            "[25,    10] loss: 0.298\n",
            "[25,    20] loss: 0.294\n",
            "[25,    30] loss: 0.300\n",
            "[25,    40] loss: 0.298\n",
            "[26,    10] loss: 0.331\n",
            "[26,    20] loss: 0.323\n",
            "[26,    30] loss: 0.332\n",
            "[26,    40] loss: 0.339\n",
            "[27,    10] loss: 0.364\n",
            "[27,    20] loss: 0.328\n",
            "[27,    30] loss: 0.313\n",
            "[27,    40] loss: 0.328\n",
            "[28,    10] loss: 0.284\n",
            "[28,    20] loss: 0.262\n",
            "[28,    30] loss: 0.275\n",
            "[28,    40] loss: 0.255\n",
            "[29,    10] loss: 0.282\n",
            "[29,    20] loss: 0.237\n",
            "[29,    30] loss: 0.241\n",
            "[29,    40] loss: 0.217\n",
            "[30,    10] loss: 0.244\n",
            "[30,    20] loss: 0.226\n",
            "[30,    30] loss: 0.215\n",
            "[30,    40] loss: 0.262\n",
            "[31,    10] loss: 0.243\n",
            "[31,    20] loss: 0.231\n",
            "[31,    30] loss: 0.237\n",
            "[31,    40] loss: 0.216\n",
            "[32,    10] loss: 0.234\n",
            "[32,    20] loss: 0.223\n",
            "[32,    30] loss: 0.218\n",
            "[32,    40] loss: 0.189\n",
            "[33,    10] loss: 0.184\n",
            "[33,    20] loss: 0.197\n",
            "[33,    30] loss: 0.201\n",
            "[33,    40] loss: 0.171\n",
            "[34,    10] loss: 0.158\n",
            "[34,    20] loss: 0.170\n",
            "[34,    30] loss: 0.177\n",
            "[34,    40] loss: 0.144\n",
            "[35,    10] loss: 0.143\n",
            "[35,    20] loss: 0.150\n",
            "[35,    30] loss: 0.136\n",
            "[35,    40] loss: 0.138\n",
            "[36,    10] loss: 0.130\n",
            "[36,    20] loss: 0.144\n",
            "[36,    30] loss: 0.142\n",
            "[36,    40] loss: 0.144\n",
            "[37,    10] loss: 0.269\n",
            "[37,    20] loss: 0.206\n",
            "[37,    30] loss: 0.165\n",
            "[37,    40] loss: 0.131\n",
            "[38,    10] loss: 0.119\n",
            "[38,    20] loss: 0.125\n",
            "[38,    30] loss: 0.138\n",
            "[38,    40] loss: 0.121\n",
            "[39,    10] loss: 0.110\n",
            "[39,    20] loss: 0.117\n",
            "[39,    30] loss: 0.110\n",
            "[39,    40] loss: 0.116\n",
            "[40,    10] loss: 0.166\n",
            "[40,    20] loss: 0.168\n",
            "[40,    30] loss: 0.139\n",
            "[40,    40] loss: 0.114\n",
            "[41,    10] loss: 0.113\n",
            "[41,    20] loss: 0.100\n",
            "[41,    30] loss: 0.104\n",
            "[41,    40] loss: 0.101\n",
            "[42,    10] loss: 0.088\n",
            "[42,    20] loss: 0.079\n",
            "[42,    30] loss: 0.087\n",
            "[42,    40] loss: 0.085\n",
            "[43,    10] loss: 0.116\n",
            "[43,    20] loss: 0.092\n",
            "[43,    30] loss: 0.085\n",
            "[43,    40] loss: 0.074\n",
            "[44,    10] loss: 0.051\n",
            "[44,    20] loss: 0.058\n",
            "[44,    30] loss: 0.072\n",
            "[44,    40] loss: 0.070\n",
            "[45,    10] loss: 0.101\n",
            "[45,    20] loss: 0.093\n",
            "[45,    30] loss: 0.079\n",
            "[45,    40] loss: 0.073\n",
            "[46,    10] loss: 0.048\n",
            "[46,    20] loss: 0.054\n",
            "[46,    30] loss: 0.049\n",
            "[46,    40] loss: 0.047\n",
            "[47,    10] loss: 0.057\n",
            "[47,    20] loss: 0.050\n",
            "[47,    30] loss: 0.044\n",
            "[47,    40] loss: 0.043\n",
            "[48,    10] loss: 0.039\n",
            "[48,    20] loss: 0.036\n",
            "[48,    30] loss: 0.032\n",
            "[48,    40] loss: 0.027\n",
            "[49,    10] loss: 0.028\n",
            "[49,    20] loss: 0.021\n",
            "[49,    30] loss: 0.030\n",
            "[49,    40] loss: 0.028\n",
            "[50,    10] loss: 0.061\n",
            "[50,    20] loss: 0.043\n",
            "[50,    30] loss: 0.043\n",
            "[50,    40] loss: 0.042\n",
            "[51,    10] loss: 0.198\n",
            "[51,    20] loss: 0.176\n",
            "[51,    30] loss: 0.116\n",
            "[51,    40] loss: 0.090\n",
            "[52,    10] loss: 0.043\n",
            "[52,    20] loss: 0.032\n",
            "[52,    30] loss: 0.034\n",
            "[52,    40] loss: 0.029\n",
            "[53,    10] loss: 0.023\n",
            "[53,    20] loss: 0.019\n",
            "[53,    30] loss: 0.015\n",
            "[53,    40] loss: 0.013\n",
            "[54,    10] loss: 0.011\n",
            "[54,    20] loss: 0.011\n",
            "[54,    30] loss: 0.018\n",
            "[54,    40] loss: 0.021\n",
            "[55,    10] loss: 0.191\n",
            "[55,    20] loss: 0.136\n",
            "[55,    30] loss: 0.098\n",
            "[55,    40] loss: 0.068\n",
            "[56,    10] loss: 0.052\n",
            "[56,    20] loss: 0.034\n",
            "[56,    30] loss: 0.030\n",
            "[56,    40] loss: 0.035\n",
            "[57,    10] loss: 0.069\n",
            "[57,    20] loss: 0.056\n",
            "[57,    30] loss: 0.034\n",
            "[57,    40] loss: 0.030\n",
            "[58,    10] loss: 0.023\n",
            "[58,    20] loss: 0.016\n",
            "[58,    30] loss: 0.010\n",
            "[58,    40] loss: 0.016\n",
            "[59,    10] loss: 0.009\n",
            "[59,    20] loss: 0.011\n",
            "[59,    30] loss: 0.009\n",
            "[59,    40] loss: 0.006\n",
            "[60,    10] loss: 0.012\n",
            "[60,    20] loss: 0.014\n",
            "[60,    30] loss: 0.008\n",
            "[60,    40] loss: 0.007\n",
            "[61,    10] loss: 0.009\n",
            "[61,    20] loss: 0.004\n",
            "[61,    30] loss: 0.006\n",
            "[61,    40] loss: 0.004\n",
            "[62,    10] loss: 0.003\n",
            "[62,    20] loss: 0.004\n",
            "[62,    30] loss: 0.003\n",
            "[62,    40] loss: 0.005\n",
            "[63,    10] loss: 0.002\n",
            "[63,    20] loss: 0.004\n",
            "[63,    30] loss: 0.002\n",
            "[63,    40] loss: 0.003\n",
            "[64,    10] loss: 0.002\n",
            "[64,    20] loss: 0.002\n",
            "[64,    30] loss: 0.002\n",
            "[64,    40] loss: 0.003\n",
            "[65,    10] loss: 0.002\n",
            "[65,    20] loss: 0.001\n",
            "[65,    30] loss: 0.002\n",
            "[65,    40] loss: 0.001\n",
            "[66,    10] loss: 0.002\n",
            "[66,    20] loss: 0.001\n",
            "[66,    30] loss: 0.001\n",
            "[66,    40] loss: 0.001\n",
            "[67,    10] loss: 0.001\n",
            "[67,    20] loss: 0.001\n",
            "[67,    30] loss: 0.001\n",
            "[67,    40] loss: 0.001\n",
            "[68,    10] loss: 0.001\n",
            "[68,    20] loss: 0.001\n",
            "[68,    30] loss: 0.001\n",
            "[68,    40] loss: 0.001\n",
            "[69,    10] loss: 0.001\n",
            "[69,    20] loss: 0.001\n",
            "[69,    30] loss: 0.001\n",
            "[69,    40] loss: 0.001\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 train images: 100 %\n",
            "Accuracy of the network on the 10000 test dataset 1: 34 %\n",
            "Accuracy of the network on the 10000 test dataset 2: 38 %\n",
            "Accuracy of the network on the 10000 test dataset 3: 47 %\n",
            "Accuracy of the network on the 10000 test dataset 4: 63 %\n",
            "Accuracy of the network on the 10000 test dataset 5: 78 %\n",
            "Accuracy of the network on the 10000 test dataset 6: 90 %\n",
            "Accuracy of the network on the 10000 test dataset 7: 98 %\n",
            "Accuracy of the network on the 10000 test dataset 8: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 9: 100 %\n",
            "Accuracy of the network on the 10000 test dataset 10: 91 %\n",
            "--------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "qlY1RknNfVjb",
        "outputId": "42a63a47-29d1-4f06-8b35-84cbe5c2645b"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "fig = plt.figure()\n",
        "for i,j in enumerate(train_loss_all):\n",
        "    plt.plot(j,label =\"dataset \"+str(i+1))\n",
        "    \n",
        "\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Training_loss\")\n",
        "\n",
        "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
        "fig.savefig(\"Figure.pdf\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdsAAAEGCAYAAAAt2j/FAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nlalgqyiKoiitTAVbRVEURWll/x+0TKdzNqF91gAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMVKmRRhfVjd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Exp_2_Attention_models_on_9_datasets_made_from_10k_mosaic_complex.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "id": "JSjG64ra4aFu",
        "outputId": "5b94262c-7376-429d-85ce-087ebb1e92ff"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!pip install torch"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.3.0+cu100)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.17.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8-7SARDZErK"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "import torch.optim as optim\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import copy\n",
        "import pickle\n",
        "# Ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "vwJv7Y8Rewez",
        "outputId": "17320efb-1605-4abb-ad4c-484248ce4c66"
      },
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=10, shuffle=True)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=10, shuffle=False)\n",
        "\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "foreground_classes = {'plane', 'car', 'bird'}\n",
        "\n",
        "background_classes = {'cat', 'deer', 'dog', 'frog', 'horse','ship', 'truck'}\n",
        "\n",
        "# print(type(foreground_classes))\n",
        "\n",
        "dataiter = iter(trainloader)\n",
        "background_data=[]\n",
        "background_label=[]\n",
        "foreground_data=[]\n",
        "foreground_label=[]\n",
        "batch_size=10\n",
        "\n",
        "for i in range(5000):   #5000*batch_size = 50000 data points\n",
        "  images, labels = dataiter.next()\n",
        "  for j in range(batch_size):\n",
        "    if(classes[labels[j]] in background_classes):\n",
        "      img = images[j].tolist()\n",
        "      background_data.append(img)\n",
        "      background_label.append(labels[j])\n",
        "    else:\n",
        "      img = images[j].tolist()\n",
        "      foreground_data.append(img)\n",
        "      foreground_label.append(labels[j])\n",
        "            \n",
        "foreground_data = torch.tensor(foreground_data)\n",
        "foreground_label = torch.tensor(foreground_label)\n",
        "background_data = torch.tensor(background_data)\n",
        "background_label = torch.tensor(background_label)\n",
        "    \n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "170500096it [00:02, 71736044.08it/s]                               \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nDYhjJse6Qq"
      },
      "source": [
        "def create_mosaic_img(bg_idx,fg_idx,fg): \n",
        "  \"\"\"\n",
        "  bg_idx : list of indexes of background_data[] to be used as background images in mosaic\n",
        "  fg_idx : index of image to be used as foreground image from foreground data\n",
        "  fg : at what position/index foreground image has to be stored out of 0-8\n",
        "  \"\"\"\n",
        "  image_list=[]\n",
        "  j=0\n",
        "  for i in range(9):\n",
        "    if i != fg:\n",
        "      image_list.append(background_data[bg_idx[j]].type(\"torch.DoubleTensor\"))\n",
        "      j+=1\n",
        "    else: \n",
        "      image_list.append(foreground_data[fg_idx].type(\"torch.DoubleTensor\"))\n",
        "      label = foreground_label[fg_idx]  #-7  # minus 7 because our fore ground classes are 7,8,9 but we have to store it as 0,1,2\n",
        "  #image_list = np.concatenate(image_list ,axis=0)\n",
        "  image_list = torch.stack(image_list) \n",
        "  return image_list,label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aivGVg14e9iZ"
      },
      "source": [
        "desired_num = 20000\n",
        "mosaic_list_of_images =[]      # list of mosaic images, each mosaic image is saved as list of 9 images\n",
        "fore_idx =[]                   # list of indexes at which foreground image is present in a mosaic image i.e from 0 to 9               \n",
        "mosaic_label=[]                # label of mosaic image = foreground class present in that mosaic\n",
        "for i in range(desired_num):\n",
        "  bg_idx = np.random.randint(0,35000,8)\n",
        "  fg_idx = np.random.randint(0,15000)\n",
        "  fg = np.random.randint(0,9)\n",
        "  fore_idx.append(fg)\n",
        "  image_list,label = create_mosaic_img(bg_idx,fg_idx,fg)\n",
        "  mosaic_list_of_images.append(image_list)\n",
        "  mosaic_label.append(label)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6Jy35SSfBS9"
      },
      "source": [
        "dict = {\"mosaic_list_of_images\": mosaic_list_of_images, \"mosaic_label\": mosaic_label , \"fore_idx\":fore_idx}\n",
        "f = open(\"/content/drive/My Drive/Research/Experiments on CIFAR mosaic/Exp_2_Attention_models_on_9_datasets_made_from_10k_mosaic/weights/file.pkl\",\"wb\")\n",
        "pickle.dump(dict,f)\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIuiboIUfViV"
      },
      "source": [
        "# Load data from saved file "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MbYf6zuBfViX"
      },
      "source": [
        "# with open('/content/drive/My Drive/Research/Experiments on CIFAR mosaic/Exp_2_Attention_models_on_9_datasets_made_from_10k_mosaic/weights/file.pkl', 'rb') as f:\n",
        "#     data = pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwZjDB5lfVib"
      },
      "source": [
        "# mosaic_list_of_images = data[\"mosaic_list_of_images\"]\n",
        "# mosaic_label = data[\"mosaic_label\"]\n",
        "# fore_idx = data[\"fore_idx\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "cog5VUzGgE5L",
        "outputId": "6dd69ad1-2c10-47a0-ba5e-68631c9b08ca"
      },
      "source": [
        "print(len(mosaic_list_of_images) , len(mosaic_label), len(mosaic_list_of_images[0:10000]))\n",
        "print(len(fore_idx))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20000 20000 10000\n",
            "20000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xX91RwMy-IP4"
      },
      "source": [
        "def create_avg_image_from_mosaic_dataset(mosaic_dataset,labels,foreground_index,dataset_number):\n",
        "  \"\"\"\n",
        "  mosaic_dataset : mosaic_dataset contains 9 images 32 x 32 each as 1 data point\n",
        "  labels : mosaic_dataset labels\n",
        "  foreground_index : contains list of indexes where foreground image is present so that using this we can take weighted average\n",
        "  dataset_number : will help us to tell what ratio of foreground image to be taken. for eg: if it is \"j\" then fg_image_ratio = j/9 , bg_image_ratio = (9-j)/8*9\n",
        "  \"\"\"\n",
        "  avg_image_dataset = []\n",
        "  for i in range(len(mosaic_dataset)):\n",
        "    img = torch.zeros([3, 32,32], dtype=torch.float64)\n",
        "    for j in range(9):\n",
        "      if j == foreground_index[i]:\n",
        "        img = img + mosaic_dataset[i][j]*dataset_number/9\n",
        "      else :\n",
        "        img = img + mosaic_dataset[i][j]*(9-dataset_number)/(8*9)\n",
        "    \n",
        "    avg_image_dataset.append(img)\n",
        "    \n",
        "  return avg_image_dataset , labels , foreground_index\n",
        "        \n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGz8Y88vIZPT"
      },
      "source": [
        "avg_image_dataset_1 , labels_1,  fg_index_1 = create_avg_image_from_mosaic_dataset(mosaic_list_of_images[0:10000], mosaic_label[0:10000], fore_idx[0:10000] , 1)\n",
        "avg_image_dataset_2 , labels_2,  fg_index_2 = create_avg_image_from_mosaic_dataset(mosaic_list_of_images[0:10000], mosaic_label[0:10000], fore_idx[0:10000] , 2)\n",
        "avg_image_dataset_3 , labels_3,  fg_index_3 = create_avg_image_from_mosaic_dataset(mosaic_list_of_images[0:10000], mosaic_label[0:10000], fore_idx[0:10000] , 3)\n",
        "avg_image_dataset_4 , labels_4,  fg_index_4 = create_avg_image_from_mosaic_dataset(mosaic_list_of_images[0:10000], mosaic_label[0:10000], fore_idx[0:10000] , 4)\n",
        "avg_image_dataset_5 , labels_5,  fg_index_5 = create_avg_image_from_mosaic_dataset(mosaic_list_of_images[0:10000], mosaic_label[0:10000], fore_idx[0:10000] , 5)\n",
        "avg_image_dataset_6 , labels_6,  fg_index_6 = create_avg_image_from_mosaic_dataset(mosaic_list_of_images[0:10000], mosaic_label[0:10000], fore_idx[0:10000] , 6)\n",
        "avg_image_dataset_7 , labels_7,  fg_index_7 = create_avg_image_from_mosaic_dataset(mosaic_list_of_images[0:10000], mosaic_label[0:10000], fore_idx[0:10000] , 7)\n",
        "avg_image_dataset_8 , labels_8,  fg_index_8 = create_avg_image_from_mosaic_dataset(mosaic_list_of_images[0:10000], mosaic_label[0:10000], fore_idx[0:10000] , 8)\n",
        "avg_image_dataset_9 , labels_9,  fg_index_9 = create_avg_image_from_mosaic_dataset(mosaic_list_of_images[0:10000], mosaic_label[0:10000], fore_idx[0:10000] , 9)\n",
        "\n",
        "test_dataset_10 , labels_10 , fg_index_10 = create_avg_image_from_mosaic_dataset(mosaic_list_of_images[10000:20000], mosaic_label[10000:20000], fore_idx[10000:20000] , 9)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSO9SFE25Lrk"
      },
      "source": [
        "class MosaicDataset(Dataset):\n",
        "  \"\"\"MosaicDataset dataset.\"\"\"\n",
        "\n",
        "  def __init__(self, mosaic_list_of_images, mosaic_label):\n",
        "    \"\"\"\n",
        "      Args:\n",
        "        csv_file (string): Path to the csv file with annotations.\n",
        "        root_dir (string): Directory with all the images.\n",
        "        transform (callable, optional): Optional transform to be applied\n",
        "            on a sample.\n",
        "    \"\"\"\n",
        "    self.mosaic = mosaic_list_of_images\n",
        "    self.label = mosaic_label\n",
        "    #self.fore_idx = fore_idx\n",
        "    \n",
        "  def __len__(self):\n",
        "    return len(self.label)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.mosaic[idx] , self.label[idx] #, self.fore_idx[idx]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obE1xeyRks1Q"
      },
      "source": [
        "batch = 256\n",
        "\n",
        "\n",
        "# training_data = avg_image_dataset_5    #just change this and training_label to desired dataset for training\n",
        "# training_label = labels_5\n",
        "\n",
        "traindata_1 = MosaicDataset(avg_image_dataset_1, labels_1 )\n",
        "trainloader_1 = DataLoader( traindata_1 , batch_size= batch ,shuffle=True)\n",
        "\n",
        "traindata_2 = MosaicDataset(avg_image_dataset_2, labels_2 )\n",
        "trainloader_2 = DataLoader( traindata_2 , batch_size= batch ,shuffle=True)\n",
        "\n",
        "traindata_3 = MosaicDataset(avg_image_dataset_3, labels_3 )\n",
        "trainloader_3 = DataLoader( traindata_3 , batch_size= batch ,shuffle=True)\n",
        "\n",
        "traindata_4 = MosaicDataset(avg_image_dataset_4, labels_4 )\n",
        "trainloader_4 = DataLoader( traindata_4 , batch_size= batch ,shuffle=True)\n",
        "\n",
        "traindata_5 = MosaicDataset(avg_image_dataset_5, labels_5 )\n",
        "trainloader_5 = DataLoader( traindata_5 , batch_size= batch ,shuffle=True)\n",
        "\n",
        "traindata_6 = MosaicDataset(avg_image_dataset_6, labels_6 )\n",
        "trainloader_6 = DataLoader( traindata_6 , batch_size= batch ,shuffle=True)\n",
        "\n",
        "traindata_7 = MosaicDataset(avg_image_dataset_7, labels_7 )\n",
        "trainloader_7 = DataLoader( traindata_7 , batch_size= batch ,shuffle=True)\n",
        "\n",
        "traindata_8 = MosaicDataset(avg_image_dataset_8, labels_8 )\n",
        "trainloader_8 = DataLoader( traindata_8 , batch_size= batch ,shuffle=True)\n",
        "\n",
        "traindata_9 = MosaicDataset(avg_image_dataset_9, labels_9 )\n",
        "trainloader_9 = DataLoader( traindata_9 , batch_size= batch ,shuffle=True)\n",
        "\n",
        "testdata_1 = MosaicDataset(avg_image_dataset_1, labels_1 )\n",
        "testloader_1 = DataLoader( testdata_1 , batch_size= batch ,shuffle=False)\n",
        "\n",
        "testdata_2 = MosaicDataset(avg_image_dataset_2, labels_2 )\n",
        "testloader_2 = DataLoader( testdata_2 , batch_size= batch ,shuffle=False)\n",
        "\n",
        "testdata_3 = MosaicDataset(avg_image_dataset_3, labels_3 )\n",
        "testloader_3 = DataLoader( testdata_3 , batch_size= batch ,shuffle=False)\n",
        "\n",
        "testdata_4 = MosaicDataset(avg_image_dataset_4, labels_4 )\n",
        "testloader_4 = DataLoader( testdata_4 , batch_size= batch ,shuffle=False)\n",
        "\n",
        "testdata_5 = MosaicDataset(avg_image_dataset_5, labels_5 )\n",
        "testloader_5 = DataLoader( testdata_5 , batch_size= batch ,shuffle=False)\n",
        "\n",
        "testdata_6 = MosaicDataset(avg_image_dataset_6, labels_6 )\n",
        "testloader_6 = DataLoader( testdata_6 , batch_size= batch ,shuffle=False)\n",
        "\n",
        "testdata_7 = MosaicDataset(avg_image_dataset_7, labels_7 )\n",
        "testloader_7 = DataLoader( testdata_7 , batch_size= batch ,shuffle=False)\n",
        "\n",
        "testdata_8 = MosaicDataset(avg_image_dataset_8, labels_8 )\n",
        "testloader_8 = DataLoader( testdata_8 , batch_size= batch ,shuffle=False)\n",
        "\n",
        "testdata_9 = MosaicDataset(avg_image_dataset_9, labels_9 )\n",
        "testloader_9 = DataLoader( testdata_9 , batch_size= batch ,shuffle=False)\n",
        "\n",
        "testdata_10 = MosaicDataset(test_dataset_10, labels_10 )\n",
        "testloader_10 = DataLoader( testdata_10 , batch_size= batch ,shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SadRzWBBZEsP"
      },
      "source": [
        "class Conv_module(nn.Module):\n",
        "    def __init__(self,inp_ch,f,s,k,pad):\n",
        "        super(Conv_module,self).__init__()\n",
        "        self.inp_ch = inp_ch\n",
        "        self.f = f\n",
        "        self.s = s \n",
        "        self.k = k \n",
        "        self.pad = pad\n",
        "        \n",
        "        \n",
        "        self.conv = nn.Conv2d(self.inp_ch,self.f,k,stride=s,padding=self.pad)\n",
        "        self.bn = nn.BatchNorm2d(self.f)\n",
        "        self.act = nn.ReLU()\n",
        "    def forward(self,x):\n",
        "        x = self.conv(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.act(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IgGYMG_ZZEsT"
      },
      "source": [
        "class inception_module(nn.Module):\n",
        "    def __init__(self,inp_ch,f0,f1):\n",
        "        super(inception_module, self).__init__()\n",
        "        self.inp_ch = inp_ch\n",
        "        self.f0 = f0\n",
        "        self.f1 = f1\n",
        "        \n",
        "        \n",
        "        \n",
        "        self.conv1 = Conv_module(self.inp_ch,self.f0,1,1,pad=0)\n",
        "        self.conv3 = Conv_module(self.inp_ch,self.f1,1,3,pad=1)\n",
        "        #self.conv1 = nn.Conv2d(3,self.f0,1)\n",
        "        #self.conv3 = nn.Conv2d(3,self.f1,3,padding=1)\n",
        "    def forward(self,x):\n",
        "        x1 = self.conv1.forward(x)\n",
        "        x3 = self.conv3.forward(x)\n",
        "        #print(x1.shape,x3.shape)\n",
        "        \n",
        "        x = torch.cat((x1,x3),dim=1)\n",
        "        \n",
        "    \n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thkdqW91Hpju"
      },
      "source": [
        "class downsample_module(nn.Module):\n",
        "    def __init__(self,inp_ch,f):\n",
        "        super(downsample_module,self).__init__()\n",
        "        self.inp_ch = inp_ch\n",
        "        self.f = f\n",
        "        self.conv = Conv_module(self.inp_ch,self.f,2,3,pad=0)\n",
        "        self.pool = nn.MaxPool2d(3,stride=2,padding=0)\n",
        "    def forward(self,x):\n",
        "        x1 = self.conv(x)\n",
        "        #print(x1.shape)\n",
        "        x2 = self.pool(x)\n",
        "        #print(x2.shape)\n",
        "        x = torch.cat((x1,x2),dim=1)\n",
        "        \n",
        "        return x,x1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1yVWgR4vFhe"
      },
      "source": [
        "class inception_net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(inception_net,self).__init__()\n",
        "        self.conv1 = Conv_module(3,96,1,3,0)\n",
        "        \n",
        "        self.incept1 = inception_module(96,32,32)\n",
        "        self.incept2 = inception_module(64,32,48)\n",
        "        \n",
        "        self.downsample1 = downsample_module(80,80)\n",
        "        \n",
        "        self.incept3 = inception_module(160,112,48)\n",
        "        self.incept4 = inception_module(160,96,64)\n",
        "        self.incept5 = inception_module(160,80,80)\n",
        "        self.incept6 = inception_module(160,48,96)\n",
        "        \n",
        "        self.downsample2 = downsample_module(144,96)\n",
        "        \n",
        "        self.incept7 = inception_module(240,176,60)\n",
        "        self.incept8 = inception_module(236,176,60)\n",
        "        \n",
        "        self.pool = nn.AvgPool2d(5)\n",
        "        \n",
        "        \n",
        "        \n",
        "        self.linear = nn.Linear(236,3)\n",
        "    def forward(self,x):\n",
        "        x = self.conv1.forward(x)\n",
        "        #act1 = x\n",
        "        \n",
        "        x = self.incept1.forward(x)\n",
        "        #act2 = x\n",
        "        \n",
        "        x = self.incept2.forward(x)\n",
        "        #act3 = x\n",
        "        \n",
        "        x,act4 = self.downsample1.forward(x)\n",
        "        \n",
        "        x = self.incept3.forward(x)\n",
        "        #act5 = x\n",
        "        \n",
        "        x = self.incept4.forward(x)\n",
        "        #act6 = x\n",
        "        \n",
        "        x = self.incept5.forward(x)\n",
        "        #act7 = x\n",
        "        \n",
        "        x = self.incept6.forward(x)\n",
        "        #act8 = x\n",
        "        \n",
        "        x,act9 = self.downsample2.forward(x)\n",
        "        \n",
        "        x = self.incept7.forward(x)\n",
        "        #act10 = x\n",
        "        x = self.incept8.forward(x)\n",
        "        #act11 = x\n",
        "        #print(x.shape)\n",
        "        x = self.pool(x)\n",
        "        #print(x.shape)\n",
        "        x = x.view(-1,1*1*236)\n",
        "        x = self.linear(x) \n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOWrnzv1fVjD"
      },
      "source": [
        "def test_all(number, testloader,inc):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    out = []\n",
        "    pred = []\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data\n",
        "            images, labels = images.to(\"cuda\"),labels.to(\"cuda\")\n",
        "            out.append(labels.cpu().numpy())\n",
        "            outputs= inc(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            pred.append(predicted.cpu().numpy())\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print('Accuracy of the network on the 10000 test dataset %d: %d %%' % (number , 100 * correct / total))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFfAJZkcZEsY"
      },
      "source": [
        "def train_all(trainloader, ds_number, testloader_list):\n",
        "    \n",
        "    print(\"--\"*40)\n",
        "    print(\"training on data set  \", ds_number)\n",
        "    \n",
        "    inc = inception_net().double()\n",
        "    inc = inc.to(\"cuda\")\n",
        "    \n",
        "    criterion_inception = nn.CrossEntropyLoss()\n",
        "    optimizer_inception = optim.SGD(inc.parameters(), lr=0.01, momentum=0.9)\n",
        "    \n",
        "    acti = []\n",
        "    loss_curi = []\n",
        "    epochs = 70\n",
        "    \n",
        "    for epoch in range(epochs): # loop over the dataset multiple times\n",
        "        ep_lossi = []\n",
        "\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            # get the inputs\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(\"cuda\"),labels.to(\"cuda\")\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer_inception.zero_grad()\n",
        "\n",
        "            # forward + backward + optimize\n",
        "            outputs = inc(inputs)\n",
        "            loss = criterion_inception(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer_inception.step()\n",
        "\n",
        "            # print statistics\n",
        "            running_loss += loss.item()\n",
        "            if i % 10 == 9:    # print every 10 mini-batches\n",
        "                print('[%d, %5d] loss: %.3f' %\n",
        "                      (epoch + 1, i + 1, running_loss / 10))\n",
        "                ep_lossi.append(running_loss/10) # loss per minibatch\n",
        "                running_loss = 0.0\n",
        "                \n",
        "        loss_curi.append(np.mean(ep_lossi))   #loss per epoch\n",
        "    #     if (epoch%5 == 0):\n",
        "    #         _,actis= inc(inputs)\n",
        "    #         acti.append(actis)\n",
        "\n",
        "\n",
        "\n",
        "    print('Finished Training')\n",
        "    torch.save(inc.state_dict(),\"/content/drive/My Drive/Research/Experiments on CIFAR mosaic/Exp_2_Attention_models_on_9_datasets_made_from_10k_mosaic/weights/train_dataset_\"+str(ds_number)+\"_\"+str(epochs)+\".pt\")\n",
        "    \n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in trainloader:\n",
        "            images, labels = data\n",
        "            images, labels = images.to(\"cuda\"), labels.to(\"cuda\")\n",
        "            outputs = inc(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print('Accuracy of the network on the 10000 train images: %d %%' % (  100 * correct / total))\n",
        "    \n",
        "    for i, j in enumerate(testloader_list):\n",
        "        test_all(i+1, j,inc)\n",
        "    \n",
        "    print(\"--\"*40)\n",
        "    \n",
        "    return loss_curi\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mI-vqhB-fVjJ",
        "outputId": "b09b2baf-4d15-46c2-c896-a058cc5e7577"
      },
      "source": [
        "train_loss_all=[]\n",
        "\n",
        "testloader_list= [ testloader_1, testloader_2, testloader_3, testloader_4, testloader_5, testloader_6,\n",
        "                 testloader_7, testloader_8, testloader_9, testloader_10]\n",
        "\n",
        "train_loss_all.append(train_all(trainloader_1, 1, testloader_list))\n",
        "train_loss_all.append(train_all(trainloader_2, 2, testloader_list))\n",
        "train_loss_all.append(train_all(trainloader_3, 3, testloader_list))\n",
        "train_loss_all.append(train_all(trainloader_4, 4, testloader_list))\n",
        "train_loss_all.append(train_all(trainloader_5, 5, testloader_list))\n",
        "train_loss_all.append(train_all(trainloader_6, 6, testloader_list))\n",
        "train_loss_all.append(train_all(trainloader_7, 7, testloader_list))\n",
        "train_loss_all.append(train_all(trainloader_8, 8, testloader_list))\n",
        "train_loss_all.append(train_all(trainloader_9, 9, testloader_list))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "training on data set   1\n",
            "[1,    10] loss: 1.088\n",
            "[1,    20] loss: 1.072\n",
            "[1,    30] loss: 1.073\n",
            "[1,    40] loss: 1.086\n",
            "[2,    10] loss: 1.063\n",
            "[2,    20] loss: 1.050\n",
            "[2,    30] loss: 1.045\n",
            "[2,    40] loss: 1.055\n",
            "[3,    10] loss: 1.041\n",
            "[3,    20] loss: 1.051\n",
            "[3,    30] loss: 1.048\n",
            "[3,    40] loss: 1.029\n",
            "[4,    10] loss: 1.014\n",
            "[4,    20] loss: 1.020\n",
            "[4,    30] loss: 1.021\n",
            "[4,    40] loss: 1.017\n",
            "[5,    10] loss: 1.007\n",
            "[5,    20] loss: 0.994\n",
            "[5,    30] loss: 1.006\n",
            "[5,    40] loss: 0.992\n",
            "[6,    10] loss: 0.980\n",
            "[6,    20] loss: 0.981\n",
            "[6,    30] loss: 0.965\n",
            "[6,    40] loss: 0.969\n",
            "[7,    10] loss: 0.941\n",
            "[7,    20] loss: 0.939\n",
            "[7,    30] loss: 0.940\n",
            "[7,    40] loss: 0.926\n",
            "[8,    10] loss: 0.895\n",
            "[8,    20] loss: 0.872\n",
            "[8,    30] loss: 0.895\n",
            "[8,    40] loss: 0.891\n",
            "[9,    10] loss: 0.877\n",
            "[9,    20] loss: 0.868\n",
            "[9,    30] loss: 0.857\n",
            "[9,    40] loss: 0.910\n",
            "[10,    10] loss: 0.798\n",
            "[10,    20] loss: 0.806\n",
            "[10,    30] loss: 0.782\n",
            "[10,    40] loss: 0.797\n",
            "[11,    10] loss: 0.793\n",
            "[11,    20] loss: 0.841\n",
            "[11,    30] loss: 0.834\n",
            "[11,    40] loss: 0.814\n",
            "[12,    10] loss: 0.717\n",
            "[12,    20] loss: 0.737\n",
            "[12,    30] loss: 0.682\n",
            "[12,    40] loss: 0.638\n",
            "[13,    10] loss: 0.607\n",
            "[13,    20] loss: 0.590\n",
            "[13,    30] loss: 0.590\n",
            "[13,    40] loss: 0.596\n",
            "[14,    10] loss: 0.550\n",
            "[14,    20] loss: 0.521\n",
            "[14,    30] loss: 0.471\n",
            "[14,    40] loss: 0.503\n",
            "[15,    10] loss: 0.541\n",
            "[15,    20] loss: 0.557\n",
            "[15,    30] loss: 0.504\n",
            "[15,    40] loss: 0.482\n",
            "[16,    10] loss: 0.380\n",
            "[16,    20] loss: 0.432\n",
            "[16,    30] loss: 0.365\n",
            "[16,    40] loss: 0.333\n",
            "[17,    10] loss: 0.359\n",
            "[17,    20] loss: 0.391\n",
            "[17,    30] loss: 0.336\n",
            "[17,    40] loss: 0.335\n",
            "[18,    10] loss: 0.331\n",
            "[18,    20] loss: 0.335\n",
            "[18,    30] loss: 0.301\n",
            "[18,    40] loss: 0.292\n",
            "[19,    10] loss: 0.325\n",
            "[19,    20] loss: 0.370\n",
            "[19,    30] loss: 0.327\n",
            "[19,    40] loss: 0.257\n",
            "[20,    10] loss: 0.193\n",
            "[20,    20] loss: 0.186\n",
            "[20,    30] loss: 0.169\n",
            "[20,    40] loss: 0.169\n",
            "[21,    10] loss: 0.196\n",
            "[21,    20] loss: 0.230\n",
            "[21,    30] loss: 0.166\n",
            "[21,    40] loss: 0.206\n",
            "[22,    10] loss: 0.274\n",
            "[22,    20] loss: 0.288\n",
            "[22,    30] loss: 0.253\n",
            "[22,    40] loss: 0.220\n",
            "[23,    10] loss: 0.180\n",
            "[23,    20] loss: 0.175\n",
            "[23,    30] loss: 0.150\n",
            "[23,    40] loss: 0.148\n",
            "[24,    10] loss: 0.145\n",
            "[24,    20] loss: 0.162\n",
            "[24,    30] loss: 0.133\n",
            "[24,    40] loss: 0.139\n",
            "[25,    10] loss: 0.159\n",
            "[25,    20] loss: 0.163\n",
            "[25,    30] loss: 0.132\n",
            "[25,    40] loss: 0.107\n",
            "[26,    10] loss: 0.062\n",
            "[26,    20] loss: 0.050\n",
            "[26,    30] loss: 0.042\n",
            "[26,    40] loss: 0.030\n",
            "[27,    10] loss: 0.009\n",
            "[27,    20] loss: 0.012\n",
            "[27,    30] loss: 0.010\n",
            "[27,    40] loss: 0.012\n",
            "[28,    10] loss: 0.006\n",
            "[28,    20] loss: 0.009\n",
            "[28,    30] loss: 0.006\n",
            "[28,    40] loss: 0.011\n",
            "[29,    10] loss: 0.022\n",
            "[29,    20] loss: 0.030\n",
            "[29,    30] loss: 0.028\n",
            "[29,    40] loss: 0.018\n",
            "[30,    10] loss: 0.010\n",
            "[30,    20] loss: 0.006\n",
            "[30,    30] loss: 0.005\n",
            "[30,    40] loss: 0.004\n",
            "[31,    10] loss: 0.002\n",
            "[31,    20] loss: 0.002\n",
            "[31,    30] loss: 0.002\n",
            "[31,    40] loss: 0.005\n",
            "[32,    10] loss: 0.003\n",
            "[32,    20] loss: 0.007\n",
            "[32,    30] loss: 0.007\n",
            "[32,    40] loss: 0.004\n",
            "[33,    10] loss: 0.002\n",
            "[33,    20] loss: 0.002\n",
            "[33,    30] loss: 0.002\n",
            "[33,    40] loss: 0.007\n",
            "[34,    10] loss: 0.017\n",
            "[34,    20] loss: 0.015\n",
            "[34,    30] loss: 0.013\n",
            "[34,    40] loss: 0.009\n",
            "[35,    10] loss: 0.003\n",
            "[35,    20] loss: 0.003\n",
            "[35,    30] loss: 0.003\n",
            "[35,    40] loss: 0.002\n",
            "[36,    10] loss: 0.001\n",
            "[36,    20] loss: 0.001\n",
            "[36,    30] loss: 0.001\n",
            "[36,    40] loss: 0.030\n",
            "[37,    10] loss: 0.155\n",
            "[37,    20] loss: 0.215\n",
            "[37,    30] loss: 0.227\n",
            "[37,    40] loss: 0.184\n",
            "[38,    10] loss: 0.107\n",
            "[38,    20] loss: 0.099\n",
            "[38,    30] loss: 0.079\n",
            "[38,    40] loss: 0.075\n",
            "[39,    10] loss: 0.067\n",
            "[39,    20] loss: 0.077\n",
            "[39,    30] loss: 0.066\n",
            "[39,    40] loss: 0.060\n",
            "[40,    10] loss: 0.062\n",
            "[40,    20] loss: 0.050\n",
            "[40,    30] loss: 0.063\n",
            "[40,    40] loss: 0.053\n",
            "[41,    10] loss: 0.067\n",
            "[41,    20] loss: 0.074\n",
            "[41,    30] loss: 0.055\n",
            "[41,    40] loss: 0.050\n",
            "[42,    10] loss: 0.024\n",
            "[42,    20] loss: 0.026\n",
            "[42,    30] loss: 0.027\n",
            "[42,    40] loss: 0.033\n",
            "[43,    10] loss: 0.104\n",
            "[43,    20] loss: 0.084\n",
            "[43,    30] loss: 0.077\n",
            "[43,    40] loss: 0.066\n",
            "[44,    10] loss: 0.087\n",
            "[44,    20] loss: 0.073\n",
            "[44,    30] loss: 0.061\n",
            "[44,    40] loss: 0.054\n",
            "[45,    10] loss: 0.022\n",
            "[45,    20] loss: 0.021\n",
            "[45,    30] loss: 0.015\n",
            "[45,    40] loss: 0.015\n",
            "[46,    10] loss: 0.006\n",
            "[46,    20] loss: 0.004\n",
            "[46,    30] loss: 0.004\n",
            "[46,    40] loss: 0.003\n",
            "[47,    10] loss: 0.002\n",
            "[47,    20] loss: 0.002\n",
            "[47,    30] loss: 0.002\n",
            "[47,    40] loss: 0.011\n",
            "[48,    10] loss: 0.024\n",
            "[48,    20] loss: 0.039\n",
            "[48,    30] loss: 0.030\n",
            "[48,    40] loss: 0.037\n",
            "[49,    10] loss: 0.013\n",
            "[49,    20] loss: 0.013\n",
            "[49,    30] loss: 0.012\n",
            "[49,    40] loss: 0.010\n",
            "[50,    10] loss: 0.003\n",
            "[50,    20] loss: 0.003\n",
            "[50,    30] loss: 0.005\n",
            "[50,    40] loss: 0.003\n",
            "[51,    10] loss: 0.001\n",
            "[51,    20] loss: 0.001\n",
            "[51,    30] loss: 0.001\n",
            "[51,    40] loss: 0.002\n",
            "[52,    10] loss: 0.001\n",
            "[52,    20] loss: 0.001\n",
            "[52,    30] loss: 0.001\n",
            "[52,    40] loss: 0.001\n",
            "[53,    10] loss: 0.001\n",
            "[53,    20] loss: 0.001\n",
            "[53,    30] loss: 0.001\n",
            "[53,    40] loss: 0.014\n",
            "[54,    10] loss: 0.039\n",
            "[54,    20] loss: 0.068\n",
            "[54,    30] loss: 0.059\n",
            "[54,    40] loss: 0.049\n",
            "[55,    10] loss: 0.031\n",
            "[55,    20] loss: 0.044\n",
            "[55,    30] loss: 0.030\n",
            "[55,    40] loss: 0.028\n",
            "[56,    10] loss: 0.017\n",
            "[56,    20] loss: 0.016\n",
            "[56,    30] loss: 0.015\n",
            "[56,    40] loss: 0.018\n",
            "[57,    10] loss: 0.066\n",
            "[57,    20] loss: 0.063\n",
            "[57,    30] loss: 0.054\n",
            "[57,    40] loss: 0.043\n",
            "[58,    10] loss: 0.031\n",
            "[58,    20] loss: 0.046\n",
            "[58,    30] loss: 0.029\n",
            "[58,    40] loss: 0.025\n",
            "[59,    10] loss: 0.011\n",
            "[59,    20] loss: 0.007\n",
            "[59,    30] loss: 0.004\n",
            "[59,    40] loss: 0.027\n",
            "[60,    10] loss: 0.083\n",
            "[60,    20] loss: 0.112\n",
            "[60,    30] loss: 0.086\n",
            "[60,    40] loss: 0.104\n",
            "[61,    10] loss: 0.227\n",
            "[61,    20] loss: 0.284\n",
            "[61,    30] loss: 0.181\n",
            "[61,    40] loss: 0.197\n",
            "[62,    10] loss: 0.152\n",
            "[62,    20] loss: 0.139\n",
            "[62,    30] loss: 0.121\n",
            "[62,    40] loss: 0.091\n",
            "[63,    10] loss: 0.040\n",
            "[63,    20] loss: 0.033\n",
            "[63,    30] loss: 0.030\n",
            "[63,    40] loss: 0.040\n",
            "[64,    10] loss: 0.064\n",
            "[64,    20] loss: 0.076\n",
            "[64,    30] loss: 0.051\n",
            "[64,    40] loss: 0.048\n",
            "[65,    10] loss: 0.043\n",
            "[65,    20] loss: 0.036\n",
            "[65,    30] loss: 0.032\n",
            "[65,    40] loss: 0.031\n",
            "[66,    10] loss: 0.017\n",
            "[66,    20] loss: 0.020\n",
            "[66,    30] loss: 0.012\n",
            "[66,    40] loss: 0.013\n",
            "[67,    10] loss: 0.004\n",
            "[67,    20] loss: 0.004\n",
            "[67,    30] loss: 0.004\n",
            "[67,    40] loss: 0.005\n",
            "[68,    10] loss: 0.002\n",
            "[68,    20] loss: 0.002\n",
            "[68,    30] loss: 0.002\n",
            "[68,    40] loss: 0.003\n",
            "[69,    10] loss: 0.001\n",
            "[69,    20] loss: 0.001\n",
            "[69,    30] loss: 0.001\n",
            "[69,    40] loss: 0.001\n",
            "[70,    10] loss: 0.001\n",
            "[70,    20] loss: 0.001\n",
            "[70,    30] loss: 0.001\n",
            "[70,    40] loss: 0.017\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 train images: 100 %\n",
            "Accuracy of the network on the 10000 test dataset 1: 100 %\n",
            "Accuracy of the network on the 10000 test dataset 2: 93 %\n",
            "Accuracy of the network on the 10000 test dataset 3: 78 %\n",
            "Accuracy of the network on the 10000 test dataset 4: 72 %\n",
            "Accuracy of the network on the 10000 test dataset 5: 71 %\n",
            "Accuracy of the network on the 10000 test dataset 6: 70 %\n",
            "Accuracy of the network on the 10000 test dataset 7: 70 %\n",
            "Accuracy of the network on the 10000 test dataset 8: 70 %\n",
            "Accuracy of the network on the 10000 test dataset 9: 70 %\n",
            "Accuracy of the network on the 10000 test dataset 10: 68 %\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "training on data set   2\n",
            "[1,    10] loss: 1.017\n",
            "[1,    20] loss: 0.917\n",
            "[1,    30] loss: 0.826\n",
            "[1,    40] loss: 0.832\n",
            "[2,    10] loss: 0.802\n",
            "[2,    20] loss: 0.782\n",
            "[2,    30] loss: 0.765\n",
            "[2,    40] loss: 0.742\n",
            "[3,    10] loss: 0.727\n",
            "[3,    20] loss: 0.707\n",
            "[3,    30] loss: 0.684\n",
            "[3,    40] loss: 0.702\n",
            "[4,    10] loss: 0.663\n",
            "[4,    20] loss: 0.683\n",
            "[4,    30] loss: 0.665\n",
            "[4,    40] loss: 0.635\n",
            "[5,    10] loss: 0.638\n",
            "[5,    20] loss: 0.630\n",
            "[5,    30] loss: 0.600\n",
            "[5,    40] loss: 0.580\n",
            "[6,    10] loss: 0.539\n",
            "[6,    20] loss: 0.542\n",
            "[6,    30] loss: 0.542\n",
            "[6,    40] loss: 0.524\n",
            "[7,    10] loss: 0.496\n",
            "[7,    20] loss: 0.494\n",
            "[7,    30] loss: 0.491\n",
            "[7,    40] loss: 0.471\n",
            "[8,    10] loss: 0.453\n",
            "[8,    20] loss: 0.480\n",
            "[8,    30] loss: 0.466\n",
            "[8,    40] loss: 0.456\n",
            "[9,    10] loss: 0.389\n",
            "[9,    20] loss: 0.402\n",
            "[9,    30] loss: 0.395\n",
            "[9,    40] loss: 0.409\n",
            "[10,    10] loss: 0.345\n",
            "[10,    20] loss: 0.330\n",
            "[10,    30] loss: 0.301\n",
            "[10,    40] loss: 0.317\n",
            "[11,    10] loss: 0.323\n",
            "[11,    20] loss: 0.367\n",
            "[11,    30] loss: 0.331\n",
            "[11,    40] loss: 0.263\n",
            "[12,    10] loss: 0.215\n",
            "[12,    20] loss: 0.200\n",
            "[12,    30] loss: 0.192\n",
            "[12,    40] loss: 0.164\n",
            "[13,    10] loss: 0.106\n",
            "[13,    20] loss: 0.095\n",
            "[13,    30] loss: 0.094\n",
            "[13,    40] loss: 0.086\n",
            "[14,    10] loss: 0.102\n",
            "[14,    20] loss: 0.110\n",
            "[14,    30] loss: 0.112\n",
            "[14,    40] loss: 0.096\n",
            "[15,    10] loss: 0.069\n",
            "[15,    20] loss: 0.080\n",
            "[15,    30] loss: 0.063\n",
            "[15,    40] loss: 0.048\n",
            "[16,    10] loss: 0.031\n",
            "[16,    20] loss: 0.032\n",
            "[16,    30] loss: 0.025\n",
            "[16,    40] loss: 0.026\n",
            "[17,    10] loss: 0.017\n",
            "[17,    20] loss: 0.014\n",
            "[17,    30] loss: 0.010\n",
            "[17,    40] loss: 0.011\n",
            "[18,    10] loss: 0.007\n",
            "[18,    20] loss: 0.007\n",
            "[18,    30] loss: 0.005\n",
            "[18,    40] loss: 0.003\n",
            "[19,    10] loss: 0.003\n",
            "[19,    20] loss: 0.002\n",
            "[19,    30] loss: 0.002\n",
            "[19,    40] loss: 0.008\n",
            "[20,    10] loss: 0.023\n",
            "[20,    20] loss: 0.031\n",
            "[20,    30] loss: 0.023\n",
            "[20,    40] loss: 0.024\n",
            "[21,    10] loss: 0.074\n",
            "[21,    20] loss: 0.096\n",
            "[21,    30] loss: 0.086\n",
            "[21,    40] loss: 0.078\n",
            "[22,    10] loss: 0.078\n",
            "[22,    20] loss: 0.087\n",
            "[22,    30] loss: 0.088\n",
            "[22,    40] loss: 0.110\n",
            "[23,    10] loss: 0.090\n",
            "[23,    20] loss: 0.075\n",
            "[23,    30] loss: 0.067\n",
            "[23,    40] loss: 0.074\n",
            "[24,    10] loss: 0.086\n",
            "[24,    20] loss: 0.077\n",
            "[24,    30] loss: 0.072\n",
            "[24,    40] loss: 0.090\n",
            "[25,    10] loss: 0.131\n",
            "[25,    20] loss: 0.161\n",
            "[25,    30] loss: 0.133\n",
            "[25,    40] loss: 0.144\n",
            "[26,    10] loss: 0.239\n",
            "[26,    20] loss: 0.225\n",
            "[26,    30] loss: 0.221\n",
            "[26,    40] loss: 0.175\n",
            "[27,    10] loss: 0.086\n",
            "[27,    20] loss: 0.079\n",
            "[27,    30] loss: 0.048\n",
            "[27,    40] loss: 0.057\n",
            "[28,    10] loss: 0.054\n",
            "[28,    20] loss: 0.068\n",
            "[28,    30] loss: 0.060\n",
            "[28,    40] loss: 0.054\n",
            "[29,    10] loss: 0.036\n",
            "[29,    20] loss: 0.031\n",
            "[29,    30] loss: 0.031\n",
            "[29,    40] loss: 0.052\n",
            "[30,    10] loss: 0.097\n",
            "[30,    20] loss: 0.107\n",
            "[30,    30] loss: 0.074\n",
            "[30,    40] loss: 0.050\n",
            "[31,    10] loss: 0.038\n",
            "[31,    20] loss: 0.035\n",
            "[31,    30] loss: 0.031\n",
            "[31,    40] loss: 0.025\n",
            "[32,    10] loss: 0.014\n",
            "[32,    20] loss: 0.013\n",
            "[32,    30] loss: 0.010\n",
            "[32,    40] loss: 0.006\n",
            "[33,    10] loss: 0.003\n",
            "[33,    20] loss: 0.003\n",
            "[33,    30] loss: 0.003\n",
            "[33,    40] loss: 0.008\n",
            "[34,    10] loss: 0.017\n",
            "[34,    20] loss: 0.019\n",
            "[34,    30] loss: 0.014\n",
            "[34,    40] loss: 0.008\n",
            "[35,    10] loss: 0.005\n",
            "[35,    20] loss: 0.003\n",
            "[35,    30] loss: 0.002\n",
            "[35,    40] loss: 0.009\n",
            "[36,    10] loss: 0.028\n",
            "[36,    20] loss: 0.035\n",
            "[36,    30] loss: 0.019\n",
            "[36,    40] loss: 0.021\n",
            "[37,    10] loss: 0.009\n",
            "[37,    20] loss: 0.013\n",
            "[37,    30] loss: 0.010\n",
            "[37,    40] loss: 0.007\n",
            "[38,    10] loss: 0.005\n",
            "[38,    20] loss: 0.004\n",
            "[38,    30] loss: 0.002\n",
            "[38,    40] loss: 0.003\n",
            "[39,    10] loss: 0.001\n",
            "[39,    20] loss: 0.001\n",
            "[39,    30] loss: 0.001\n",
            "[39,    40] loss: 0.001\n",
            "[40,    10] loss: 0.001\n",
            "[40,    20] loss: 0.001\n",
            "[40,    30] loss: 0.001\n",
            "[40,    40] loss: 0.001\n",
            "[41,    10] loss: 0.001\n",
            "[41,    20] loss: 0.001\n",
            "[41,    30] loss: 0.001\n",
            "[41,    40] loss: 0.003\n",
            "[42,    10] loss: 0.010\n",
            "[42,    20] loss: 0.007\n",
            "[42,    30] loss: 0.004\n",
            "[42,    40] loss: 0.004\n",
            "[43,    10] loss: 0.001\n",
            "[43,    20] loss: 0.002\n",
            "[43,    30] loss: 0.001\n",
            "[43,    40] loss: 0.001\n",
            "[44,    10] loss: 0.001\n",
            "[44,    20] loss: 0.001\n",
            "[44,    30] loss: 0.001\n",
            "[44,    40] loss: 0.001\n",
            "[45,    10] loss: 0.001\n",
            "[45,    20] loss: 0.000\n",
            "[45,    30] loss: 0.001\n",
            "[45,    40] loss: 0.000\n",
            "[46,    10] loss: 0.000\n",
            "[46,    20] loss: 0.001\n",
            "[46,    30] loss: 0.000\n",
            "[46,    40] loss: 0.000\n",
            "[47,    10] loss: 0.000\n",
            "[47,    20] loss: 0.000\n",
            "[47,    30] loss: 0.000\n",
            "[47,    40] loss: 0.001\n",
            "[48,    10] loss: 0.000\n",
            "[48,    20] loss: 0.000\n",
            "[48,    30] loss: 0.000\n",
            "[48,    40] loss: 0.002\n",
            "[49,    10] loss: 0.001\n",
            "[49,    20] loss: 0.002\n",
            "[49,    30] loss: 0.001\n",
            "[49,    40] loss: 0.001\n",
            "[50,    10] loss: 0.001\n",
            "[50,    20] loss: 0.000\n",
            "[50,    30] loss: 0.001\n",
            "[50,    40] loss: 0.000\n",
            "[51,    10] loss: 0.000\n",
            "[51,    20] loss: 0.000\n",
            "[51,    30] loss: 0.000\n",
            "[51,    40] loss: 0.000\n",
            "[52,    10] loss: 0.000\n",
            "[52,    20] loss: 0.000\n",
            "[52,    30] loss: 0.000\n",
            "[52,    40] loss: 0.000\n",
            "[53,    10] loss: 0.000\n",
            "[53,    20] loss: 0.000\n",
            "[53,    30] loss: 0.000\n",
            "[53,    40] loss: 0.011\n",
            "[54,    10] loss: 0.031\n",
            "[54,    20] loss: 0.060\n",
            "[54,    30] loss: 0.039\n",
            "[54,    40] loss: 0.018\n",
            "[55,    10] loss: 0.016\n",
            "[55,    20] loss: 0.010\n",
            "[55,    30] loss: 0.007\n",
            "[55,    40] loss: 0.012\n",
            "[56,    10] loss: 0.021\n",
            "[56,    20] loss: 0.012\n",
            "[56,    30] loss: 0.016\n",
            "[56,    40] loss: 0.015\n",
            "[57,    10] loss: 0.005\n",
            "[57,    20] loss: 0.005\n",
            "[57,    30] loss: 0.003\n",
            "[57,    40] loss: 0.004\n",
            "[58,    10] loss: 0.001\n",
            "[58,    20] loss: 0.001\n",
            "[58,    30] loss: 0.001\n",
            "[58,    40] loss: 0.001\n",
            "[59,    10] loss: 0.001\n",
            "[59,    20] loss: 0.001\n",
            "[59,    30] loss: 0.001\n",
            "[59,    40] loss: 0.001\n",
            "[60,    10] loss: 0.000\n",
            "[60,    20] loss: 0.000\n",
            "[60,    30] loss: 0.000\n",
            "[60,    40] loss: 0.003\n",
            "[61,    10] loss: 0.003\n",
            "[61,    20] loss: 0.003\n",
            "[61,    30] loss: 0.001\n",
            "[61,    40] loss: 0.005\n",
            "[62,    10] loss: 0.006\n",
            "[62,    20] loss: 0.009\n",
            "[62,    30] loss: 0.006\n",
            "[62,    40] loss: 0.012\n",
            "[63,    10] loss: 0.021\n",
            "[63,    20] loss: 0.022\n",
            "[63,    30] loss: 0.022\n",
            "[63,    40] loss: 0.035\n",
            "[64,    10] loss: 0.024\n",
            "[64,    20] loss: 0.023\n",
            "[64,    30] loss: 0.020\n",
            "[64,    40] loss: 0.016\n",
            "[65,    10] loss: 0.009\n",
            "[65,    20] loss: 0.005\n",
            "[65,    30] loss: 0.005\n",
            "[65,    40] loss: 0.009\n",
            "[66,    10] loss: 0.036\n",
            "[66,    20] loss: 0.048\n",
            "[66,    30] loss: 0.032\n",
            "[66,    40] loss: 0.020\n",
            "[67,    10] loss: 0.021\n",
            "[67,    20] loss: 0.020\n",
            "[67,    30] loss: 0.020\n",
            "[67,    40] loss: 0.016\n",
            "[68,    10] loss: 0.009\n",
            "[68,    20] loss: 0.009\n",
            "[68,    30] loss: 0.004\n",
            "[68,    40] loss: 0.003\n",
            "[69,    10] loss: 0.001\n",
            "[69,    20] loss: 0.001\n",
            "[69,    30] loss: 0.001\n",
            "[69,    40] loss: 0.002\n",
            "[70,    10] loss: 0.001\n",
            "[70,    20] loss: 0.001\n",
            "[70,    30] loss: 0.001\n",
            "[70,    40] loss: 0.001\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 train images: 100 %\n",
            "Accuracy of the network on the 10000 test dataset 1: 73 %\n",
            "Accuracy of the network on the 10000 test dataset 2: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 3: 98 %\n",
            "Accuracy of the network on the 10000 test dataset 4: 95 %\n",
            "Accuracy of the network on the 10000 test dataset 5: 93 %\n",
            "Accuracy of the network on the 10000 test dataset 6: 92 %\n",
            "Accuracy of the network on the 10000 test dataset 7: 91 %\n",
            "Accuracy of the network on the 10000 test dataset 8: 91 %\n",
            "Accuracy of the network on the 10000 test dataset 9: 91 %\n",
            "Accuracy of the network on the 10000 test dataset 10: 88 %\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "training on data set   3\n",
            "[1,    10] loss: 0.963\n",
            "[1,    20] loss: 0.767\n",
            "[1,    30] loss: 0.663\n",
            "[1,    40] loss: 0.638\n",
            "[2,    10] loss: 0.620\n",
            "[2,    20] loss: 0.610\n",
            "[2,    30] loss: 0.567\n",
            "[2,    40] loss: 0.522\n",
            "[3,    10] loss: 0.488\n",
            "[3,    20] loss: 0.505\n",
            "[3,    30] loss: 0.473\n",
            "[3,    40] loss: 0.464\n",
            "[4,    10] loss: 0.470\n",
            "[4,    20] loss: 0.441\n",
            "[4,    30] loss: 0.431\n",
            "[4,    40] loss: 0.416\n",
            "[5,    10] loss: 0.419\n",
            "[5,    20] loss: 0.377\n",
            "[5,    30] loss: 0.416\n",
            "[5,    40] loss: 0.371\n",
            "[6,    10] loss: 0.326\n",
            "[6,    20] loss: 0.315\n",
            "[6,    30] loss: 0.267\n",
            "[6,    40] loss: 0.285\n",
            "[7,    10] loss: 0.314\n",
            "[7,    20] loss: 0.326\n",
            "[7,    30] loss: 0.279\n",
            "[7,    40] loss: 0.268\n",
            "[8,    10] loss: 0.197\n",
            "[8,    20] loss: 0.183\n",
            "[8,    30] loss: 0.180\n",
            "[8,    40] loss: 0.192\n",
            "[9,    10] loss: 0.206\n",
            "[9,    20] loss: 0.199\n",
            "[9,    30] loss: 0.192\n",
            "[9,    40] loss: 0.158\n",
            "[10,    10] loss: 0.132\n",
            "[10,    20] loss: 0.118\n",
            "[10,    30] loss: 0.105\n",
            "[10,    40] loss: 0.104\n",
            "[11,    10] loss: 0.120\n",
            "[11,    20] loss: 0.120\n",
            "[11,    30] loss: 0.095\n",
            "[11,    40] loss: 0.090\n",
            "[12,    10] loss: 0.112\n",
            "[12,    20] loss: 0.119\n",
            "[12,    30] loss: 0.088\n",
            "[12,    40] loss: 0.120\n",
            "[13,    10] loss: 0.246\n",
            "[13,    20] loss: 0.249\n",
            "[13,    30] loss: 0.175\n",
            "[13,    40] loss: 0.150\n",
            "[14,    10] loss: 0.121\n",
            "[14,    20] loss: 0.099\n",
            "[14,    30] loss: 0.079\n",
            "[14,    40] loss: 0.083\n",
            "[15,    10] loss: 0.105\n",
            "[15,    20] loss: 0.091\n",
            "[15,    30] loss: 0.071\n",
            "[15,    40] loss: 0.111\n",
            "[16,    10] loss: 0.219\n",
            "[16,    20] loss: 0.214\n",
            "[16,    30] loss: 0.206\n",
            "[16,    40] loss: 0.154\n",
            "[17,    10] loss: 0.110\n",
            "[17,    20] loss: 0.097\n",
            "[17,    30] loss: 0.085\n",
            "[17,    40] loss: 0.078\n",
            "[18,    10] loss: 0.064\n",
            "[18,    20] loss: 0.057\n",
            "[18,    30] loss: 0.041\n",
            "[18,    40] loss: 0.037\n",
            "[19,    10] loss: 0.017\n",
            "[19,    20] loss: 0.013\n",
            "[19,    30] loss: 0.009\n",
            "[19,    40] loss: 0.014\n",
            "[20,    10] loss: 0.011\n",
            "[20,    20] loss: 0.010\n",
            "[20,    30] loss: 0.011\n",
            "[20,    40] loss: 0.011\n",
            "[21,    10] loss: 0.017\n",
            "[21,    20] loss: 0.010\n",
            "[21,    30] loss: 0.007\n",
            "[21,    40] loss: 0.012\n",
            "[22,    10] loss: 0.023\n",
            "[22,    20] loss: 0.015\n",
            "[22,    30] loss: 0.013\n",
            "[22,    40] loss: 0.019\n",
            "[23,    10] loss: 0.051\n",
            "[23,    20] loss: 0.047\n",
            "[23,    30] loss: 0.035\n",
            "[23,    40] loss: 0.027\n",
            "[24,    10] loss: 0.015\n",
            "[24,    20] loss: 0.007\n",
            "[24,    30] loss: 0.007\n",
            "[24,    40] loss: 0.007\n",
            "[25,    10] loss: 0.003\n",
            "[25,    20] loss: 0.002\n",
            "[25,    30] loss: 0.002\n",
            "[25,    40] loss: 0.002\n",
            "[26,    10] loss: 0.001\n",
            "[26,    20] loss: 0.001\n",
            "[26,    30] loss: 0.001\n",
            "[26,    40] loss: 0.001\n",
            "[27,    10] loss: 0.001\n",
            "[27,    20] loss: 0.001\n",
            "[27,    30] loss: 0.001\n",
            "[27,    40] loss: 0.002\n",
            "[28,    10] loss: 0.001\n",
            "[28,    20] loss: 0.002\n",
            "[28,    30] loss: 0.001\n",
            "[28,    40] loss: 0.001\n",
            "[29,    10] loss: 0.001\n",
            "[29,    20] loss: 0.001\n",
            "[29,    30] loss: 0.001\n",
            "[29,    40] loss: 0.001\n",
            "[30,    10] loss: 0.001\n",
            "[30,    20] loss: 0.001\n",
            "[30,    30] loss: 0.001\n",
            "[30,    40] loss: 0.002\n",
            "[31,    10] loss: 0.001\n",
            "[31,    20] loss: 0.002\n",
            "[31,    30] loss: 0.001\n",
            "[31,    40] loss: 0.002\n",
            "[32,    10] loss: 0.001\n",
            "[32,    20] loss: 0.001\n",
            "[32,    30] loss: 0.001\n",
            "[32,    40] loss: 0.001\n",
            "[33,    10] loss: 0.001\n",
            "[33,    20] loss: 0.001\n",
            "[33,    30] loss: 0.001\n",
            "[33,    40] loss: 0.001\n",
            "[34,    10] loss: 0.000\n",
            "[34,    20] loss: 0.001\n",
            "[34,    30] loss: 0.000\n",
            "[34,    40] loss: 0.001\n",
            "[35,    10] loss: 0.000\n",
            "[35,    20] loss: 0.000\n",
            "[35,    30] loss: 0.000\n",
            "[35,    40] loss: 0.001\n",
            "[36,    10] loss: 0.000\n",
            "[36,    20] loss: 0.001\n",
            "[36,    30] loss: 0.000\n",
            "[36,    40] loss: 0.001\n",
            "[37,    10] loss: 0.000\n",
            "[37,    20] loss: 0.000\n",
            "[37,    30] loss: 0.000\n",
            "[37,    40] loss: 0.004\n",
            "[38,    10] loss: 0.015\n",
            "[38,    20] loss: 0.015\n",
            "[38,    30] loss: 0.008\n",
            "[38,    40] loss: 0.008\n",
            "[39,    10] loss: 0.021\n",
            "[39,    20] loss: 0.015\n",
            "[39,    30] loss: 0.018\n",
            "[39,    40] loss: 0.084\n",
            "[40,    10] loss: 0.402\n",
            "[40,    20] loss: 0.293\n",
            "[40,    30] loss: 0.291\n",
            "[40,    40] loss: 0.263\n",
            "[41,    10] loss: 0.207\n",
            "[41,    20] loss: 0.142\n",
            "[41,    30] loss: 0.142\n",
            "[41,    40] loss: 0.122\n",
            "[42,    10] loss: 0.094\n",
            "[42,    20] loss: 0.086\n",
            "[42,    30] loss: 0.064\n",
            "[42,    40] loss: 0.048\n",
            "[43,    10] loss: 0.028\n",
            "[43,    20] loss: 0.022\n",
            "[43,    30] loss: 0.018\n",
            "[43,    40] loss: 0.012\n",
            "[44,    10] loss: 0.006\n",
            "[44,    20] loss: 0.006\n",
            "[44,    30] loss: 0.006\n",
            "[44,    40] loss: 0.109\n",
            "[45,    10] loss: 0.220\n",
            "[45,    20] loss: 0.200\n",
            "[45,    30] loss: 0.141\n",
            "[45,    40] loss: 0.085\n",
            "[46,    10] loss: 0.051\n",
            "[46,    20] loss: 0.039\n",
            "[46,    30] loss: 0.031\n",
            "[46,    40] loss: 0.022\n",
            "[47,    10] loss: 0.009\n",
            "[47,    20] loss: 0.007\n",
            "[47,    30] loss: 0.009\n",
            "[47,    40] loss: 0.008\n",
            "[48,    10] loss: 0.005\n",
            "[48,    20] loss: 0.005\n",
            "[48,    30] loss: 0.004\n",
            "[48,    40] loss: 0.005\n",
            "[49,    10] loss: 0.006\n",
            "[49,    20] loss: 0.007\n",
            "[49,    30] loss: 0.004\n",
            "[49,    40] loss: 0.003\n",
            "[50,    10] loss: 0.002\n",
            "[50,    20] loss: 0.002\n",
            "[50,    30] loss: 0.002\n",
            "[50,    40] loss: 0.003\n",
            "[51,    10] loss: 0.002\n",
            "[51,    20] loss: 0.001\n",
            "[51,    30] loss: 0.001\n",
            "[51,    40] loss: 0.002\n",
            "[52,    10] loss: 0.001\n",
            "[52,    20] loss: 0.001\n",
            "[52,    30] loss: 0.001\n",
            "[52,    40] loss: 0.001\n",
            "[53,    10] loss: 0.001\n",
            "[53,    20] loss: 0.002\n",
            "[53,    30] loss: 0.001\n",
            "[53,    40] loss: 0.001\n",
            "[54,    10] loss: 0.001\n",
            "[54,    20] loss: 0.001\n",
            "[54,    30] loss: 0.001\n",
            "[54,    40] loss: 0.001\n",
            "[55,    10] loss: 0.001\n",
            "[55,    20] loss: 0.001\n",
            "[55,    30] loss: 0.001\n",
            "[55,    40] loss: 0.008\n",
            "[56,    10] loss: 0.022\n",
            "[56,    20] loss: 0.023\n",
            "[56,    30] loss: 0.014\n",
            "[56,    40] loss: 0.012\n",
            "[57,    10] loss: 0.004\n",
            "[57,    20] loss: 0.005\n",
            "[57,    30] loss: 0.004\n",
            "[57,    40] loss: 0.002\n",
            "[58,    10] loss: 0.002\n",
            "[58,    20] loss: 0.001\n",
            "[58,    30] loss: 0.001\n",
            "[58,    40] loss: 0.001\n",
            "[59,    10] loss: 0.001\n",
            "[59,    20] loss: 0.001\n",
            "[59,    30] loss: 0.001\n",
            "[59,    40] loss: 0.001\n",
            "[60,    10] loss: 0.001\n",
            "[60,    20] loss: 0.001\n",
            "[60,    30] loss: 0.001\n",
            "[60,    40] loss: 0.063\n",
            "[61,    10] loss: 0.130\n",
            "[61,    20] loss: 0.114\n",
            "[61,    30] loss: 0.075\n",
            "[61,    40] loss: 0.057\n",
            "[62,    10] loss: 0.023\n",
            "[62,    20] loss: 0.014\n",
            "[62,    30] loss: 0.018\n",
            "[62,    40] loss: 0.018\n",
            "[63,    10] loss: 0.015\n",
            "[63,    20] loss: 0.015\n",
            "[63,    30] loss: 0.012\n",
            "[63,    40] loss: 0.013\n",
            "[64,    10] loss: 0.011\n",
            "[64,    20] loss: 0.007\n",
            "[64,    30] loss: 0.005\n",
            "[64,    40] loss: 0.003\n",
            "[65,    10] loss: 0.002\n",
            "[65,    20] loss: 0.002\n",
            "[65,    30] loss: 0.001\n",
            "[65,    40] loss: 0.002\n",
            "[66,    10] loss: 0.001\n",
            "[66,    20] loss: 0.001\n",
            "[66,    30] loss: 0.001\n",
            "[66,    40] loss: 0.001\n",
            "[67,    10] loss: 0.001\n",
            "[67,    20] loss: 0.001\n",
            "[67,    30] loss: 0.001\n",
            "[67,    40] loss: 0.002\n",
            "[68,    10] loss: 0.001\n",
            "[68,    20] loss: 0.001\n",
            "[68,    30] loss: 0.001\n",
            "[68,    40] loss: 0.001\n",
            "[69,    10] loss: 0.001\n",
            "[69,    20] loss: 0.001\n",
            "[69,    30] loss: 0.001\n",
            "[69,    40] loss: 0.001\n",
            "[70,    10] loss: 0.001\n",
            "[70,    20] loss: 0.001\n",
            "[70,    30] loss: 0.001\n",
            "[70,    40] loss: 0.001\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 train images: 100 %\n",
            "Accuracy of the network on the 10000 test dataset 1: 51 %\n",
            "Accuracy of the network on the 10000 test dataset 2: 92 %\n",
            "Accuracy of the network on the 10000 test dataset 3: 100 %\n",
            "Accuracy of the network on the 10000 test dataset 4: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 5: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 6: 98 %\n",
            "Accuracy of the network on the 10000 test dataset 7: 98 %\n",
            "Accuracy of the network on the 10000 test dataset 8: 97 %\n",
            "Accuracy of the network on the 10000 test dataset 9: 97 %\n",
            "Accuracy of the network on the 10000 test dataset 10: 93 %\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "training on data set   4\n",
            "[1,    10] loss: 0.927\n",
            "[1,    20] loss: 0.688\n",
            "[1,    30] loss: 0.568\n",
            "[1,    40] loss: 0.604\n",
            "[2,    10] loss: 0.557\n",
            "[2,    20] loss: 0.521\n",
            "[2,    30] loss: 0.512\n",
            "[2,    40] loss: 0.511\n",
            "[3,    10] loss: 0.513\n",
            "[3,    20] loss: 0.450\n",
            "[3,    30] loss: 0.418\n",
            "[3,    40] loss: 0.428\n",
            "[4,    10] loss: 0.378\n",
            "[4,    20] loss: 0.381\n",
            "[4,    30] loss: 0.349\n",
            "[4,    40] loss: 0.308\n",
            "[5,    10] loss: 0.269\n",
            "[5,    20] loss: 0.261\n",
            "[5,    30] loss: 0.293\n",
            "[5,    40] loss: 0.273\n",
            "[6,    10] loss: 0.269\n",
            "[6,    20] loss: 0.233\n",
            "[6,    30] loss: 0.231\n",
            "[6,    40] loss: 0.216\n",
            "[7,    10] loss: 0.204\n",
            "[7,    20] loss: 0.209\n",
            "[7,    30] loss: 0.180\n",
            "[7,    40] loss: 0.182\n",
            "[8,    10] loss: 0.163\n",
            "[8,    20] loss: 0.151\n",
            "[8,    30] loss: 0.148\n",
            "[8,    40] loss: 0.114\n",
            "[9,    10] loss: 0.088\n",
            "[9,    20] loss: 0.078\n",
            "[9,    30] loss: 0.073\n",
            "[9,    40] loss: 0.076\n",
            "[10,    10] loss: 0.054\n",
            "[10,    20] loss: 0.047\n",
            "[10,    30] loss: 0.052\n",
            "[10,    40] loss: 0.055\n",
            "[11,    10] loss: 0.088\n",
            "[11,    20] loss: 0.073\n",
            "[11,    30] loss: 0.071\n",
            "[11,    40] loss: 0.084\n",
            "[12,    10] loss: 0.123\n",
            "[12,    20] loss: 0.107\n",
            "[12,    30] loss: 0.103\n",
            "[12,    40] loss: 0.099\n",
            "[13,    10] loss: 0.103\n",
            "[13,    20] loss: 0.091\n",
            "[13,    30] loss: 0.067\n",
            "[13,    40] loss: 0.091\n",
            "[14,    10] loss: 0.153\n",
            "[14,    20] loss: 0.132\n",
            "[14,    30] loss: 0.103\n",
            "[14,    40] loss: 0.095\n",
            "[15,    10] loss: 0.087\n",
            "[15,    20] loss: 0.075\n",
            "[15,    30] loss: 0.052\n",
            "[15,    40] loss: 0.043\n",
            "[16,    10] loss: 0.031\n",
            "[16,    20] loss: 0.028\n",
            "[16,    30] loss: 0.024\n",
            "[16,    40] loss: 0.041\n",
            "[17,    10] loss: 0.132\n",
            "[17,    20] loss: 0.085\n",
            "[17,    30] loss: 0.066\n",
            "[17,    40] loss: 0.057\n",
            "[18,    10] loss: 0.034\n",
            "[18,    20] loss: 0.025\n",
            "[18,    30] loss: 0.016\n",
            "[18,    40] loss: 0.020\n",
            "[19,    10] loss: 0.015\n",
            "[19,    20] loss: 0.014\n",
            "[19,    30] loss: 0.008\n",
            "[19,    40] loss: 0.009\n",
            "[20,    10] loss: 0.005\n",
            "[20,    20] loss: 0.004\n",
            "[20,    30] loss: 0.004\n",
            "[20,    40] loss: 0.004\n",
            "[21,    10] loss: 0.002\n",
            "[21,    20] loss: 0.002\n",
            "[21,    30] loss: 0.002\n",
            "[21,    40] loss: 0.004\n",
            "[22,    10] loss: 0.003\n",
            "[22,    20] loss: 0.006\n",
            "[22,    30] loss: 0.003\n",
            "[22,    40] loss: 0.006\n",
            "[23,    10] loss: 0.007\n",
            "[23,    20] loss: 0.007\n",
            "[23,    30] loss: 0.003\n",
            "[23,    40] loss: 0.005\n",
            "[24,    10] loss: 0.007\n",
            "[24,    20] loss: 0.005\n",
            "[24,    30] loss: 0.004\n",
            "[24,    40] loss: 0.002\n",
            "[25,    10] loss: 0.002\n",
            "[25,    20] loss: 0.002\n",
            "[25,    30] loss: 0.001\n",
            "[25,    40] loss: 0.001\n",
            "[26,    10] loss: 0.001\n",
            "[26,    20] loss: 0.001\n",
            "[26,    30] loss: 0.001\n",
            "[26,    40] loss: 0.001\n",
            "[27,    10] loss: 0.001\n",
            "[27,    20] loss: 0.001\n",
            "[27,    30] loss: 0.001\n",
            "[27,    40] loss: 0.005\n",
            "[28,    10] loss: 0.015\n",
            "[28,    20] loss: 0.024\n",
            "[28,    30] loss: 0.019\n",
            "[28,    40] loss: 0.008\n",
            "[29,    10] loss: 0.007\n",
            "[29,    20] loss: 0.004\n",
            "[29,    30] loss: 0.003\n",
            "[29,    40] loss: 0.003\n",
            "[30,    10] loss: 0.001\n",
            "[30,    20] loss: 0.001\n",
            "[30,    30] loss: 0.002\n",
            "[30,    40] loss: 0.038\n",
            "[31,    10] loss: 0.210\n",
            "[31,    20] loss: 0.184\n",
            "[31,    30] loss: 0.129\n",
            "[31,    40] loss: 0.096\n",
            "[32,    10] loss: 0.061\n",
            "[32,    20] loss: 0.045\n",
            "[32,    30] loss: 0.024\n",
            "[32,    40] loss: 0.041\n",
            "[33,    10] loss: 0.062\n",
            "[33,    20] loss: 0.047\n",
            "[33,    30] loss: 0.040\n",
            "[33,    40] loss: 0.030\n",
            "[34,    10] loss: 0.029\n",
            "[34,    20] loss: 0.015\n",
            "[34,    30] loss: 0.013\n",
            "[34,    40] loss: 0.010\n",
            "[35,    10] loss: 0.006\n",
            "[35,    20] loss: 0.005\n",
            "[35,    30] loss: 0.004\n",
            "[35,    40] loss: 0.008\n",
            "[36,    10] loss: 0.008\n",
            "[36,    20] loss: 0.005\n",
            "[36,    30] loss: 0.004\n",
            "[36,    40] loss: 0.006\n",
            "[37,    10] loss: 0.005\n",
            "[37,    20] loss: 0.003\n",
            "[37,    30] loss: 0.003\n",
            "[37,    40] loss: 0.002\n",
            "[38,    10] loss: 0.001\n",
            "[38,    20] loss: 0.001\n",
            "[38,    30] loss: 0.001\n",
            "[38,    40] loss: 0.006\n",
            "[39,    10] loss: 0.008\n",
            "[39,    20] loss: 0.007\n",
            "[39,    30] loss: 0.005\n",
            "[39,    40] loss: 0.004\n",
            "[40,    10] loss: 0.002\n",
            "[40,    20] loss: 0.003\n",
            "[40,    30] loss: 0.002\n",
            "[40,    40] loss: 0.001\n",
            "[41,    10] loss: 0.001\n",
            "[41,    20] loss: 0.001\n",
            "[41,    30] loss: 0.001\n",
            "[41,    40] loss: 0.012\n",
            "[42,    10] loss: 0.059\n",
            "[42,    20] loss: 0.039\n",
            "[42,    30] loss: 0.023\n",
            "[42,    40] loss: 0.040\n",
            "[43,    10] loss: 0.116\n",
            "[43,    20] loss: 0.062\n",
            "[43,    30] loss: 0.044\n",
            "[43,    40] loss: 0.053\n",
            "[44,    10] loss: 0.081\n",
            "[44,    20] loss: 0.068\n",
            "[44,    30] loss: 0.035\n",
            "[44,    40] loss: 0.093\n",
            "[45,    10] loss: 0.189\n",
            "[45,    20] loss: 0.173\n",
            "[45,    30] loss: 0.100\n",
            "[45,    40] loss: 0.071\n",
            "[46,    10] loss: 0.062\n",
            "[46,    20] loss: 0.044\n",
            "[46,    30] loss: 0.035\n",
            "[46,    40] loss: 0.030\n",
            "[47,    10] loss: 0.041\n",
            "[47,    20] loss: 0.036\n",
            "[47,    30] loss: 0.033\n",
            "[47,    40] loss: 0.035\n",
            "[48,    10] loss: 0.076\n",
            "[48,    20] loss: 0.062\n",
            "[48,    30] loss: 0.040\n",
            "[48,    40] loss: 0.023\n",
            "[49,    10] loss: 0.009\n",
            "[49,    20] loss: 0.009\n",
            "[49,    30] loss: 0.007\n",
            "[49,    40] loss: 0.008\n",
            "[50,    10] loss: 0.004\n",
            "[50,    20] loss: 0.006\n",
            "[50,    30] loss: 0.005\n",
            "[50,    40] loss: 0.004\n",
            "[51,    10] loss: 0.002\n",
            "[51,    20] loss: 0.003\n",
            "[51,    30] loss: 0.002\n",
            "[51,    40] loss: 0.002\n",
            "[52,    10] loss: 0.001\n",
            "[52,    20] loss: 0.002\n",
            "[52,    30] loss: 0.002\n",
            "[52,    40] loss: 0.008\n",
            "[53,    10] loss: 0.033\n",
            "[53,    20] loss: 0.030\n",
            "[53,    30] loss: 0.020\n",
            "[53,    40] loss: 0.008\n",
            "[54,    10] loss: 0.004\n",
            "[54,    20] loss: 0.004\n",
            "[54,    30] loss: 0.003\n",
            "[54,    40] loss: 0.006\n",
            "[55,    10] loss: 0.004\n",
            "[55,    20] loss: 0.006\n",
            "[55,    30] loss: 0.004\n",
            "[55,    40] loss: 0.008\n",
            "[56,    10] loss: 0.014\n",
            "[56,    20] loss: 0.017\n",
            "[56,    30] loss: 0.008\n",
            "[56,    40] loss: 0.005\n",
            "[57,    10] loss: 0.004\n",
            "[57,    20] loss: 0.002\n",
            "[57,    30] loss: 0.002\n",
            "[57,    40] loss: 0.002\n",
            "[58,    10] loss: 0.001\n",
            "[58,    20] loss: 0.001\n",
            "[58,    30] loss: 0.002\n",
            "[58,    40] loss: 0.003\n",
            "[59,    10] loss: 0.001\n",
            "[59,    20] loss: 0.002\n",
            "[59,    30] loss: 0.001\n",
            "[59,    40] loss: 0.004\n",
            "[60,    10] loss: 0.006\n",
            "[60,    20] loss: 0.007\n",
            "[60,    30] loss: 0.003\n",
            "[60,    40] loss: 0.006\n",
            "[61,    10] loss: 0.006\n",
            "[61,    20] loss: 0.009\n",
            "[61,    30] loss: 0.006\n",
            "[61,    40] loss: 0.003\n",
            "[62,    10] loss: 0.002\n",
            "[62,    20] loss: 0.001\n",
            "[62,    30] loss: 0.002\n",
            "[62,    40] loss: 0.001\n",
            "[63,    10] loss: 0.001\n",
            "[63,    20] loss: 0.001\n",
            "[63,    30] loss: 0.001\n",
            "[63,    40] loss: 0.003\n",
            "[64,    10] loss: 0.007\n",
            "[64,    20] loss: 0.002\n",
            "[64,    30] loss: 0.002\n",
            "[64,    40] loss: 0.002\n",
            "[65,    10] loss: 0.001\n",
            "[65,    20] loss: 0.001\n",
            "[65,    30] loss: 0.001\n",
            "[65,    40] loss: 0.001\n",
            "[66,    10] loss: 0.001\n",
            "[66,    20] loss: 0.000\n",
            "[66,    30] loss: 0.000\n",
            "[66,    40] loss: 0.001\n",
            "[67,    10] loss: 0.000\n",
            "[67,    20] loss: 0.000\n",
            "[67,    30] loss: 0.001\n",
            "[67,    40] loss: 0.001\n",
            "[68,    10] loss: 0.000\n",
            "[68,    20] loss: 0.000\n",
            "[68,    30] loss: 0.000\n",
            "[68,    40] loss: 0.000\n",
            "[69,    10] loss: 0.000\n",
            "[69,    20] loss: 0.000\n",
            "[69,    30] loss: 0.000\n",
            "[69,    40] loss: 0.024\n",
            "[70,    10] loss: 0.052\n",
            "[70,    20] loss: 0.047\n",
            "[70,    30] loss: 0.027\n",
            "[70,    40] loss: 0.020\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 train images: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 1: 47 %\n",
            "Accuracy of the network on the 10000 test dataset 2: 77 %\n",
            "Accuracy of the network on the 10000 test dataset 3: 96 %\n",
            "Accuracy of the network on the 10000 test dataset 4: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 5: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 6: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 7: 98 %\n",
            "Accuracy of the network on the 10000 test dataset 8: 98 %\n",
            "Accuracy of the network on the 10000 test dataset 9: 98 %\n",
            "Accuracy of the network on the 10000 test dataset 10: 93 %\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "training on data set   5\n",
            "[1,    10] loss: 0.884\n",
            "[1,    20] loss: 0.637\n",
            "[1,    30] loss: 0.553\n",
            "[1,    40] loss: 0.473\n",
            "[2,    10] loss: 0.475\n",
            "[2,    20] loss: 0.401\n",
            "[2,    30] loss: 0.426\n",
            "[2,    40] loss: 0.378\n",
            "[3,    10] loss: 0.406\n",
            "[3,    20] loss: 0.355\n",
            "[3,    30] loss: 0.338\n",
            "[3,    40] loss: 0.365\n",
            "[4,    10] loss: 0.352\n",
            "[4,    20] loss: 0.328\n",
            "[4,    30] loss: 0.297\n",
            "[4,    40] loss: 0.226\n",
            "[5,    10] loss: 0.207\n",
            "[5,    20] loss: 0.217\n",
            "[5,    30] loss: 0.196\n",
            "[5,    40] loss: 0.210\n",
            "[6,    10] loss: 0.227\n",
            "[6,    20] loss: 0.175\n",
            "[6,    30] loss: 0.174\n",
            "[6,    40] loss: 0.162\n",
            "[7,    10] loss: 0.155\n",
            "[7,    20] loss: 0.145\n",
            "[7,    30] loss: 0.137\n",
            "[7,    40] loss: 0.145\n",
            "[8,    10] loss: 0.157\n",
            "[8,    20] loss: 0.147\n",
            "[8,    30] loss: 0.119\n",
            "[8,    40] loss: 0.128\n",
            "[9,    10] loss: 0.122\n",
            "[9,    20] loss: 0.096\n",
            "[9,    30] loss: 0.096\n",
            "[9,    40] loss: 0.098\n",
            "[10,    10] loss: 0.102\n",
            "[10,    20] loss: 0.089\n",
            "[10,    30] loss: 0.071\n",
            "[10,    40] loss: 0.056\n",
            "[11,    10] loss: 0.031\n",
            "[11,    20] loss: 0.030\n",
            "[11,    30] loss: 0.027\n",
            "[11,    40] loss: 0.036\n",
            "[12,    10] loss: 0.084\n",
            "[12,    20] loss: 0.077\n",
            "[12,    30] loss: 0.057\n",
            "[12,    40] loss: 0.047\n",
            "[13,    10] loss: 0.030\n",
            "[13,    20] loss: 0.028\n",
            "[13,    30] loss: 0.021\n",
            "[13,    40] loss: 0.016\n",
            "[14,    10] loss: 0.010\n",
            "[14,    20] loss: 0.007\n",
            "[14,    30] loss: 0.008\n",
            "[14,    40] loss: 0.008\n",
            "[15,    10] loss: 0.010\n",
            "[15,    20] loss: 0.012\n",
            "[15,    30] loss: 0.010\n",
            "[15,    40] loss: 0.009\n",
            "[16,    10] loss: 0.014\n",
            "[16,    20] loss: 0.012\n",
            "[16,    30] loss: 0.009\n",
            "[16,    40] loss: 0.005\n",
            "[17,    10] loss: 0.004\n",
            "[17,    20] loss: 0.003\n",
            "[17,    30] loss: 0.002\n",
            "[17,    40] loss: 0.005\n",
            "[18,    10] loss: 0.030\n",
            "[18,    20] loss: 0.023\n",
            "[18,    30] loss: 0.013\n",
            "[18,    40] loss: 0.018\n",
            "[19,    10] loss: 0.016\n",
            "[19,    20] loss: 0.014\n",
            "[19,    30] loss: 0.011\n",
            "[19,    40] loss: 0.012\n",
            "[20,    10] loss: 0.017\n",
            "[20,    20] loss: 0.017\n",
            "[20,    30] loss: 0.016\n",
            "[20,    40] loss: 0.009\n",
            "[21,    10] loss: 0.006\n",
            "[21,    20] loss: 0.004\n",
            "[21,    30] loss: 0.003\n",
            "[21,    40] loss: 0.007\n",
            "[22,    10] loss: 0.021\n",
            "[22,    20] loss: 0.013\n",
            "[22,    30] loss: 0.012\n",
            "[22,    40] loss: 0.009\n",
            "[23,    10] loss: 0.006\n",
            "[23,    20] loss: 0.006\n",
            "[23,    30] loss: 0.004\n",
            "[23,    40] loss: 0.004\n",
            "[24,    10] loss: 0.002\n",
            "[24,    20] loss: 0.002\n",
            "[24,    30] loss: 0.002\n",
            "[24,    40] loss: 0.002\n",
            "[25,    10] loss: 0.001\n",
            "[25,    20] loss: 0.002\n",
            "[25,    30] loss: 0.001\n",
            "[25,    40] loss: 0.001\n",
            "[26,    10] loss: 0.001\n",
            "[26,    20] loss: 0.001\n",
            "[26,    30] loss: 0.000\n",
            "[26,    40] loss: 0.001\n",
            "[27,    10] loss: 0.000\n",
            "[27,    20] loss: 0.000\n",
            "[27,    30] loss: 0.001\n",
            "[27,    40] loss: 0.011\n",
            "[28,    10] loss: 0.127\n",
            "[28,    20] loss: 0.108\n",
            "[28,    30] loss: 0.089\n",
            "[28,    40] loss: 0.059\n",
            "[29,    10] loss: 0.033\n",
            "[29,    20] loss: 0.034\n",
            "[29,    30] loss: 0.028\n",
            "[29,    40] loss: 0.033\n",
            "[30,    10] loss: 0.028\n",
            "[30,    20] loss: 0.021\n",
            "[30,    30] loss: 0.016\n",
            "[30,    40] loss: 0.021\n",
            "[31,    10] loss: 0.037\n",
            "[31,    20] loss: 0.036\n",
            "[31,    30] loss: 0.022\n",
            "[31,    40] loss: 0.016\n",
            "[32,    10] loss: 0.010\n",
            "[32,    20] loss: 0.013\n",
            "[32,    30] loss: 0.010\n",
            "[32,    40] loss: 0.006\n",
            "[33,    10] loss: 0.003\n",
            "[33,    20] loss: 0.003\n",
            "[33,    30] loss: 0.002\n",
            "[33,    40] loss: 0.009\n",
            "[34,    10] loss: 0.042\n",
            "[34,    20] loss: 0.029\n",
            "[34,    30] loss: 0.026\n",
            "[34,    40] loss: 0.012\n",
            "[35,    10] loss: 0.010\n",
            "[35,    20] loss: 0.007\n",
            "[35,    30] loss: 0.004\n",
            "[35,    40] loss: 0.003\n",
            "[36,    10] loss: 0.002\n",
            "[36,    20] loss: 0.002\n",
            "[36,    30] loss: 0.002\n",
            "[36,    40] loss: 0.003\n",
            "[37,    10] loss: 0.006\n",
            "[37,    20] loss: 0.007\n",
            "[37,    30] loss: 0.006\n",
            "[37,    40] loss: 0.005\n",
            "[38,    10] loss: 0.010\n",
            "[38,    20] loss: 0.010\n",
            "[38,    30] loss: 0.006\n",
            "[38,    40] loss: 0.008\n",
            "[39,    10] loss: 0.011\n",
            "[39,    20] loss: 0.012\n",
            "[39,    30] loss: 0.008\n",
            "[39,    40] loss: 0.007\n",
            "[40,    10] loss: 0.015\n",
            "[40,    20] loss: 0.010\n",
            "[40,    30] loss: 0.012\n",
            "[40,    40] loss: 0.012\n",
            "[41,    10] loss: 0.067\n",
            "[41,    20] loss: 0.093\n",
            "[41,    30] loss: 0.078\n",
            "[41,    40] loss: 0.031\n",
            "[42,    10] loss: 0.020\n",
            "[42,    20] loss: 0.014\n",
            "[42,    30] loss: 0.009\n",
            "[42,    40] loss: 0.018\n",
            "[43,    10] loss: 0.028\n",
            "[43,    20] loss: 0.037\n",
            "[43,    30] loss: 0.018\n",
            "[43,    40] loss: 0.019\n",
            "[44,    10] loss: 0.009\n",
            "[44,    20] loss: 0.006\n",
            "[44,    30] loss: 0.004\n",
            "[44,    40] loss: 0.006\n",
            "[45,    10] loss: 0.004\n",
            "[45,    20] loss: 0.003\n",
            "[45,    30] loss: 0.004\n",
            "[45,    40] loss: 0.004\n",
            "[46,    10] loss: 0.002\n",
            "[46,    20] loss: 0.002\n",
            "[46,    30] loss: 0.002\n",
            "[46,    40] loss: 0.004\n",
            "[47,    10] loss: 0.021\n",
            "[47,    20] loss: 0.011\n",
            "[47,    30] loss: 0.005\n",
            "[47,    40] loss: 0.005\n",
            "[48,    10] loss: 0.003\n",
            "[48,    20] loss: 0.002\n",
            "[48,    30] loss: 0.002\n",
            "[48,    40] loss: 0.002\n",
            "[49,    10] loss: 0.001\n",
            "[49,    20] loss: 0.001\n",
            "[49,    30] loss: 0.001\n",
            "[49,    40] loss: 0.001\n",
            "[50,    10] loss: 0.001\n",
            "[50,    20] loss: 0.001\n",
            "[50,    30] loss: 0.001\n",
            "[50,    40] loss: 0.001\n",
            "[51,    10] loss: 0.001\n",
            "[51,    20] loss: 0.001\n",
            "[51,    30] loss: 0.001\n",
            "[51,    40] loss: 0.008\n",
            "[52,    10] loss: 0.050\n",
            "[52,    20] loss: 0.030\n",
            "[52,    30] loss: 0.033\n",
            "[52,    40] loss: 0.017\n",
            "[53,    10] loss: 0.015\n",
            "[53,    20] loss: 0.008\n",
            "[53,    30] loss: 0.004\n",
            "[53,    40] loss: 0.005\n",
            "[54,    10] loss: 0.002\n",
            "[54,    20] loss: 0.002\n",
            "[54,    30] loss: 0.001\n",
            "[54,    40] loss: 0.001\n",
            "[55,    10] loss: 0.001\n",
            "[55,    20] loss: 0.001\n",
            "[55,    30] loss: 0.001\n",
            "[55,    40] loss: 0.001\n",
            "[56,    10] loss: 0.000\n",
            "[56,    20] loss: 0.001\n",
            "[56,    30] loss: 0.000\n",
            "[56,    40] loss: 0.001\n",
            "[57,    10] loss: 0.000\n",
            "[57,    20] loss: 0.001\n",
            "[57,    30] loss: 0.001\n",
            "[57,    40] loss: 0.001\n",
            "[58,    10] loss: 0.000\n",
            "[58,    20] loss: 0.001\n",
            "[58,    30] loss: 0.000\n",
            "[58,    40] loss: 0.000\n",
            "[59,    10] loss: 0.000\n",
            "[59,    20] loss: 0.000\n",
            "[59,    30] loss: 0.000\n",
            "[59,    40] loss: 0.005\n",
            "[60,    10] loss: 0.015\n",
            "[60,    20] loss: 0.008\n",
            "[60,    30] loss: 0.007\n",
            "[60,    40] loss: 0.025\n",
            "[61,    10] loss: 0.087\n",
            "[61,    20] loss: 0.057\n",
            "[61,    30] loss: 0.055\n",
            "[61,    40] loss: 0.024\n",
            "[62,    10] loss: 0.015\n",
            "[62,    20] loss: 0.011\n",
            "[62,    30] loss: 0.009\n",
            "[62,    40] loss: 0.006\n",
            "[63,    10] loss: 0.003\n",
            "[63,    20] loss: 0.002\n",
            "[63,    30] loss: 0.002\n",
            "[63,    40] loss: 0.002\n",
            "[64,    10] loss: 0.001\n",
            "[64,    20] loss: 0.001\n",
            "[64,    30] loss: 0.001\n",
            "[64,    40] loss: 0.001\n",
            "[65,    10] loss: 0.001\n",
            "[65,    20] loss: 0.001\n",
            "[65,    30] loss: 0.000\n",
            "[65,    40] loss: 0.001\n",
            "[66,    10] loss: 0.000\n",
            "[66,    20] loss: 0.001\n",
            "[66,    30] loss: 0.000\n",
            "[66,    40] loss: 0.001\n",
            "[67,    10] loss: 0.000\n",
            "[67,    20] loss: 0.000\n",
            "[67,    30] loss: 0.000\n",
            "[67,    40] loss: 0.029\n",
            "[68,    10] loss: 0.144\n",
            "[68,    20] loss: 0.113\n",
            "[68,    30] loss: 0.079\n",
            "[68,    40] loss: 0.062\n",
            "[69,    10] loss: 0.058\n",
            "[69,    20] loss: 0.035\n",
            "[69,    30] loss: 0.026\n",
            "[69,    40] loss: 0.015\n",
            "[70,    10] loss: 0.006\n",
            "[70,    20] loss: 0.005\n",
            "[70,    30] loss: 0.004\n",
            "[70,    40] loss: 0.011\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 train images: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 1: 45 %\n",
            "Accuracy of the network on the 10000 test dataset 2: 74 %\n",
            "Accuracy of the network on the 10000 test dataset 3: 94 %\n",
            "Accuracy of the network on the 10000 test dataset 4: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 5: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 6: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 7: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 8: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 9: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 10: 95 %\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "training on data set   6\n",
            "[1,    10] loss: 0.903\n",
            "[1,    20] loss: 0.644\n",
            "[1,    30] loss: 0.542\n",
            "[1,    40] loss: 0.493\n",
            "[2,    10] loss: 0.478\n",
            "[2,    20] loss: 0.443\n",
            "[2,    30] loss: 0.422\n",
            "[2,    40] loss: 0.404\n",
            "[3,    10] loss: 0.402\n",
            "[3,    20] loss: 0.344\n",
            "[3,    30] loss: 0.333\n",
            "[3,    40] loss: 0.348\n",
            "[4,    10] loss: 0.351\n",
            "[4,    20] loss: 0.306\n",
            "[4,    30] loss: 0.259\n",
            "[4,    40] loss: 0.253\n",
            "[5,    10] loss: 0.239\n",
            "[5,    20] loss: 0.212\n",
            "[5,    30] loss: 0.205\n",
            "[5,    40] loss: 0.211\n",
            "[6,    10] loss: 0.194\n",
            "[6,    20] loss: 0.172\n",
            "[6,    30] loss: 0.159\n",
            "[6,    40] loss: 0.154\n",
            "[7,    10] loss: 0.198\n",
            "[7,    20] loss: 0.191\n",
            "[7,    30] loss: 0.162\n",
            "[7,    40] loss: 0.161\n",
            "[8,    10] loss: 0.179\n",
            "[8,    20] loss: 0.152\n",
            "[8,    30] loss: 0.132\n",
            "[8,    40] loss: 0.141\n",
            "[9,    10] loss: 0.148\n",
            "[9,    20] loss: 0.118\n",
            "[9,    30] loss: 0.105\n",
            "[9,    40] loss: 0.108\n",
            "[10,    10] loss: 0.124\n",
            "[10,    20] loss: 0.109\n",
            "[10,    30] loss: 0.080\n",
            "[10,    40] loss: 0.086\n",
            "[11,    10] loss: 0.107\n",
            "[11,    20] loss: 0.099\n",
            "[11,    30] loss: 0.069\n",
            "[11,    40] loss: 0.054\n",
            "[12,    10] loss: 0.033\n",
            "[12,    20] loss: 0.029\n",
            "[12,    30] loss: 0.026\n",
            "[12,    40] loss: 0.020\n",
            "[13,    10] loss: 0.012\n",
            "[13,    20] loss: 0.010\n",
            "[13,    30] loss: 0.007\n",
            "[13,    40] loss: 0.030\n",
            "[14,    10] loss: 0.098\n",
            "[14,    20] loss: 0.094\n",
            "[14,    30] loss: 0.079\n",
            "[14,    40] loss: 0.045\n",
            "[15,    10] loss: 0.038\n",
            "[15,    20] loss: 0.027\n",
            "[15,    30] loss: 0.024\n",
            "[15,    40] loss: 0.039\n",
            "[16,    10] loss: 0.123\n",
            "[16,    20] loss: 0.087\n",
            "[16,    30] loss: 0.073\n",
            "[16,    40] loss: 0.061\n",
            "[17,    10] loss: 0.065\n",
            "[17,    20] loss: 0.045\n",
            "[17,    30] loss: 0.038\n",
            "[17,    40] loss: 0.032\n",
            "[18,    10] loss: 0.064\n",
            "[18,    20] loss: 0.038\n",
            "[18,    30] loss: 0.030\n",
            "[18,    40] loss: 0.027\n",
            "[19,    10] loss: 0.022\n",
            "[19,    20] loss: 0.018\n",
            "[19,    30] loss: 0.014\n",
            "[19,    40] loss: 0.012\n",
            "[20,    10] loss: 0.008\n",
            "[20,    20] loss: 0.006\n",
            "[20,    30] loss: 0.004\n",
            "[20,    40] loss: 0.007\n",
            "[21,    10] loss: 0.014\n",
            "[21,    20] loss: 0.009\n",
            "[21,    30] loss: 0.006\n",
            "[21,    40] loss: 0.004\n",
            "[22,    10] loss: 0.002\n",
            "[22,    20] loss: 0.002\n",
            "[22,    30] loss: 0.002\n",
            "[22,    40] loss: 0.002\n",
            "[23,    10] loss: 0.001\n",
            "[23,    20] loss: 0.001\n",
            "[23,    30] loss: 0.001\n",
            "[23,    40] loss: 0.001\n",
            "[24,    10] loss: 0.001\n",
            "[24,    20] loss: 0.001\n",
            "[24,    30] loss: 0.001\n",
            "[24,    40] loss: 0.001\n",
            "[25,    10] loss: 0.001\n",
            "[25,    20] loss: 0.001\n",
            "[25,    30] loss: 0.001\n",
            "[25,    40] loss: 0.001\n",
            "[26,    10] loss: 0.001\n",
            "[26,    20] loss: 0.001\n",
            "[26,    30] loss: 0.001\n",
            "[26,    40] loss: 0.001\n",
            "[27,    10] loss: 0.001\n",
            "[27,    20] loss: 0.001\n",
            "[27,    30] loss: 0.001\n",
            "[27,    40] loss: 0.001\n",
            "[28,    10] loss: 0.001\n",
            "[28,    20] loss: 0.001\n",
            "[28,    30] loss: 0.001\n",
            "[28,    40] loss: 0.002\n",
            "[29,    10] loss: 0.003\n",
            "[29,    20] loss: 0.005\n",
            "[29,    30] loss: 0.002\n",
            "[29,    40] loss: 0.002\n",
            "[30,    10] loss: 0.002\n",
            "[30,    20] loss: 0.001\n",
            "[30,    30] loss: 0.001\n",
            "[30,    40] loss: 0.001\n",
            "[31,    10] loss: 0.001\n",
            "[31,    20] loss: 0.001\n",
            "[31,    30] loss: 0.001\n",
            "[31,    40] loss: 0.001\n",
            "[32,    10] loss: 0.000\n",
            "[32,    20] loss: 0.001\n",
            "[32,    30] loss: 0.001\n",
            "[32,    40] loss: 0.001\n",
            "[33,    10] loss: 0.001\n",
            "[33,    20] loss: 0.001\n",
            "[33,    30] loss: 0.001\n",
            "[33,    40] loss: 0.001\n",
            "[34,    10] loss: 0.001\n",
            "[34,    20] loss: 0.001\n",
            "[34,    30] loss: 0.001\n",
            "[34,    40] loss: 0.001\n",
            "[35,    10] loss: 0.000\n",
            "[35,    20] loss: 0.000\n",
            "[35,    30] loss: 0.000\n",
            "[35,    40] loss: 0.001\n",
            "[36,    10] loss: 0.000\n",
            "[36,    20] loss: 0.000\n",
            "[36,    30] loss: 0.000\n",
            "[36,    40] loss: 0.000\n",
            "[37,    10] loss: 0.000\n",
            "[37,    20] loss: 0.000\n",
            "[37,    30] loss: 0.000\n",
            "[37,    40] loss: 0.003\n",
            "[38,    10] loss: 0.021\n",
            "[38,    20] loss: 0.012\n",
            "[38,    30] loss: 0.009\n",
            "[38,    40] loss: 0.007\n",
            "[39,    10] loss: 0.006\n",
            "[39,    20] loss: 0.005\n",
            "[39,    30] loss: 0.003\n",
            "[39,    40] loss: 0.006\n",
            "[40,    10] loss: 0.003\n",
            "[40,    20] loss: 0.004\n",
            "[40,    30] loss: 0.002\n",
            "[40,    40] loss: 0.002\n",
            "[41,    10] loss: 0.001\n",
            "[41,    20] loss: 0.001\n",
            "[41,    30] loss: 0.001\n",
            "[41,    40] loss: 0.001\n",
            "[42,    10] loss: 0.001\n",
            "[42,    20] loss: 0.001\n",
            "[42,    30] loss: 0.001\n",
            "[42,    40] loss: 0.001\n",
            "[43,    10] loss: 0.000\n",
            "[43,    20] loss: 0.000\n",
            "[43,    30] loss: 0.000\n",
            "[43,    40] loss: 0.001\n",
            "[44,    10] loss: 0.000\n",
            "[44,    20] loss: 0.001\n",
            "[44,    30] loss: 0.000\n",
            "[44,    40] loss: 0.008\n",
            "[45,    10] loss: 0.060\n",
            "[45,    20] loss: 0.036\n",
            "[45,    30] loss: 0.025\n",
            "[45,    40] loss: 0.014\n",
            "[46,    10] loss: 0.009\n",
            "[46,    20] loss: 0.007\n",
            "[46,    30] loss: 0.004\n",
            "[46,    40] loss: 0.005\n",
            "[47,    10] loss: 0.002\n",
            "[47,    20] loss: 0.004\n",
            "[47,    30] loss: 0.002\n",
            "[47,    40] loss: 0.002\n",
            "[48,    10] loss: 0.001\n",
            "[48,    20] loss: 0.001\n",
            "[48,    30] loss: 0.001\n",
            "[48,    40] loss: 0.003\n",
            "[49,    10] loss: 0.013\n",
            "[49,    20] loss: 0.005\n",
            "[49,    30] loss: 0.003\n",
            "[49,    40] loss: 0.002\n",
            "[50,    10] loss: 0.002\n",
            "[50,    20] loss: 0.001\n",
            "[50,    30] loss: 0.001\n",
            "[50,    40] loss: 0.001\n",
            "[51,    10] loss: 0.001\n",
            "[51,    20] loss: 0.001\n",
            "[51,    30] loss: 0.001\n",
            "[51,    40] loss: 0.001\n",
            "[52,    10] loss: 0.001\n",
            "[52,    20] loss: 0.001\n",
            "[52,    30] loss: 0.000\n",
            "[52,    40] loss: 0.004\n",
            "[53,    10] loss: 0.005\n",
            "[53,    20] loss: 0.002\n",
            "[53,    30] loss: 0.003\n",
            "[53,    40] loss: 0.003\n",
            "[54,    10] loss: 0.002\n",
            "[54,    20] loss: 0.001\n",
            "[54,    30] loss: 0.001\n",
            "[54,    40] loss: 0.001\n",
            "[55,    10] loss: 0.001\n",
            "[55,    20] loss: 0.000\n",
            "[55,    30] loss: 0.001\n",
            "[55,    40] loss: 0.001\n",
            "[56,    10] loss: 0.001\n",
            "[56,    20] loss: 0.000\n",
            "[56,    30] loss: 0.000\n",
            "[56,    40] loss: 0.000\n",
            "[57,    10] loss: 0.000\n",
            "[57,    20] loss: 0.000\n",
            "[57,    30] loss: 0.000\n",
            "[57,    40] loss: 0.001\n",
            "[58,    10] loss: 0.000\n",
            "[58,    20] loss: 0.000\n",
            "[58,    30] loss: 0.000\n",
            "[58,    40] loss: 0.001\n",
            "[59,    10] loss: 0.000\n",
            "[59,    20] loss: 0.000\n",
            "[59,    30] loss: 0.000\n",
            "[59,    40] loss: 0.007\n",
            "[60,    10] loss: 0.039\n",
            "[60,    20] loss: 0.037\n",
            "[60,    30] loss: 0.028\n",
            "[60,    40] loss: 0.017\n",
            "[61,    10] loss: 0.012\n",
            "[61,    20] loss: 0.007\n",
            "[61,    30] loss: 0.005\n",
            "[61,    40] loss: 0.006\n",
            "[62,    10] loss: 0.002\n",
            "[62,    20] loss: 0.002\n",
            "[62,    30] loss: 0.002\n",
            "[62,    40] loss: 0.004\n",
            "[63,    10] loss: 0.012\n",
            "[63,    20] loss: 0.011\n",
            "[63,    30] loss: 0.008\n",
            "[63,    40] loss: 0.005\n",
            "[64,    10] loss: 0.002\n",
            "[64,    20] loss: 0.003\n",
            "[64,    30] loss: 0.002\n",
            "[64,    40] loss: 0.004\n",
            "[65,    10] loss: 0.003\n",
            "[65,    20] loss: 0.002\n",
            "[65,    30] loss: 0.001\n",
            "[65,    40] loss: 0.001\n",
            "[66,    10] loss: 0.001\n",
            "[66,    20] loss: 0.001\n",
            "[66,    30] loss: 0.001\n",
            "[66,    40] loss: 0.001\n",
            "[67,    10] loss: 0.001\n",
            "[67,    20] loss: 0.000\n",
            "[67,    30] loss: 0.001\n",
            "[67,    40] loss: 0.000\n",
            "[68,    10] loss: 0.000\n",
            "[68,    20] loss: 0.000\n",
            "[68,    30] loss: 0.000\n",
            "[68,    40] loss: 0.000\n",
            "[69,    10] loss: 0.000\n",
            "[69,    20] loss: 0.000\n",
            "[69,    30] loss: 0.000\n",
            "[69,    40] loss: 0.002\n",
            "[70,    10] loss: 0.001\n",
            "[70,    20] loss: 0.002\n",
            "[70,    30] loss: 0.001\n",
            "[70,    40] loss: 0.005\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 train images: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 1: 44 %\n",
            "Accuracy of the network on the 10000 test dataset 2: 70 %\n",
            "Accuracy of the network on the 10000 test dataset 3: 89 %\n",
            "Accuracy of the network on the 10000 test dataset 4: 98 %\n",
            "Accuracy of the network on the 10000 test dataset 5: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 6: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 7: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 8: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 9: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 10: 95 %\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "training on data set   7\n",
            "[1,    10] loss: 0.886\n",
            "[1,    20] loss: 0.617\n",
            "[1,    30] loss: 0.519\n",
            "[1,    40] loss: 0.463\n",
            "[2,    10] loss: 0.457\n",
            "[2,    20] loss: 0.425\n",
            "[2,    30] loss: 0.385\n",
            "[2,    40] loss: 0.412\n",
            "[3,    10] loss: 0.379\n",
            "[3,    20] loss: 0.318\n",
            "[3,    30] loss: 0.312\n",
            "[3,    40] loss: 0.314\n",
            "[4,    10] loss: 0.287\n",
            "[4,    20] loss: 0.244\n",
            "[4,    30] loss: 0.251\n",
            "[4,    40] loss: 0.235\n",
            "[5,    10] loss: 0.219\n",
            "[5,    20] loss: 0.199\n",
            "[5,    30] loss: 0.171\n",
            "[5,    40] loss: 0.169\n",
            "[6,    10] loss: 0.139\n",
            "[6,    20] loss: 0.134\n",
            "[6,    30] loss: 0.126\n",
            "[6,    40] loss: 0.143\n",
            "[7,    10] loss: 0.130\n",
            "[7,    20] loss: 0.121\n",
            "[7,    30] loss: 0.105\n",
            "[7,    40] loss: 0.104\n",
            "[8,    10] loss: 0.075\n",
            "[8,    20] loss: 0.064\n",
            "[8,    30] loss: 0.064\n",
            "[8,    40] loss: 0.065\n",
            "[9,    10] loss: 0.092\n",
            "[9,    20] loss: 0.117\n",
            "[9,    30] loss: 0.100\n",
            "[9,    40] loss: 0.115\n",
            "[10,    10] loss: 0.121\n",
            "[10,    20] loss: 0.111\n",
            "[10,    30] loss: 0.097\n",
            "[10,    40] loss: 0.070\n",
            "[11,    10] loss: 0.038\n",
            "[11,    20] loss: 0.036\n",
            "[11,    30] loss: 0.033\n",
            "[11,    40] loss: 0.024\n",
            "[12,    10] loss: 0.021\n",
            "[12,    20] loss: 0.018\n",
            "[12,    30] loss: 0.015\n",
            "[12,    40] loss: 0.038\n",
            "[13,    10] loss: 0.190\n",
            "[13,    20] loss: 0.151\n",
            "[13,    30] loss: 0.134\n",
            "[13,    40] loss: 0.105\n",
            "[14,    10] loss: 0.059\n",
            "[14,    20] loss: 0.047\n",
            "[14,    30] loss: 0.032\n",
            "[14,    40] loss: 0.037\n",
            "[15,    10] loss: 0.066\n",
            "[15,    20] loss: 0.077\n",
            "[15,    30] loss: 0.063\n",
            "[15,    40] loss: 0.060\n",
            "[16,    10] loss: 0.086\n",
            "[16,    20] loss: 0.073\n",
            "[16,    30] loss: 0.052\n",
            "[16,    40] loss: 0.070\n",
            "[17,    10] loss: 0.179\n",
            "[17,    20] loss: 0.114\n",
            "[17,    30] loss: 0.086\n",
            "[17,    40] loss: 0.065\n",
            "[18,    10] loss: 0.047\n",
            "[18,    20] loss: 0.039\n",
            "[18,    30] loss: 0.029\n",
            "[18,    40] loss: 0.024\n",
            "[19,    10] loss: 0.021\n",
            "[19,    20] loss: 0.010\n",
            "[19,    30] loss: 0.010\n",
            "[19,    40] loss: 0.017\n",
            "[20,    10] loss: 0.030\n",
            "[20,    20] loss: 0.022\n",
            "[20,    30] loss: 0.013\n",
            "[20,    40] loss: 0.035\n",
            "[21,    10] loss: 0.083\n",
            "[21,    20] loss: 0.071\n",
            "[21,    30] loss: 0.051\n",
            "[21,    40] loss: 0.041\n",
            "[22,    10] loss: 0.025\n",
            "[22,    20] loss: 0.022\n",
            "[22,    30] loss: 0.017\n",
            "[22,    40] loss: 0.012\n",
            "[23,    10] loss: 0.007\n",
            "[23,    20] loss: 0.005\n",
            "[23,    30] loss: 0.005\n",
            "[23,    40] loss: 0.006\n",
            "[24,    10] loss: 0.004\n",
            "[24,    20] loss: 0.003\n",
            "[24,    30] loss: 0.002\n",
            "[24,    40] loss: 0.004\n",
            "[25,    10] loss: 0.003\n",
            "[25,    20] loss: 0.003\n",
            "[25,    30] loss: 0.003\n",
            "[25,    40] loss: 0.002\n",
            "[26,    10] loss: 0.001\n",
            "[26,    20] loss: 0.001\n",
            "[26,    30] loss: 0.001\n",
            "[26,    40] loss: 0.001\n",
            "[27,    10] loss: 0.001\n",
            "[27,    20] loss: 0.001\n",
            "[27,    30] loss: 0.001\n",
            "[27,    40] loss: 0.001\n",
            "[28,    10] loss: 0.001\n",
            "[28,    20] loss: 0.001\n",
            "[28,    30] loss: 0.001\n",
            "[28,    40] loss: 0.001\n",
            "[29,    10] loss: 0.001\n",
            "[29,    20] loss: 0.001\n",
            "[29,    30] loss: 0.001\n",
            "[29,    40] loss: 0.001\n",
            "[30,    10] loss: 0.001\n",
            "[30,    20] loss: 0.001\n",
            "[30,    30] loss: 0.001\n",
            "[30,    40] loss: 0.027\n",
            "[31,    10] loss: 0.059\n",
            "[31,    20] loss: 0.047\n",
            "[31,    30] loss: 0.038\n",
            "[31,    40] loss: 0.026\n",
            "[32,    10] loss: 0.017\n",
            "[32,    20] loss: 0.014\n",
            "[32,    30] loss: 0.011\n",
            "[32,    40] loss: 0.006\n",
            "[33,    10] loss: 0.004\n",
            "[33,    20] loss: 0.003\n",
            "[33,    30] loss: 0.004\n",
            "[33,    40] loss: 0.003\n",
            "[34,    10] loss: 0.002\n",
            "[34,    20] loss: 0.002\n",
            "[34,    30] loss: 0.001\n",
            "[34,    40] loss: 0.001\n",
            "[35,    10] loss: 0.001\n",
            "[35,    20] loss: 0.001\n",
            "[35,    30] loss: 0.001\n",
            "[35,    40] loss: 0.027\n",
            "[36,    10] loss: 0.101\n",
            "[36,    20] loss: 0.077\n",
            "[36,    30] loss: 0.046\n",
            "[36,    40] loss: 0.046\n",
            "[37,    10] loss: 0.090\n",
            "[37,    20] loss: 0.056\n",
            "[37,    30] loss: 0.038\n",
            "[37,    40] loss: 0.027\n",
            "[38,    10] loss: 0.017\n",
            "[38,    20] loss: 0.010\n",
            "[38,    30] loss: 0.011\n",
            "[38,    40] loss: 0.009\n",
            "[39,    10] loss: 0.004\n",
            "[39,    20] loss: 0.005\n",
            "[39,    30] loss: 0.002\n",
            "[39,    40] loss: 0.002\n",
            "[40,    10] loss: 0.001\n",
            "[40,    20] loss: 0.001\n",
            "[40,    30] loss: 0.001\n",
            "[40,    40] loss: 0.005\n",
            "[41,    10] loss: 0.003\n",
            "[41,    20] loss: 0.002\n",
            "[41,    30] loss: 0.002\n",
            "[41,    40] loss: 0.002\n",
            "[42,    10] loss: 0.001\n",
            "[42,    20] loss: 0.001\n",
            "[42,    30] loss: 0.001\n",
            "[42,    40] loss: 0.001\n",
            "[43,    10] loss: 0.001\n",
            "[43,    20] loss: 0.001\n",
            "[43,    30] loss: 0.001\n",
            "[43,    40] loss: 0.002\n",
            "[44,    10] loss: 0.001\n",
            "[44,    20] loss: 0.001\n",
            "[44,    30] loss: 0.001\n",
            "[44,    40] loss: 0.001\n",
            "[45,    10] loss: 0.001\n",
            "[45,    20] loss: 0.001\n",
            "[45,    30] loss: 0.000\n",
            "[45,    40] loss: 0.001\n",
            "[46,    10] loss: 0.001\n",
            "[46,    20] loss: 0.000\n",
            "[46,    30] loss: 0.001\n",
            "[46,    40] loss: 0.002\n",
            "[47,    10] loss: 0.005\n",
            "[47,    20] loss: 0.002\n",
            "[47,    30] loss: 0.002\n",
            "[47,    40] loss: 0.003\n",
            "[48,    10] loss: 0.003\n",
            "[48,    20] loss: 0.003\n",
            "[48,    30] loss: 0.002\n",
            "[48,    40] loss: 0.003\n",
            "[49,    10] loss: 0.001\n",
            "[49,    20] loss: 0.001\n",
            "[49,    30] loss: 0.001\n",
            "[49,    40] loss: 0.001\n",
            "[50,    10] loss: 0.001\n",
            "[50,    20] loss: 0.000\n",
            "[50,    30] loss: 0.001\n",
            "[50,    40] loss: 0.000\n",
            "[51,    10] loss: 0.000\n",
            "[51,    20] loss: 0.000\n",
            "[51,    30] loss: 0.000\n",
            "[51,    40] loss: 0.000\n",
            "[52,    10] loss: 0.000\n",
            "[52,    20] loss: 0.000\n",
            "[52,    30] loss: 0.000\n",
            "[52,    40] loss: 0.000\n",
            "[53,    10] loss: 0.000\n",
            "[53,    20] loss: 0.000\n",
            "[53,    30] loss: 0.000\n",
            "[53,    40] loss: 0.000\n",
            "[54,    10] loss: 0.000\n",
            "[54,    20] loss: 0.000\n",
            "[54,    30] loss: 0.000\n",
            "[54,    40] loss: 0.001\n",
            "[55,    10] loss: 0.000\n",
            "[55,    20] loss: 0.000\n",
            "[55,    30] loss: 0.001\n",
            "[55,    40] loss: 0.001\n",
            "[56,    10] loss: 0.000\n",
            "[56,    20] loss: 0.000\n",
            "[56,    30] loss: 0.000\n",
            "[56,    40] loss: 0.000\n",
            "[57,    10] loss: 0.000\n",
            "[57,    20] loss: 0.000\n",
            "[57,    30] loss: 0.000\n",
            "[57,    40] loss: 0.011\n",
            "[58,    10] loss: 0.056\n",
            "[58,    20] loss: 0.044\n",
            "[58,    30] loss: 0.044\n",
            "[58,    40] loss: 0.027\n",
            "[59,    10] loss: 0.012\n",
            "[59,    20] loss: 0.009\n",
            "[59,    30] loss: 0.007\n",
            "[59,    40] loss: 0.034\n",
            "[60,    10] loss: 0.139\n",
            "[60,    20] loss: 0.105\n",
            "[60,    30] loss: 0.064\n",
            "[60,    40] loss: 0.056\n",
            "[61,    10] loss: 0.084\n",
            "[61,    20] loss: 0.064\n",
            "[61,    30] loss: 0.039\n",
            "[61,    40] loss: 0.038\n",
            "[62,    10] loss: 0.025\n",
            "[62,    20] loss: 0.019\n",
            "[62,    30] loss: 0.011\n",
            "[62,    40] loss: 0.072\n",
            "[63,    10] loss: 0.122\n",
            "[63,    20] loss: 0.078\n",
            "[63,    30] loss: 0.067\n",
            "[63,    40] loss: 0.047\n",
            "[64,    10] loss: 0.021\n",
            "[64,    20] loss: 0.013\n",
            "[64,    30] loss: 0.012\n",
            "[64,    40] loss: 0.015\n",
            "[65,    10] loss: 0.006\n",
            "[65,    20] loss: 0.004\n",
            "[65,    30] loss: 0.003\n",
            "[65,    40] loss: 0.003\n",
            "[66,    10] loss: 0.002\n",
            "[66,    20] loss: 0.002\n",
            "[66,    30] loss: 0.002\n",
            "[66,    40] loss: 0.038\n",
            "[67,    10] loss: 0.037\n",
            "[67,    20] loss: 0.043\n",
            "[67,    30] loss: 0.025\n",
            "[67,    40] loss: 0.022\n",
            "[68,    10] loss: 0.008\n",
            "[68,    20] loss: 0.005\n",
            "[68,    30] loss: 0.005\n",
            "[68,    40] loss: 0.004\n",
            "[69,    10] loss: 0.003\n",
            "[69,    20] loss: 0.002\n",
            "[69,    30] loss: 0.001\n",
            "[69,    40] loss: 0.002\n",
            "[70,    10] loss: 0.001\n",
            "[70,    20] loss: 0.002\n",
            "[70,    30] loss: 0.001\n",
            "[70,    40] loss: 0.001\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 train images: 100 %\n",
            "Accuracy of the network on the 10000 test dataset 1: 44 %\n",
            "Accuracy of the network on the 10000 test dataset 2: 70 %\n",
            "Accuracy of the network on the 10000 test dataset 3: 89 %\n",
            "Accuracy of the network on the 10000 test dataset 4: 97 %\n",
            "Accuracy of the network on the 10000 test dataset 5: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 6: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 7: 100 %\n",
            "Accuracy of the network on the 10000 test dataset 8: 100 %\n",
            "Accuracy of the network on the 10000 test dataset 9: 100 %\n",
            "Accuracy of the network on the 10000 test dataset 10: 95 %\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "training on data set   8\n",
            "[1,    10] loss: 0.884\n",
            "[1,    20] loss: 0.637\n",
            "[1,    30] loss: 0.515\n",
            "[1,    40] loss: 0.489\n",
            "[2,    10] loss: 0.506\n",
            "[2,    20] loss: 0.464\n",
            "[2,    30] loss: 0.406\n",
            "[2,    40] loss: 0.386\n",
            "[3,    10] loss: 0.351\n",
            "[3,    20] loss: 0.326\n",
            "[3,    30] loss: 0.290\n",
            "[3,    40] loss: 0.295\n",
            "[4,    10] loss: 0.275\n",
            "[4,    20] loss: 0.233\n",
            "[4,    30] loss: 0.214\n",
            "[4,    40] loss: 0.205\n",
            "[5,    10] loss: 0.211\n",
            "[5,    20] loss: 0.203\n",
            "[5,    30] loss: 0.190\n",
            "[5,    40] loss: 0.263\n",
            "[6,    10] loss: 0.297\n",
            "[6,    20] loss: 0.242\n",
            "[6,    30] loss: 0.223\n",
            "[6,    40] loss: 0.209\n",
            "[7,    10] loss: 0.218\n",
            "[7,    20] loss: 0.184\n",
            "[7,    30] loss: 0.147\n",
            "[7,    40] loss: 0.148\n",
            "[8,    10] loss: 0.168\n",
            "[8,    20] loss: 0.133\n",
            "[8,    30] loss: 0.092\n",
            "[8,    40] loss: 0.172\n",
            "[9,    10] loss: 0.198\n",
            "[9,    20] loss: 0.186\n",
            "[9,    30] loss: 0.159\n",
            "[9,    40] loss: 0.158\n",
            "[10,    10] loss: 0.189\n",
            "[10,    20] loss: 0.169\n",
            "[10,    30] loss: 0.118\n",
            "[10,    40] loss: 0.132\n",
            "[11,    10] loss: 0.123\n",
            "[11,    20] loss: 0.102\n",
            "[11,    30] loss: 0.085\n",
            "[11,    40] loss: 0.067\n",
            "[12,    10] loss: 0.074\n",
            "[12,    20] loss: 0.078\n",
            "[12,    30] loss: 0.069\n",
            "[12,    40] loss: 0.044\n",
            "[13,    10] loss: 0.037\n",
            "[13,    20] loss: 0.031\n",
            "[13,    30] loss: 0.026\n",
            "[13,    40] loss: 0.017\n",
            "[14,    10] loss: 0.012\n",
            "[14,    20] loss: 0.010\n",
            "[14,    30] loss: 0.009\n",
            "[14,    40] loss: 0.007\n",
            "[15,    10] loss: 0.005\n",
            "[15,    20] loss: 0.004\n",
            "[15,    30] loss: 0.005\n",
            "[15,    40] loss: 0.004\n",
            "[16,    10] loss: 0.003\n",
            "[16,    20] loss: 0.003\n",
            "[16,    30] loss: 0.003\n",
            "[16,    40] loss: 0.003\n",
            "[17,    10] loss: 0.002\n",
            "[17,    20] loss: 0.002\n",
            "[17,    30] loss: 0.002\n",
            "[17,    40] loss: 0.002\n",
            "[18,    10] loss: 0.001\n",
            "[18,    20] loss: 0.002\n",
            "[18,    30] loss: 0.001\n",
            "[18,    40] loss: 0.001\n",
            "[19,    10] loss: 0.001\n",
            "[19,    20] loss: 0.001\n",
            "[19,    30] loss: 0.001\n",
            "[19,    40] loss: 0.014\n",
            "[20,    10] loss: 0.133\n",
            "[20,    20] loss: 0.092\n",
            "[20,    30] loss: 0.080\n",
            "[20,    40] loss: 0.098\n",
            "[21,    10] loss: 0.233\n",
            "[21,    20] loss: 0.160\n",
            "[21,    30] loss: 0.118\n",
            "[21,    40] loss: 0.137\n",
            "[22,    10] loss: 0.124\n",
            "[22,    20] loss: 0.113\n",
            "[22,    30] loss: 0.084\n",
            "[22,    40] loss: 0.063\n",
            "[23,    10] loss: 0.031\n",
            "[23,    20] loss: 0.027\n",
            "[23,    30] loss: 0.023\n",
            "[23,    40] loss: 0.040\n",
            "[24,    10] loss: 0.061\n",
            "[24,    20] loss: 0.050\n",
            "[24,    30] loss: 0.043\n",
            "[24,    40] loss: 0.051\n",
            "[25,    10] loss: 0.083\n",
            "[25,    20] loss: 0.051\n",
            "[25,    30] loss: 0.036\n",
            "[25,    40] loss: 0.026\n",
            "[26,    10] loss: 0.011\n",
            "[26,    20] loss: 0.009\n",
            "[26,    30] loss: 0.008\n",
            "[26,    40] loss: 0.013\n",
            "[27,    10] loss: 0.029\n",
            "[27,    20] loss: 0.020\n",
            "[27,    30] loss: 0.013\n",
            "[27,    40] loss: 0.011\n",
            "[28,    10] loss: 0.008\n",
            "[28,    20] loss: 0.006\n",
            "[28,    30] loss: 0.005\n",
            "[28,    40] loss: 0.005\n",
            "[29,    10] loss: 0.003\n",
            "[29,    20] loss: 0.004\n",
            "[29,    30] loss: 0.002\n",
            "[29,    40] loss: 0.003\n",
            "[30,    10] loss: 0.002\n",
            "[30,    20] loss: 0.002\n",
            "[30,    30] loss: 0.001\n",
            "[30,    40] loss: 0.022\n",
            "[31,    10] loss: 0.059\n",
            "[31,    20] loss: 0.044\n",
            "[31,    30] loss: 0.032\n",
            "[31,    40] loss: 0.025\n",
            "[32,    10] loss: 0.031\n",
            "[32,    20] loss: 0.021\n",
            "[32,    30] loss: 0.013\n",
            "[32,    40] loss: 0.008\n",
            "[33,    10] loss: 0.005\n",
            "[33,    20] loss: 0.004\n",
            "[33,    30] loss: 0.004\n",
            "[33,    40] loss: 0.003\n",
            "[34,    10] loss: 0.002\n",
            "[34,    20] loss: 0.001\n",
            "[34,    30] loss: 0.001\n",
            "[34,    40] loss: 0.005\n",
            "[35,    10] loss: 0.011\n",
            "[35,    20] loss: 0.011\n",
            "[35,    30] loss: 0.006\n",
            "[35,    40] loss: 0.043\n",
            "[36,    10] loss: 0.103\n",
            "[36,    20] loss: 0.053\n",
            "[36,    30] loss: 0.039\n",
            "[36,    40] loss: 0.029\n",
            "[37,    10] loss: 0.013\n",
            "[37,    20] loss: 0.017\n",
            "[37,    30] loss: 0.009\n",
            "[37,    40] loss: 0.007\n",
            "[38,    10] loss: 0.004\n",
            "[38,    20] loss: 0.003\n",
            "[38,    30] loss: 0.002\n",
            "[38,    40] loss: 0.016\n",
            "[39,    10] loss: 0.050\n",
            "[39,    20] loss: 0.049\n",
            "[39,    30] loss: 0.024\n",
            "[39,    40] loss: 0.024\n",
            "[40,    10] loss: 0.011\n",
            "[40,    20] loss: 0.012\n",
            "[40,    30] loss: 0.006\n",
            "[40,    40] loss: 0.005\n",
            "[41,    10] loss: 0.003\n",
            "[41,    20] loss: 0.003\n",
            "[41,    30] loss: 0.003\n",
            "[41,    40] loss: 0.002\n",
            "[42,    10] loss: 0.001\n",
            "[42,    20] loss: 0.001\n",
            "[42,    30] loss: 0.001\n",
            "[42,    40] loss: 0.057\n",
            "[43,    10] loss: 0.094\n",
            "[43,    20] loss: 0.079\n",
            "[43,    30] loss: 0.059\n",
            "[43,    40] loss: 0.059\n",
            "[44,    10] loss: 0.095\n",
            "[44,    20] loss: 0.078\n",
            "[44,    30] loss: 0.057\n",
            "[44,    40] loss: 0.036\n",
            "[45,    10] loss: 0.012\n",
            "[45,    20] loss: 0.009\n",
            "[45,    30] loss: 0.007\n",
            "[45,    40] loss: 0.007\n",
            "[46,    10] loss: 0.003\n",
            "[46,    20] loss: 0.003\n",
            "[46,    30] loss: 0.003\n",
            "[46,    40] loss: 0.005\n",
            "[47,    10] loss: 0.005\n",
            "[47,    20] loss: 0.004\n",
            "[47,    30] loss: 0.002\n",
            "[47,    40] loss: 0.005\n",
            "[48,    10] loss: 0.006\n",
            "[48,    20] loss: 0.006\n",
            "[48,    30] loss: 0.005\n",
            "[48,    40] loss: 0.005\n",
            "[49,    10] loss: 0.002\n",
            "[49,    20] loss: 0.002\n",
            "[49,    30] loss: 0.001\n",
            "[49,    40] loss: 0.001\n",
            "[50,    10] loss: 0.001\n",
            "[50,    20] loss: 0.001\n",
            "[50,    30] loss: 0.001\n",
            "[50,    40] loss: 0.007\n",
            "[51,    10] loss: 0.011\n",
            "[51,    20] loss: 0.009\n",
            "[51,    30] loss: 0.003\n",
            "[51,    40] loss: 0.004\n",
            "[52,    10] loss: 0.002\n",
            "[52,    20] loss: 0.002\n",
            "[52,    30] loss: 0.001\n",
            "[52,    40] loss: 0.001\n",
            "[53,    10] loss: 0.001\n",
            "[53,    20] loss: 0.001\n",
            "[53,    30] loss: 0.001\n",
            "[53,    40] loss: 0.001\n",
            "[54,    10] loss: 0.001\n",
            "[54,    20] loss: 0.001\n",
            "[54,    30] loss: 0.001\n",
            "[54,    40] loss: 0.006\n",
            "[55,    10] loss: 0.007\n",
            "[55,    20] loss: 0.006\n",
            "[55,    30] loss: 0.005\n",
            "[55,    40] loss: 0.020\n",
            "[56,    10] loss: 0.027\n",
            "[56,    20] loss: 0.027\n",
            "[56,    30] loss: 0.012\n",
            "[56,    40] loss: 0.080\n",
            "[57,    10] loss: 0.182\n",
            "[57,    20] loss: 0.134\n",
            "[57,    30] loss: 0.085\n",
            "[57,    40] loss: 0.059\n",
            "[58,    10] loss: 0.047\n",
            "[58,    20] loss: 0.040\n",
            "[58,    30] loss: 0.022\n",
            "[58,    40] loss: 0.022\n",
            "[59,    10] loss: 0.026\n",
            "[59,    20] loss: 0.020\n",
            "[59,    30] loss: 0.016\n",
            "[59,    40] loss: 0.018\n",
            "[60,    10] loss: 0.031\n",
            "[60,    20] loss: 0.025\n",
            "[60,    30] loss: 0.019\n",
            "[60,    40] loss: 0.012\n",
            "[61,    10] loss: 0.007\n",
            "[61,    20] loss: 0.004\n",
            "[61,    30] loss: 0.004\n",
            "[61,    40] loss: 0.003\n",
            "[62,    10] loss: 0.002\n",
            "[62,    20] loss: 0.002\n",
            "[62,    30] loss: 0.002\n",
            "[62,    40] loss: 0.001\n",
            "[63,    10] loss: 0.001\n",
            "[63,    20] loss: 0.002\n",
            "[63,    30] loss: 0.001\n",
            "[63,    40] loss: 0.007\n",
            "[64,    10] loss: 0.018\n",
            "[64,    20] loss: 0.011\n",
            "[64,    30] loss: 0.006\n",
            "[64,    40] loss: 0.004\n",
            "[65,    10] loss: 0.003\n",
            "[65,    20] loss: 0.002\n",
            "[65,    30] loss: 0.002\n",
            "[65,    40] loss: 0.004\n",
            "[66,    10] loss: 0.002\n",
            "[66,    20] loss: 0.002\n",
            "[66,    30] loss: 0.002\n",
            "[66,    40] loss: 0.003\n",
            "[67,    10] loss: 0.003\n",
            "[67,    20] loss: 0.002\n",
            "[67,    30] loss: 0.001\n",
            "[67,    40] loss: 0.001\n",
            "[68,    10] loss: 0.001\n",
            "[68,    20] loss: 0.001\n",
            "[68,    30] loss: 0.001\n",
            "[68,    40] loss: 0.018\n",
            "[69,    10] loss: 0.056\n",
            "[69,    20] loss: 0.039\n",
            "[69,    30] loss: 0.020\n",
            "[69,    40] loss: 0.014\n",
            "[70,    10] loss: 0.006\n",
            "[70,    20] loss: 0.006\n",
            "[70,    30] loss: 0.003\n",
            "[70,    40] loss: 0.003\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 train images: 100 %\n",
            "Accuracy of the network on the 10000 test dataset 1: 44 %\n",
            "Accuracy of the network on the 10000 test dataset 2: 69 %\n",
            "Accuracy of the network on the 10000 test dataset 3: 88 %\n",
            "Accuracy of the network on the 10000 test dataset 4: 96 %\n",
            "Accuracy of the network on the 10000 test dataset 5: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 6: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 7: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 8: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 9: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 10: 96 %\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "training on data set   9\n",
            "[1,    10] loss: 0.883\n",
            "[1,    20] loss: 0.597\n",
            "[1,    30] loss: 0.518\n",
            "[1,    40] loss: 0.457\n",
            "[2,    10] loss: 0.443\n",
            "[2,    20] loss: 0.431\n",
            "[2,    30] loss: 0.424\n",
            "[2,    40] loss: 0.370\n",
            "[3,    10] loss: 0.413\n",
            "[3,    20] loss: 0.345\n",
            "[3,    30] loss: 0.330\n",
            "[3,    40] loss: 0.302\n",
            "[4,    10] loss: 0.288\n",
            "[4,    20] loss: 0.277\n",
            "[4,    30] loss: 0.253\n",
            "[4,    40] loss: 0.264\n",
            "[5,    10] loss: 0.254\n",
            "[5,    20] loss: 0.227\n",
            "[5,    30] loss: 0.194\n",
            "[5,    40] loss: 0.190\n",
            "[6,    10] loss: 0.237\n",
            "[6,    20] loss: 0.181\n",
            "[6,    30] loss: 0.180\n",
            "[6,    40] loss: 0.186\n",
            "[7,    10] loss: 0.202\n",
            "[7,    20] loss: 0.181\n",
            "[7,    30] loss: 0.150\n",
            "[7,    40] loss: 0.173\n",
            "[8,    10] loss: 0.161\n",
            "[8,    20] loss: 0.121\n",
            "[8,    30] loss: 0.097\n",
            "[8,    40] loss: 0.101\n",
            "[9,    10] loss: 0.089\n",
            "[9,    20] loss: 0.077\n",
            "[9,    30] loss: 0.068\n",
            "[9,    40] loss: 0.053\n",
            "[10,    10] loss: 0.039\n",
            "[10,    20] loss: 0.037\n",
            "[10,    30] loss: 0.028\n",
            "[10,    40] loss: 0.032\n",
            "[11,    10] loss: 0.036\n",
            "[11,    20] loss: 0.034\n",
            "[11,    30] loss: 0.027\n",
            "[11,    40] loss: 0.023\n",
            "[12,    10] loss: 0.047\n",
            "[12,    20] loss: 0.034\n",
            "[12,    30] loss: 0.024\n",
            "[12,    40] loss: 0.022\n",
            "[13,    10] loss: 0.010\n",
            "[13,    20] loss: 0.009\n",
            "[13,    30] loss: 0.009\n",
            "[13,    40] loss: 0.006\n",
            "[14,    10] loss: 0.004\n",
            "[14,    20] loss: 0.003\n",
            "[14,    30] loss: 0.003\n",
            "[14,    40] loss: 0.011\n",
            "[15,    10] loss: 0.081\n",
            "[15,    20] loss: 0.083\n",
            "[15,    30] loss: 0.074\n",
            "[15,    40] loss: 0.063\n",
            "[16,    10] loss: 0.041\n",
            "[16,    20] loss: 0.026\n",
            "[16,    30] loss: 0.021\n",
            "[16,    40] loss: 0.013\n",
            "[17,    10] loss: 0.007\n",
            "[17,    20] loss: 0.007\n",
            "[17,    30] loss: 0.005\n",
            "[17,    40] loss: 0.015\n",
            "[18,    10] loss: 0.092\n",
            "[18,    20] loss: 0.090\n",
            "[18,    30] loss: 0.070\n",
            "[18,    40] loss: 0.050\n",
            "[19,    10] loss: 0.027\n",
            "[19,    20] loss: 0.022\n",
            "[19,    30] loss: 0.018\n",
            "[19,    40] loss: 0.023\n",
            "[20,    10] loss: 0.065\n",
            "[20,    20] loss: 0.047\n",
            "[20,    30] loss: 0.033\n",
            "[20,    40] loss: 0.045\n",
            "[21,    10] loss: 0.071\n",
            "[21,    20] loss: 0.060\n",
            "[21,    30] loss: 0.048\n",
            "[21,    40] loss: 0.049\n",
            "[22,    10] loss: 0.115\n",
            "[22,    20] loss: 0.107\n",
            "[22,    30] loss: 0.057\n",
            "[22,    40] loss: 0.103\n",
            "[23,    10] loss: 0.234\n",
            "[23,    20] loss: 0.148\n",
            "[23,    30] loss: 0.106\n",
            "[23,    40] loss: 0.083\n",
            "[24,    10] loss: 0.034\n",
            "[24,    20] loss: 0.035\n",
            "[24,    30] loss: 0.026\n",
            "[24,    40] loss: 0.028\n",
            "[25,    10] loss: 0.019\n",
            "[25,    20] loss: 0.022\n",
            "[25,    30] loss: 0.016\n",
            "[25,    40] loss: 0.009\n",
            "[26,    10] loss: 0.006\n",
            "[26,    20] loss: 0.004\n",
            "[26,    30] loss: 0.004\n",
            "[26,    40] loss: 0.005\n",
            "[27,    10] loss: 0.003\n",
            "[27,    20] loss: 0.002\n",
            "[27,    30] loss: 0.003\n",
            "[27,    40] loss: 0.003\n",
            "[28,    10] loss: 0.002\n",
            "[28,    20] loss: 0.001\n",
            "[28,    30] loss: 0.001\n",
            "[28,    40] loss: 0.003\n",
            "[29,    10] loss: 0.002\n",
            "[29,    20] loss: 0.003\n",
            "[29,    30] loss: 0.002\n",
            "[29,    40] loss: 0.002\n",
            "[30,    10] loss: 0.001\n",
            "[30,    20] loss: 0.002\n",
            "[30,    30] loss: 0.001\n",
            "[30,    40] loss: 0.002\n",
            "[31,    10] loss: 0.001\n",
            "[31,    20] loss: 0.001\n",
            "[31,    30] loss: 0.001\n",
            "[31,    40] loss: 0.002\n",
            "[32,    10] loss: 0.002\n",
            "[32,    20] loss: 0.002\n",
            "[32,    30] loss: 0.001\n",
            "[32,    40] loss: 0.005\n",
            "[33,    10] loss: 0.005\n",
            "[33,    20] loss: 0.004\n",
            "[33,    30] loss: 0.002\n",
            "[33,    40] loss: 0.002\n",
            "[34,    10] loss: 0.001\n",
            "[34,    20] loss: 0.001\n",
            "[34,    30] loss: 0.001\n",
            "[34,    40] loss: 0.001\n",
            "[35,    10] loss: 0.001\n",
            "[35,    20] loss: 0.001\n",
            "[35,    30] loss: 0.001\n",
            "[35,    40] loss: 0.013\n",
            "[36,    10] loss: 0.098\n",
            "[36,    20] loss: 0.043\n",
            "[36,    30] loss: 0.028\n",
            "[36,    40] loss: 0.019\n",
            "[37,    10] loss: 0.010\n",
            "[37,    20] loss: 0.009\n",
            "[37,    30] loss: 0.006\n",
            "[37,    40] loss: 0.006\n",
            "[38,    10] loss: 0.004\n",
            "[38,    20] loss: 0.003\n",
            "[38,    30] loss: 0.002\n",
            "[38,    40] loss: 0.018\n",
            "[39,    10] loss: 0.018\n",
            "[39,    20] loss: 0.026\n",
            "[39,    30] loss: 0.017\n",
            "[39,    40] loss: 0.010\n",
            "[40,    10] loss: 0.006\n",
            "[40,    20] loss: 0.004\n",
            "[40,    30] loss: 0.005\n",
            "[40,    40] loss: 0.002\n",
            "[41,    10] loss: 0.002\n",
            "[41,    20] loss: 0.002\n",
            "[41,    30] loss: 0.001\n",
            "[41,    40] loss: 0.001\n",
            "[42,    10] loss: 0.001\n",
            "[42,    20] loss: 0.001\n",
            "[42,    30] loss: 0.001\n",
            "[42,    40] loss: 0.001\n",
            "[43,    10] loss: 0.001\n",
            "[43,    20] loss: 0.001\n",
            "[43,    30] loss: 0.001\n",
            "[43,    40] loss: 0.001\n",
            "[44,    10] loss: 0.001\n",
            "[44,    20] loss: 0.001\n",
            "[44,    30] loss: 0.000\n",
            "[44,    40] loss: 0.001\n",
            "[45,    10] loss: 0.000\n",
            "[45,    20] loss: 0.001\n",
            "[45,    30] loss: 0.000\n",
            "[45,    40] loss: 0.001\n",
            "[46,    10] loss: 0.001\n",
            "[46,    20] loss: 0.000\n",
            "[46,    30] loss: 0.000\n",
            "[46,    40] loss: 0.001\n",
            "[47,    10] loss: 0.000\n",
            "[47,    20] loss: 0.001\n",
            "[47,    30] loss: 0.000\n",
            "[47,    40] loss: 0.000\n",
            "[48,    10] loss: 0.000\n",
            "[48,    20] loss: 0.000\n",
            "[48,    30] loss: 0.000\n",
            "[48,    40] loss: 0.001\n",
            "[49,    10] loss: 0.000\n",
            "[49,    20] loss: 0.000\n",
            "[49,    30] loss: 0.000\n",
            "[49,    40] loss: 0.009\n",
            "[50,    10] loss: 0.015\n",
            "[50,    20] loss: 0.012\n",
            "[50,    30] loss: 0.007\n",
            "[50,    40] loss: 0.008\n",
            "[51,    10] loss: 0.003\n",
            "[51,    20] loss: 0.003\n",
            "[51,    30] loss: 0.002\n",
            "[51,    40] loss: 0.003\n",
            "[52,    10] loss: 0.001\n",
            "[52,    20] loss: 0.001\n",
            "[52,    30] loss: 0.001\n",
            "[52,    40] loss: 0.001\n",
            "[53,    10] loss: 0.001\n",
            "[53,    20] loss: 0.001\n",
            "[53,    30] loss: 0.000\n",
            "[53,    40] loss: 0.040\n",
            "[54,    10] loss: 0.246\n",
            "[54,    20] loss: 0.153\n",
            "[54,    30] loss: 0.129\n",
            "[54,    40] loss: 0.081\n",
            "[55,    10] loss: 0.046\n",
            "[55,    20] loss: 0.038\n",
            "[55,    30] loss: 0.035\n",
            "[55,    40] loss: 0.026\n",
            "[56,    10] loss: 0.019\n",
            "[56,    20] loss: 0.011\n",
            "[56,    30] loss: 0.011\n",
            "[56,    40] loss: 0.014\n",
            "[57,    10] loss: 0.020\n",
            "[57,    20] loss: 0.012\n",
            "[57,    30] loss: 0.008\n",
            "[57,    40] loss: 0.017\n",
            "[58,    10] loss: 0.028\n",
            "[58,    20] loss: 0.018\n",
            "[58,    30] loss: 0.015\n",
            "[58,    40] loss: 0.006\n",
            "[59,    10] loss: 0.004\n",
            "[59,    20] loss: 0.003\n",
            "[59,    30] loss: 0.002\n",
            "[59,    40] loss: 0.003\n",
            "[60,    10] loss: 0.002\n",
            "[60,    20] loss: 0.002\n",
            "[60,    30] loss: 0.001\n",
            "[60,    40] loss: 0.001\n",
            "[61,    10] loss: 0.001\n",
            "[61,    20] loss: 0.001\n",
            "[61,    30] loss: 0.001\n",
            "[61,    40] loss: 0.001\n",
            "[62,    10] loss: 0.001\n",
            "[62,    20] loss: 0.001\n",
            "[62,    30] loss: 0.001\n",
            "[62,    40] loss: 0.001\n",
            "[63,    10] loss: 0.001\n",
            "[63,    20] loss: 0.001\n",
            "[63,    30] loss: 0.001\n",
            "[63,    40] loss: 0.001\n",
            "[64,    10] loss: 0.001\n",
            "[64,    20] loss: 0.001\n",
            "[64,    30] loss: 0.001\n",
            "[64,    40] loss: 0.020\n",
            "[65,    10] loss: 0.042\n",
            "[65,    20] loss: 0.037\n",
            "[65,    30] loss: 0.027\n",
            "[65,    40] loss: 0.058\n",
            "[66,    10] loss: 0.183\n",
            "[66,    20] loss: 0.144\n",
            "[66,    30] loss: 0.087\n",
            "[66,    40] loss: 0.103\n",
            "[67,    10] loss: 0.136\n",
            "[67,    20] loss: 0.092\n",
            "[67,    30] loss: 0.053\n",
            "[67,    40] loss: 0.053\n",
            "[68,    10] loss: 0.046\n",
            "[68,    20] loss: 0.021\n",
            "[68,    30] loss: 0.018\n",
            "[68,    40] loss: 0.021\n",
            "[69,    10] loss: 0.012\n",
            "[69,    20] loss: 0.017\n",
            "[69,    30] loss: 0.009\n",
            "[69,    40] loss: 0.006\n",
            "[70,    10] loss: 0.003\n",
            "[70,    20] loss: 0.003\n",
            "[70,    30] loss: 0.003\n",
            "[70,    40] loss: 0.003\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 train images: 100 %\n",
            "Accuracy of the network on the 10000 test dataset 1: 44 %\n",
            "Accuracy of the network on the 10000 test dataset 2: 69 %\n",
            "Accuracy of the network on the 10000 test dataset 3: 87 %\n",
            "Accuracy of the network on the 10000 test dataset 4: 95 %\n",
            "Accuracy of the network on the 10000 test dataset 5: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 6: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 7: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 8: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 9: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 10: 95 %\n",
            "--------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbZaQekCfVjN"
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "ouBomi5DfVjR",
        "outputId": "408cc39c-5205-4871-ea00-06a1a453368a"
      },
      "source": [
        "fig = plt.figure()\n",
        "for i,j in enumerate(train_loss_all):\n",
        "    plt.plot(j,label =\"dataset \"+str(i+1))\n",
        "    \n",
        "\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Training_loss\")\n",
        "\n",
        "plt.legend()\n",
        "fig.savefig(\"Figure.pdf\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nOzdd3iUZbr48e8zk0nPpE56IZQE0iaE\nQOiKSLO3ddFVD/uznj3qsRxcV7e4etx1V7cc++q6svbCuoiKYgGlSAsQAkkghBBIIL33TCbP749J\nYoD0mckk5Plc11wh77zzvvdk17nnafcjpJQoiqIo45fG0QEoiqIojqUSgaIoyjinEoGiKMo4pxKB\noijKOKcSgaIoyjjn5OgAhiMgIEBOmDDB0WEoiqKMKXv37q2QUhrOPj4mE8GECRNIT093dBiKoihj\nihDiRG/HVdeQoijKOKcSgaIoyjinEoGiKMo4NybHCBRFOT+ZTCaKiopoaWlxdChjmqurK+Hh4eh0\nukGdrxKBoiijRlFREV5eXkyYMAEhhKPDGZOklFRWVlJUVER0dPSgXqO6hhRFGTVaWlrw9/dXScAK\nQgj8/f2H1KpSiUBRlFFFJQHrDfVvOK4SwaFTtTy/6aijw1AURRlVxlUi+DjjFM98mcsrW445OhRF\nUcaAxx57jGeeeabfc9atW0d2drZN71tQUMA777zT5/PLly/Hx8eHyy67zCb3G1eJ4OEV07g0MYTf\nbTjMv/YWOTocRVHOA45IBKtXr+bNN9+02f3GVSLQagR//rGReZP9eehfmWw6XOrokBRFGWWefPJJ\nYmJimD9/PkeOHOk+/uqrrzJz5kyMRiPXXnstTU1NfP/996xfv57Vq1eTnJzMsWPHej0P4MMPPyQh\nIQGj0cjChQsBMJvNrF69mpkzZ5KUlMTf/vY3AB5++GG2bt1KcnIyf/nLX86JcfHixXh5ednsPY+7\n6aMuTlr+dnMqK1/Zwc/e3sfbt6UxI8rP0WEpinKW336SRfbpOpteMy5Uz28uj+/z+b179/Lee++R\nkZFBe3s7KSkpzJgxA4BrrrmG22+/HYBf/vKXvPbaa9xzzz1cccUVXHbZZVx33XUA+Pj49Hre448/\nzsaNGwkLC6OmpgaA1157DW9vb/bs2UNrayvz5s1j6dKlPPXUUzzzzDN8+umnNn3/fRlXLYIuni5O\nrPnpLIL1rvz09T18mnkac4fau1lRxrutW7dy9dVX4+7ujl6v54orruh+7tChQyxYsIDExETefvtt\nsrKyer1GX+fNmzePVatW8eqrr2I2mwH48ssveeONN0hOTiYtLY3KykqOHh35CS3jrkXQJcDThTdv\nTWPV67u5+539RAfkcufCiVydEoaLk9bR4SnKuNffN3dHWLVqFevWrcNoNLJmzRq+/fbbIZ338ssv\ns2vXLj777DNmzJjB3r17kVLy3HPPsWzZsjOu0de17WVctgi6RPi58+X9F/DiT1LwcNHy8EcHWfjH\nzWogWVHGqYULF7Ju3Tqam5upr6/nk08+6X6uvr6ekJAQTCYTb7/9dvdxLy8v6uvrBzzv2LFjpKWl\n8fjjj2MwGCgsLGTZsmW89NJLmEwmAHJzc2lsbDznmvY2blsEXbQawSWJIaxICGZbXgV//fooD354\ngPoWE6vmDW55tqIo54eUlBR+/OMfYzQaCQwMZObMmd3PPfHEE6SlpWEwGEhLS+v+oF65ciW33347\nzz77LGvXru3zvNWrV3P06FGklCxevBij0UhSUhIFBQWkpKQgpcRgMLBu3TqSkpLQarUYjUZWrVrF\n/ffff0acCxYs4PDhwzQ0NBAeHs5rr712TqtiKISUY69vPDU1VdprY5q29g7ueXcfG7NKeezyOJUM\nFGUE5eTkMG3aNEeHcV7o7W8phNgrpUw9+9xx3TXUG2cnDc/dkMKy+CAe+ySbNduPOzokRVEUu1KJ\noBcqGSiKMp6oRNCHs5PBY+uzaG03OzosRVEUm7NrIhBC/EMIUSaEONTH80II8awQIk8IkSmESLFn\nPEPl7KTh+RtT+Om8Caz5voDrX95BYVWTo8NSFEWxKXu3CNYAy/t5fgUwpfNxB/CSneMZMp1Ww28u\nj+flm2aQX9HIJc9u5YtDJY4OS1EUxWbsmgiklFuAqn5OuRJ4Q1rsBHyEECH2jGm4licEs+HeBUQH\neHDXW3v5v69VOWtFUc4Pjh4jCAMKe/xe1HnsHEKIO4QQ6UKI9PLy8hEJ7mwRfu58eNccrkkJ4y9f\n5/LaNjWIrCjns9FYhjojI4M5c+YQHx9PUlIS77//vtX3c3QiGDQp5StSylQpZarBYHBYHC5OWv54\nbRLL44N54tNstQpZUca5kU4E7u7uvPHGG2RlZfHFF19w3333dRexGy5HJ4JTQESP38M7j41qTloN\n/3dDcnc566+yVTlrRTlfjPYy1DExMUyZMgWA0NBQAgMDsbaXxNElJtYDdwsh3gPSgFopZbGDYxqU\nrnLWP3l1J//1zj7++dNZzJnk7+iwFOX88fnDUHLQttcMToQVT/X59FgrQ717927a2tqYNGmSVX8W\ne08ffRfYAcQKIYqEELcKIe4SQtzVecoGIB/IA14FfmbPeGytq5x1lJ87d7yRzumaZkeHpCiKFcZS\nGeri4mJuvvlmXn/9dTQa6z7K7doikFLeMMDzEvgve8Zgb74ezrz2HzNZ9tct/HLdIV77j1SEEI4O\nS1HGvn6+uTvCaCpDXVdXx6WXXsqTTz7J7NmzrX5vjh4jOC9E+rvzP8ti2XS4jPUHTjs6HEVRhmks\nlKFua2vj6quv5pZbbunujrKWSgQ2smruBJIjfHhsfRaVDa2ODkdRlGHoWYZ6xYoVvZahnjdvHlOn\nTu0+vnLlSp5++mmmT5/OsWPH+jxv9erVJCYmkpCQwNy5czEajdx2223ExcWRkpJCQkICd955J+3t\n7WeUoT57sPiDDz5gy5YtrFmzhuTkZJKTk8nIyLDqfasy1DaUW1rPpc9uZXlCCM/dMN3R4SjKmKPK\nUNuOKkPtIDFBXty9aAqfHDitppQqijJmqERgY/954SSmBnvxy3UHqWsxOTocRVGUAalEYGPOThr+\ncG0SZfWtvLA5z9HhKIqiDEglAjswRvhweVIob+44QVVjm6PDURRF6ZdKBHZy7+LJNJvM/H1rvqND\nURRF6ZdKBHYyOdCLSxND+Of3BVSrVoGiKKOYSgR2dO/iKTSZzKpctaKMUaOxDPWJEydISUkhOTmZ\n+Ph4Xn75ZavvpxKBHcUEeXFJQghrvi+gpkm1ChTlfDTSiSAkJIQdO3aQkZHBrl27eOqppzh92rqK\nBuMrEdSchGObR/SW9yyeTENrO/9QrQJFGRNGexlqZ2dnXFxcAGhtbaWjo8Pq9+zoMtQja8cLsO8N\n+EURaLQjcsupwXpWJATz+vYCbp0/EW933YjcV1HGuj/s/gOHqw7b9JpT/aby81k/7/P5sVKGurCw\nkEsvvZS8vDyefvppQkNDrfq7jK8WQXASmJqg8tiI3vbexVOob23n+c1HMXeMvZIeijJejJUy1BER\nEWRmZpKXl8c///lPSkutq2QwvloEIUbLz5JMMMSM2G2nhei5whjKq1uP80VWCbfMnsD1qRGqdaAo\n/ejvm7sjjKYy1F1CQ0NJSEhg69atVlUiHV8tAkMsaF2g+MCI3/rP1xt58ScphOjdeHJDDrN//w2/\n/vgQbe3W9+8pimIbY6EMdVFREc3Nlk2wqqur2bZtG7GxsVa97/HVItDqIHCaQxKBk1bDJYkhXJIY\nQtbpWl7bepw3dpxgRpQvVyaHjXg8iqKcq2cZ6sDAwF7LUBsMBtLS0ro/qFeuXMntt9/Os88+y9q1\na/s8b/Xq1Rw9ehQpJYsXL8ZoNJKUlERBQQEpKSlIKTEYDKxbt+6MMtSrVq3i/vvv744jJyeHBx98\nECEEUkr+53/+h8TERKve9/grQ73+Hsj5BB46Dg7cSayjQzLzya+ZPyWA/1upSlYrCqgy1LakylD3\nJzgJmquhtsihYWg0ggtiDXyXW64GkBVFcajxlwhCki0/SzIdGwdw0dRAappM7D9Z7ehQFEUZx8Zf\nIgiKB6GBYscnggVTDGg1gm8Olzk6FEVRxrHxlwic3cF/ikMGjM/m7aZj5gRfNqtEoCiKA42/RAAQ\nkjQquobA0j10uKSeUzXNjg5FUZRxanwmguAkqDsFjZWOjoSLpgYCqFaBoigOMz4TQfcKY8d3D00y\neBLh56YSgaKMQqOxDHWXuro6wsPDufvuu62+3/hMBMGdiy9GwYCxEIKLYgPZfqyCFpP5nOc71NRS\nRRnVHJUIfvWrX3VXMbXW+EwE7n7gHTkqBowBFk0NpMXUwY5jP3RVSSn5wxeHmfW7r2lqa3dgdIoy\nvoz2MtRgqZJaWlrK0qVLbfKe7V5iQgixHPg/QAv8XUr51FnPRwL/BHw6z3lYSrnB3nGNpgHj2RP9\ncdNp2XS4jEWdYwZ//iqXl761VEnNKa5nRpSvI0NUlBFX8rvf0Zpj2zLULtOmEvzII30+PxbKUHd0\ndPDggw/y1ltv8fXXX9vk72LXFoEQQgu8AKwA4oAbhBBxZ532S+ADKeV0YCXwoj1j6hZitJSjbm0Y\nkdv1x1WnZd7kADYdLrNUI/zmKM9tymNJXBAA2adrHRyhoowPY6EM9Ysvvsgll1xCeHi4jd61/VsE\ns4A8KWU+gBDiPeBKoGeHmgT0nf/2Bqzbc22wgpMsty49BJGzR+SW/bloaiBf55TyyL8P8e7uk1wz\nPYynf2Rkxv9+RXZxnaPDU5QR1983d0cYLWWod+zYwdatW3nxxRdpaGigra0NT09PnnrqqT5fMxB7\njxGEAYU9fi/qPNbTY8BNQogiYANwT28XEkLcIYRIF0Kkl5eXWx9ZSJLl56gZJzAA8O7uk1yWFMIf\nr0tCqxHEh+rJOq0SgaKMhLFQhvrtt9/m5MmTFBQU8Mwzz3DLLbdYlQRgdAwW3wCskVKGA5cAbwoh\nzolLSvmKlDJVSplqMBisv6tXCLgHjIqZQwAh3m4sjQvi6ulh/OXHyThpLX+CuBA9h0vqaTerfQsU\nxd56lqFesWJFr2Wo582bx9SpU7uPr1y5kqeffprp06dz7NixPs9bvXo1iYmJJCQkMHfuXIxGI7fd\ndhtxcXGkpKSQkJDAnXfeSXt7+xllqHsbLLY1u5ahFkLMAR6TUi7r/P0XAFLK3/c4JwtYLqUs7Pw9\nH5gtpexzYr1VZah7evNqaCyHu7ZZfy07Wbf/FPe9n8HG+xYSG+zl6HAUxa5UGWrbGU1lqPcAU4QQ\n0UIIZyyDwevPOucksLgzyGmAK2CDvp9BCDFC2WFobxuR2w1HXKhl+CS7WA0YK4piH3ZNBFLKduBu\nYCOQg2V2UJYQ4nEhRNdw/IPA7UKIA8C7wCo5UrvlBCdBhwnKc0bkdsMxMcADFycNWafUOIGiKPZh\n93UEnWsCNpx17Nc9/p0NzLN3HL0K7dwZ7MT3P5SdGGWctBqmhqgBY0VR7Gc0DBY7jl80BCXAwbWO\njqRfcSF6sovrGIvbiiqKMvqNq0SwpWgLz+9//syDiT+CU+lQle+YoAYhPlRPbbNJlapWFMUuxlUi\n2F+2n9cOvkZ7R4/aPYmWZeGjuVXQPWCsuocURbGDcZUIIrwiaJftlDSW/HDQOxyi5kHmBzBKu16m\nBevRCNQ4gaKMsNFahlqr1ZKcnExycvIZZTCGa1wlgnBPS22OwvrCM59I/BFUHh01q4zP5uasJTrA\nQ5WaUJRRyBGJwM3NjYyMDDIyMli//uwZ+UM3rhJBhFcE0EsiiLsSNDo4+KEDohqc+FBv1TWkKCNg\nLJShtjW7Tx8dTQLdA9FpdBQ1FJ35hLsfTFkCh/4FSx4HjdYxAfYjLlTP+gOnqWlqw8fd2dHhKIrd\nbf0gl4pC21YHDojwZMH1MX0+PxbKUAO0tLSQmpqKk5MTDz/8MFdddZVVf5dxlQi0Gi1hnmEU1Red\n+2Tij+DIBijYBhMvGPngBhDfY8B47uQAB0ejKOennmWogXPKUP/yl7+kpqaGhoaGcyqGDnReVxnq\n66+/nmuuuQawlKHOzMxk7VrLZJXa2lqOHj2Ks3P/X/ZOnDhBWFgY+fn5XHTRRSQmJjJp0qRhv+9x\nlQgAwr3Cz+0aAohdAc6elu6hUZgI4kK6Sk2oRKCMD/19c3eE0VKGGiAszFLEeeLEiVx44YXs37/f\nqkQwrsYIwDJOUFRfdO7iLJ0bTLscsteDqcUxwfXD39OFYL2rmjmkKHY0FspQV1dX09raCkBFRQXb\nt28nLu7s/b6GZvy1CDzDaTA1UNNag6/rWds/Jv4IDrwLeV9ZksIoExeqVwPGimJHPctQBwYG9lqG\n2mAwkJaW1v1BvXLlSm6//XaeffZZ1q5d2+d5q1ev5ujRo0gpWbx4MUajkaSkJAoKCkhJSUFKicFg\nYN26dWeUoV61ahX3339/dxw5OTnceeedaDQaOjo6ePjhh61OBHYtQ20v1pSh3nxyM/duvpe3L3mb\nJEPSmU+a2+HP0yA8FW541waR2tafvjzCi98eI+u3y3DVjb4BbUWxlipDbTs2L0MthPi9EEIvhHAS\nQmwUQpQKIW60UbwjpiU3l7D0kwC9DxhrnSD1p5ZB4yIb7HdgY3EheswdktzS3puMiqIowzHYMYIV\nUso64DIsewpPBX5ut6jspPajf9Px+F8RUvY+YAww9x7wCISNj466lcbxod4AHFIlqRVFsaHBJoKu\nsYRLgA+llNVYNp0fU5yjIpEtLUxq9+87Ebh4wUWPQuFOyLF+xZ4tRfi54efhzN4T1Y4ORVGU88hg\nE8HnQohDQBrwlRAiAGi1X1j2oYuMBCC+2e/cRWU9Tb8ZAuPgq9+Mqt3LhBDMmuDHruOVjg5FUZTz\nyKASgZRyNXARMENKaQKagWvsGZg9OEdNAGBig3vfLQKwrCxe8gRUH4c9r45McIOUNtGPoupmVZJa\nURSbGexg8TVAs5SyXQjxMPA6YLBrZHagCwkGnY7QGkFZUxkt7f2sF5hyMUy6CL77IzRVjVyQA5gV\n7QfArnzVKlAUxTYG2zX0mJSyXggxF8s4wdvAy/YLyz6EVotzeDj+lZbFG6cbTvf/gqX/C611sKX/\nMrQjaWqwHr2rE7vyR09yUpTz1WgtQ33y5EmWLl3KtGnTiIuLo6CgwKr7DTYRmDt/Xgb8TUr5MeBi\n1Z0dxDkyEvfSWqCXKqRnC4qH5J/A7legusD+wQ2CViOYFa3GCRRltHBEIrjllltYvXo1OTk57N69\nm8DAQKvuN9hEUCyEeAFYCWwQQjgP4bWjii4qEm1RGUjZ/4Bxl0WPgBCw7a/2D26Q0qL9KahsorRu\n9JXCUJSxbrSXoc7Ozqa9vZ0lS5YA4Onp2V0kb7gGW2LieixdQs9JKauFEKHAw1bd2UF0EZHI5mZC\nWj0HbhEA6ENh+k2w/y1YuBq8w+wf5ADSJnaOExyv4gpjqIOjURT72LzmFcpO2HYv8cCoiSxadUef\nz4+FMtS5ubn4+PhwzTXXcPz4cS6++GKeeuoptNrhVxsY7KyhBiALuFAIcRfgK6X8fNh3dZBt773B\nh5stf9iEloDBJQKAefdBhxm+f86O0Q1eXIgeTxcnNWCsKDbWswy1Xq8/pwz1ggULSExM5O233yYr\nK6vXa/R1XlcZ6ldffRWz2dLb/uWXX/LGG2+QnJxMWloalZWVHD16tN8Y29vb2bp1K8888wx79uwh\nPz+fNWvWWPW+B9UiEELcDfwMWNd56AMhxAtSyhetuvsIa22GxsYGzEIwudGTjb2VmeiNbxQYV8Le\nNbDgAfC0rj/OWk5aDakTfNl1XA0YK+ev/r65O8JoKUMdHh5OcnIyEydOBOCqq65i586d3HrrrcN+\nb4Pt578DmCWlfERK+QiWhWV3DfuuDlJSbsLk7U+zqzMRtU4U1RfRITsG9+L590N7C+x4wb5BDlJa\ntD95ZQ1UNIy5dX2KMmqNhTLUM2fOpKamhvLycgA2bdpkdfXRwSYCAfRcYmvqPDamNDtLWoKjaAqN\nJKDSRFtHG2VNZYN7ccAUiL8a9vx9VKwr6Bon2K1aBYpiMz3LUK9YsaLXMtTz5s1j6tSp3cdXrlzJ\n008/zfTp0zl27Fif561evZrExEQSEhKYO3cuRqOR2267jbi4OFJSUkhISODOO++kvb39jDLUZw8W\na7VannnmGRYvXkxiYiJSyu4xieEaVBlqIcRDwA3AvzoPXQ28K6V0yAT74ZahXv+3j9hXnEnKyWKi\nK05w47WneH3Z66QGn1OVtXclh+DleXDBw7DoF0O+vy2ZzB0Yf/slP5oRzm+vTHBoLIpiK6oMte3Y\nvAy1lPKPwJ1AU+fjLkclAWsE6AQenpXUeHjidLoC+qtC2pvgBIi9FHa9BC2OrQCq02qYEaXGCRRF\nsV6/iaBzDwK9EEIPHAb+3vk40nlsQEKI5UKII0KIvM7yFL2dc70QIlsIkSWE6HsVhZWcwjaTlPQV\nrToXaGjEp0U7tEQAsPBBaKmF9H/YJ8ghSIv243BJPdWNo6cwnqIoY89As4aysJSb7hoP6OpHEp3/\njuzvxUIILfACsAQoAvYIIdZLKbN7nDMF+AUwr3ONgt2m5Hj7TKS+0YTZ2TJ1K26gKqS9CZsBEWlw\ncC3Mv88OUQ5e2kR/AHYXVLEsPtihsSiKMnb12yKQUkZIKSM7f3b9u+v37iQghJjaxyVmAXlSynwp\nZRvwHnDlWefcDrzQuccBUspBjt4OnZfvBAA0HpbKnbFN+t53KhvItCug9CBU2Xaxy1AlhXvj4qRR\ndYcURbGKrcpE9NWdEwb07Hsp6jzWUwwQI4TYLoTYKYRY3tuFhBB3CCHShRDpXdOmhsrdLwoAjUcj\nUggi63RD7xoCmHaZ5WfOJ/2fZ2cuTlpSIn3ZqRaWKYpiBVslAmumkjoBU4ALscxMelUI4XP2SVLK\nV6SUqVLKVINheBWw3fWWRKB1b4TQEAKrOqhpraG+bYh7APtOgOAkhycCgHmT/ckurqO8Xq0nUBRl\neGyVCPqag3oKiOjxe3jnsZ6KgPVSSpOU8jiQiyUx2JxO54s069C6N9AaHoq+3FIMaljdQ3FXQNEe\nqBuglLWdXRhrGVLZkju8VpKiKH0bjWWoN2/eTHJycvfD1dWVdevW9XruYNm7gugeYIoQIrqzYulK\n4OyNgNdhaQ3QuQVmDGCXznchBLLVBxfXBqoDAnEptvStn6g/MfSLTeusQXL4MxtGOHRxIXoCPF34\nViUCRXGIkU4EixYtIiMjg4yMDDZt2oS7uztLly616n62SgTm3g5KKduBu4GNQA7wgZQySwjxuBCi\nq5rTRqBSCJENbAZWSynt1umtbTfg4tJIlYsn1Nbj2+ZMZnnm0C9kiIWAGIdvcK/RCC6IMbD1aDnm\njoEXByqK0r/RXoa6p7Vr17JixYqRKUMthEjq5XAtUCil7JBSzuzleQCklBuADWcd+3WPf0vggc6H\n3bk5hdGmO0axsOTABXIS6SVDX6UMwLTLLfsUNFaCh78NoxyaC2MN/GtfERmFNcyI8nVYHIpiSzWf\nHKPtdKNNr+kc6oHP5ZP6fH4slKHu6b333uOBB6z/6Bxsi+A1YC/wBvAmkA58DBwVQiy2OooRpPea\ngE7XSpuwNGJSTKEcrjpMXdswVgpPuwKkGY5sGPhcO1owJQCNgO+O2G3mraKMC2OhDHWX4uJiDh48\neE7l0uEY7MY0BcCtUspMACFEIvAr4BFgLZBsdSQjRO8/kZIywNUyU2hSowfSU7K/dD8XRFwwtIuF\nGME70jJ7KOVm2wc7SD7uzkyP9OXb3HIeWBrrsDgUxZb6++buCKOlDHWXDz74gKuvvhqdTmflOxt8\ni2BaVxIAkFIeBOKklHlWRzDCPPwtU0iFWwNOwcH4V7Sh0+hILx1G95AQlu6h/M0Orz10YYyBzKJa\nVZZaUawwFspQd3n33Xe54YYbbPK+B5sIDgshnhNCzOt8PNt5zAVot0kkI8Td54e1BE4REZgLT5EY\nkMiekj3Du+C0y8HcBke/tGGUQ6emkSqK9cZCGWqwzCoqLCzkgguG2IvRh8GWoXYH7gHmdx7aDjwH\ntACeUspam0QzSMMtQw0gZQfffDOVU0VTmXN4Gmzdzsbnf8zfD/6d7Su34+nsObQLdpjhT1Mhag5c\n/8awYrKFjg7JrN99zdxJATx7w3SHxaEo1lBlqG3HHmWom6SUf5BSXt75eEpK2SilNI90ErCWEBpk\nizcurg3U+vhgrqxkplcCHbKD/WX7h35BjdZScuLoV9BcbfuABxuGRrAwxsAWNY1UUZQhGlQiEELM\nFkJ83lkqOrfrYe/g7EVj8sfFtZFynSsAU5t9cNI4DW+cACD1VjA1wc6XbBjl0F0YG0hNk4kDRTUO\njUNRlLFlsGMErwMvAhcDC3o8xiRnEYKrSyPV7Zb9ikXBKRL8E4afCIITYOplsPNlaHbch/DCzmmk\n3x45c5ygobWd1vZe1/wpiqIMOhHUSSk/kVKellKWdj3sGpkdeblNwNmlmWZTK1pfXxq+/ZbU4FSy\nKrJoMjUN76IX/Bxaa2HXy7YNdgh83J1JjvDhuyNlSCnZfbyK+97bT8oTX/HQ2mGsnlYUZVwYbCLY\nJIT4vRBiphAiqeth18jsyMtnAgDtohqviy+mYfNmUn2NmKWZjLKM4V00JMmyjeXOFy07mDnIhbGB\nZJ6qZelftnD933bwTU4ZAR7O7D3huPELRVFGt8Emgvmdjz9j2XHsBeB5ewVlb16BEwAQLvV4LVtG\nR1MTMbmNaIV2+N1DABestiSBXa/YJtBhWBYfjEYI3J21/PHaJHY9upiVsyIpqm6msXVMzfRVFGWE\nDHbW0IJeHgvtHZy9eBqiAdC6N+CRNguttzdt33xHvH+8dYkgdDrELIcdzztsgVlssBeHHlvGx3fP\n5/qZEbg7OxETZJkSm1fW4JCYFGWsGo1lqAEeeugh4uPjmTZtGvfeey+DWQbQn4E2r7+h8+e9vT2s\nurMDubqHIjsEWrcG2tpNeF68mIZNm5npn8zBioM0tzcP/+IXPAQtNbDbca0CN2ftGb9PCfIC4KhK\nBIpicyOdCL7//nu2b99OZsGoIoIAACAASURBVGYmhw4dYs+ePXz33XdW3W+gFkFXKUtDH48xSaNx\nQrbpcXFt5PSJE+iXLaOjoYHZRe60d7RzoPzA8C8eNgMmL7G0ClqHuPOZnUT5ueOs1XC0dHTEoyij\n2WgvQy2EoKWlhba2NlpbWzGZTAQFBVn1nvstOielfLHz56+sussopGn1w9WlgZJThUxYcCEavZ6Q\n3cfRJGhIL0lndsjs4V/8wofh74vhk/vgmlcsi84cyEmrYaLBg1yVCJQx5PPPP6ekpMSm1wwODmbF\nihV9Pj8WylDPmTOHRYsWERISgpSSu+++2+rV2INdUBYghHhICPGiEOKVrodVd3YwnTkIF9dGSotK\nEM7OeF10ES3fbiVeH8u3hd/SITuGf/HwVFj8azi0Fv59l6UMhYPFBHmRW6q6hhSlP2OhDHVeXh45\nOTkUFRVx6tQpNm3axNatW61634MtQ/0xsBPYRh+7kY017s6RtLnsorS8AgCvZUupXbeOVa0zeLDl\nLTYc38BlEy8b/g0WPAiyAzb9r6VK6VUvObRlEBPkyfoDp2lsbcfDZbD/syuK4/T3zd0RRksZ6n//\n+9/Mnj0bT0/LJJAVK1awY8cOFiwY/hrfwU4f9ZBSPiilfEdK+X7XY9h3HQXcPCMRQtLWbkkEHvPm\nofHwICGznml+0/jr3r9aN2gMsHA1LPolZL4P637m0JaBGjBWlIGNhTLUkZGRfPfdd7S3t2Mymfju\nu+9GpmsI+FwIYd3uyKOMPsAyhRSdZfGXxtkZz4suouHrr3lo+gOUNpXyRpYNqolesBoWPQqZ78Hn\nP7f+esMU05kI1DiBovRtLJShvu6665g0aRKJiYkYjUaMRiOXX365Ve97sGWoqwFvoAloAwSW7Yb9\nrLr7MFlThrpLXeVR9hxYzrHsedxxt+UDv/6bbyj6r7uJeO3v/KptLdtPb+fTqz8l0D3Q+qA/uQ/2\nvwkPHgGPAOuvN0TmDkncr7/gljlRPHpp3IjfX1EGQ5Whth2bl6EGAgAdlmRg6Px9zE4fBfDwiQRA\n69ZAe7tlxa3HvHlo3N2p/2Ij98+4H1OHief2P2ebG866Azra4eBa21xviLQawSSDpxowVhTlHAMt\nKJvS+c/4Ph5jllbrgmz1xNW1kbLi0wBoXF3xmD+fhu3biNRH8pOpP+HjvI/JrrTBYpGgOAhOggPv\nWn+tYYoJ8lRrCRRFOcdALYKHO3++0MtjzNYa6iJa/HBxbaC48GT3MfdZs2g/XUxb0SnuMN6Bj4sP\nz6Q/Y/USbgCSb4TiDCjLsf5awzAlyIvTtS3Ut5gccn9FUUanfhOBlPLWzp/nVa2hLlqTAVfXRkpO\nneo+5t45ONS0Zw96Zz0/S/4Ze0r2sLlws/U3TLgONE4OaxXEqJlDiqL0YrBjBAghpgohrhFC3Nj1\nsGdgI8FVE46LSyOFBSe6j7lMmYzW25umPZbN7K+NuZYJ+gn8dd9fae+wsnqnp8FSfiLzA4dMJe0q\nPqe6hxRF6WmwK4t/CbwCvAysAP4KXGfHuEaEm1sEGk0HDc0/7LEjNBrcZqZ2JwKdRsd9M+7jeO1x\nPjr6kfU3Na6E+mLI/9b6aw1RhK87rjqNGjBWFOUMg20R/BhYBBRLKW8GjICH3aIaIW7elplDGo9m\nKooKu4+7p6ZiKizE1Fnn5KKIi5geOJ0XM14c/g5mXWJXgKs3HHjPuusMg0YjmBzoqdYSKMogjdYy\n1D//+c9JSEggISGB99+3fm3vYBNBs5TSDLQLIbyAEiDK6rs7WMDEWABc3evJ/P6HWh0/jBNY1ioI\nIXgw9UEqWyr5Z9Y/rbupkwskXAs5nzhkz4IpgV4cVS0CRbGZkU4En332Gfv27SMjI4Ndu3bxzDPP\nUFdn3WfJYBPBfiGED/APIB3Y3fkYkBBiuRDiiBAiTwjxcD/nXSuEkEKIcxY72ItvSByi1YcA/5Pk\nHjrUfdx16lQ0Xl7d3UMARoORJVFLeD3rdSqaK6y7sfFGaG+GnPXWXWcYpgR5UlLXQm2zmjmkKL0Z\n7WWos7OzWbhwIU5OTnh4eJCUlMQXX3xh1XsesPqYEEIAj0kpa4AXhBAbAb2Uct8gXqvFMtV0CVAE\n7BFCrJdSZp91nhfw38CuYbyHYRNCg2/HIsx+H1PQVElLQwOunp4IrRb3lJQzEgHAfSn3sfnkZl7M\neJFfz/n18G8cngp+kyDjXZh+k5XvYmhiAi0zh/LK6pkR5ZCF4YoyKLm5T1DfYNup1l6e04iJ6buq\n/lgoQ200Gvntb3/Lgw8+SFNTE5s3byYuzrpqAQO2CKRlAv1XPX7PG0wS6DQLyJNS5ksp24D3gCt7\nOe8J4A9AyyCvazPh0deg0XTgH1XLsb0/fPC7z5pJ2/HjtJeXdx+L1Edyfez1fHT0I/Jr8od/UyEg\n+QY4sQ2qTwx8vg39UHNIdQ8pytnGQhnqpUuXcskllzB37lxuuOEG5syZg1ZrXWXjwdYjzhBCTJdS\n7h/i9cOAwh6/FwFpPU8QQqQAEVLKz4QQq/u6kBDiDuAOsFTfsxX/KWmY833xCS0k4+stxF+wCLAM\nGAM0paej71EO9y7jXazLW8cb2W/w2NzHhn/jhOssJapz1sPce6x5C0MS7uuGm06rBoyVUa+/b+6O\nMFrKUAM8+uijPProowDceOONxMTEWPXeBiox0ZUopmPp1jkihNgnhNgvhBhsq6C/62uAPwMPDnSu\nlPIVKWWqlDLVYLBdmSONTotTw0y8fUopKzuI2WRZK+AaF4dwd+8eMO7i6+pLWkgau0sGNUTSN79o\nCE60DBqPoK6ZQ2rAWFHONRbKUJvNZiorKwHIzMwkMzOTpUutKw49UItgN5ACXDHAeX05BUT0+D28\n81gXLyAB+NYyFEEwsF4IcYWU0rryokMQ7H0pxeJL/Cc3sm/jTmZeNh+h0+E+ffo54wQAqUGpbC7c\nTGljKUEeVuwVOu1K2Py/UF8CXsFWvIOhmRLkybajVg54K8p5qGcZ6sDAwF7LUBsMBtLS0ro/qFeu\nXMntt9/Os88+y9q1a/s8b/Xq1Rw9ehQpJYsXL8ZoNJKUlERBQQEpKSlIKTEYDKxbt+6MMtSrVq3i\n/vvv747DZDJ1b0Kj1+t56623cHKycrMpKWWfD2B/f88P9MCSaPKBaMAZOADE93P+t0DqQNedMWOG\ntKXmolr58ccz5CdrZ8pX//t33cfLX3pZZsdOlaaqqjPOz6rIkglrEuSnxz617salOVL+Ri/l7let\nu84QvfRtnoz6+afy0KmaEb2vogwkOzvb0SGcN3r7WwLpspfP1IEGiw1CiAf6egwiybQDdwMbgRzg\nAylllhDicSHEcFsZNucS6kVDeSxuvpW0NO2jvNCSwd1n/jBO0FOsbyxeOi/SS61stBhiwX/KiHcP\nXW4MJVjvyg2v7GTvieoRvbeiKKPPQIlAC3hi6cLp7TEgKeUGKWWMlHKSlPLJzmO/llKeM4leSnmh\nHMEuoS5CCJxbLGPY3tGF7F5vGf5wTUxEuLic0z2k1WhJCUohvcTKUIWAaZfD8a3QVGXdtYYgzMeN\ntf85B39PF276+y625JYP/CJFUc5bAyWCYinl41LK3/b2GJEIR0hwcDx1tQb8pzSTv283bc3taJyd\ncUtOPqdFAJZxgoK6AsqbrPwQnXY5SDPkWrcgZKjCfd354M45RAd4cOs/9/BZZvGI3l9R+iJtUfJ9\nnBvq33CgRCCGH8rYEh4fTXn5BFz8GtC4ZpC3rwywlJtozTmM+awl3KnBlm6jvaV7rbtx6HTQh494\n9xCAwcuFd++YTXKED3e/u48NB1UyUBzL1dWVyspKlQysIKWksrISV1fXQb9moKHmxdaFNHaERIdR\n/q8oJk5Kx2fCUQ5uziJuXiges9OoeP55Gr/fgX75D3N9p/pNxUPnQXppOsujlw//xl3dQ+n/gNYG\ncPG0wbsZPG83HW/8vzRWvrqTX607xLzJAXi76UY0BkXpEh4eTlFREeXlqrvSGq6uroSHhw/6/H4T\ngZRy5DquHczFxQVPbTCtNeH4Tioj9+Od1JbPR5+cjMbbm4bNm89IBE4aJ6YHTmdPybnTS4ds2uWw\n6yXI+wrir7b+ekPk5qzlyasSuOL5bfzlq1weu2JM70KqjGE6nY7o6GhHhzHuDHpjmvEgJCiI8opI\nXHxa0bnv5/DO0wgnJzwXLqRhyxak+czNZFKDUsmvzaeyudK6G0fOBvcAh3QPdUkI8+am2VG8saOA\nrNO1DotDUZSRpxJBD6GTIjhdaVnYpY88zaHNu5FS4nnhBZirq2k+kHnG+TYbJ9BoYeqlkLsRTCNe\nbqnbg0ti8XV35tcfZ9HRofpoFWW8UImgh9DwMNra3NE0ROEzsZH68gyK82rxXLAAnJxo2HzmvsVx\n/nG4OblZv54AYNoV0NbgkJ3Luni76/j5iqnsPVHNR/tPDfwCRVHOCyoR9BAcbGkNtJVMxt3QhEaX\nRfa2ArR6Pe4zZtDw7ZmJQKfRkWxIts04QfRCcPGG92+CvybCa8vgg/+ArX+GEZxBcV1KOCmRPvx+\nQ073ngVSSoqqm9h2tAKTuWPEYlEUZWSoRNCDu7s7ei89lZWW6qb6qCqO7NhOe5sZz0UX0no0j7ai\nojNeMzN4Jnk1eVS3WLlC18kZrl8Dc34GEbNBq4OiPfDNb6G6wLprD4FGI3j8ygSqm9q46829/Mc/\ndjPjf79m/h82c9Nru9QUU0U5D6lEcJbwiHBOmZxwbg3Ff0obbY2HyD9QjtciS3nqhs3fnnF+1zjB\nvlKri7HCpItgyeNw7auw6lP48ZuW46WH+n+djSWEefPTedHsLqiirL6Vi6cF8sSV8Tg7acg+PfLb\na45Xnx//nFVfrFJz6hW7U4ngLJMnT6axowXt6Xjcg2oRmgIObc7BOSoK5+joc8YJEvwTcNW62mac\n4GyBcSA0UHLQ9tcewC8vncbhJ5bz+X8v4I/XGbl5zgQmGzw5ovYxGDF7S/eyt3Sv9VujKsoAVCI4\ny+TJkwGoqYgE0YE+soHC7O9prG3Fc9EiGvfswdzwQy1/nVaH0WBk26ltfFv4LXtL93Kk6ghlTWXW\nB6Nzg4AYKM4c+FwbE0Kg0575f4+YIE9yS1QiGClVLZZlPAV1BY4NRDnvqURwFr1eT3BwMIVtHji1\n+xKUIDC3ZlOYXYnXogvBZKJx2/YzXrMgfAEFdQXcs+keVn2xius+uY7FHy7ms/zPrA8oONEhLYLe\nxAR7cbq2hboWtfH9SOhan3K89riDI1HOdyoR9GLy5MmUmGtwK0nCLbACRDW5uzJwmz69e5VxT7fE\n3cKGqzfw3qXv8cqSV/jTBX/Cx8WHHad3WB9McCLUFY1oddK+xHbud3xUdQ+NiK4WwYm6kd3XWhl/\nVCLoxZQpU5BImssmIUUrXuHNFGXvBa2211XGQggi9BHEB8QzJ3QOSycsJdmQzIHyA9YHE5xo+TkK\nWgVdG98fKVHbXI6EyhZLi0B1DSn2phJBL8LDw3F1deV0cwCaDncC4zW01B+mtrypz1XGZ0syJFFQ\nV0Btq5XlGoJGTyII83HDw1ltfD8STGYT9W2Wv3NBbYFjg1HOeyoR9EKr1TJp0iSKRA0e5Ul4hVUg\nZQ052w92rzKu27Ch32sYDUYA61sFngbwChkViUCjEUwJ8lKJYAR0dQv5ufpxquEUJrMal1HsRyWC\nPkyePJnG9mYoiUNqGvAKbyRv9w60ej3eV11J9bvv0nwoq8/XJwQkoBEaMsttMONnFA0Yx6pEMCK6\nEkFKYApmaaawodDBESnnM5UI+tA1jbSsLhwnsx/BKc1UFmYipSTooYdw8ven+JFHkG1tvb7eXefO\nFJ8pthsnqDji0IJ0XaYEeVLR0EZFQ+ugzm8xmTGrAnZD1jU+kBKUAqjuIcW+VCLog5eXFyEhIZxy\nrce3cDEewRXoPAspyMxDq9cT/NvHaM3NpeKVV/u8htFg5GDFQcwd5j7PGZTgROhoh/LD1l3HBmKD\nLQPGg2kVtLV3sPhP3/GnL4/YO6zzTneLoCsRqAFjxY5UIujHlClTKGmuxPX4XAQuGBKrOPjNFgC8\nFi1Cf/nlVLz8Mi1Hev+gMwYaaTQ1kl+bb10gwUmWn6Oge6hrCulgFpZtzCrhVE0zX+eU2jus805V\nsyURRHlF4e/qr1oEil2pRNCPrmmkJdp2/JoW4xtTz+ljPywmC3rkF2i9vSl65FFqKs4tA2CzAWPf\naNB5jIpEYPBywcddx5HSgaeQvr3LMv89t7Rh0F1JikVVSxUuWhc8dB5E6aNUi0CxK5UI+hEWFoab\nmxvFvg3oMy9Ao+3AMyqLus4PfSdfX/wffZSNQYE8//wLNDY2nvH6SK9IfFx8rE8EGg0EJ4yKRCCE\nIGYQA8Z5ZQ3szK9iaVwQALvyHb8gbiypbKnEz9UPIQTR3tFqUZliVyoR9EOj0TBp0iRONJegqwvE\npWk6AfHVZHz1FQBms5kva2soDwykHcnB778/4/VCCJIMSbadOdTh+P0AYoO8yC2p77cq5tu7TqDT\nCp64KgEPZy078lXhtKHoSgQAE/QTqGqpsn5NiqL0QSWCAcTGxtLU0sQ3nll4lyxD52am+PRapJR8\n+umnHDlyhKXz5uFdV0fGd1vOeb3RYCS/Nt/6/4iDE6GtHmoc/80wJtiL+tZ2Sup6n8XU3GbmX3uL\nWJ4QQpDelZnRfuxULYIhqWqu+iEReE8A1ICxYj8qEQwgPj6eiy++mKKOCtYXFtNS54dHVA6ff/Yp\n+/fvZ+HChcxdsoSpgYGUOGkp3bnzjNcnGSwDvQcrrOzWGU2lJgI9ATjSx4Dxp5mnqWtp5ydplg1+\nZk/0J6+sgbJ6x09/HSuqWqrOaBGAmkI6EtILqiisanJ0GCNOJYIBaDQa5s+fz53/cTtB0ocTxVNx\n82sl99gnzJgxg0WdG9bMvOlmAHa/9dYZXSaJAYlohMb6cQIH7k1wtq6aQ32NE7y96ySTDB6kRVs+\nyOZM9AewrlXQMn42xJFSUtVShb+b5e8W5hWGk3BS4wR2JqXk/63ZwxOfZjs6lBFn90QghFguhDgi\nhMgTQjzcy/MPCCGyhRCZQohvhBBR9o5pOAxRwVwRcSFx5VfQ2uLG1NhtzJsXjBACgIDwMELc3MjT\nOtHwzTfdr/PQeTDZZ7L14wRdexOMgkTg6+FMoJdLr8XnDp2qJaOwhp+kRXX/beJD9Xi5OLEzv3J4\nNyzYBn+YAGU5VkQ9djSYGjB1mLpbBDqNjnCvcNU1ZGcnq5qoa2lnd0EVHeNsEaRdE4EQQgu8AKwA\n4oAbhBBxZ522H0iVUiYBa4E/2jMma3ikBjO5KZjaTTdjbu5gf8ZNlJR83P188sKF1Pr6cOT5F5Cm\nH2rDGA1GDpYfpENaOdDbS6mJmpYaTtadtO66wxAb3PvMobd3ncRVp+HalPDuY05ajWWc4NgwE8GB\nd0Ga4dTe4YY7pnTtQ9CVCMDSPaT2JbCvrM5tWGuaTONuJz57twhmAXlSynwpZRvwHnBlzxOklJul\nlF2dcjuBcEYpt3h/hLOWBN8LyPvYSHOZF1nZD3As/y9IKYlPTEQAx3Q6qt//oPt1SYYk6k315NdY\nu7Ds3L0Jfr/79/z0i5+O+L62MUFeHC2rP6N8RH2LiY8zTnF5Uije7rozzp8z0Z/8ikZK+xhg7pPZ\nBIc7N/gZBSurR0LXqmJ/V//uYxO8J3Cy7qT1q9SVPmWdrqWzETv81usYZe9EEAb0rJZV1HmsL7cC\nn/f2hBDiDiFEuhAivby83IYhDp7GWYtbYgB+bR2ETryc3I+DMNfMpKDgeQ5l/TdublomTppEYUwM\n5c8/j7ne8q2ia2FZZoWV3UNnDRhLKdlZvJOy5jKKGooGfZktRVuY886c7g+c4YgN8qLF1NE9sFbb\nZOLON/fS1Gbmptnn9u7N7h4nGOJ/YMe/g+ZqEFooH7ulKtILqkgvGNzfuzsRuPVIBPoJtHW0UdJU\nYpf4FEuLICbQi0g/d5UIHEUIcROQCjzd2/NSyleklKlSylSDwTCywfXgnhKIbDWz/JLFOLsFcehD\nZ5yab6WsbAN79/6YuLhgGnROlGm1VL3+OmD5j9jbxZutRVvJKMtgd/Futp/azu7i3UP7Jt9VaqLY\nMvCcX5vf/aExlDGIzYWbaTA1sK903+DvfZaYzppDR0rrOV7RyNUvbmdPQRV/+pERY4TPOefHherR\nuzqxY6jdQ9kfg7MXxK4Y0y2Cxz/N5hcfDW58p2cJ6i5RektyVTOH7CfrdB3xoXpmT/Rj1/HxNU5g\n70RwCojo8Xt457EzCCEuBh4FrpBSjupaBC7R3mj9XGneU8bFt65CdlRx4F/e6Oofo6n5BI1Nj+Dr\nW0HxgvlUvfkW5vp6hBBMD5zO1ye/5ubPb+bWL2/lrq/v4tYvb2XbqW2Dv7lHAPhNhJOWLTB3l+wG\nwEk4DSkRZJRlALC/bP/g732WKZ1TSD9ML+KqF7ZT3dTG27fN5toZvffsaTWCWdH+7BjKNy2zCXI+\nhdjlEGKEmpPQ1jjw60ahk1VNHC1roLZp4H0FuiqP+rj+kFDVWgL7Kqtvoby+lbhQPbMn+o+7cQJ7\nJ4I9wBQhRLQQwhlYCazveYIQYjrwNyxJoMzO8VhNaASec0NpO1FHdLiRgMgJaNhD5oZgQvWvodN5\nE5+wkbpJRbQ0NVL99jsA/GbOb3juouf428V/4x/L/sGbK97ES+fFlye+HFoAUfPgxPfQ0cGekj2E\neIQwPWj6oBNBbWsteTV5AGSUZwzt3j14uDgR7uvG1zmlGLxcWPdf85gV7dfva2ZP9ONEZROna5oH\nd5OCrdBcBXFXgSHWcqzi6LBjdpT6FhM1nQlg38nqAc+vbK7E28UbneaHcRZ/V3+8dF5qwNhOugaK\nE8K8SRtuN+YYZtdEIKVsB+4GNgI5wAdSyiwhxONCiCs6T3sa8AQ+FEJkCCHW93G5UcNjZhDCRUvD\n9tPM/dGNtDZW4OKWy3drDlK2cz51xd5ET9rBwRsjKV+zho7GRgLcArgw4kLmhs1lZvBMQmQIF5df\nzN4jezF1DGH3qQnzoaWGjtKDpJekMzN4JkaDkcNVh2lpH3ggtms9Q1JAEtmV2bSah98Au2Z6GJcm\nhfCv/5xLlL/HgOfPmTTE/8Cy1oGzJ0xeDIaplmNjcJygsOqHxLf3xMCJoOdisi5CCCZ4T1AtAjvJ\n7kwEcaF6wnzciPBzU4nAlqSUG6SUMVLKSVLKJzuP/VpKub7z3xdLKYOklMmdjyv6v6LjaVyc8EgL\npvlQBRMmTycwehJ1xRuoK/kXBfsO49W6iqZGP/SJpaT7u1P13vtnvN5kMrF27Vo01RqmF05n476N\ng7951DwA8nI/pbq1mpnBM0kKSKJdtpNTNfA8+32l+3ASTtwUdxPtHe1kVw5/8cwDS2N54cYUvN10\nA58MTAvW4+2mG9w4gbkdDn8KMcstayj8JoLGaUyOExRWWwbU3XRa0k8MPGBc1VJ1xoyhLhP0E9Si\nMjvJOl1LpJ87elfL/5dnR/uzexyNE4yaweKxxnNuKACNO4pZcvvdJC+7lImzbkPncTspy1cxcdLt\neHjWUj03mC0fvUtHyw/f1r/66ivKy8u59IpLaXVqJX1DOseOHRvcjX0iwCeSPUWWukYzg2eSaLDM\nJhpM99D+sv3E+ceRFpLW/buttLW1kZmZSUcfhfE0GkFatB/b8yqobxmgFXRiGzRVQlznbGOtDvwn\nj9EWgSURLIsP4kBhLSZz/+tJemsRgGXAuKSxhCaTdSUQGk2NtimEeB7pGijuMnuiP9VNJnLLxsc4\ngUoEw+Tk44pbooHG3SUEhkWz+P/9J8vvvAQXd2e+e+cIsVNuBjwJmnKKI+FhbPvDkwDk5uaye/du\nZs+ezcyUmWjSNDToGnjnnXc40scGN+eIms+ehpOEeYQS5hlGgFsAYZ5hA5axaDO3cajiEMmByfi5\n+hGlj+oeOLaFXbt28dFHH3H0aN/9+NenRlBa38oVz2/vbo73KmudZQ+GKUt+OBYQY/MWQc+Ff/ZS\nVN2Mp4sTF00LotlkJqe4/3IZfSWCrgHjk/XWLSB8K/stbvn8Fmpaaqy6zvmirsXEicqmMxJB2kTL\n33/YiyDHGJUIrOA1PwzZaqYx3bIDl5unM3OvnUzxsVpyd1UTGfkT/P2LEJF+7MzLYf9XX/Dxxx8T\nFBTE4sWLAVgyZQmbgjbh6efJ+++/P6hk0BE1j3SdYKZPTPexJEPSgIkguzKbto42UgIt2x8aDUYO\nlB+wyWK0jo4O9u2zTEfNzOz72+bFcUG8e/tsmtraufrF7by7++S59ze3Q84nELPM0i3UxTAVqo9D\nu20mlrXmH+dIygya+4nXFgqrmgj3dSM1yhfof5zA1GGitrUWP7deEkFn8TlrB4yzK7MxS/OguhId\nobz8ayoqNo/Y/XI6v5DEh3p3Hwv3dSfc123cVM1VicAKzhFeOE/Q07D9FNJs+TCbOieY0Ck+fP9R\nHu6aqxBCEB6ZjylkAp9t/JKWlhauvfZadDpLX+T8sPk4uTjRkNRAQEAAGzdu7LNrpctRvzBqtVpm\ndugw17dhKm3EaDBS1lRGSWPfC466uoGMgZYFbsmByVS1VFFYX9jnawaroKCA6upqfHx8OHz4MM3N\nfc8MmhXtx2f3LmBWtB+/+OggD3xwgKzTtbS1d77vE9uhqQLirzrzhYZYkB1QmWd1vADNGRlIk4nG\n7dsHPtkKhdVNRPi5E+rjRqi3K+n9JILqFstzvY0RTPSeiIfOg13Fu6yK50i15cvG4arROd6Sd+yP\n5B37w4jdL6s7EejPOD57oj+7jleOi3EClQis5DU/DHN1K83Zlo1XhBBccGMs5vYO1v2xiOayFIKC\n8zF7utLuqcerrhK9h3v3692c3JgfNp9NxZuYN28eVVVV5Of3X4piT5NlFfGsqmJqP8un/JWDGAMs\ni8366/vdX7afKH0UOkENbwAAIABJREFUAW4BAEw3TO8+bq19+/bh6urK1VdfjdlsJju790Ho08Vr\nSd/7Y/w9dKz56SweWBLDuoxTXPrsNuJ/8wWX/N9Wvv/kNcxaN5i85MwXd88css0HWGueJaE0H7Bf\ni0BKSWFVMxG+lv/NZ0zwY18/iaC38hJddFodc0PnsqVoy7DrVtW11XGqwbKUZzS2CMzmVpqbC2hs\nPEZ7+8isGck6XUeApwuBetczjo+ncQKVCKzkGueP1t+V+u+KkJ3faP1CPLjpiTlc+JNYNM1XotE2\nEq5vwKNGh7kon40v/fWM7pAlUUuoaK6gzdCGu7s7e/bs6feeu0v2ECFcCC7cQ1thPR2NJiaZInHW\nOPeZCKSUZJRlkGxI7j420WciXjovq9YTADQ1NZGTk0NSUhKRkZH4+/v32j1kNjeRl/cHamvTaWrK\nR6sR3Lt4CltWL+K5G6Zz6/yJhHpIEqq/4quOVGTPbiGwDBYLjc0GjFvzLGMZzZmZdqvVVNnYRrPJ\nTISf5b3MiPShuLaFU32spejatL63riGACyMupLy5nJzK4X2I51blgpT4SY9R2SJoaspHSjPQQX19\n1ojcM+t0LQlh+nOOd5VRHw/jBCoRWEloBPqLIjEVNVDxj0N0dC4c8vB2IX5BGCtW3YCHeyyTErNw\nb5lNiFcSeXt2kv7JR93XWBi+EGeNM5uKNjFjxgxyc3Opqel9IM/cYSa9NJ2ZPjF0NDTSXmmZjWQ+\n2UScf1yf9YwK6gqobq0mJSil+5hGaEgKTLJ6wPjAgQOYzWZSUlIQQmA0Gjlx4gTV1Wd+8y0qehOT\nyfJBV1v3wz0j/Ny53BjKwyum8vfUU+hpYk3rBRwrP+sboc4VfKNt1iJoyzsGOh3mqipMp85Z8G4T\nXTOGuloEqRMsHy7/n733Do+jPBe373dme9Gqd8kqlmS5SJZt3MFgU40TCJAECAQOIQnhJOGEk0q+\nFAKckwoJSU4SAoQeSuhgqo0bcrdsS7JVrd6llXa1fXdmvj9Wli3LNsbY4PzY+7rW3n13duaZ0cw8\n8z71WH6Cg1nFR3MWA5yddTaSkFjXue6k5KkfrueSHRr33++jr7/lI0cgnWq83kOBBu7Rj9jD4wQI\nhBUa+z2TzEIQPS+z4k+Nn8DprPzYFNvJEFMEpwDr3DQSvlBMsM1N/593Ex44dHEJIcjJ+TIRcYD8\ntE04WUb29Hls/OejdO6rif5eb2VJ1hLeaXuHOXOjN+odO3YcdVsNww2MhkY5a8r5hNTC8fFQq4uy\nlDJqB2sJK5MjYQ6af2anzp4wPjtlNs0jzbhDJ9f4RdM0du3aRVZWFunp6QCUlY2ZqQ6bFUQiHtra\n/05i4tnIsg23+xjmmF2PEXYUsEUtPXo5ipRpp2RGoHq9hLu7sY81FvLvOT03nY7h6JN/TmJUEUxL\nt2MxyOw8RgG6o9UZOpwEUwLlKeWs71h/UvLUD9WxcpfA4AtT2q7SMNxwUus5XXi8DQihw2hMP/Y5\ncgpp6ItW0D3cUXw4p8JPoGkqNbW3sbf6VlQ1dNLrOZ3EFMEpwjonjZSvzkINKPT/eQ+BxkNPfOnp\nl6HTOUhbtg9z0IlveD6O1HRe+8OvGO7tBqLmoT5fH+3hdoqLi9m1axeRSGTSdg7WFzpr6krC+qiN\n35AfR7DVTXlKOUkDQRpv+jLBlomRJVX9VcQb48mPy58wXpFagYZ20nHlnZ2dDAwMMGfOoZlGfHw8\neXl57NlzKCKpo/NRwuFhCgtuJy6uDLf7KLOQgQZor0R31g1kOsxsbj5Kw/uUEhhqjtYh+ggEx/I2\n4lauRBiNBE5T5NDBGUF2QtQ0pJMlZufEs/MYpSaGAkPoJT02ve2Y6zwn+xz2O/fT5+370PL4qqpI\nc0ZLWc9q1c4485DX24jFko8jrgK3+/Q3YTqWo/ggiwujfoL9vSffIW90tJZw2Ekg0El397Mf/INP\ngJgiOIUY8xyk/udsZIeBwX/UEGiIXuyybCYr61pG2Mn08NP4AkYySq5BiUR46o7baavezbKcZegk\nHfftvA9boQ2fz0dt7eSp5I7eHUyJm0KaNZ2QaS6y5MQ8IxllJMhMpvKdlxTE1t0MPfjghN9V9Vcx\nO3X2eNewgxxspbm77+TMQ7t27UKv1zNz5swJ42VlZTidTrq6uohERmlvf5Dk5BXExZXhiCvH46lH\nUY4oibHrUZB0iNnXsrAwiS0HjpLZmVICahicHy2EMtgYdRSbSqdhmj79tDmMO4d9JFkNWI268bG5\nUxLY3zOKNzhZ0Tv90RaVR/6dDufc7HMBWN/54WYFETVC4aZWIkYd5jlzmN0mnXmKwNOA1VpEXNws\nAoEOQqETM8s0NDTw8ssvf2hfT223C7tRN266O5IlU6OBFe83HeWh5ARxOqNRaTbbdFpa/4SinGCt\nrY+RmCI4xegSTaR+oxxdggnXGy3jJ2Zuzk3IsgX9FV5yO9bQslfj3C/cgS0hief/56c0r13PbRW3\n0TDcwB377sBv8PPKulfY3rudV5tf5U9Vf+K77/03ltc2sEKJ5g+EwznoacCYMPa08vA75PXDQF48\n7ldfI+KMXkRD/iHa3G3j+QOHY9FbmG0v49yXCxnd+OHs5IFAgJqaGmbNmoXRaJzw3fTp09HpdOzZ\ns4f2jn8QibgpyL8NgLi42WhahNHRmkM/iASjnchKLgFbKosKknB6Q5MjNg4Wn/uIfoJgUxPCaESf\nnY25rIzAvn1ooVM/be9w+slOnHiTmTslAUXV2NMx2Q90rGSywymMLyTLlvWhFUFLbx3z9yv4zp6N\nfcUKMgcUOlvOnAxjRfHhD3RgtRZjj4uaF0dHT2xWsHnzZqqqqviwvUpqu92UZsYhSUdXvOkOE0Wp\nNjY2fhRFsBGbrZSS4p8RCg3Q2fnYSa/rdBFTBKcByaTDvjyXcI+XwL6ondtgSCQ7+8u4Eg+Q43+Z\nIpuK470RVliv5cKp/0Hn87soeC/CO5e/xW/O/Q2RrAjKsMLtr9zOHZvu4O/Vfyf32UpufCPExfe8\nh/utNURGDRikJvT+rSBrBOoH2LPIwsPnudBCIUaefQ44VHa6IrXiqPJeP7CK+KAN784P1/SkpqaG\ncDg8wSx0EJPJxLRp09i/fwft7Q+RknIRdvsMAOIcUT+F232YXb7u9WhJiTk3AocK1E2qS5Q8lkT3\nEf0EwaYmDAUFCFnGPLscLRQiUH/q7eUdwz5yEiZGP1XkJiAER80nOBFFIITg3Jxz2dqzFX/kxJ8u\nu19/AUsI4q+4AuviRQBYdjd9uKKHpxGvtwnQsFmLibPPBASuE/AT+P1+WltbAY6b1X4kiqpR1zN6\nTLPQQZZMTWZbi5NA+MN3h1MUHyOunSQmLiE+fh5JSctobXuASOTMCkmNKYLThGV2KrokE+41hzJn\np+R+BVm2ELjawjSh4lI1uiMaibYsKpKWU9BVQvUvnmexdQ53f+FudHod19mu4+XLXua9hLu44L0R\n4lauxDS1iN5f/B4AnWmI8Jq/E+ndhz61kGDZCDvTZZQcwcCTj9Mx3Mrmns0YJAPTk45sFw2hHi8l\nLRkM6UaI9PqIDJ34jWXv3r2kpKSQlXX0pnPl5eUkJe9CUTzjswEAoyEZkylrQuQQux4FRw4URp23\n2QkWchLNVB6pCAxWiM+dNCNQwirO7hOPOw82N2GcOhUA85hz27/31DqMFVWje8Q/7ig+iMOspzjV\nftTIoaHA0AcqAoBl2csIKsEPlVwmv7GOvnhB/rJLMZaUoDiszDgQ+egtVE8RByOGrNYidDo7Fksh\noyegCBobG9E0DaPR+KEUQcugB39YOaaj+CBnFyUTjKjHzf84FsMj29C0MImJZwNQUHA7kcgI7e0P\nTVhOVVRe+O1OGnd8eL/PqSCmCE4TQhbRWUG3l8D+qIlGr08gJ/vLeAuHCcUNYF/qYLdPYc1oBNst\nZUQyIE3J4ckffIfOmt2UzSpj4MAAuppu+v+/n2KeO5eMX/4vUx5/DMviSwBw1vroerkX1dmIsGYz\n57NPAvCbBQIGhrjrV5fyTP0zzEyeiUE2TJBR0zRGXmlCmGR+lv0XAPy1JxYzHQqF6OzspLi4+Jj2\n7MwUhaysOoK+Umy2kgnfxcWVH5oRDLfCgXVQcT1I8vgyiwuS2XpgaEJfZCAaOTQ4cUawe007T9+9\nDffgBysyxeMl0t0zrgh0mZnIycmn3GHc6w4QVrSj2p/nTElgV/vwBB+IpmlRH8FRksmOZF7aPKx6\nK+s61p2QLKHOTpL29VA7PwWDbEBIEvr5c6MO45PMSTjVeLwNSJIBszkXgLi4WbhHPzjHo76+HqvV\nyrx582hvbycQOLG+2FUHhliJnrIM+/jYt9Z8i0dqHpmw3IKCJHSSYONJ+Amczk1IkoF4xzwA4uwz\nSU1dSXvHw4RCh661/vZReppc7H7no9WROlliiuA0Ypmdipxkwv1u2/jJnC5/EUkxMpj+EKbGNVx2\nWwUBb4RXHqgl7uxi9JKBwtQ5vPLbe9D1dmDU63nm1VcJZ2SQ/cf7kQwGJJMJ87wVIEcY3d1KwGkg\n/uqoYigMTOXRix/lhtJ8wnEqt+zP4CcLf8LPFv1sXC6f28WuN17Ft7ufUIub+IvyyVGg09SLr+bE\nbKydnZ2oqkpeXt7EL9w9sP0htCeuoOndC5FQqKkuxv/GzyDoGV/METebQKCLYGgQdj0eTRSr+NKE\nVS0qTMIdiEwu0pZSEm1Qc1gj9/ZaJ5qqUbe55wNlDzVHHcXGoqgiEEJgLis75Q7j8RyCRPOk7xYW\nJDIaiHD/2sbxc8Mb9hJSQxN6FR+LD5tl7HrxJVQBrhVzx8dSzllBghe6qrec6C6dVqIRQ4VIUtSx\nHhdXRig0SDB47L9pJBKhsbGRkpISSkpKUFX1hCv5jmzr5Q7MpO6NPqi1udtY17mOf9b9c4LysRl1\nVOTGs+kk/ARO5ybiHfOR5UNZywX5/4Wi+Glr+9v4WOf+6Gyjv22U4d6PvwtfTBGcRoQsiDvv0KxA\nUzQ8rwyQ0Hsx3sI2BrY+T2qulcu/U0EoGOHVZxsRNj0VJRcy99LL2L/mDWw1uwgYDFReuhLVdiik\nMNztwVSUSs7f/07GPfeQcMVykAWhNjfZ/njOW/l/ZBcHMdd38ZnwDAriC8Z/W/nsk2x89GGGXqpD\nn2XDkAPf+L92PN3bCLWPoox+sNO0tbV1LEfisE6kB9bBfTPg9dvpD9czmKgn3fwZfME4areugT/P\nh9oXQdOIs0TLRbh3/DpqFpp6Pjgmtrk8pp8gZRpEAjASrc0fDin0trgAqNvci3asmO/dT8HbPxkv\nLWEsPJSHYS4rI9TaiuJyfeC+nyhHJpMdzqWzMriiIovfv9vIna/uQ1W1D8whOJKjZRk39I2y6o8b\neWxzK5GxcteaquJ88QVqpghyiw75c+xLlgKgbjt1pcg/Ch5PAzbroUKKcXHRmljHCyNtbW0lFApR\nUlJCdnY2ZrOZhoYP9vWEFZW0nujMwbO+k2Cbm/fao4Xuur3dk8pvLJ2aQk23i2FvCNUfOaHopECw\nF6+3kcSkpRPGrdZCMjKuoLPr8eiDENBZ78SeZEIIqN/y4Xx1p4KYIjjNWCpSkROjvgLP5m7C3V7y\ny76BpJkYmd+H85FHScm1c/l3KoiENboiGsHGEZZefj3zbcn4Ql7mJCfQMzTEiy++iKqqqCGFSL8P\nfaYV29IlxF95BUIvYci2M1rbw+M/+DbVO/fhuP4WhE6l/cUfjIew+Ufd1K5fQ2n8IqSghOX8DIaf\nehKhKGRU70YgGNzTjttdTWfXU8fcr9bWVjIzMzGZDqvP8v79YEsl8vU1NBTFYbfPoHTRr0lJSWFP\n6lVgToTnboT7ZmL/yyqEquFueDz627O/O2kbaXEmCpKtVB6ZT5B8MHIoah7qbXKhRjRKFqYz6gzQ\n2XAUW653EFZ/HyrvJ1j1/njE0EHM5Qf9BKcudr1j2I8QkBk/eUagkyV++/lyblqSzyOVrXz3uT30\n+6L7eaKK4GhZxv/3XhO1XSP89OVaLvnDRjY0DODbth21u4d1swQliYdMdPrMTEbT7CTXdJ907aJT\nRSQySjDYg9VaND5mt01DCP3EoIIjqK+vR6/XU1BQgCRJFBYW0tTU9IGFG3c1DFKmSQwVO5AdRpzP\n1rOpdQO59lwkIbGmfc2E5ZcWJaFpsGNPLz3/sxXvCcw8h8euucSEpZO+y5tyC6oaorvracIhhZ5m\nF4UVKeRMT6J+23EeZk4TMUVwmhGyIG55DuEuD67VLRiLE4grLyQ37yYCczQ6n/4VrtdeJznbzoLL\nCqjr94MKfb95gqT3txJvi8PZ3cZFF13E/v37effddwn3eEEDQ5Z9wraM+XEwFEEWOja/8DTa2d8k\nboaeror91FTfhr9jkIZn1lFkrqA0cSGtnlq2bXyOkWefw1hSgjTcybDaT+u2Wpqaf0V9/U8Y9UwO\n0zzoH5gyZcqhwaFmaF4Dc2+kyf0KodAg00ruQZb1lJeX09E/gvOqF2DlbyFzNvKS72Iz5OCasRS+\n2wi5C456/BYVJrG9dXj86RaAlIORQ1HZOuudSJJgyVVTMZh11FUe5SJd/2sI+8DoIFi1CUNhNGLo\nIKZZs0CIU+YwVkMKnU4fGXEmDLqjX2aSJPjJqlK+e2ExL1R1cc+b0WTB4ykCJXLoOBzMMn6n9R00\nTaPPHWDHnv3cYNrJrZYqFne/x2u/vo9377mXkNnAthJBcULxhPWF55ZS0hah09n60Xf6IzDuKLYd\nkk+SjNhsJbhHj10/q76+nsLCwvFqvsXFxXi9Xrq7u4+7vdat3egQ5J6TTeIXilGGApxVM5VVhauY\nmzaXNW0TFUF5djx2ow4qu9HCKp4tPR84KxhybsJgSJ7kHwOwWPJJTDybru5/0tM4hBrRyJ6WSMnC\nNDzOIN2NH2+viJgi+Bg4OCtAgoTLChFCkJv7VQz6ZEa/ZqTrxz/EW1lJ6aIMNIcRj6YQ7tOTfPNX\nOOvaL9Pf2kx2nI05FXOprKxk09ubAdBnTcw+FelGBBKFefPwDA1SveE9tJuvQrNAWBmm6ZX7SWpK\noCxxGfpkK5EymT1r3mYkHCTjzp9jnDYNeWA/ia4Iw8NRu3F7+4OT9ueo/oGd/wAhM1I0h66up8jJ\nuYG4uGjntFmzxjqo1e6D+V+Fq5+E5T8mLm0Zbl8DGse+oBYVJuEJRqjuOsxkY3JA/BTY8Dt46VY6\n97STlm/HbDNQfFYazVUDBP2HJWsNNcOOh2DOl2Hxtwh2j2DMSp6wHdlmw1BYcEp6E4T7vHTfuZkL\n6kaZZZ88GzgcIQTfXF7E3ZfPpKYvevN6aaeLQc/kngvuIT8P/fdG6rceMh1cWXQlza5m3u9+nyc2\nt3KJtwpjIEB8extGfRBLtoGquaW8cfZckuIzcRgnRsjEL1mGKQwtlW9+5P3+KHjGFMHhpiFgLAu9\nGu0oM5aenh7cbjclJYdutFOnTkUI8YHRQ+YWNyMyxBckYCyIp2emj0tHzuaC8BJW5K6g2dU8oe+D\nTpa4NDuBksEQcpKJSL+PcKfnmOvXNBWn830SE5YgxNFvs9nZ1xMM9tJ24HUkSZAx1UF+eQp6kzzh\nb/xxEFMEHwNClki+YTopX5mFLil6Y9Dr4ygu+TnBJC+BK+x0fvNbhBvrKM1w0RoAXWIBCV/+BqVn\nn4fJHseuN17B4sxHF4pjS+d2AmYF2TExCqitNxphMWfOSrJLZ1L18qv0uFuQQlaMPYkMTn2Vt3oe\nJHKlmfTb5zL/i59Hp6rUl+RhKi8n/vNXIddtxp+xA9BISjqPvr5XCQQmPmEf9A/k5kajOwj7oeoJ\nPBm3UrPrHoy6NAryvzO+vMPhID8/f0LJCQBHXDmK4sHrO7Zzb2HBmJ/gyLpD1/wTpl9GsHYNA70K\n2c6n4N07mbYgCSWs0rj9sDC8tXeBbIBzf4gy/VoifhmjNjkz2VxWTmDPR69E6tnSg4ZGkV/jh90K\noxs60T6gPeV1C6fwhfnRxjV/W9vH4l+u5XvP7eGN6h7+uKaRbzyxk//v15sJBxXWPt84Hm20Mn8l\nqeZUHq5+BPcT/yCcYGGmw8F/3H0X37v9dm65/nosiZmMJmWRriuYtN385Z9BFeCprPxI+/xR8Xoa\nkCQzJtPEUOQ4e/Qc8flaJ/2mvr4eIQTFxcVomkZYUbFYLGRnZx9XEXQPepgehJFsC2IskeyJ1Nfp\nMvdjecvP8uRzASaZhz7vlwgAoc8VIvQS3h3Rm/WzOzq49+36CRFgHk8d4fAQiYmTzUIHSU46F5Mp\nG6/yPGkFcRhMOvQGmcI5qTTt6icc+vB5CydLTBF8TOjTrBjzJz6NpaZcTHLy+bjOGUHNs9D+lZux\n/P3HDAS9aIB/7yB6g5Hy8y+mafsW9lc2MD1zAREU3hcNk8I2ayrX4tFGMLj1LL70GhZYL8Bt30GS\ncTH2V12oejf2cidT5kUjR5TtOyjuHmQAhYYt7+NYtQrN140r433Co2mMxq1C01Q6Oh+dsJ3W1lYy\nMjLG/QP+vQ/RkCbYkfsqQWsn6R3/gSxPdJCWl5czPDxMR8ehJjhxcdHEsoH+rbjdbhRl8omfbDNS\nnGab7DBOmwGX/5mui9aiIZOdC2y6l9QNN5KYbjoUPdS5M+qgXvwtsKcT6opGRRmDe6F/otnLXFaG\nMjJCuOPkG/WoIYXOXc08Z9nCbVIXA4lGXKtb6P9jFaHO4ycRWS1+4gxxvHv7cr4wL5vX9vbwjSd3\n8bt3GmjocjHVAwE9qO4wlRuiMuplPV+a/iXEhs2UBlsRwDm33oo+PR19SgrphYVc9pkV6DUZ2pIn\nbdOSkEJnjglz1YnH358OvN5GbNYihJB4o7qHF6uiPTcOziqPZh6qr68nJycHq9XKL9+sY/4977Kn\nY4SioiK6u7sZHT368a7Z2IkJQfq8DAB8YR+b+t5nz4IeIqMhfA91MTuhfIJ5KNTjJa3bz3OE2DQ0\ninlmMr7dAzyyvpnv/2sv969t4gfP7x1XBk7nJgASE5ccc5+FkElPvQZ93H4ySg+ZgqYtSCccUGjd\nc/LZzB+WmCL4BBFCUFJyJ0LS4/tuGpqqYMnPZdZlJQyGVdxbe9E0jZLFFwACg6GWz35lAeXKFFrU\nbqqrDjV/Ge7tprt+H1KGkVCbG92aIFpmK5ouQHrp1RjnXYZvwERyYdNYvXcYeuQRCo02UnLzWP/4\nQ6gmI/or5xOyd5HWdR73Vz1BSsrFdHX9czwTMhQK0dXVRV5eHiMjO6jafSOVrvvoyJExj5RQ5P8V\npuqS8dyJg5SWlqLT6SZUJLVY8gEL71c+yb333stdd93FL3/5S/74xz+ydu3a8eUWFyazo3X4UAez\nw+hs9qIzSKR99bfwuQcQHVsoFS/Q1+LG2e2Bd34K1pSoIuBQMxpjkh42/m7CusYdxh8hjNS3u5+N\nWi3C2kSJ/gDty9JJur4U1Rdh8NFa1KPUFzrIkD+aTDY11c7dl89iy49W8OKti6m58yJ+v6AYWYFL\nvjoLt6yx7qVmAuHouj4Tms6tr0FjUSElJSU4HBMfOCL2AE6jk6RhPXuPUtbCVTaFtDY3yjFunKrX\nS/99v6fx7HPw7To9EUYeb7TGkNMb4vZn9/CdZ/bw05drMJoKkGXLJIfxyMgIvb29lJSUUNPl4u8b\nDuAORLjuwa0o9mgV3Kamo3eyi9Q58QiNgoo0ACq7KwmpITICFezxRTCOBPlO03XUD9SNd/xzv9OG\nMMm8FyfY1DiIZV4aWlCh8o1mLpqRxreWT+W5nZ3jysDp3ITVWozRmHbc/VZcK1Ajegwpb42PZRbF\nY0s0UvcxRg/FFMEnjMmYztTC7zESqML65DfJe/YZZq7Ip1+WYDREqH2UnW8MIhtLCI7uJdA5zOxI\nHkbFzGuvvU54rPn6vvVrEEIieX4RWlhFCyl4K/YQ9sm0bO1nv2LDWZWIZg/R8fZP8VfX4N+xk8Tr\nr2f5TbcwOjTAw9+5hfqcdlAhoWcxCZ1GtgfjURQPXd3PAFH/gKIopKcPsavqerzuWrJbkijYcC+l\nab8j5+Ir0KWYca1uGW/UA2A0GiktLaWmpgZf0IemaaxZs5bhYQfJyW5WrVrFeeedx6xZs7DZbGzY\nsGF8er+4MAl/WGF19WQncGfdMJlT45F1EpR/Ea77F8XyaiQU9v/rdWjbBMt+AMaoYz3Y2IQwmdCf\newNa9UuMrtuLGowqRmNREcJsZuiBB2j/+tdpuerzNC5fTvOlqwh+QNe4g2zdsAUtfTdlZe8yu7AS\nnK2YZySTdP101NEw7jXHnm0cWV7CYdFTkZuAVS+zZ20HaflxTC9LYerZmSQENO5/upZwby/93/wu\nTfkFKHoDheVFk9Zb56yj2d6MA40H35iciWxYcBaSBh2//y2BhoZx05imabhefZXmS1Yy9Le/oXg8\n9N19N9oHROR8WMLhYUKhAay2Yv7xfguBiMKVc7J5bHMbX3msCrN1+qQQ0oO9vYuLS/jJyzUkWAy8\n9q2lJNoMfOulA5gsVvZV7iHinJhcFgxGyHdF6EwyIOmiwQJr29eSIqXTt0EhmGVnb1Altd/KHV03\ns7ZlDaGOUQL7hrCfnU1FUTKVzUM80TVEFyrXW6386do53H5BMd9eUcRzOzu544XtjIxsP65Z6CA9\n9SqeroW4fasJh6P5MkISlMxPp2O/E6/r1PTn/iBiiuAMICvrWhyOOTR3/56I5EFnkMlYnouiaTQ9\n20Bz1QDlF64iHPTTvn4nOmTmFCwiqHh5/eW30VSV2g1rmVI2m4R5U7AtziTh5kJ8Yhequ5BtL/6L\nA7u2U1h6Dfo+aBt6loE/3Y9ksRB/1ZVkl87k0m9/j8ziacgpbbg7bKgBA9c1L+XxrW9isM6io+Mf\nqGqY1tZW4uN7GXLehdU6lXkDC7A3/QRLYib2c7IQsoTj0gIig348WybeuEtnlhIIBLj+0et54tkn\n2LRpEzbrLPRUwdlXAAAgAElEQVT6ASoqZrBs2TIuvfRSrr/+epKSkli9ejXhcJjl01KpyI3nZ6/U\n0uc+dGF7XUGGe7xklSQc2kjBuVhufpYp1hrq6/QoCUUw98bxr4NNTRgLChBLvs1e5Sbuf+9l1vzt\nZVRVReh0OFatQhkdRRkYRE5IwHrWfJSRETq+9nUig8efqg819rDVu5upBdVoSGRm1dPTuJpQKIQh\nx45lbhqe97sm9Ks4HKVPIUVLmTTeVjuEq99P2fJouOvlVxSj6AW9W/uou+VbhD1edpTNZ1Q/ylrv\n2km/rx+ux+lwgs6Au6OOpiMK+WUvXEFNrsD/5LO0fPYympadS/eP7qDt2i/R/b3vo0tJYcpTT5Hx\nizsJ7NuH68WXjnscPiweT1ThC10Bj7zfyg/ymvldWRe/vGIWlU2DrG1KZHS0FvWwmkj19fUkJyez\nttVHVfsIP1pZSmlGHM98bREpdhN+j53W/g76HtyD4jmUF1O7pRM7AsvMqJksrIZZ37meiwavI+SP\ncN5104hfmsUev8JCTxnJb2q43mpFsuiwLc1kaVEyLn+Yu1fX0ZhiIN+rIlwhhBB85/wivr2iiN7e\nF1C1EAlHCRs9ks76YQzKZaiqn57e58fHSxamo6naBF+XGgrR97//O15M8lQSUwRnAEJITJv2PyiK\nl81bLmDnrmvRT/kHXVkbQN1OaVk1hUvbmHohDIhHCSZ1c941i7EqGeyu2cGe9zcxOjjA9GUrkIwy\n8Z8txCVtRlWDlJR/lZDfh6zXM+fya8lzXEg4Q2PIuT7qHLZHn5SnLVnG0hvORWcJUqI7m0B/Nbmm\nEm6sWsDqVg/BYC99/a/T07OBGTPXYTbnUjHtfnxVGajEkfCFUoQcPZ1MJQkYi+Jxv9uO4o1evL6w\nj3tb7yUgB5jRPYPm/c0sXbaUuXOvApQJU3+dTseqVasYHh5m48aN6GSJ332+nGBE4QfPH3LmdtVH\n8wWypx2mCADSZ1J61SX41Xh2JPyGUPiQLyXY1ISxaCrO9ghvSg5UVN537uXVh55HVVUy7voFReve\nI/+F58n9+wNk/uqX5Pz1L0QGB+m49T9R/ccuYbH6tdWkZ+1DZ3DTFP4h3rCF7Kx1VFZG48kdF+ch\ndBIjrx6Y5JB+c8ubFLQUYNtjw3VEUtvetR1YHQYK56QCoDfIzDs/l8KIjkDHEH+ddzlWWcGUb+KF\nxhcmNBnyhX3s7t/N1KSpzKmoIFca5tG/P8bD37mFkd6oop6WNpNfXm/mLz+rIOUXP8M8Zw6ja9cS\nam8n4567yXvuWZz5Jdzpy0abPpP+39+H4jm57NcDVQNs+lfjhDDYg6Gjq+tMTAk18PXeO+Hpa7na\nsoPHbprPnv5CNC3Ee1u/jaqGqaqq4sCBAxQWT+OXb9QxPy+RK+dEnczpDhNPX13BeZEUwkJh72gz\nA4/Woo45Xod3D+BHo2xpDkokzNqNzyMPW7A1ZDPjnCySsmzMuXgK3ZLEXpOX6YNTCDaNYD83B8mo\nY8nUZCwGmYtnpHPVjbNBgHdn9GYthODrSwxcW/oqtYMl3LHaEi01HvLCY5fBE1dFM+KBUJeHgZeb\ncff5yMqbi8Mxl87Ox8ejoxLSrRizPazf9B7BQPQa6rvnf3A++hj+XbtO6tgfD/nnP//5KV/p6eaB\nBx74+de+9rVPWoxTisGQNFaVUxAM9jM8spFQynY8WZWQvBXn8HoMCYOojl5cae+TkDyf1Lgy6ppr\naGltxhDwcPHXvoksR9Pzm5t/h6L6KZ/zawIeD3mz51BQcRb2govo3vdXQlkRCqfMRle0JFreAWht\n/TM+fytlC/+C6/d3YZi6iALzbLS6EKK0C//oTvSGLUhSEgsWPIey+j3cneXYF5iwnnWo4Y0QAkOm\nDc/7XRBWCebJ3PLOLewZ2MOKjBX4B/3sSN6BJ9PDJYVX0dX1FH19ryHJJuz2mQghk5CQgNPpZMeO\nHUyfPp3slATsJj2PVLaSFmdiVraDPWs6GB0KsOTzRZMc53GZiTTuH6KlOkjN+i6C3jBxdhXXX/6I\nWH4Zr+/cywhe8kczyJA1qr1d9LV0UzKzFPmw/AIAfVoaxqIinI88gqehlV2DuShhlaTDwnf3V+9j\nS81aZszYREryOYzuhV2+QkrTt9Hc5GTatAsx2swInYR3Sw+GLBv6lKhDfd3+dax7ZR0+kw+rZqWp\nqYmysjJ0Oh1D3R4qn29m7iVTyCo+pPDiVCc1lf302jNozjORaoxw5RVX8FrNm+i748nPzuaRfY/w\nvQ3fo8vTxXXTr+OC0uVs27YVY18Tkb4uWvfsYtqSZVgtdrLsWTzY8hQDOXau+NpvSLrpJpJuugnz\njBl4ghGue3Abm5qGqDUks7wmOuuwLlr0oc7x3hYXr/15D71NLpw9XgoqUpAkQXfP8/h8B/jdhoU8\nYfoNcRZzNBhg2wPkzFrK7LmX8cqeQfJMr7Jj/3tsXj9Mfv5Udqr57Opw8eAN80ixH0psDL5yAMuQ\nYItuiD7RTbOnC12jn4yKPPyvHaDVIjH9/Hze+usfqP3Xy+QP5mK3zmLlN8rRG2R0BhlN1ajZ6WZb\n+hryEnPIvHwWQpaw6GVuyOzkc0Uy+oREgl1Bgk0j2BZnAirV1beiKsOE4n7Lw5UDrN/fwxWNP0DX\nuh68A7D1r/h7bAy9AZE2N3GSIPfSAuyJDrq7n8FsycNum0ZDQwOVe9/BzzCNO7sp7G5k6E9/JOmr\nN5N43XUf6rgfzp133tnz85///IEjx2OK4AzCYs4lJXkFWZlfYMqUW0hP/xxJCeeTIX2RpObLSNi6\nCkfX2TiTN9Iz8DR6JR5/91SckW7C8SlkZeeSlJxEOOyirv4nZGZ8nqSkc8ivmEfO9Gj0hZBk9AYb\nfdoG3P5tJGx7BX3mIhSTnf11PyAl5QIy8q6ibW8PrU0tONKSyDPNYmRkkEjaboJBK+nibgyvOfE0\nJqA39pH41RXjYXgHkW0GFE8Y75Yefur8FXu81fz23N9y5YIrmTt3LqG4EE/uf5JkaybLS/8br7eB\nrq4n6O9/E6ulALM5l5ycHHbu3ElPTw/l5eWUZcezo83Jszs6uGR6KtWr28goiKf4rIkOOZcvzPdf\nqOavrb0EkvScle7gwLY+qjf20Z2+iF5NpVXfRUa4GJOpkJGBNEqMCtWeVtoOtDJteul4gtJBjAUF\nRCzxrN+XTNeAjuaqAWSdIKPQQTAY5MnHn6AgZycWRx9lTSpzOl9gweh+RvIKMNrqaW3JoqhoBoZs\nG/6aQQJ1w9jmZ7ChYwOvP/s6spD56k1fZWbRTLZs2UJvby8zZ85k2ystOHu8XHjTDHSGqILSVJXe\n79yG3x3CmTILc1wT0wqmE65NIbtqHmqTnXWVO3g+8Bhzcyu4e+ndXJR3EUrAz471awlZHYRmXoRo\n2EJXXS3TlixjWnIpeknPE/ufIKJGWJi1CCEEiqrxjSd2UtUxwu8+X85Ovx5dbxepG98i/rOfQY47\nfgnng3hdQV65rwqjRc/s83OoWd/FcK+PFGc1rbW/I6jFUdLRzNlUwdVPRJ37jW/DjodInHk+ixde\nz7paD5mWd7DFDxDJuJE/bejmK0vzuWLOoQzxQMMw7rfaiD8/j2VXr+C1Bi+qp48mXwv7N9cgRcK0\nJ3nYvW0NDa1tBJJSMLoHiI93MfvC85DGHgKSc2zsr+xhOATvzX+XS4tWIXxD8PzNGNbfhah6HDbd\nhwg68bnKMfrW0mPcRnfvc0wruYclMy6gPNvB9O13MNO1jo4l9+C48g94Gh0M15ehl9rps1hJU3QY\nNY3EuXMZHFpDV9dTDA01sHp1EynJmaQ6cuh0NtCxe4jSbDuZ/3MPQjp5Q05MEfybIYRAr4/HYs3B\nkpKFbVYOtvmZGOIT0VsrGHK9R1C3EXfTASLtLiK2HPbu343H5cceV8vg0DsUF//0qFELtvjZmM05\ndHs20WVzYqx8EL/3AL1qHflTfsCOVwPsanEwbM+nOWBEFxmlSJ1Da9hFbetM5tZPQ/V48BqaiSxf\nQHxOEpIcVQSqFu2D+2bLm7zoeZ2Z7VNY2D+Ty4svZ/7ss5F0MkajkTlpc9g/tJ+n65/mnCkrKS/4\nOva4mQwNvkdH5yOMjtaSEF+Kw5HDtm3bSExMJCEhnjRlAFfzbpp3rMOtdZKWE0/h1AycD/+DwQf+\nRsNImC+93cueThc3Ls5n54Cb1SNuLvnMVPLcPUgj3dQldjElLpv/+PH1zFiWS9jnpaPeSJneTK2v\nherqahISEkhKOtQpzOcO8fY7YdwhEzNqH0Kflsj+Og3n0DCbq9fi8TUxtaSSLKeOzKY2fsUNFFj8\n5HVV051tZGiolczMlVisVnTJZryV3RzwtvCPjf8kRbFwycUGPIN/QtX2kpe3lG3b6vCO+mjZEKJk\nQTpF8w79HYcff4KRZ59lytevocrZR1jvItKQg39YI+UsHauNT1EydBbzRi7gP867huLcPFRF4aVf\n34VnoJ+wI5H1Lge5pTPx7VrLSF8PRfMXMSdtLv2+fp7Y/wQplhRmJM3g7tf282JVF3ddNpMvnpXL\n5RVZ/MvnYOb2t6ndVUfelZ89ZlOXgygRldf/by/uwQCX/ddsiualoTdK7F3bSe+6HZiWbMOyx0fF\nlk5sn70BseiroDdB6SrY9xLseoyRtCVUbmon6I8nLa2GQPBf2OK28NlSG7LSi2d0P67hnXRsfo7R\n7C2MZK2hr/cxzp+dRb1hGXJbhIg2SKvcj9c/yOioG8lsQpWshBOS8A73MFCzm6L5i5B1OmSdhE4v\nMbAjwjbW8/fm+9i86680utsYmf1FEhZ8E0tmBTqDC0/vVLwje2m2/JUk00wKp/0cIQR5u3/HtI6n\neVj3Rb7RtJg5HRrG1hRMeRJJpl/zVks5cSYvhm4QiiDv7JvxB3wMDb1AamojCxeuZOHMc6lds52B\nZA1mXELpvMnBAB+GYykC8VGTZz4IIcTFwB8AGXhQ07RfHvG9EXgMmAsMAV/UNK31eOucN2+edqzm\n7p8WgsFBtm35IsFwO+76WeTN/C/WvlODR9fP7NkbsFn9GPx/RadYCfoiKOFDNlkB6AwyKYU+POov\nGPXvRRdRQZHpeud/GfYlMXtZMnM+M42WdXXUvVGDToujK6mRgBYgvbOfXkWPzliGkDMI60foS66m\nP34fw1ofIzod+SPzKB1aSGrIwQyzTLpeQjXriF+Zj31eGkIIXEEXV792NSE1xK/O/hXlqeVImkpH\nx8O0tf+NSGSU5OQL2L07l84ODVVVURQFvclKl8tOpuQhovdg8fuZuWcvaYPDWDwu2pNzyPyv25h5\n5UqGvCF++NB6wtu3caOzlqq5ZSg6wX/e/i0sduv4MWne3snex/eTYw2wybCPEeFnak4uKz93OQZh\n5eV7t+Me8rMs/l6S91Uz0GRn8/wr6MwxIgvB4tItSAmNLKkOM3LxP5j/uJcfXlTELaZ32df4W3qy\nBW2N55GWOAu/ZiKhCrwWLyN5a8hIb0aSw+i0ciJaM5rwMtJfTkNLIfrBGVx/22dInxKNJgq1t9N8\n2eUMLV1C9fTp9PX1YROpXHzeZZQsSEdvlKPlxft8vPHXakb6/Sy+ohB33xq2v/I8F936Hd7euZeR\niMxLI1mUeeqp6Ktk3mevYtmXbiSshPn+8z/DUy8oNJ7FG4MKyxeX8LPLD7UiVVSNl27/BdPffJp/\nrLiZqStXcP5cB4bI+3h9TSQmLCEx8ezxipvr/1lPzfouLvzKDIrOSkP1eum+48fU1EXoPyeHrEUP\nkLBJh/mfEob8fLL/9CeMBdHkt8jAAVoe/gqv+ysIaDqU4DvYhZ/py8KIifmUUTSBQZeCyZoBSLjd\nVZjNufRrN9P2ago2yY+7/VFCZjMYP4slkoBlTg8dXS3I3lEKrAau/N6PMVltKGGFJ3/6Pt5QP87k\nR9mb2klXRCbBKeO1KGQUFLMkYSm59YkYkv6MzuRi1tb5JGa5kPNmwqZ7icz8BkPJt9LwXjv5QY3X\nDRGkFbmszInnpV/v4bzkx8jQMvAqK9Ev1vNs23ZUtZ2z5u/D76/D3O1AtzHE+/nX4PIrnDtnFede\nNrkR1IkihNipadq8SeOnUxEIIWSgAbgA6AS2A9domrbvsGVuBco0TbtFCHE18DlN0754vPXGFEGU\ncNjF7j03HdYIXhAJJSDrh+nomEFbawVClTFqDgzYkNAha3ok9CgBgRICoUFORSWO3FcZajwX177z\nOdf2N6YYq1FSZzBizGFEl0K/y8T6IR3pnX3Yu9rot0sox3kSFHIqdksGORYjKHYC5FFkTsChk/Er\nISJqALQAqjZKh3c/Lt0oHiuYU1NIm1JMenYmAXkNbv8rqKoXr2cqRlMBiY4czFIK2zeMghNy2p+n\nvmAaHkscXsVAglBJcfagd49gtJlQJCt+YcVrdODS6whqKp64eRjyspmSZCQj3ojHP4pz1I1nsJ8Z\nB7pI0ekY0g9wQNeLhkp8JAHN7yBH248bidG0xfT1dRE2eMmUNIqtOkaLXyawdSGBzflIfie60AiW\n9BxImYLXaMZw3h/QWybH6WuqhKN3IYbGRfT2hXALL+ZZOzEVVaMh6O2dis/nwKQmEK8lYW5w0qDT\nMWSPI16vca6pjln6dqSs2ZAxGzIrIH0mmt6K1+1l7SO7aNmzh4jvbWzJ88iruIphpZW6nuj1o6In\n4gtg8/QgyXZ0kh2dZEWSjWiShCogIkUIGYJoBgVJVpDwoFcGSejtxJ7qwl7gQZ8RDXPUFAkhq6iK\nDtdwNv0D2fg9KVileApT4knR2VHWPoM3u5XwYsAQJOy10fL2NVh0KVg76rB7OpDmTaNDC9Mty0Qk\nCX0oyLJ1G0hyOtEkgW5GPsHhNohouBIcdOWnMM9+A36jB295N4np00jOLMMXqKa9+4/4gi1IagkH\n1oUIew3I+s9gjEQoGW0gydNJi9nOlqxcFEA/PIBVCZKourEK6FaWEFKGULU+VHxosgwOE+YMBWuC\nm/j4XhISetm37xwCA8XkKIlkab1YdfkQSkQg0DmMDGXbeLndhdoXICeiQ4+GcYmBjJGNKL0G2jEy\nKnysUuJI8nkZjN+Ie0YrSoKCpgncrjQG+3PJSr+JSy+/4KTuGZ+UIlgE/FzTtIvGPv8IQNO0/z1s\nmbfGltkshNABvUCKdhzBYorgEKoaZtSzD7+vFZ+/Db+vFa+nH4ft27g9Ej193XR1deFyuY7bsEOn\nCxKJ6DleIJmRANdNl0nKvxznu+9SX7mR0VEXxoiCwMCooxSPNQudv4EQ/bhMerTDlIVAMMU2gxxr\nCSbZhlm2YZQtSMeoxQKg6LwMT3kbV9YGIgYXSJ9slcxjEXLaqHtxGpqqJzrnAlDRNAWhhdGZ/Vgz\nPehksOiM6PUCVUDGe2AxT8eYPRurLX+8Lk3IPMDA1OfwpO0E6TilBjQBmoxQJYQmgSaDJiEQ0e8O\n+1/AWFUngYqKioaCiiqi78dXqYmxpaK/PfhrITQkvR9ZN7G1pdudxNBQDkODuQQCdhyOPtKSOklM\nbkdnPHqorKoKnM5s+nqm4nZmo4ztyuGYVB1ZkXiyIgmkq3FImhiTMvqvQTJilA/VcgopAd7qfoaA\nciiiSdMiaCJIUukQGfMG0JmPcyxVUDUJVZMPOw6C6J0oeiw0QAgNg+HQtRRwG3A2xtPVWAxxqYSs\nVtTjW8smIRQFq6qnPJhFkZSPTtKP7alG0N6GJ3UnI2k7UGw9DDdcxFW3/N+H28DB7XxCiuAq4GJN\n024e+3w9sEDTtG8etkzN2DKdY5+bx5YZPGJdXwO+BpCbmzu3ra3ttMn9/yqqqhIIBPD7/QQCASKR\nCJFIhHA4TDgcRlEUIpHI+P+SJOFwOIiPjyc+zo5ZVhHmiVmrmqKguN2oLheKy4Xq8yHHxyMnJaFa\nzPQ0NeB1jRAOBomEgkSCQSLhMJqmEvSFCXrCKF4VLayhhcIQDBMJhEBRkdGjQ4dO6NEJPZLQEPog\nwhRAGANoFgWMgBRBkxWEpKAJHUgyCoKwphEy+fEaRwhJo0iRMJKqRm/TGmgqKApoqgyqTCQiE4kI\nFEVCY+x7FUyaCbNBQq9T0esj6PQhFEUwHA7hD0gEfXqCPh2ykEADEZYRERlhAMkiIet1GDRBRmo2\nFyxYhV7RUANB8I+gj9ehNysIfz/K4ACKV0OLQHAkiKfPh88bIWQOEzT7CRj9eI1+jHo9kmwccxpq\ngAooaERAC6FpYYQEkgBJAiFU9LIAIQ7eQ9G08bdogKJpyHIYSQ4jUFC0CBFVRdMEKgJNFYQVPTrF\njqwkEg7H4Q+aCY+mIAcdaBEFIgqyAhZVRqcJNE1Fc3SjmYfR9EEiBj8Bg5+Qosc3UExEtaJJEpok\nIUk6dJpAUgS6iIRVMeBQzUiaGBdWPvwhRYMIYQJagKAWwK/4cQcG8Ed8aGoEVVXQVBUhBEKKviRD\nmNTMURblK4gEI6oeIpKKqjei6g2oej2qrEM12glrAv+wE+/QIKGAD4MsIwkQmkY4IqMLZyGHp6KF\niwlGjPg8ThS/j7DPg9c/gk/1E0FB05TocUBFEEbChyCMRnS/5HAQOeRHRKJZ4kGjDq/NiGyJIx47\nVsmMTtYh62R0sg5hdpN31lzKzzuu0eSYHEsR6E5qbZ8AmqY9ADwA0RnBJyzOvyWSJGGxWLBYJjdK\nOVmELKNLSICEhKN+nzd77lHHY0xGHntBVL+dWDxOjBgfndOdUNYFHNbCiuyxsaMuM2YachB1GseI\nESNGjI+B060ItgNFQoh8IYQBuBp45YhlXgFuGHt/FbD2eP6BGDFixIhxajmtpiFN0yJCiG8CbxGd\n9T6saVqtEOIXwA5N014BHgIeF0I0AU6iyiJGjBgxYnxMnHYfgaZpq4HVR4z99LD3AeDzp1uOGDFi\nxIhxdGJF52LEiBHjU05MEcSIESPGp5yYIogRI0aMTzkxRRAjRowYn3JOe9G504EQYgA42dTiZODj\n6wr90YnJe/r5d5M5Ju/p5f9leado2uQ2eP+WiuCjIITYcbQU6zOVmLynn383mWPynl4+jfLGTEMx\nYsSI8SknpghixIgR41POp1ERTOrOc4YTk/f08+8mc0ze08unTt5PnY8gRowYMWJM5NM4I4gRI0aM\nGIcRUwQxYsSI8SnnU6UIhBAXCyHqhRBNQogfftLyHIkQ4mEhRP9Y17aDY4lCiHeEEI1j/x+9A8wn\ngBAiRwjxnhBinxCiVghx29j4GSmzEMIkhNgmhNgzJu+dY+P5QoitY+fFM2Ml088YhBCyEKJKCPHa\n2OczVl4hRKsQoloIsVsIsWNs7Iw8HwCEEPFCiH8JIeqEEPuFEIvOVHmFECVjx/Xgyy2E+K9TIe+n\nRhEIIWTgz8AlwHTgGiHE9E9Wqkk8Alx8xNgPgTWaphUBa8Y+nylEgP/WNG06sBD4z7FjeqbKHASW\na5pWDswGLhZCLAR+BdynadpUYBj4yico49G4Ddh/2OczXd7zNE2bfVhs+5l6PgD8AXhT07RpQDnR\n43xGyqtpWv3YcZ0NzAV8wIucCnk1TftUvIBFwFuHff4R8KNPWq6jyJkH1Bz2uR7IGHufAdR/0jIe\nR/aXgQv+HWQGLMAuYAHRrEzd0c6TT/pFtKvfGmA58BogznB5W4HkI8bOyPOBaDfEFsaCZs50eY+Q\n8ULg/VMl76dmRgBkAR2Hfe4cGzvTSdM0rWfsfS+Q9kkKcyyEEHlABbCVM1jmMTPLbqAfeAdoBkY0\nTYuMLXKmnRe/B75PtEM9QBJntrwa8LYQYqcQ4v9v7/5CrKriKI5/V2ohGmp/iGCMKZKCSHIIH0oi\n6CkLXwpMfJAQIon+vIRF0FNPPQRNRVBBBIlC/0R6kGqUCIq0fw6WUBFCijoaWEyEiKwe9h49TjM0\n5oz3xFkfGO6++1wO68IZfnfvc+/eD9W+tl4P1wJHgTfr1NsbkubR3rxNDwCba/u883apEPzvuZT8\n1n3fV9J84D3gCdt/NI+1LbPtUy5D6z5gOXBjjyNNStK9wIjtr3ud5RyssD1AmYJ9RNIdzYMtux5m\nAwPAq7aXAX8yblqlZXkBqPeEVgHvjD/2X/N2qRAcBBY3nvfVvrY7IulqgPo40uM8Z5E0h1IENtl+\nv3a3OjOA7ePATsrUykJJY7v1tem6uB1YJWk/sIUyPfQi7c2L7YP1cYQyf72c9l4PB4ADtr+sz9+l\nFIa25h1zN/CN7SP1+Xnn7VIh2A0sqd+4uJgytNrW40xTsQ1YV9vrKPPwrSBJlD2n99l+oXGolZkl\nXSlpYW3PpdzP2EcpCPfXl7Umr+2nbffZ7qdcrztsr6WleSXNk3TpWJsyj72Xll4Ptg8Dv0q6oXbd\nBfxAS/M2rOHMtBBMR95e3/S4wDdYVgI/UuaFn+l1ngnybQYOAScpn1bWU+aEh4CfgE+Ay3qds5F3\nBWUYOgx8V/9WtjUzsBT4tubdCzxb+68DdgE/U4bbl/Q66wTZ7wQ+bHPemmtP/ft+7H+srddDzXYL\n8FW9JrYCi1qedx7wG7Cg0XfeebPEREREx3VpaigiIiaQQhAR0XEpBBERHZdCEBHRcSkEEREdl0IQ\n0SDp1LgVHqdtwTFJ/c2VZSPaYva/vySiU/5yWYIiojMyIoiYgrrO/vN1rf1dkq6v/f2SdkgaljQk\n6Zraf5WkD+reB3sk3VZPNUvS63U/hI/qL5yR9Fjd12FY0pYevc3oqBSCiLPNHTc1tLpx7HfbNwMv\nU1YFBXgJeMv2UmATMFj7B4FPXfY+GKD80hZgCfCK7ZuA48B9tf8pYFk9z8Mz9eYiJpJfFkc0SBq1\nPX+C/v2UTW1+qQvtHbZ9uaRjlLXgT9b+Q7avkHQU6LN9onGOfuBjlw1EkLQRmGP7OUnbgVHKMgdb\nbY/O8FuNOC0jgoip8yTtc3Gi0T7Fmft091B20BsAdjdWF42YcSkEEVO3uvH4RW1/TlkZFGAt8Flt\nDwEb4MfXZdUAAACVSURBVPRmOAsmO6mki4DFtncCGyk7Z/1jVBIxU/KpI+Jsc+sOZmO22x77Cuki\nScOUT/Vrat+jlB2unqTsdvVg7X8ceE3Seson/w2UlWUnMgt4uxYLAYMu+yVEXBC5RxAxBfUewa22\nj/U6S8R0y9RQRETHZUQQEdFxGRFERHRcCkFERMelEEREdFwKQUREx6UQRER03N9OK/ii2XKqFQAA\nAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlY1RknNfVjb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMVKmRRhfVjd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
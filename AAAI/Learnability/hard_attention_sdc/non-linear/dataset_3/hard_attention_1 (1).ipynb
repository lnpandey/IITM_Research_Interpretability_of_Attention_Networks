{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0vlCAi2JLSzD"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "PiPNZm1iTgHy"
      },
      "outputs": [],
      "source": [
        "# path=\"/content/drive/MyDrive/Research/Hard_Attention/dataset_2/m_5_size_100/run_\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "SrZgZMlK-GDe"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2_J4Rw2r0SQ",
        "outputId": "1f7a50c0-54d5-4ca1-95cb-a168d3cca8b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "from tqdm import tqdm\n",
        "%matplotlib inline\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn import functional as F\n",
        "from tqdm import tqdm as tqdm\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "-Dmy2iPWlgnc"
      },
      "outputs": [],
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "writer = SummaryWriter('runs/hard_attention')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6fjud_Fr0Sa"
      },
      "source": [
        "# Generate dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqdXHO0Cr0Sd",
        "outputId": "ebe3c0d6-18c2-4e06-9a9e-5857eecee165"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 500\n",
            "1 500\n",
            "2 500\n",
            "3 500\n",
            "4 500\n",
            "5 500\n",
            "6 500\n",
            "7 500\n",
            "8 500\n",
            "9 500\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(1)\n",
        "\n",
        "y = np.concatenate((np.zeros(500),np.ones(500),np.ones(500)*2,np.ones(500)*3,np.ones(500)*4,\n",
        "                    np.ones(500)*5,np.ones(500)*6,np.ones(500)*7,np.ones(500)*8,np.ones(500)*9))\n",
        "#y = np.random.randint(0,3,6000)\n",
        "idx= []\n",
        "for i in range(10):\n",
        "    print(i,sum(y==i))\n",
        "    idx.append(y==i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ddhXyODwr0Sk"
      },
      "outputs": [],
      "source": [
        "x = np.zeros((5000,2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "DyV3N2DIr0Sp"
      },
      "outputs": [],
      "source": [
        "x = np.zeros((5000,2))\n",
        "\n",
        "\n",
        "np.random.seed(12)\n",
        "x[idx[0],:] = np.random.multivariate_normal(mean = [5,5],cov=[[0.1,0],[0,0.1]],size=sum(idx[0]))\n",
        "x[idx[1],:] = np.random.multivariate_normal(mean = [6,6],cov=[[0.1,0],[0,0.1]],size=sum(idx[1]))\n",
        "x[idx[2],:] = np.random.multivariate_normal(mean = [5.5,6.5],cov=[[0.1,0],[0,0.1]],size=sum(idx[2]))\n",
        "x[idx[3],:] = np.random.multivariate_normal(mean = [-1,0],cov=[[0.1,0],[0,0.1]],size=sum(idx[3]))\n",
        "x[idx[4],:] = np.random.multivariate_normal(mean = [0,2],cov=[[0.1,0],[0,0.1]],size=sum(idx[4]))\n",
        "x[idx[5],:] = np.random.multivariate_normal(mean = [1,0],cov=[[0.1,0],[0,0.1]],size=sum(idx[5]))\n",
        "x[idx[6],:] = np.random.multivariate_normal(mean = [0,-1],cov=[[0.1,0],[0,0.1]],size=sum(idx[6]))\n",
        "x[idx[7],:] = np.random.multivariate_normal(mean = [0,0],cov=[[0.1,0],[0,0.1]],size=sum(idx[7]))\n",
        "x[idx[8],:] = np.random.multivariate_normal(mean = [-0.5,-0.5],cov=[[0.1,0],[0,0.1]],size=sum(idx[8]))\n",
        "x[idx[9],:] = np.random.multivariate_normal(mean = [0.4,0.2],cov=[[0.1,0],[0,0.1]],size=sum(idx[9]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qh1mDScsU07I",
        "outputId": "663a3661-5ee2-4fc0-e09b-4e7afd079250"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([5.14957125, 4.78451422]), array([5.59513544, 6.5252764 ]))"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "x[idx[0]][0], x[idx[2]][5] "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Vr5ErQ_wSrV",
        "outputId": "9fe4bbf3-d0c0-41cd-e0e9-58a75267595c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5000, 2) (5000,)\n"
          ]
        }
      ],
      "source": [
        "print(x.shape,y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "NG-3RpffwU_i"
      },
      "outputs": [],
      "source": [
        "idx= []\n",
        "for i in range(10):\n",
        "  idx.append(y==i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "id": "hJ8Jm7YUr0St",
        "outputId": "0574edad-360d-4df9-826c-8e85d08ae5d7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7efff46b8bd0>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "for i in range(10):\n",
        "    plt.scatter(x[idx[i],0],x[idx[i],1],label=\"class_\"+str(i))\n",
        "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "fNWgnhUJnWLV"
      },
      "outputs": [],
      "source": [
        "x = ( x -  np.mean(x,axis=0,keepdims=True) ) / np.std(x,axis=0,keepdims=True) \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "id": "8-VLhUfDDeHt",
        "outputId": "8660d373-59c2-47b8-d492-d86d2d81234c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7efff38fe390>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAckAAAD4CAYAAACHbh3NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXxU9bk/8M8zW3aykEBCgCRAEhK2IqhQwSjcioosBUQst2pbf1ysvkSwrVSUS9FWtFIF0SrXavFWQUQrRKhsKgIVL1HZQ1giQhICCdm3Wb+/P2YmmZmcM5nJ7JPn/XpFkjNnzvnOJObJd3seEkKAMcYYY50pAt0AxhhjLFhxkGSMMcZkcJBkjDHGZHCQZIwxxmRwkGSMMcZkqALdAGeSk5NFZmZmoJvBGGMh45tvvqkWQqQEuh3hIqiDZGZmJoqKigLdDMYYCxlE9EOg2xBOeLiVMcYYk8FBkjHGGJPBQZIxxhiTwUGSMcYYk8FBkjHGGJPhcZAkogFE9DkRnSKik0S0SOIcIqK1RHSOiI4R0XWe3pcxFp62l27HbVtuw8gNI3HbltuwvXR7oJvEejBvbAExAHhcCPEtEcUB+IaIdgshTtmccweAbMvHjQD+avmXMcbabS/djhX/XoE2YxsA4HLzZaz49woAwNRBU712jzXfrkFlcyVSY1Kx6LpFXrs2Cz8e9ySFEJeFEN9aPm8EUAwg3eG0GQDeEWaHACQQUZqn92aMhZc1365pD5BWbcY2PHngyU49y+70OK1B+HLzZQiI9iDMvVUmh7xZT5KIMgF8CWC4EKLB5vgnAFYJIQ5Yvt4L4AkhRKdMAUS0AMACABg4cOCYH37gfbGM9RQjN4yEQNe/k6JV0dAZdTAIQ/uxSGUkVvx4hdNe4W1bbsPl5sudjqfFpGHXnF3da3SQIaJvhBBjA92OcOG1jDtEFAvgQwCP2QZIdwkh1gNYDwBjx47litCM9SCpMamSQcxRi6Gl07E2YxuW7l+KpfuXgkCYmzsXAPDBmQ9gEian16tsrpQ8zkOzzCtBkojUMAfId4UQH0mcUg5ggM3X/S3HGGOs3c39b8b7Je97fB0B4dZ1UmNSOx3zx/woC34eB0kiIgB/A1AshPiLzGnbADxCRJtgXrBTL4To+s9FxliPsb10O7ae2xqQe2fEZQDHNgN7VwL1ZUB8f6zpmyA5P7rm2zUcJHsQb/QkbwLwcwDHieiI5diTAAYCgBDidQA7ANwJ4ByAFgC/8MJ9GWMhRm74cnvpdjx54Mkuh0V95VDlIWw/uQtTG+rMB+ovoTIRAFGnc61DszwU2zN4HCQti3E6/yTZnyMAPOzpvRhjoUtu+PK7q99h67mtAQuQVmt6RXcESQCpBiMuqzv/ikyNSeWh2B6EM+4wxvxCbnvHB2c+6HQ8EC6rlHg2KQETBqRjROYAXFYpAYfV/5HKSCy6bpHsa1nz7Rp/Npn5QVDXk2SMhS7H4Ui5VauB7kG2I8L7veIkh1gB8zYR65Dq7/f/XvIcuVWyLHRxkGSMeWx76Xas+r9VqNOahyujlFEwCAP0Jj0AuLStIyg4CZC2+yjlgr7UKlkW2ni4lTHmke2l2/H0wafbAyQAtBpb2wNkOHDsIS66bhEiSW13LNJkwqIrFeZVsixscE+SMeaRNd+uCauAKMWxhzi1qRmovoY1vaJRqVIi1WDEoto6TG1uAQofNZ80cm4AWsq8jYMkY8wj4T4Pp1aoseg6h+JG/3oCU1vr7FbDttO3mvdbcpAMCxwkGWMecTWVXCgiEJ656Rnzto5PlgDf/B0Qxq6fWF/m87Yx/+A5ScaYRxZdtwhqhbrrE0OQgMDS/Utx2/9ej+3FG10LkAAQ3988N/nScGBFgvlfnqsMSdyTZIx5xLp53nZ1a7wmHvW6+kA2y6sum9qwIjkJAMzzjs6oo4Ds28xzk/pW87H6SzxXGaK8WirL28aOHSuKijpV02KMhYAJGyeEVaAEgDS9AbvKKuRPiB9gDpByw7LxA4DFJ3zWPoBLZXkbD7cyxrxue+l2yXJWoa5SpXR+QvZtwNH35Idlea4y5HCQZIx5XbhuC0k1dDEn+c3fO4ZYJQmenwwxPCfJGPO6cNwWEmkyYVGtxJYPWy6tfPX//OQ333zTR6VSvQlgOLhz5MgE4ITBYHhwzJgxVx0f5CDJGOsWZ6WiwmpbiBCIMgn897WarhftuMrPeylVKtWbqampeSkpKbUKhSJ4F6IEgMlkoqqqqvzKyso3AUx3fJz/omCMuc1aKupy82UIiPZSUdtLtwOwpG1TRga4lV5CBOGQ0nV7TDRu698PIzMH4Lb+/bA9Jtr969Zf6hh29f12keEpKSkNHCA7UygUIiUlpR7mXnbnx/3cHsZYGOiqVNTUQVOx4scrAtAy32hTKLAmMQGAOUCuSE7CZbUKggiX1SqsSE7qXqAsfBTYMB34aIE5aEJ0DMd6N1AqOEDKs7w3kvGQgyRjzG1yc462x6cOmopxqeP81SSfs65sXZOYgDaF/a9O2yDqFn0r8P0+AKLz8b0ru9lS5k0cJBljbpMrCWV7fHvpdhypOuKvJvlcqsGI7THR5mLMErrcHuIu3i4SFDhIMsbcJjXnGKmMtEsELjUkG6oiTSbc3NJizrojU3Oyy+0h7orv793rueEfh35IuuGPe0ZkLd0+5oY/7hnxj0M/JPniPkuWLOm3fPnyvr64ttWWLVt6ZWZmDh84cODwJ5980u2Cn7y6lTHmNusqVrnVrUCYbAMRAmkGI25uacEHveJgkgmQLm0PAQCFEjDZBlNCp6FWq8nL3W6uN/zj0A9Jz3xyKkNrMCkA4GqjVvPMJ6cyAOA/x2XUBKRR3WQwGLB48eKBO3fuPDNo0CD9qFGj8mbPnl03ZswYl/9680pPkojeIqKrRCSZb4mIbiGieiI6YvkIzHefMeY1UwdNxa45u3Ds/mPYNWeXXYAE5IdkoygZQZwN0w4BWFRbh61xsbIBEkJgRbWL20Mi4s2p6UDmf8f+ElBqJE4M3CDf2r1n060B0kprMCnW7j2b7um1161b1zsnJyc/Nzc3f+bMmVm2j61evTp5+PDhebm5uflTpkwZ3NjYqACAt956KzE7O3tYbm5u/tixY3MBoKioKHLEiBF5Q4cOzc/Jyck/fvx4hNT9vvjii5iMjAxtfn6+LjIyUsyaNatmy5Ytbk0ee+s78XcAt3dxzn4hxI8sHzwjzVgY+/i7ctSW/QeEyb46iBIa1JRNhjn8BL9Ug1FyoY6tNIPR9f2TrTXmucb4/uae4l1/ATSxEieagH8uDEhmnqpGrVTUlj3uqqKiosgXX3wxbd++fWdKSkpOvfHGGxdtH58/f37tiRMniktKSk7l5ua2rl27NhkAVq1albZr164zJSUlpz799NNzAPDKK6+k/PrXv75y+vTpU8eOHSvOysrSSd3z0qVLmvT09PbH+vfvrysvL3frdXglSAohvgQQUt1wxphvfPxdOX7/0XFUVQ5D2+VZMOkSIAQQr+6DlvKfwtAwGrJDjEHEOoTqbEGOy8Osdhy2ebTWypxm9MVWkC6lxEVIBhy5467auXNnr2nTptWmpaUZAKBv3752k7jffPNN1JgxY3JzcnLyP/zww94nT56MBICxY8c2zZ8/P3P16tXJBoMBADB+/Pjm1atXpy1btiz17NmzmtjYWJ/9QPmzTz+eiI4S0b+IaJjcSUS0gIiKiKioqqrKj81jjHnDn3eWoFVv/v1naBiN5vNL0XR6FcTFZdA1jAYACH03tku4y5MxXSEwo7EJU5tbZBfkKNwZZpVi3ebhbIFOALaCPDo5uzxCpTDZHotQKUyPTs4u9+V9FyxYkLVu3bqLZ86cOfXEE09UaLVaBQC89957F5999tmKS5cuacaMGZNfWVmpXLhwYc3WrVvPRUVFme66667sbdu2xUldc8CAAXY9x7KyMruepSv8FSS/BZAhhBgF4BUAH8udKIRYL4QYK4QYm5KS4qfmMca8paJOOsF3RV0rlJZ5PW3VlE5DsUGFCF9Gm5MDLKqtQ6TJLmYg0mTCn6queZ6mrv4SoGvu4hz/bgX5z3EZNU/flf9Dn7gIHQHoExehe/qu/B88XbQzZcqUhsLCwsTKykolAFy5csWui97S0qIYOHCgXqvV0qZNm9pX0548eTJi0qRJzS+//HJFYmKiobS0VHPq1ClNXl6e9qmnnro6ZcqUuiNHjkRJ3bOgoKD5woULkadPn9a0tbXRRx99lDR79my3uv5+Wd0qhGiw+XwHEb1GRMlCiGp/3J8x5nsff1eOP+8skR1I7ZcQhVuHpuAfhy7C0DAabQAiUnaC1HXSuyqEkN1u4TIPrmEdZrUGwjWJCahUKZFqMGJRbZ338ri2dhF7ArAV5D/HZdR4eyXr2LFj2x5//PHLEydOHKpQKMTw4cNbMjIy2nt1S5curbjhhhvykpKSDNddd11TU1OTEgAWL17c/8KFCxFCCJowYULDuHHjWp966qnUzZs391apVCIlJUX/zDPPSCYKVqvVWL169cXbb789x2g04mc/+1n12LFj3dqX5LWiy0SUCeATIUSn/HdElArgihBCENENALbA3LN0enMuusxYcLMGxnKZ3qNVlFqJ52aNAAA89r59goGYwaug0HT+4z7BaEQrKaBV2AQ5N4JegtGIKU3NeL9XXLcCZZcFlv1BHQVMW+tWInSpostHjx69MGrUKO6UOHH06NHkUaNGZToe90pPkog2ArgFQDIRlQH4bwBqABBCvA5gDoCHiMgAoBXAvK4CJGMsuFkX6FjnH+UkRKlBBCx+/wgUEsFKWzUFkWkfgRQd9SeFSY3LlfcAADL6bESVipBqMKKFCPUuZrapUyjwZXQ0ooRAq7tBUgjc3NKC7THRvutBOkUdK2D9VCmESfNKkBRC3NvF4+sArPPGvRhjwcF2gY4zWoOp/TyjxN/GjkOvQp8AbdUUyypYYLf2z7B2Jq3JxZ1tyWhnST6uFsL9YVcifBgXi496EfSW51kTmQPwbaCMHwAsltxyzmRUVlYqb7nlllzH41988UVJamqqR6mQOOMOY6yddfi0oq4V/RKi8NspuZg5WnoPudwCHUeuBFJDw+j2oNjpPiIZ/ck8Umg7P3hZpXQp8OmJkGA0QgigXmkOrgRAdPFcg0QgtiYy9zhIRiUBw34KHH3PvILVSh0VsEw7oSw1NdV4+vTpU764NgdJxhiAzsOn5XWt+P1HxwFAMlD2S4jqci7SG14wzMUq9ZuIJvMaj6nNLbizuQVT+vfDZbVrv8LqFQocu3Cp/Wu3eqQOPEtkTsAKm/nXgePMWzxsEwzw8GpQ4SDJGAMgPXzaqjfizztL2oOk7UIdTxeeumqbaQKgB1ao30EimkDUkS6uU6CTGVZ13OvYqUcKuDwc61Eic8eVqiPnclAMchwkGevBbIdX5VbSVdS14uPvyrFi20nUtdosrvHj0rttpgn4ndiMJEVT+zGprRk3t7Rga1ysXeCUy4oztbml/RpSPUuVyQSijjlJZ9dyCQ+lhiQOkoz1UC6vTo1W47cfHIXeFNgF6f2o8w6Gqc0tuKOpBW3QtA/Hjtbq3F6RKrcXUuqYW/ORpACEJRGBSnK/OwtyHCQZ66FcWZ0apVaiTW8MeIAE7BfwOB5/wTC3fTjWtocoRW6hq9zzur1IR6E238ho2S/fWmPOxQoE/xDr4b8lYd/z6Wi6qkFsHx0KnijH9b/yen7uJUuW9IuNjTWuXLnyirevbXX33Xdn7t27N753796Gs2fPnnT3+Vx0mbEeqqvVqQoCZo9JR6ve5PQ8f3nBMBctwr6AQ4vQ4AXDXGwzTcB12vVYpP91l8PAJp/PpVrKYEXEdQRIqwDkYnXb4b8lYefvM9B0RQMIoOmKBjt/n4HDf/NJ4WVf++Uvf1m9bdu2s919PgdJxnqgj78rl9zYb8skgPcPX3J6jj9tM03AUv2DKDMlwyQIZaZkLNU/aF7YY3NOuUh2eh25X3rW7ZQeiR9gXr26+IR8dQ8/52J1277n02HQ2r9NBq0C+54PuXqSAHDHHXc0paSkGLrbZg6SjPUwT318HIvfPyK5sd+R3hj4YVZb20wTMEG3FoO072KCbq1dgLSS6nHaIplalnpSoUZI1XZ0kePCHLmcqwHIxeqWpqvSb57ccRcFop6kN3CQZKwH+fi7crx76GIIVHPsvm2mCfjAeDOMgjr1DIXNfx1pYEDvmO7EAcvwqmOO1cnLzYHTViiscI3tIx1w5I67iOtJMsaCnrMqHeFiuuIA7lZ+CSWJ9gU6QsC1XmJrrTkbjqtsh1cdF+OMnGsOnPEDIBtIg1HBE+VQRdhPRKsiTCh4IuTqSXoDB0nGehBXU8mFst+pNrdvB7EiAlpEJMpNzucrEd8fuOP5zj1AhRpQOvQyXekVjpxrDqBygTQYXf+rGkx57gfE9tUBBMT21WHKcz94uro1EPUkvYG3gDDWgzhLJRelVgTNSlZPSO2nNB+/hsf0D9mluLNjDXrWQOaYLk7qWCgEve64/lc13t7yEYh6kgAwbdq0rEOHDsXV1taq+vbtO3Lp0qUVixcvdrlsmNfqSfoC15NkzLukEggQgPnjBuLZmSOQtXR7yA/HHtA8iv6Kzr8Dy0zJmKBbi+mKA/idajP6UTVMUEBJJlD8gLAJelxPsnt8Wk+SMRYarDlY5Sp9+CtpuS85JkQHOvZTAuaFPdt0Hati0xOicHDxJL+3k4UGDpKM9TAzR6fLlr/67ZRcl1LVBTNrQnRzb/EaKkTv9oQDUsrrWnHTqs9cKg/GghPXk2SM+YU1OPyh8CRqW/RdnB28HHuLzhDQ3nvuqjwYC06+rCfJq1sZY3Zmjk5HtCY8/35WKuwTCRA675q0lgdjDOAgyRiTEK5bRRQAEqPVIJjnIp2VB2MM4CDJGJPQLyE8yzrpTQLRGhW+XzUVB5dOQrrM6wzX18/c55UgSURvEdFVIjoh8zgR0VoiOkdEx4joOm/clzHmG7+dkosotbLrE0OQbS9R6nVGqZX47ZROa0BYD+WtnuTfAdzu5PE7AGRbPhYA+KuX7ssY84GZo9Px3KwRMqnAQ5ttL9H6OtMTotqHYJ+bNaLHL9p5v+T9pFs33zpi5IaRY27dfOuI90ve90mZrCVLlvRbvnx5X19cGwDOnTunvvHGG3MGDx48bMiQIcOeeeaZPu5ewyuz80KIL4ko08kpMwC8I8yZCw4RUQIRpQkhZLMkMMYCL9QTCziS6iU62xLTE71f8n7SC4dfyNAZdQoAqG6t1rxw+IUMALgn9x6vF172JbVajdWrV5dNmDChpba2VjF69Oj8O++8s2HMmDFtrl7DX3OS6QBsC9OVWY51QkQLiKiIiIqqqqr80jjGmD1rZp5wwr1E17x+9PV0a4C00hl1itePvh5y9SQzMjL0EyZMaAGAxMRE0+DBg1svXrzoVqmXoFvnLYRYD2A9YE5LF+DmMNYj/XlnSUgnFLBSEmH13FEcGN1wrfWaZBCRO+4qaz3Jr7766nRaWprhypUryueff759qHX+/Pm1jz/+eDUAPProo/3Wrl2bvGzZsqvWepJZWVn66upqJdBRT/Khhx6qaWtrI2sJLWdKSko0p06dii4oKGhyp93+6kmWAxhg83V/yzHGWBAKhy0QUWolB8hu6B3VW7JupNxxVwWynmR9fb1i1qxZg1etWnUpKSnJrSz+/gqS2wDcZ1nlOg5APc9HMha8QnULBBF4AY6HFo5aWK5RauwCiUapMS0ctTAk60lqtVqaOnXq4Lvvvrvm/vvvr3O3XV4ZbiWijQBuAZBMRGUA/huAGgCEEK8D2AHgTgDnALQA+IU37ssY8w2pHK5RaiUiVArUtQZnurootZIDoxdYF+e8fvT19Gut1zS9o3rrFo5aWO7pop0pU6Y0zJkzZ8iyZcsqU1NTjV3Vk0xLS9MDHfUkJ02a1Lxnz5740tJSTU1NjTEvL087bNiwqxcvXtQcOXIkavr06Y2O9zSZTJg3b15GTk5O24oVK650p93eWt16bxePCwAPe+NejDHfk6sWAiAoE6BHqRUcIL3ontx7ary9kjUQ9SR3794d+/HHH/fOzs5uHTp0aD4A/OEPfyi/55576l1tN9eTZIy55ePvyvHnnSUor2uVzH0aCNyL7MD1JLuH60kyxrzCdl+hbcAMJGtScrkgaW0nl8Ni7uIgyRjrNmvAtO6rdJzDVBDQrPNsaDY9IQotOkOXpbvkVuQ6to3LYYUfrifJGAtqzuYwH//gKIwm+UFZZwuC0hOicHDpJMkg7EhuRa7Uns+uep4stPiyniQHScaYV0ild/v4u3IoANiGKAWA+Gg16lr0ThcE2aaQsw3CUnOhzpKSy/Uww2EvKPM9DpKMMZ/5884S6B16kSYA0RoVvlt+m+T5cvOGUnOhrswx9kuIkpwzDdW9oMy/OEgyxnzGnV6cO4nG3TlXbs8nl8NiruAgyRjzKttenoIIRoltZv7sxcnNl/J8JHMFB0nGmNc4LrCRCpCB6MVxOSz31GzclHTttdfSDdXVGlVysq73r39dnnTvPK+XyVqyZEm/2NhY48qVK7uVDacrLS0tdOONNw7V6XRkNBpp2rRptS+99FKFO9fgIMkY8xq56iFKIpiE4F5cCKjZuCnp6qpVGcKSO9VQVaW5umpVBgD4IlD6UmRkpDhw4EBJfHy8SavV0vXXX5+7d+/e+smTJze7eg1/JThnjPUAcnOQJiHw/aqpOLh0EgfIIHfttdfSrQHSSmi1imuvvRZy9SQVCgXi4+NNAKDT6chgMBARudVmDpKMMa+Rm2vklaShw1BdLVk3Uu64q6z1JPft23empKTk1BtvvHHR9vH58+fXnjhxorikpORUbm5u69q1a5MBwFpPsqSk5NSnn356DuioJ3n69OlTx44dK87KypIt42UwGDB06ND8vn37jiooKGiYNGmSy71IgIMkY8yLfjslF1Fqu+IOvJI0xKiSkyUDjtxxVwWqnqRKpcLp06dPXbx48di3334bc/jw4Uh32s1BkjHmNTNHp+O5WSOQnhDFdR1DVO9f/7qcIiLs6klSRISp969/HZL1JK2Sk5ONEydObCwsLIx3p128cIcx5lW8kjS0WRfneHt1ayDqSVZUVKg0Go1ITk42NjU10eeff97rN7/5TaU77eYgyRhjzE7SvfNqvL2SNRD1JC9duqR+4IEHsoxGI4QQNGPGjJp7773X5VqSANeTZIyxsML1JLtHrp4kz0kyxhhjMni4lTHGWEjjepKMMcaYDF/Wk/TKcCsR3U5EJUR0joiWSjz+ABFVEdERy8eD3rgvY4wx5kse9ySJSAngVQA/AVAG4DARbRNCOEb194UQj3h6P8YYY8xfvDHcegOAc0KIUgAgok0AZgDwSdeXee7M15X4aut5NNVoEZsUgfEzBiPnxtRAN4sxxoKON4Zb0wFcsvm6zHLM0WwiOkZEW4hogNzFiGgBERURUVFVVZUXmsdsnfm6Ep+/expNNVoAQFONFp+/expnvnZrfy1jjHlsyZIl/ZYvX97X1/cxGAzIy8vLv/XWW4e4+1x/LdwpBLBRCKElov8CsAHAJKkThRDrAawHzPsk/dS+kOCNHuBXW8/DoLPLOAWDzoSvtp7n3iRjDABwfF9ZUtGOC+kt9TpNdLxGN/bOzPIRBf1DqkyWrWeffbbvkCFDWq0JCtzhjZ5kOQDbnmF/y7F2QohrQgit5cs3AYzxwn17FG/1AK3Pd/U4Y6xnOb6vLOngB+cyWup1GgBoqddpDn5wLuP4vrIkT6/t71JZAHD+/Hn1zp074//f//t/3Uqm4I0geRhANhFlEZEGwDwA22xPIKI0my+nAyj2wn17FGc9wK6c+boSG548iFcXfgaS+Y7HJsn+jDHGepCiHRfSjQaT3W8Ko8GkKNpxwaOEvIEqlfXwww8PeOGFF8oUiu6FO4+DpBDCAOARADthDn6bhRAniWglEU23nPYoEZ0koqMAHgXwgKf37Wm62wN07IEKU+dzVBoFxs8Y7HEbGWOhz9qDdPW4qwJRKmvjxo3xycnJhokTJ7Z0t91e2ScphNghhMgRQgwWQvzRcmy5EGKb5fPfCyGGCSFGCSFuFUKc9sZ9exK5nl5kjPNpZakeKID2HmVsUgRunT+U5yMZYwCA6HiNZK9M7ri3+KJU1oEDB2J3796dkJ6ePuKBBx4YdOjQobgZM2ZkSZ0rh3O3BgHb4dANTx6UnGfMHN5b8rnaNoPTeUm5nqYwAQ+/Pgn3/+kmDpCMsXZj78wsV6oUdn9ZK1UK09g7Mz2qJzllypSGwsLCxMrKSiUAdFUqy3rcWirr5ZdfrkhMTDSUlpZqTp06pcnLy9M+9dRTV6dMmVJ35MiRKKl7vvrqq+VXrlw5Vl5efvzvf/976bhx4xq3bt36vTvt5rR0AWYdDrX29qwLcgC0B699753GiS8rJJ8vjMCXm0tkV73GJkVIBkrbnqkrq2Z5byVjPYN1Fau3V7cGolSWN3CprADb8ORB2SB2/59uwpmvK7H7bffyMqg0ivYhVMcg7O7jQOdADgCkBCIiVWhrNnDQZCyIcKms7uFSWUGqqwU5rqxedWS76jXnxlTcOn9oe8/RcQ7SlVWzUucII9DWbGhvKyckYIyFIx5u9RFXhye7Gg7t7v7FphotNjx5sP2+cr08V1bNutIGTkjAGAsULpUVYlyZZ7QaP2Ow5HBn5vDe2PDkQaf3iYhRgkDtPTpHTTVa7HnnFPZvPoO2ZgMiY1QQENA2G9sDNymkt4VYX0fOjamygVzqfowx5m9BXyqL2XNn47/UcOjQcak4fajSadDpn5sAdYRKNkBa2Q6LtjUboG02/1FlDdxyARJA+xDq+BmDodJ0/aPCCQkYY+GGe5I+4O7Gf8fh0A1PHpTc22iVmBqFyu8bnJ7jCoPO5LQnaQ3s9//pJgBoHz5WaghGnf2CL05IwBgLRxwkfcCVbRdSnG31sFVb2drttjly1pMEOgK7NZBbh5IB+yA5dJz8vCdjjIUqDpI+IDfPKNXTsl3gE6xeXfhZ+xymXAafCyeuoSAAbWOMMV/iIOkDttsrutqg7xhMg8eWz8gAACAASURBVJV1DlOurcEc5BljwWnJkiX9YmNjjStXrrziq3ukp6ePiImJMSoUCqhUKnHixAm3CmxwkPQRZ9surPZvPhMSAdLK2RwmL9phLHwc2b0j6dCWjenNdbWamIRE3bg595b/6Cd3hmw9yX379p2xJlZ3FwdJL3J1b+SZryvbt2WEGmEyDx27MpTMGAs9R3bvSPpiw/9kGPV6BQA019VqvtjwPxkA4GmgXLduXe+1a9f2JSLk5eW1Dho0qH0IavXq1clvv/12il6vp8zMTO2WLVu+j4uLM7311luJzz33XD+FQiHi4uKMRUVFJUVFRZG/+MUvsvR6PZlMJnz44YfnR4wY4ZPhLN4C4iVdFUW2TWK+++1TIRkggY6MPXIZfBhjoe3Qlo3p1gBpZdTrFYe2bAzJepIAMHny5Oxhw4blvfjii8nutpt7kl7S1d7IUJl77EpTrRaXz9e1bwthjIWX5rpaybqRcsdd5Uo9yeXLl6c3NjYqm5ublQUFBfVARz3J2bNn186fP78WMNeTfPHFF9PKyso08+bNq3XWizxw4MDprKwsfXl5uWrSpEk5w4YNa7vjjjuaXG039yS9xNneSLkVoSFJACe+rMC+97gkKGPhKCYhUbJXJnfcW3xRTxIAsrKy9ACQnp5umDp1at1XX30V4067OEh6idzCFVdTuoWakwe63s/JGAs94+bcW65Uq+3rSarVpnFz7g25epINDQ2K2tpahfXzzz//vNfIkSPd2mjOw61e4mxvZLDvg+wOYTLvn7SuduVyWYyFB+viHG+vbg1EPcmysjLVT3/60yEAYDQaafbs2dfmzJnT4E67uZ6kF0mtbgWAz94t7pTGLRw51qFkjPkf15PsHrl6ktyT9CLHvZHWFa89IUACXC6LMRZ+vBIkieh2AGsAKAG8KYRY5fB4BIB3AIwBcA3APUKIC964dzALqwU7Lgq3YWXGWPAL6nqSRKQE8CqAnwAoA3CYiLYJIWxre/0KQK0QYggRzQPwPIB7PL13sOuJAYMz7zDG/C3Y60neAOCcEKJUCKEDsAnADIdzZgDYYPl8C4DJREReuHdQ62kBgzPvMMbCjTeCZDqASzZfl1mOSZ4jhDAAqAfQW+piRLSAiIqIqKiqqsoLzQscV4sVhzKyvDzOvMMYC0dBt3BHCLEewHrAvLo1wM3xiDVgfLm5BNpmj4bFg1JsUgRn3mGMhTVvBMlyAANsvu5vOSZ1ThkRqQDEw7yAJ2zIJTfPuTEVX209H5ZBsqlGiw1PHuT9kYyxsOWNscDDALKJKIuINADmAdjmcM42APdbPp8D4DMRzBs03dRVcvNwXsDj+FoZY8xVS5Ys6bd8+fK+vrxHdXW18vbbbx+UlZU1bNCgQcP27NnjVlo6j3uSQggDET0CYCfMW0DeEkKcJKKVAIqEENsA/A3A/xLROQA1MAfSsOEsuXnOjalhm5rOivdHMhZemg5VJDXsvZRuatRpFHEaXa/JA8pjx/ULyXqSCxYsGHDbbbc1fPrpp6VtbW3U1NTkVufQK6tKhBA7hBA5QojBQog/Wo4ttwRICCHahBB3CyGGCCFuEEKUeuO+wcJZcnOgZyzgCec/AhjrSZoOVSTVffJ9hqlRpwEAU6NOU/fJ9xlNhyqSunpuV9atW9c7JycnPzc3N3/mzJlZto+tXr06efjw4Xm5ubn5U6ZMGdzY2KgAgLfeeisxOzt7WG5ubv7YsWNzAXPZrREjRuQNHTo0PycnJ//48eOSWwmuXbum/Prrr+Mee+yxagCIjIwUycnJbs19hfdvbj9xltwcMC/gsa3BKIs6njf85n5QKINrl4yz9vS07S6MhauGvZfSYTDZxwaDSdGw91LI1ZMsKSnRJCUlGe6+++7MvLy8/HvuuSejoaHB/z3Jnk6qp+i4ZzDnxtQuV4L+5IF8PPz6JNz/p5tQ8LOhmHxfXnvgDISIGCUiY8wj8rFJEVBHyv+48P5IxsKDtQfp6nFXuVJPcsyYMbk5OTn5H374Ye+TJ09GAh31JFevXp1sMJiL1Y8fP7559erVacuWLUs9e/asJjY2VnKNi8FgoOLi4uiHH364qri4+FR0dLTp6aefdmteiIOkFzj2FOX2DDpb3BIZo+p0fs6NqYiIVso8w/ceXF2AX62+uT1wO1uhy/ORjIUHRZxGslcmd9xbfFFPMjMzU9e3b1/dpEmTmgHgnnvuqT169Gi0O+0Kun2SocoxubmUr7ael31s4twcyeOB2joiNXwqtwCJh1oZCx+9Jg8or/vk+wy7IVeVwtRr8gCP60nOmTNnyLJlyypTU1ONXdWTTEtL0wMd9SQnTZrUvGfPnvjS0lJNTU2NMS8vTzts2LCrFy9e1Bw5ciRq+vTpjY73HDhwoCE1NVV39OjRiFGjRml37drVKzc3t82ddnOQ9CNni1vkAqy1XqO/GbRGnPm60q5dzmpmMsbCg3UVq7dXtwainiQAvPLKKxfnz58/SKfT0cCBA7UbN2684E67uZ6kH2148qBsT0xuvvLVhZ95tQ3qCCVu+Zk5Wb41+UFkjAp6vbFTSS+p+pC2SRPO5Efji5HRuCpMSI9Q4/eD0jA71eMFcIwxD3A9ye6RqyfJc5J+5MoCH0feHsrUa43Y+04xAOD+P92Eh1+fhF+tvhlRsZ3n5K37H21ZFyClrvgRto2KwhVhggBQptXjNyWX8GFlSG6lYowxSTzc6kfWHplU+jo542cMxu633a8Ac3ygBp+PjEJ9tALxLSbceqwVIy6aRzZMRtFp839Xez0dPVd6Ga0m+55nq0ngudLL3JtkjPlVUNeTZO5xZYGP4/m73z7lNOg5Op0die0jo6BXmfeP1Mcosf16cyYm63Mcg9+Z/Gh8Okjd6fpyPdlyrd6t44wx5iu+rCfJQTIEnMmPxva8CKdBD+iYQ3zbeA16h2ClVxE+HxnVfr5t8PuwsgYfj4iENWxar69QER4rkB4KTlAqUGvsvKIoQckj+Iyx8NEjfqPVFxbi7KTJKM7Lx9lJk1FfWBjU13X0xcjo9gBppVcRDlwfK7k3U643Vx9t/nYrlGQ3D/pc6WU4DqrqVYSDN8TJ93rlamaHfy1txlgPEvY9yfrCQlx+ejlEm3lrjKGiApefXg4AiJ82rfvX/OOfIOrq2o9547pyrsrsAbmmQqdVsR9W1kABQGoQPr7FhMgYFSbOzbELfnJB1XrfDytr8FzpZZRr9e2rWOsM0sP8dZaMGABQvP9z7N/0DhqvVSOudzImzrsPeRNvdfJKGWMsuIR9T/LqSy+3B0gr0daGqy+93K3rtQddmwDpjetKOXbsGF566SXEtLVIPp4eobb7+sPKGjx2+pJkgIQQqI9W4Pnb1NhOle3nj/33SchtAkqPUOOJkot4pPgiyrR6u1WscsOqcU31KN7/OYr3f45d69ehsboKEAKN1VXYtX4divd/7tqLZ4yxIBD2PUnDZek9pnLHuyIVdJ1d99ixY9i7dy/q6+sRHx+PyZMnY+TIkV3e59ixYygsLIRer8eNpSexL3c0DMqOb1cEgB+VHMGKXR+0X/epeoJeat+rEO3DoA2aKKxuMODkwSPYa6BOK1Rt1ej02FDReUtHq0lAC2F3XQBQ6XWYeGgX9lZ+D01kJAw6+0Fcg06L/Zve4d4kYwyAuZ5kbGysceXKlVd8cf2jR49G3HPPPe1zS2VlZRG/+93vypcvX37V1WuEfZBUpaXBUFEhebw7ugyuQuDspMnos/gx/JCR0R7oAKC+vh6FlnnLkSNHor6wEFdfehmGy5ehSktDn8WPtQ/V7t27t/152VXmbFBfDxqGpogoJAkjxp47jv6Xf7C7bu2Pp0q3yWGe0KBU4ROt6DJ5eouTPBMm2+taArNBpcaOSbNxvOw8qlLS0RppTpEY2daMyQd3IP/cMTRWV6F4/+ccKBkLYocPH07at29felNTkyY2NlZXUFBQfv3114fcJuhRo0ZprateDQYDUlNTR82bN6/zMKATYT/c2mfxY6DISLtjFBmJPosf69b1XAmu1vnJXR991B7orPR6vblnaRm2NVRUAEK0P8e6+Ke+vl72+jqdDkabuT8A2DtwaHuwcok3F9gQtX8IhRIXB2SjNSqm/VhbVCx23DILp4aYe9A7Xv0LXn3wXqyeNw3rH/4FD8EyFkQOHz6ctHPnzoympiYNADQ1NWl27tyZcfjw4ZCrJ2lr27ZtvQYOHKjNyclxK1F72AfJ+GnTkPbMSqj69QOIoOrXD2nPrHR5cU19YSFOjxuP4qF5KB6aB0NVlUvPE21taJIJWvX19U7nSo8dO2Z3/GxKOvbljkZTZDRAhKbIaOzLHY2zKentj59KHxQ8K0sl2iFUKuy/8SeWLwTaGhvb5yo/fX0NB0rGgsS+ffvSDQaDXWwwGAyKffv2hVw9SVsbN25MmjNnzjV32x32w62AOVA6BkWpoU4AdsdiC25G3ab37XtoDj1DEMn24KJbWtASE9O5PfHxssO251Uq/N/HH9sd+3rQMLv5SMA8ZLo3byz25o3tNDcYrBpiEySPmwwGfLZhPQ/BMhYErD1IV4+7ypV6ksuXL09vbGxUNjc3KwsKCuqBjnqSs2fPrp0/f34tYK4n+eKLL6aVlZVp5s2bVztixAj56hEA2traaM+ePfF/+ctfytxtd48IkrbqCwtx+b9XQLR0rBg1VFSg4re/szvPUFGBuo2bur6gkyHOkUeP4fAN18Oo6niblQYDsrOzUThjOloiIhDd0oKRR48h46L5j6pvrhsNk8l+y0dTRJT0DayBMQQCJAD0apKfCmhrtK9yw9tHGAuM2NhYnVRAjI2N9Xk9yS1btpwbP35869q1a3vv27cvDjDXk/zss89itm3bFj9mzJj8b7755tTChQtrJk6c2PzPf/4z/q677sp+5ZVXfpAqlWW1ZcuW+Pz8/JYBAwYY5M6RE3bDrY7Do2fGje+Y5yssxOUnl9kFSF9TGgzmQCoE1G1tyCwtxXeHDqElMhIgQktMDA6NH4ei60YDAPQREnUcta1+a69XSP3hYDRi4te7nT7NOuQqtX3k09fXYN2vpOcxi/d/jvUP/4LnOBnzgoKCgnKVSmX3l7pKpTIVFBR4XE+ysLAwsbKyUgkAXdWTtB631pN8+eWXKxITEw2lpaWaU6dOafLy8rRPPfXU1SlTptQdOXJEpidhtmnTpqS5c+d2a+GRRz1JIkoC8D6ATAAXAMwVQtRKnGcEcNzy5UUhxHRP7iunvrAQFb9/ErBZ1GKsq0PF0t932vzvaz8MHNipF2lSqXBp4EC7YwAAIpzPzkZKtfRw+Y2lJ83DqiHSY5RqZ5SuDfnnjkmc3GHX/6xD3sRbsX/TO522j5gMBmibzH8oNlZXYce61dj79/UYOn4iTu7b236+dT8mAO55MtYN1lWs3l7dGqh6kg0NDYoDBw702rBhww/dabdH9SSJ6AUANUKIVUS0FECiEOIJifOahBCx7l7f3XqSZydNltzuEQiF0+6SnI90On9o+V6c7dMfB4aMhFZtHvGI1OvQptaETpCUIgR++8bTXZ4WGRfXaei1O+KSU7Dg1bc9vg5joYbrSXaPr+pJzgCwwfL5BgAzPbyeR7qbIMAXWqKj3X8SEc726Y/Phl4HrSaiYwuFxrs1JQPB2XykLW8ESABovMa/DxhjnvN04U5fIYQ1MlUC6CtzXiQRFQEwAFglhPhY5jwQ0QIACwBg4MCBbjVGLnGAP/0wcCCOjXKSUUeiJ5mSUorMrCOIiGjGP+jvEKTs/LwQ70V2NR/pbXG9k/16P8ZY4AS0niQR7QEgVQpime0XQghBRHJjtxlCiHIiGgTgMyI6LoQ4L3WiEGI9gPWAebi1q/bZ6rP4MVQ8sRQwSScE9zWpeUhbSoMBRqV9AExJKUV2ziEolUYcxAQ0we1R6ZDQ1XykN6k0EZg47z6/3Y8xFlgBrScphPgPuceI6AoRpQkhLhNRGgDJfHhCiHLLv6VE9AWA0QAkg6THFIrOQVLqmA8cGzUSSWkXMXhwEVRq80ISo1EJk0kJtVoHY5MG5y5dj6rqQe3Pycw6AqXS/IfOZswP7R5jAKg0ERhWMBml3x3m7SKMMa/zdLh1G4D7Aayy/LvV8QQiSgTQIoTQElEygJsAvODhfSVdfellu5Wt7axDnB4sUnJFTMYV5OR+BYWiIyCrVEZYC1ep4nTIzTuIwfoinD8/FlVVgxAR0dx+bjV4iNAdEbFxmPzAAg6IjDGf8TRIrgKwmYh+BeAHAHMBgIjGAlgohHgQQB6AN4jIBPNCoVVCCJ90i2UX7ngYHFvGGtE4wwhjEqCsAeK2KhFd1HneMCvrW7sAKYUIUGu0yB16ELlDD9o9loxqVKOPR20NVm/MfxwTv97t1WFXo86ne5sZY8yz1a1CiGtCiMlCiGwhxH8IIWosx4ssARJCiH8LIUYIIUZZ/v2bNxoupbuVPZxpGWtE/XwjjL0BEGDsDdTPN6JlbOe5YE2k65v+bXKCt5uLd6ER8mW4QhYRGuISsbNgZnuSc2+wlt5ijDFfCauMO1IVPzzVOMMI4bADQ0SYj9u69EvPt2nchAN4EH8FicAsPPI1g1rTkeTcSxqrXUs4zxgLPkuWLOm3fPlyuV0RXvGHP/yhz5AhQ4ZlZ2cPmzZtWlZLS4tbCz/CKkhaK35AKbGFopuMMsVhjEnAlWd0qHhVhyvP6KAY0+iVNTc34QB8O3MaWHJJzj3BaegY866ysneT9h8YP2LvZ0PG7D8wfkRZ2bsel8kKhO+//169fv36vkeOHDl19uzZk0ajkd588023XktYBUnAHCj7rXrOaz1KpZNETLZDsN6UjPDdCO9qUgF3/Ou1lzhQMuYlZWXvJp0998cMne6qBhDQ6a5qzp77Y4Y3AmUg6kkajUZqbm5W6PV6tLa2Kvr376+XO1dK2AVJoKNHqUzwvNcSt1UJOL6lAoBDr9GbOzfm4l2fr8QNCB8lFRAmE3atX8eBkjEv+P7CunSTSWsXG0wmreL7C+tCrp5kVlaW/uGHH67Mysoa2adPn1FxcXHGWbNmNbjT7rAMkoA5UOYc+gqIcpocvl3LWKPd8KnUwhxfOogJWIS/Yj4+MO+XDDdCYOClsz5LKsCLeBjzDp2uSrJupNxxV7lST3LMmDG5OTk5+R9++GHvkydPRgId9SRXr16dbLBs8Rs/fnzz6tWr05YtW5Z69uxZTWxsrGSvoqqqSrl9+/aEc+fOHa+srDzW0tKieO2113r2cGsnrV2vOHW2grVxhhFQOzzBy/v9D2IC3sRDqKY+ACnM/4bbzCQRKtIyvbq61VFjdRWXzGLMQxpNimSvTO64tyxYsCBr3bp1F8+cOXPqiSeeqNBqzb3Z99577+Kzzz5bcenSJc2YMWPyKysrlQsXLqzZunXruaioKNNdd92VvW3btjipaxYWFvYaOHCgtl+/foaIiAgxc+bMun//+99upTUL6yBprSPZFWcrWOUW7njTZsyHjhzmUEkRdkOuvljd6si2BiUPwTLmvqzMR8oVigi7JfYKRYQpK/ORkKsnmZmZqfv2229jGxsbFSaTCZ999llcXl6eW/vsPE0mENSuvvSyS+c5W8HqD/KZdgSSRRWqkRJ66epkSoL5YnWrHOsQLGfkYcx1/fvPrwHMc5M6XZVGo0nRZWU+Um493l2BqCc5adKk5mnTptWOHDkyT6VSYdiwYS1Llixxa9+YR/Ukfc3depKOivPyXeqNXXlGJ71C1QjAe7tJZC3CXy1DrA6EQDKq0IZINFEv3zfEm2SCZK/GWvzXu6u9couI2Dhom5sQ1ztZfr8kER7f5NqIAmPhgOtJdo9cPcmw7UnWFxaaE5sbu16AE7dVifr59kOupAWER9PUrpuLd/GmeEhiyJVQjT5QCp3zYs2B4qRNkW3NMKg0MKg73kSVXme3uvXUkJHYf+NP0BCbgF5NdW6nrbPN27r+4V9IBkoumcUY80RYBsn6wkJcfnq5ZIB0lofV8XjjDKPX90BKuQkHAACbxXzJoVUjaQBhgtdXDHmIjAYIpeVHyKbNKr0Okw/uAADZIHhqyEjsLJjZHkStaesA18tq2Q6lTpx3H3atXweDTtvRDi6ZxViPENB6kqHo6ksvQ7R1npu1rmK19hitq1gBILqoc9Ly5nGWhTt+iE034QBuwgHMxweyN9SINvvepiVwxqIBTejl/Z6m7VC1xLWFUoXfvvG00x6hXMDbf+NP7HqZQMfCHleDZOO1jtEja7Dcv+kdLpnFWA8T0HqSoUiuGoizVaxSVT30ufB7502uEkgsGnEf3rL0NpORjGrMxbvtvVBnwbVbhEB+2XmkV17AnjGTIKTmFy3Zc/LPHXN7/6PcAh7H48qICBi1WslzHYdS8ybeykGRMeZVYRkkVWlpMFRUdDru9irWAGyQmYt38YZ42DzEaqMN0QCANXhI8nmyZba6O5dJhOL0LJxKH2Se23XgOL/orl5NdWiIS5Q83t4EhUI27CtUKh5KZYz5XFjuk5SqBkKRkVDWSb9c2fysASjGcRMOIAqdh4oNpHaaiUeqzJanZbeEQikZIGEyYenxRqww3I67+i/EwJg8t6898evdUOnt9yY7Bl5hMsEg04tUR0Zxr5Ex5nNh2ZOMnzYNgHlu0nD5MlRpaeiz+DFolEW4oPuH3apV0lrys0qI2k9oLRB+H3JtgnRCCPn9lI6Lf5IRiyYAAjp4XsKrEyLMrNIABMSo43F98h0AgIvNxS5fwjo8293VrdrmJvfbzRhjbgrLIAmYA6U1WLYfwzRgB3CxbiOM8cZOq1sdJW5WA9CjdaIw97n9FCzlhk67qg5iXfxjTXPXaUuJj6gUaoxMLHArSALdm8u04q0djIW+JUuW9IuNjTWuXLnyiq/u8cwzz/R55513UoQQuO+++6qWL19+1Z3nh22QlDPozj9gEP4AADg9bjxEnX3pJqktIomblfIJB3xAat+kRrSZq4O4QDLNHQAIAQVMMJGLGRJk5jPj9J1XVEer/JfsgLd2MOZbG8qrk/5yoTL9qs6g6aNR6ZZkppbfn57sUcadQDh8+HDkO++8k/Ltt98WR0ZGmgoKCnJmzZpVP3z4cOl5HAlhOSfpqrRlT9rNXcolOq96RAdjIvyWc/wmHMCD+CuSxVVAmJAsruJB/LV9SLUrztLcLcTaznOVQgDChFjRgFhRb/68rQX55aVQmOwDospkwu+KO5djazG4VX1GFikUABEi4+JAEsWzI2LjcNuCR3g+kjEf2VBenbT8XHnGFZ1BIwBc0Rk0y8+VZ2worw65epLHjx+PGj16dFNcXJxJrVbjpptuaty0aZNbuTF7XE/SluPcZeNMk+QWEX0e/D4vaR067Q5nw7U34QB0+kj8U3U3rlFSp60kANDWFoPD/zcLEMCYOoHdgwfhSiShb5vAgtON+EmlEVB0lEYxmPQ4VrsPdz7yuHmfolyKOJi3dKg1GrQ1Nko+LoRoTyNXvP9z3vfImJ/95UJlutYk7DpQWpNQ/OVCZbonvUlrPcmvvvrqdFpamuHKlSvK559/vq/18fnz59c+/vjj1QDw6KOP9lu7dm3ysmXLrlrrSWZlZemrq6uVQEc9yYceeqimra2NrCW0HP3oRz9qXblyZXplZaUyJiZG7N69O37UqFHN7rTboyBJRHcDWAEgD8ANQgjJRKtEdDuANTBnQn1TCLHKk/t6k+3cZcXewdInBVeimy45G641mYC0c61YM/S/JF+WEMCF738ECOAWfT6GlPfFovJmm8cJh6r/hZGJBYhW9UKLoQHHavfhYnMx7p74ZwDolPnGVnRcLyx49W2X0sjxvkfG/O+qziCZkFPuuKtcqSe5fPny9MbGRmVzc7OyoKCgHuioJzl79uza+fPn1wLmepIvvvhiWllZmWbevHm1I0aMkPyFc91117UtWrSocvLkyTlRUVGmYcOGtSglRqic8XS49QSAWQC+lDuBiJQAXgVwB4B8APcSUb6H9/WJyMh+gW6CVzgbrlUQofrqIGi1MZLPNegjUFU1SPbaraIZF5uL8UnZ69h84QV8UvY6LjYXIy45BYA5sN224BHZ51uz5Eycdx9UGvtuO881MhZ4fTQqybqRcse9xRf1JAFg8eLF1SdPniwuKioqSUxMNObk5PivVJYQohgAyPlm9RsAnBNClFrO3QRgBgCfpBDyxKDBv8Hp08tgMtkUahYIuZ4k4Gy4VqBAn4/i0gsYnPtvKJUdf8wZjUqcP28pHkBAkaoUQ3Rpds9WZUbjLtNDiFbGtfciK/SldsEtb+KtssOu1p6iO2nkeNiVMf9ZkplavvxceYbtkGuEgkxLMlM9ric5Z86cIcuWLatMTU01dlVPMi0tTQ901JOcNGlS8549e+JLS0s1NTU1xry8PO2wYcOuXrx4UXPkyJGo6dOnS87hlJeXq9LT0w1nz57VbN++PeHw4cOn3Wm3P+Yk0wFcsvm6DMCNcicT0QIACwBg4MCBvm2Zg7TUGQCA0vMvok1rSW1HwVtKrDtMgvCdqhQDKn+MS1AhddD/ISKiGVptDC58/yO7XmQTdf6DS1OuhMaykjVGHY8bUqZCNxrInniL3XmuJBx3ZTi1eP/ndtexFlO2Pp8x5l3WeUdvr24NRD1JAJg+ffrguro6lUqlEi+//PLF5ORktxKed1lPkoj2AEiVeGiZEGKr5ZwvAPxGak6SiOYAuF0I8aDl658DuFEIIT8mZ+FpPUlP7f1MZo4yRAkBVFRko/T8OCiFAhP1QzHElIb/jdgHLXWe+I41RWKe7qYur6uIVqHf8vGdjnujByg7d5mcggWvvu3WtRjrCbieZPd0u56kEOI/PLx3OYABNl/3txxjfmQbIAHASKb24dTx+hzsV5+GkTry8CmFAmMN8nOTtkwt0ivLvLHwxrbShyvHGWPMm/wx3HoYQDYRZcEcHOcBrATxWQAACi5JREFU+Jkf7usxlSoRBkNtoJvhFVptTHuAtLIOpw4xpQF68xxkE7UhVkRirGGQ+bgFqRUQeufJbJu/u4qGnRdgrNNCmRCBXlMyETNaIum6G+J6J3MxZcaYU0FbT5KIfgrgFQApALYT0REhxBQi6gfzVo87hRAGInoEwE6Yt4C8JYQ46cl9/SUn52kUFz8BITpvnpcV6IU+JpW5EQrbBTkq87YOB7GiY4vIEFNap0U6thJmZaNu2zmI1s4/bxSlRPN3V1H30dn2QGqs06L2/RJof6hH0szsbr8cLqbMmFeYTCYTKRSK8FpkYeFpPUmTyUSQKWnh6erWfwL4p8TxCgB32ny9A8AOT+4VCNaFPGe++xMMmmoo9LEwqZoBmZ8zhSIKJmOr5GNeY721VCA2KZB64pcAgOrsD2GIvAZVW2/QucmoqYkCujmcar1XwvQhqP2gxP5HSWE+3rDzgmRPs+VQJSIy4rvdo+Riyox5xYmqqqr8lJSU+nANlN1lMpmoqqoqHuYtjZ10uXAnkAK9cMfKtpdUn/pvXB36HkzqJkvwIAACkRH9MGjwb8wBNUJivsyxh9nNHqeqtTeSz87GlWF/h1B2bFsiowZ9Tz6A+MofSz7vnOKy0+HUrpBagYRZ5h6h1JBq2dL9ss9VJkQgbekNLt+LMdZ9Ugt3vvnmmz4qlepNAMPRw9ORSjABOGEwGB4cM2ZMp+TnPTotnausvaCGnRcQX/ljJLXdKjvfpk1sQGnjnzoFsCjtULREnjD35oTCrlfnKjJqkHx2dnsgtO0t2h6X0tVwaleE3oSGnReQtvQGydetTIiAsU46y47cccaYf1h++U8PdDtCEQdJF8WM7uPSkGHmuJ8Dh4AfatfBoKmGSpeMjMRHkPmTn+PK/xyF/rw5Efj5iY/DEHWt8wXkepgCdj3F+MofOw2KvuAs2PWakona90skH1Mm+KCmJWOM+QEHSR/IHPdzZOLnnY7Hjk1D7cVGQC9kh0zJqIFJ07mgsKqtt9+DoiNnwS5mdB9of6hHy6FKu+OkVqDXlEwft4wxxnyDg6SfWOc1oTfPAcsNmQKQDJ7WxwLFlWCXNDMbERnxXt8GwhhjgcJB0k+kVn86GzJ1Z77R19wJdq4OSzPGWCjgIOkn7ixeCcR8oxxemcoY68l4KbCfhOLiFZ5PZIz1dBwk/SQUgg1FKduDuTIhAgmzsnnolDHWo/Fwq5/EjO4jm9YtWIhWI9L+OziGeRljLBhwT9KPEqYPkX7HnX0X/JgHNhSHhBljzJc4SPpRzOg+SLw7FxTVUZBbEa1C4t25iB7XuWQnqRVInJuL/qsmSj7uTTz/yBhjnXHu1iDiSqmp5u+uonZzSUeic0dKAowufk/NaWd5PyNjYUQqdyvrPp6TDCKu7DG0Pm5blsqOC3/0RI9L9ah8FWOM9RQcJEOIbU+TopSAAZ17lCa09xA7ISBxbi73GBljzEUcJEOEY1Fjp6tkJQKktdQVB0jGGHMdB8kQIVfU2BU858gYY93DQTJEdLcmI6eVY4yx7uMtICFCbg+jIlrldH8jFzxmjLHu4yAZInpNyQSp7b9dpFYgftpgpC29QTZQcoIAxhjrPo+CJBHdTUQnichERLL7cojoAhEdJ6IjRNRzNj56UczoPkiYlS2bW1UuiHKCAMYY6z5P5yRPAJgF4A0Xzr1VCFHt4f16NGf7KK3HueAxY4x5j0dBUghRDABEfkwwymRxwWPGGPMuf81JCgC7iOgbIlrg7EQiWkBERURUVFVV5afmMcYYY5112ZMkoj0ApLJrLxNCbHXxPhOEEOVE1AfAbiI6LYT4UupEIcR6AOsBc+5WF6/PGGOMeV2XQVII8R+e3kQIUW759yoR/RPADQAkgyRjjDEWLHw+3EpEMUQUZ/0cwG0wL/hhjDHGgpqnW0B+SkRlAMYD2E5EOy3H+xHRDstpfQEcIKKjAP4PwHYhxKee3Jcxxhjzh6CuJ0lEVQB+CHQ7XJAMIBS3t4Riu7nN/hOK7eY2AxlCiBQvXq9HC+ogGSqIqCgUi5yGYru5zf4Tiu3mNjNv47R0jDHGmAwOkowxxpgMDpLesT7QDeimUGw3t9l/QrHd3GbmVTwnyRhjjMngniRjjDEmg4MkY4wxJoODZDe4UUfzdiIqIaJzRLTUn22UaU8SEe0morOWfxNlzjNaan8eIaJt/m6npQ1O3zsiiiCi9y2Pf01Emf5vZac2ddXmB4ioyua9fTAQ7XRo01tEdJWIJLNgkdlay2s6RkTX+buNEm3qqs23EFG9zfu83N9tlGjTACL6nIhOWX53LJI4J+jeawZACMEfbn4AyAOQC+ALAGNlzlECOA9gEAANgKMA8gPc7hcALLV8vhTA8zLnNQW4nV2+dwB+DeB1y+fzALwfAm1+AMC6QLZTot03A7gOwAmZx+8E8C8ABGAcgK9DoM23APgk0O10aFMagOssn8cBOCPx8xF07zV/CO5JdocQolgIUdLFaTcAOCeEKBVC6ABsAjDD961zagaADZbPNwCYGcC2OOPKe2f7WrYAmEyBLWwajN/vLglzNZ4aJ6fMAPCOMDsEIIGI0vzTOmkutDnoCCEuCyG+tXzeCKAYQLrDaUH3XjMebvWldACXbL4uQ+f/KfytrxDisuXzSpjz6kqJtNT0PEREgQikrrx37ecIIQwA6gH09kvrpLn6/Z5tGUrbQkQD/NM0jwTjz7ErxhPRUSL6FxENC3RjbFmmBkYD+NrhoVB9r8Nal6Wyeiov1dH0O2fttv1CCCGISG7/T4Yw1/8cBOAzIjouhDjv7bb2QIUANgohtET0XzD3hCcFuE3h6FuYf4abiOhOAB8DyA5wmwAARBQL4EMAjwkhGgLdHtY1DpIyhOd1NMsB2PYU+luO+ZSzdhPRFSJKE0JctgzjXJW5hrX+ZykRfQHzX73+DJKuvHfWc8qISAUgHsA1/zRPUpdtFkLYtu9NmOeIg11Afo49YRt8hBA7iOg1IkoWQgQ08TkRqWEOkO8KIT6SOCXk3uuegIdbfecwgGwiyiIiDcyLSwKyUtTGNgD3Wz6/H0CnHjERJRJRhOXzZAA3ATjltxaaufLe2b6WOQA+E0IEMjNGl212mF+aDvO8VLDbBuA+y8rLcQDqbYbsgxIRpVrnp4noBph/zwXyDyhY2vM3AMVCiL/InBZy73WPEOiVQ6H4AeCnMM8XaAFcAbDTcrwfgB02590J8yq28zAP0wa63b0B7AVwFsAeAEmW42MBvGn5/McAjsO8OvM4gF8FqK2d3jsAKwFMt3weCeADAOdgrlM6KAje367a/ByAk5b39nMAQ4OgzRsBXAagt/xM/wrAQgALLY8TgFctr+k4ZFZzB1mbH7F5nw8B+HEQtHkCAAHgGIAjlo87g/295g/BaekYY4wxOTzcyhhjjMngIMkYY4zJ4CDJGGOMyeAgyRhjjMngIMkYY4zJ4CDJGGOMyeAgyRhjjMn4/72RXl4EB81SAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "for i in range(10):\n",
        "    plt.scatter(x[idx[i],0],x[idx[i],1],label=\"class_\"+str(i))\n",
        "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "UfFHcZJOr0Sz"
      },
      "outputs": [],
      "source": [
        "foreground_classes = {'class_0','class_1' }\n",
        "\n",
        "background_classes = {'bg_classes',}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jqbvfbwVr0TN",
        "outputId": "ee31cbbb-ff1f-4a03-9873-458652991f6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 1100/1100 [00:00<00:00, 3036.66it/s]\n"
          ]
        }
      ],
      "source": [
        "desired_num = 1100\n",
        "mosaic_list_of_images =[]\n",
        "mosaic_label = []\n",
        "fore_idx=[]\n",
        "m = 5\n",
        "for j in tqdm(range(desired_num)):\n",
        "    np.random.seed(j)\n",
        "    fg_class  = np.random.randint(0,3)\n",
        "    fg_idx = np.random.randint(0,m)\n",
        "    a = []\n",
        "    for i in range(m):\n",
        "        if i == fg_idx:\n",
        "            b = np.random.choice(np.where(idx[fg_class]==True)[0],size=1)\n",
        "            a.append(x[b])\n",
        "#             print(\"foreground \"+str(fg_class)+\" present at \" + str(fg_idx))\n",
        "        else:\n",
        "            bg_class = np.random.randint(3,10)\n",
        "            b = np.random.choice(np.where(idx[bg_class]==True)[0],size=1)\n",
        "            a.append(x[b])\n",
        "#             print(\"background \"+str(bg_class)+\" present at \" + str(i))\n",
        "    a = np.concatenate(a,axis=0)\n",
        "    mosaic_list_of_images.append(np.reshape(a,(m,2)))\n",
        "    mosaic_label.append(fg_class)\n",
        "    fore_idx.append(fg_idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "BOsFmWfMr0TR"
      },
      "outputs": [],
      "source": [
        "# mosaic_list_of_images = np.concatenate(mosaic_list_of_images,axis=1).T\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2aIPMgLXNiXW",
        "outputId": "e0df682d-3a50-43bf-d0a1-8bb46dafcf7f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1100, array([[ 1.29420953,  0.98948331],\n",
              "        [-0.62723678, -1.08186056],\n",
              "        [-0.85061921,  0.02673107],\n",
              "        [-0.83096682, -0.93025336],\n",
              "        [-0.94404393, -0.55234797]]), (5, 2))"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "len(mosaic_list_of_images), mosaic_list_of_images[0],mosaic_list_of_images[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "iPoIwbMHx44n"
      },
      "outputs": [],
      "source": [
        "class MosaicDataset(Dataset):\n",
        "  \"\"\"MosaicDataset dataset.\"\"\"\n",
        "\n",
        "  def __init__(self, mosaic_list_of_images, mosaic_label, fore_idx):\n",
        "    \"\"\"\n",
        "      Args:\n",
        "        csv_file (string): Path to the csv file with annotations.\n",
        "        root_dir (string): Directory with all the images.\n",
        "        transform (callable, optional): Optional transform to be applied\n",
        "            on a sample.\n",
        "    \"\"\"\n",
        "    self.mosaic = mosaic_list_of_images\n",
        "    self.label = mosaic_label\n",
        "    self.fore_idx = fore_idx\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.label)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.mosaic[idx] , self.label[idx], self.fore_idx[idx]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "fOPAJQJeW8Ah"
      },
      "outputs": [],
      "source": [
        "batch = 50\n",
        "msd1 = MosaicDataset(mosaic_list_of_images[0:100], mosaic_label[0:100] , fore_idx[0:100])\n",
        "train_loader = DataLoader( msd1 ,batch_size= batch ,shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "aWBIcyvGApLt"
      },
      "outputs": [],
      "source": [
        "data,_,_=iter(train_loader).next()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cauJIvKEAxKM",
        "outputId": "00e3b029-6fed-40ca-937a-9e103b51ce2c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([50, 5, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "qjNiQgxZW8bA"
      },
      "outputs": [],
      "source": [
        "batch = 250\n",
        "msd2 = MosaicDataset(mosaic_list_of_images[100:], mosaic_label[100:] , fore_idx[100:])\n",
        "test_loader = DataLoader( msd2 ,batch_size= batch ,shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "yda1E5ApiKpH"
      },
      "outputs": [],
      "source": [
        "class Focus(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Focus, self).__init__()\n",
        "        self.fc1 = nn.Linear(2,1, bias=False)\n",
        "        torch.nn.init.xavier_normal_(self.fc1.weight)\n",
        "        #self.fc2 = nn.Linear(64, 1, bias=False)\n",
        "        #torch.nn.init.xavier_normal_(self.fc2.weight)\n",
        "\n",
        "    def forward(self,z):\n",
        "        #print(\"data\",z)\n",
        "        batch = z.size(0)\n",
        "        patches = z.size(1)\n",
        "        z = z.view(batch,patches,2*1)\n",
        "        alp1,ft1 = self.helper(z)\n",
        "\n",
        "        alpha = F.softmax(alp1,dim=1)\n",
        "        #print(self.training)\n",
        "        \n",
        "        if self.training:\n",
        "            alpha =alpha[:,:,0]\n",
        "            y = ft1 \n",
        "            return alpha,y\n",
        "        else:\n",
        "            #alpha_cumsum = torch.cumsum(alpha, dim = 1)\n",
        "            #print(alpha_cumsum)\n",
        "            #len_batch = alpha_cumsum.size(0)\n",
        "            #patches = alpha_cumsum.size(1)\n",
        "            #rand_prob = torch.rand(len_batch,patches, 1).to(device)\n",
        "            #alpha_relu = F.relu(rand_prob-alpha_cumsum)\n",
        "            #print(alpha_relu)\n",
        "            #alpha_index = torch.count_nonzero(alpha_relu,dim=1)\n",
        "            #alpha_hard = F.one_hot(alpha_index,num_classes=patches)\n",
        "            #print(alpha_hard)\n",
        "            #alpha_hard = torch.transpose(alpha_hard,dim0=1,dim1=2)\n",
        "            #print(ft1,\"alpha_hard\",alpha_hard) \n",
        "            #y = torch.sum(alpha_hard*ft1,dim=1)\n",
        "            #print(alpha,alpha.shape)\n",
        "         \n",
        "        \n",
        "            index = torch.argmax(alpha,dim=1)\n",
        "            hard_alpha = torch.nn.functional.one_hot(index[:,0], patches)\n",
        "            y = torch.sum(hard_alpha[:,:,None]*ft1,dim=1)\n",
        "            alpha = alpha[:,:,0]\n",
        "            return alpha,y\n",
        "    \n",
        "    def helper(self, x):\n",
        "        x1 = x\n",
        "        x = self.fc1(x)\n",
        "        return x,x1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "0dYXnywAD-4l"
      },
      "outputs": [],
      "source": [
        "class Classification(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Classification, self).__init__()\n",
        "    self.fc1 = nn.Linear(2, 50)\n",
        "    self.fc2 = nn.Linear(50,3)\n",
        "    torch.nn.init.xavier_normal_(self.fc1.weight)\n",
        "    torch.nn.init.zeros_(self.fc1.bias)\n",
        "    torch.nn.init.xavier_normal_(self.fc2.weight)\n",
        "    torch.nn.init.zeros_(self.fc2.bias)\n",
        "  def forward(self, x):\n",
        "    #print(x.shape)\n",
        "    #x = x.view(-1, 1)\n",
        "    #print(x.shape)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = self.fc2(x)\n",
        "    # print(x.shape)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "lSa6O9f6XNf4"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(13)\n",
        "focus_net = Focus().double()\n",
        "focus_net = focus_net.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "36k3H2G-XO9A"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(13)\n",
        "classify = Classification().double()\n",
        "classify = classify.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "bK78aII8-GDl"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "optimizer_classify = optim.Adam(classify.parameters(), lr=0.01 ) #, momentum=0.9)\n",
        "optimizer_focus = optim.Adam(focus_net.parameters(), lr=0.01 ) #, momentum=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "h0mjWiFG-GDl"
      },
      "outputs": [],
      "source": [
        "def my_cross_entropy(output,target,alpha):\n",
        "    criterion = nn.CrossEntropyLoss(reduce=False)\n",
        "    \n",
        "    batch = output.size(0)\n",
        "    #print(batch)\n",
        "    patches = output.size(1)\n",
        "    classes = output.size(2)\n",
        "    \n",
        "    \n",
        "    \n",
        "    output = torch.reshape(output,(batch*patches,classes))\n",
        "    \n",
        "    \n",
        "    target = target.repeat_interleave(patches)\n",
        "    \n",
        "    loss = criterion(output,target)\n",
        "    \n",
        "    #print(loss,loss.shape)\n",
        "    loss = torch.reshape(loss,(batch,patches))\n",
        "    #print(loss.size())\n",
        "    final_loss = torch.sum(torch.mul(loss,alpha),dim=1)\n",
        "    #print(final_loss.shape)\n",
        "    final_loss = torch.mean(final_loss,dim=0)\n",
        "    \n",
        "    \n",
        "    \n",
        "    #print(final_loss)\n",
        "    return final_loss\n",
        "    \n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "pjD2VZuV9Ed4"
      },
      "outputs": [],
      "source": [
        "col1=[]\n",
        "col2=[]\n",
        "col3=[]\n",
        "col4=[]\n",
        "col5=[]\n",
        "col6=[]\n",
        "col7=[]\n",
        "col8=[]\n",
        "col9=[]\n",
        "col10=[]\n",
        "col11=[]\n",
        "col12=[]\n",
        "col13=[]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "eEVrBg7d-GDl"
      },
      "outputs": [],
      "source": [
        "def plot_attended_data(trainloader,net,epoch):\n",
        "    attd_data =[]\n",
        "    lbls = []\n",
        "    for data in trainloader:\n",
        "        inputs, labels , fore_idx = data\n",
        "        inputs = inputs.double()\n",
        "        inputs, labels , fore_idx = inputs.to(device),labels.to(device), fore_idx.to(device)\n",
        "        alphas, avg_images = focus_net(inputs)\n",
        "        attd_data.append(avg_images.numpy())\n",
        "        lbls.append(labels)\n",
        "    attd_data = np.concatenate(attd_data,axis=0)\n",
        "    lbls = np.concatenate(lbls,axis=0)\n",
        "    plt.figure(figsize=(6,8))\n",
        "    plt.scatter(attd_data[:,0],attd_data[:,1],c=lbls)\n",
        "    plt.title(\"EPOCH_\"+str(epoch))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "uALi25pmzQHV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24a87429-1a81-4b1a-993c-5db7de33eebc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1.1133, device='cuda:0', dtype=torch.float64)\n",
            "tensor(1.1222, device='cuda:0', dtype=torch.float64)\n",
            "====================================================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        }
      ],
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "count = 0\n",
        "flag = 1\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "  for data in train_loader:\n",
        "    inputs, labels , fore_idx = data\n",
        "    inputs = inputs.double()\n",
        "    inputs, labels , fore_idx = inputs.to(device),labels.to(device), fore_idx.to(device)\n",
        "    alphas, avg_images = focus_net(inputs)\n",
        "    outputs = classify(avg_images)\n",
        "    loss = my_cross_entropy(outputs,labels,alphas)\n",
        "    print(loss)\n",
        "    # print(outputs.shape)\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "\n",
        "print(\"=\"*100)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "4vmNprlPzTjP"
      },
      "outputs": [],
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "count = 0\n",
        "flag = 1\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "  for data in test_loader:\n",
        "    inputs, labels , fore_idx = data\n",
        "    inputs = inputs.double()\n",
        "    inputs, labels , fore_idx = inputs.to(device),labels.to(device), fore_idx.to(device)\n",
        "    alphas, avg_images = focus_net(inputs)\n",
        "    outputs = classify(avg_images)\n",
        "\n",
        "    _, predicted = torch.max(outputs.data, 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "_nvicAzw-GDm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed9011cb-c02e-44c4-ca44-515f5d26ef17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('fc1.weight', Parameter containing:\n",
            "tensor([[-0.4153,  0.0408]], device='cuda:0', dtype=torch.float64,\n",
            "       requires_grad=True))\n"
          ]
        }
      ],
      "source": [
        "for param in focus_net.named_parameters():\n",
        "    print(param)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "Yl41sE8vFERk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5c50486-408e-48e6-c677-91dbea654e8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,     2] loss: 1.105\n",
            "[2,     2] loss: 1.090\n",
            "[3,     2] loss: 1.088\n",
            "[4,     2] loss: 1.086\n",
            "[5,     2] loss: 1.085\n",
            "[6,     2] loss: 1.084\n",
            "[7,     2] loss: 1.084\n",
            "[8,     2] loss: 1.083\n",
            "[9,     2] loss: 1.081\n",
            "[10,     2] loss: 1.080\n",
            "[11,     2] loss: 1.081\n",
            "[12,     2] loss: 1.078\n",
            "[13,     2] loss: 1.077\n",
            "[14,     2] loss: 1.076\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[15,     2] loss: 1.076\n",
            "[16,     2] loss: 1.075\n",
            "[17,     2] loss: 1.074\n",
            "[18,     2] loss: 1.071\n",
            "[19,     2] loss: 1.070\n",
            "[20,     2] loss: 1.066\n",
            "[21,     2] loss: 1.065\n",
            "[22,     2] loss: 1.061\n",
            "[23,     2] loss: 1.058\n",
            "[24,     2] loss: 1.053\n",
            "[25,     2] loss: 1.048\n",
            "[26,     2] loss: 1.041\n",
            "[27,     2] loss: 1.033\n",
            "[28,     2] loss: 1.025\n",
            "[29,     2] loss: 1.015\n",
            "[30,     2] loss: 1.001\n",
            "[31,     2] loss: 0.987\n",
            "[32,     2] loss: 0.972\n",
            "[33,     2] loss: 0.956\n",
            "[34,     2] loss: 0.946\n",
            "[35,     2] loss: 0.922\n",
            "[36,     2] loss: 0.902\n",
            "[37,     2] loss: 0.880\n",
            "[38,     2] loss: 0.862\n",
            "[39,     2] loss: 0.843\n",
            "[40,     2] loss: 0.820\n",
            "[41,     2] loss: 0.802\n",
            "[42,     2] loss: 0.787\n",
            "[43,     2] loss: 0.762\n",
            "[44,     2] loss: 0.749\n",
            "[45,     2] loss: 0.728\n",
            "[46,     2] loss: 0.711\n",
            "[47,     2] loss: 0.691\n",
            "[48,     2] loss: 0.678\n",
            "[49,     2] loss: 0.662\n",
            "[50,     2] loss: 0.646\n",
            "[51,     2] loss: 0.637\n",
            "[52,     2] loss: 0.619\n",
            "[53,     2] loss: 0.607\n",
            "[54,     2] loss: 0.591\n",
            "[55,     2] loss: 0.583\n",
            "[56,     2] loss: 0.566\n",
            "[57,     2] loss: 0.555\n",
            "[58,     2] loss: 0.548\n",
            "[59,     2] loss: 0.533\n",
            "[60,     2] loss: 0.525\n",
            "[61,     2] loss: 0.516\n",
            "[62,     2] loss: 0.507\n",
            "[63,     2] loss: 0.496\n",
            "[64,     2] loss: 0.511\n",
            "[65,     2] loss: 0.484\n",
            "[66,     2] loss: 0.494\n",
            "[67,     2] loss: 0.463\n",
            "[68,     2] loss: 0.471\n",
            "[69,     2] loss: 0.468\n",
            "[70,     2] loss: 0.460\n",
            "[71,     2] loss: 0.439\n",
            "[72,     2] loss: 0.447\n",
            "[73,     2] loss: 0.439\n",
            "[74,     2] loss: 0.441\n",
            "[75,     2] loss: 0.418\n",
            "[76,     2] loss: 0.416\n",
            "[77,     2] loss: 0.417\n",
            "[78,     2] loss: 0.405\n",
            "[79,     2] loss: 0.405\n",
            "[80,     2] loss: 0.400\n",
            "[81,     2] loss: 0.398\n",
            "[82,     2] loss: 0.392\n",
            "[83,     2] loss: 0.384\n",
            "[84,     2] loss: 0.397\n",
            "[85,     2] loss: 0.382\n",
            "[86,     2] loss: 0.382\n",
            "[87,     2] loss: 0.382\n",
            "[88,     2] loss: 0.368\n",
            "[89,     2] loss: 0.378\n",
            "[90,     2] loss: 0.375\n",
            "[91,     2] loss: 0.370\n",
            "[92,     2] loss: 0.363\n",
            "[93,     2] loss: 0.357\n",
            "[94,     2] loss: 0.358\n",
            "[95,     2] loss: 0.356\n",
            "[96,     2] loss: 0.352\n",
            "[97,     2] loss: 0.349\n",
            "[98,     2] loss: 0.345\n",
            "[99,     2] loss: 0.346\n",
            "[100,     2] loss: 0.343\n",
            "[101,     2] loss: 0.342\n",
            "[102,     2] loss: 0.340\n",
            "[103,     2] loss: 0.340\n",
            "[104,     2] loss: 0.336\n",
            "[105,     2] loss: 0.333\n",
            "[106,     2] loss: 0.334\n",
            "[107,     2] loss: 0.330\n",
            "[108,     2] loss: 0.334\n",
            "[109,     2] loss: 0.329\n",
            "[110,     2] loss: 0.326\n",
            "[111,     2] loss: 0.331\n",
            "[112,     2] loss: 0.333\n",
            "[113,     2] loss: 0.323\n",
            "[114,     2] loss: 0.332\n",
            "[115,     2] loss: 0.321\n",
            "[116,     2] loss: 0.321\n",
            "[117,     2] loss: 0.320\n",
            "[118,     2] loss: 0.317\n",
            "[119,     2] loss: 0.321\n",
            "[120,     2] loss: 0.316\n",
            "[121,     2] loss: 0.319\n",
            "[122,     2] loss: 0.316\n",
            "[123,     2] loss: 0.313\n",
            "[124,     2] loss: 0.314\n",
            "[125,     2] loss: 0.323\n",
            "[126,     2] loss: 0.311\n",
            "[127,     2] loss: 0.311\n",
            "[128,     2] loss: 0.322\n",
            "[129,     2] loss: 0.305\n",
            "[130,     2] loss: 0.311\n",
            "[131,     2] loss: 0.310\n",
            "[132,     2] loss: 0.304\n",
            "[133,     2] loss: 0.307\n",
            "[134,     2] loss: 0.304\n",
            "[135,     2] loss: 0.318\n",
            "[136,     2] loss: 0.304\n",
            "[137,     2] loss: 0.304\n",
            "[138,     2] loss: 0.308\n",
            "[139,     2] loss: 0.302\n",
            "[140,     2] loss: 0.299\n",
            "[141,     2] loss: 0.309\n",
            "[142,     2] loss: 0.302\n",
            "[143,     2] loss: 0.298\n",
            "[144,     2] loss: 0.298\n",
            "[145,     2] loss: 0.300\n",
            "[146,     2] loss: 0.298\n",
            "[147,     2] loss: 0.306\n",
            "[148,     2] loss: 0.297\n",
            "[149,     2] loss: 0.297\n",
            "[150,     2] loss: 0.296\n",
            "[151,     2] loss: 0.301\n",
            "[152,     2] loss: 0.293\n",
            "[153,     2] loss: 0.299\n",
            "[154,     2] loss: 0.297\n",
            "[155,     2] loss: 0.290\n",
            "[156,     2] loss: 0.297\n",
            "[157,     2] loss: 0.299\n",
            "[158,     2] loss: 0.291\n",
            "[159,     2] loss: 0.300\n",
            "[160,     2] loss: 0.291\n",
            "[161,     2] loss: 0.290\n",
            "[162,     2] loss: 0.289\n",
            "[163,     2] loss: 0.296\n",
            "[164,     2] loss: 0.287\n",
            "[165,     2] loss: 0.290\n",
            "[166,     2] loss: 0.293\n",
            "[167,     2] loss: 0.285\n",
            "[168,     2] loss: 0.290\n",
            "[169,     2] loss: 0.291\n",
            "[170,     2] loss: 0.285\n",
            "[171,     2] loss: 0.289\n",
            "[172,     2] loss: 0.289\n",
            "[173,     2] loss: 0.286\n",
            "[174,     2] loss: 0.294\n",
            "[175,     2] loss: 0.287\n",
            "[176,     2] loss: 0.284\n",
            "[177,     2] loss: 0.283\n",
            "[178,     2] loss: 0.284\n",
            "[179,     2] loss: 0.286\n",
            "[180,     2] loss: 0.288\n",
            "[181,     2] loss: 0.282\n",
            "[182,     2] loss: 0.285\n",
            "[183,     2] loss: 0.283\n",
            "[184,     2] loss: 0.289\n",
            "[185,     2] loss: 0.285\n",
            "[186,     2] loss: 0.280\n",
            "[187,     2] loss: 0.283\n",
            "[188,     2] loss: 0.281\n",
            "[189,     2] loss: 0.286\n",
            "[190,     2] loss: 0.299\n",
            "[191,     2] loss: 0.284\n",
            "[192,     2] loss: 0.279\n",
            "[193,     2] loss: 0.280\n",
            "[194,     2] loss: 0.281\n",
            "[195,     2] loss: 0.291\n",
            "[196,     2] loss: 0.287\n",
            "[197,     2] loss: 0.280\n",
            "[198,     2] loss: 0.291\n",
            "[199,     2] loss: 0.278\n",
            "[200,     2] loss: 0.282\n",
            "[201,     2] loss: 0.286\n",
            "[202,     2] loss: 0.277\n",
            "[203,     2] loss: 0.284\n",
            "[204,     2] loss: 0.279\n",
            "[205,     2] loss: 0.277\n",
            "[206,     2] loss: 0.312\n",
            "[207,     2] loss: 0.284\n",
            "[208,     2] loss: 0.282\n",
            "[209,     2] loss: 0.283\n",
            "[210,     2] loss: 0.279\n",
            "[211,     2] loss: 0.280\n",
            "[212,     2] loss: 0.276\n",
            "[213,     2] loss: 0.275\n",
            "[214,     2] loss: 0.278\n",
            "[215,     2] loss: 0.275\n",
            "[216,     2] loss: 0.283\n",
            "[217,     2] loss: 0.280\n",
            "[218,     2] loss: 0.274\n",
            "[219,     2] loss: 0.276\n",
            "[220,     2] loss: 0.276\n",
            "[221,     2] loss: 0.274\n",
            "[222,     2] loss: 0.273\n",
            "[223,     2] loss: 0.285\n",
            "[224,     2] loss: 0.280\n",
            "[225,     2] loss: 0.274\n",
            "[226,     2] loss: 0.275\n",
            "[227,     2] loss: 0.272\n",
            "[228,     2] loss: 0.277\n",
            "[229,     2] loss: 0.272\n",
            "[230,     2] loss: 0.278\n",
            "[231,     2] loss: 0.274\n",
            "[232,     2] loss: 0.271\n",
            "[233,     2] loss: 0.274\n",
            "[234,     2] loss: 0.284\n",
            "[235,     2] loss: 0.278\n",
            "[236,     2] loss: 0.279\n",
            "[237,     2] loss: 0.281\n",
            "[238,     2] loss: 0.281\n",
            "[239,     2] loss: 0.276\n",
            "[240,     2] loss: 0.274\n",
            "[241,     2] loss: 0.278\n",
            "[242,     2] loss: 0.270\n",
            "[243,     2] loss: 0.274\n",
            "[244,     2] loss: 0.272\n",
            "[245,     2] loss: 0.272\n",
            "[246,     2] loss: 0.275\n",
            "[247,     2] loss: 0.269\n",
            "[248,     2] loss: 0.271\n",
            "[249,     2] loss: 0.272\n",
            "[250,     2] loss: 0.276\n",
            "[251,     2] loss: 0.272\n",
            "[252,     2] loss: 0.274\n",
            "[253,     2] loss: 0.272\n",
            "[254,     2] loss: 0.269\n",
            "[255,     2] loss: 0.270\n",
            "[256,     2] loss: 0.270\n",
            "[257,     2] loss: 0.270\n",
            "[258,     2] loss: 0.270\n",
            "[259,     2] loss: 0.269\n",
            "[260,     2] loss: 0.268\n",
            "[261,     2] loss: 0.269\n",
            "[262,     2] loss: 0.270\n",
            "[263,     2] loss: 0.268\n",
            "[264,     2] loss: 0.274\n",
            "[265,     2] loss: 0.270\n",
            "[266,     2] loss: 0.267\n",
            "[267,     2] loss: 0.269\n",
            "[268,     2] loss: 0.270\n",
            "[269,     2] loss: 0.272\n",
            "[270,     2] loss: 0.272\n",
            "[271,     2] loss: 0.267\n",
            "[272,     2] loss: 0.270\n",
            "[273,     2] loss: 0.275\n",
            "[274,     2] loss: 0.267\n",
            "[275,     2] loss: 0.274\n",
            "[276,     2] loss: 0.273\n",
            "[277,     2] loss: 0.264\n",
            "[278,     2] loss: 0.274\n",
            "[279,     2] loss: 0.272\n",
            "[280,     2] loss: 0.268\n",
            "[281,     2] loss: 0.268\n",
            "[282,     2] loss: 0.266\n",
            "[283,     2] loss: 0.266\n",
            "[284,     2] loss: 0.268\n",
            "[285,     2] loss: 0.267\n",
            "[286,     2] loss: 0.266\n",
            "[287,     2] loss: 0.268\n",
            "[288,     2] loss: 0.266\n",
            "[289,     2] loss: 0.265\n",
            "[290,     2] loss: 0.266\n",
            "[291,     2] loss: 0.266\n",
            "[292,     2] loss: 0.266\n",
            "[293,     2] loss: 0.269\n",
            "[294,     2] loss: 0.267\n",
            "[295,     2] loss: 0.266\n",
            "[296,     2] loss: 0.265\n",
            "[297,     2] loss: 0.274\n",
            "[298,     2] loss: 0.266\n",
            "[299,     2] loss: 0.264\n",
            "[300,     2] loss: 0.268\n",
            "[301,     2] loss: 0.265\n",
            "[302,     2] loss: 0.270\n",
            "[303,     2] loss: 0.275\n",
            "[304,     2] loss: 0.265\n",
            "[305,     2] loss: 0.265\n",
            "[306,     2] loss: 0.264\n",
            "[307,     2] loss: 0.265\n",
            "[308,     2] loss: 0.265\n",
            "[309,     2] loss: 0.264\n",
            "[310,     2] loss: 0.266\n",
            "[311,     2] loss: 0.271\n",
            "[312,     2] loss: 0.270\n",
            "[313,     2] loss: 0.266\n",
            "[314,     2] loss: 0.265\n",
            "[315,     2] loss: 0.265\n",
            "[316,     2] loss: 0.265\n",
            "[317,     2] loss: 0.264\n",
            "[318,     2] loss: 0.269\n",
            "[319,     2] loss: 0.280\n",
            "[320,     2] loss: 0.266\n",
            "[321,     2] loss: 0.268\n",
            "[322,     2] loss: 0.278\n",
            "[323,     2] loss: 0.268\n",
            "[324,     2] loss: 0.284\n",
            "[325,     2] loss: 0.269\n",
            "[326,     2] loss: 0.264\n",
            "[327,     2] loss: 0.281\n",
            "[328,     2] loss: 0.282\n",
            "[329,     2] loss: 0.271\n",
            "[330,     2] loss: 0.266\n",
            "[331,     2] loss: 0.267\n",
            "[332,     2] loss: 0.271\n",
            "[333,     2] loss: 0.269\n",
            "[334,     2] loss: 0.266\n",
            "[335,     2] loss: 0.265\n",
            "[336,     2] loss: 0.269\n",
            "[337,     2] loss: 0.265\n",
            "[338,     2] loss: 0.262\n",
            "[339,     2] loss: 0.263\n",
            "[340,     2] loss: 0.267\n",
            "[341,     2] loss: 0.265\n",
            "[342,     2] loss: 0.265\n",
            "[343,     2] loss: 0.263\n",
            "[344,     2] loss: 0.269\n",
            "[345,     2] loss: 0.266\n",
            "[346,     2] loss: 0.264\n",
            "[347,     2] loss: 0.267\n",
            "[348,     2] loss: 0.263\n",
            "[349,     2] loss: 0.263\n",
            "[350,     2] loss: 0.268\n",
            "[351,     2] loss: 0.264\n",
            "[352,     2] loss: 0.267\n",
            "[353,     2] loss: 0.267\n",
            "[354,     2] loss: 0.263\n",
            "[355,     2] loss: 0.267\n",
            "[356,     2] loss: 0.266\n",
            "[357,     2] loss: 0.273\n",
            "[358,     2] loss: 0.270\n",
            "[359,     2] loss: 0.260\n",
            "[360,     2] loss: 0.268\n",
            "[361,     2] loss: 0.266\n",
            "[362,     2] loss: 0.261\n",
            "[363,     2] loss: 0.272\n",
            "[364,     2] loss: 0.275\n",
            "[365,     2] loss: 0.266\n",
            "[366,     2] loss: 0.262\n",
            "[367,     2] loss: 0.265\n",
            "[368,     2] loss: 0.262\n",
            "[369,     2] loss: 0.261\n",
            "[370,     2] loss: 0.263\n",
            "[371,     2] loss: 0.262\n",
            "[372,     2] loss: 0.261\n",
            "[373,     2] loss: 0.263\n",
            "[374,     2] loss: 0.261\n",
            "[375,     2] loss: 0.261\n",
            "[376,     2] loss: 0.262\n",
            "[377,     2] loss: 0.265\n",
            "[378,     2] loss: 0.266\n",
            "[379,     2] loss: 0.261\n",
            "[380,     2] loss: 0.262\n",
            "[381,     2] loss: 0.274\n",
            "[382,     2] loss: 0.261\n",
            "[383,     2] loss: 0.263\n",
            "[384,     2] loss: 0.267\n",
            "[385,     2] loss: 0.261\n",
            "[386,     2] loss: 0.267\n",
            "[387,     2] loss: 0.270\n",
            "[388,     2] loss: 0.263\n",
            "[389,     2] loss: 0.274\n",
            "[390,     2] loss: 0.266\n",
            "[391,     2] loss: 0.280\n",
            "[392,     2] loss: 0.279\n",
            "[393,     2] loss: 0.259\n",
            "[394,     2] loss: 0.276\n",
            "[395,     2] loss: 0.271\n",
            "[396,     2] loss: 0.257\n",
            "[397,     2] loss: 0.271\n",
            "[398,     2] loss: 0.271\n",
            "[399,     2] loss: 0.259\n",
            "[400,     2] loss: 0.270\n",
            "[401,     2] loss: 0.262\n",
            "[402,     2] loss: 0.262\n",
            "[403,     2] loss: 0.265\n",
            "[404,     2] loss: 0.266\n",
            "[405,     2] loss: 0.272\n",
            "[406,     2] loss: 0.261\n",
            "[407,     2] loss: 0.259\n",
            "[408,     2] loss: 0.273\n",
            "[409,     2] loss: 0.265\n",
            "[410,     2] loss: 0.275\n",
            "[411,     2] loss: 0.269\n",
            "[412,     2] loss: 0.257\n",
            "[413,     2] loss: 0.274\n",
            "[414,     2] loss: 0.280\n",
            "[415,     2] loss: 0.260\n",
            "[416,     2] loss: 0.263\n",
            "[417,     2] loss: 0.260\n",
            "[418,     2] loss: 0.263\n",
            "[419,     2] loss: 0.260\n",
            "[420,     2] loss: 0.259\n",
            "[421,     2] loss: 0.276\n",
            "[422,     2] loss: 0.264\n",
            "[423,     2] loss: 0.261\n",
            "[424,     2] loss: 0.264\n",
            "[425,     2] loss: 0.264\n",
            "[426,     2] loss: 0.263\n",
            "[427,     2] loss: 0.268\n",
            "[428,     2] loss: 0.262\n",
            "[429,     2] loss: 0.259\n",
            "[430,     2] loss: 0.261\n",
            "[431,     2] loss: 0.259\n",
            "[432,     2] loss: 0.260\n",
            "[433,     2] loss: 0.259\n",
            "[434,     2] loss: 0.259\n",
            "[435,     2] loss: 0.259\n",
            "[436,     2] loss: 0.264\n",
            "[437,     2] loss: 0.260\n",
            "[438,     2] loss: 0.260\n",
            "[439,     2] loss: 0.263\n",
            "[440,     2] loss: 0.274\n",
            "[441,     2] loss: 0.266\n",
            "[442,     2] loss: 0.259\n",
            "[443,     2] loss: 0.259\n",
            "[444,     2] loss: 0.259\n",
            "[445,     2] loss: 0.261\n",
            "[446,     2] loss: 0.264\n",
            "[447,     2] loss: 0.260\n",
            "[448,     2] loss: 0.263\n",
            "[449,     2] loss: 0.259\n",
            "[450,     2] loss: 0.258\n",
            "[451,     2] loss: 0.260\n",
            "[452,     2] loss: 0.259\n",
            "[453,     2] loss: 0.262\n",
            "[454,     2] loss: 0.263\n",
            "[455,     2] loss: 0.259\n",
            "[456,     2] loss: 0.258\n",
            "[457,     2] loss: 0.262\n",
            "[458,     2] loss: 0.259\n",
            "[459,     2] loss: 0.267\n",
            "[460,     2] loss: 0.261\n",
            "[461,     2] loss: 0.258\n",
            "[462,     2] loss: 0.261\n",
            "[463,     2] loss: 0.260\n",
            "[464,     2] loss: 0.258\n",
            "[465,     2] loss: 0.260\n",
            "[466,     2] loss: 0.259\n",
            "[467,     2] loss: 0.258\n",
            "[468,     2] loss: 0.259\n",
            "[469,     2] loss: 0.258\n",
            "[470,     2] loss: 0.259\n",
            "[471,     2] loss: 0.259\n",
            "[472,     2] loss: 0.258\n",
            "[473,     2] loss: 0.259\n",
            "[474,     2] loss: 0.261\n",
            "[475,     2] loss: 0.270\n",
            "[476,     2] loss: 0.261\n",
            "[477,     2] loss: 0.259\n",
            "[478,     2] loss: 0.258\n",
            "[479,     2] loss: 0.261\n",
            "[480,     2] loss: 0.265\n",
            "[481,     2] loss: 0.261\n",
            "[482,     2] loss: 0.260\n",
            "[483,     2] loss: 0.260\n",
            "[484,     2] loss: 0.273\n",
            "[485,     2] loss: 0.259\n",
            "[486,     2] loss: 0.259\n",
            "[487,     2] loss: 0.263\n",
            "[488,     2] loss: 0.265\n",
            "[489,     2] loss: 0.262\n",
            "[490,     2] loss: 0.259\n",
            "[491,     2] loss: 0.259\n",
            "[492,     2] loss: 0.264\n",
            "[493,     2] loss: 0.275\n",
            "[494,     2] loss: 0.261\n",
            "[495,     2] loss: 0.258\n",
            "[496,     2] loss: 0.258\n",
            "[497,     2] loss: 0.259\n",
            "[498,     2] loss: 0.266\n",
            "[499,     2] loss: 0.259\n",
            "[500,     2] loss: 0.257\n",
            "[501,     2] loss: 0.259\n",
            "[502,     2] loss: 0.263\n",
            "[503,     2] loss: 0.260\n",
            "[504,     2] loss: 0.266\n",
            "[505,     2] loss: 0.259\n",
            "[506,     2] loss: 0.266\n",
            "[507,     2] loss: 0.259\n",
            "[508,     2] loss: 0.265\n",
            "[509,     2] loss: 0.262\n",
            "[510,     2] loss: 0.257\n",
            "[511,     2] loss: 0.259\n",
            "[512,     2] loss: 0.262\n",
            "[513,     2] loss: 0.256\n",
            "[514,     2] loss: 0.261\n",
            "[515,     2] loss: 0.261\n",
            "[516,     2] loss: 0.258\n",
            "[517,     2] loss: 0.263\n",
            "[518,     2] loss: 0.262\n",
            "[519,     2] loss: 0.257\n",
            "[520,     2] loss: 0.258\n",
            "[521,     2] loss: 0.258\n",
            "[522,     2] loss: 0.263\n",
            "[523,     2] loss: 0.260\n",
            "[524,     2] loss: 0.257\n",
            "[525,     2] loss: 0.262\n",
            "[526,     2] loss: 0.258\n",
            "[527,     2] loss: 0.257\n",
            "[528,     2] loss: 0.260\n",
            "[529,     2] loss: 0.261\n",
            "[530,     2] loss: 0.258\n",
            "[531,     2] loss: 0.273\n",
            "[532,     2] loss: 0.263\n",
            "[533,     2] loss: 0.261\n",
            "[534,     2] loss: 0.259\n",
            "[535,     2] loss: 0.263\n",
            "[536,     2] loss: 0.259\n",
            "[537,     2] loss: 0.256\n",
            "[538,     2] loss: 0.261\n",
            "[539,     2] loss: 0.260\n",
            "[540,     2] loss: 0.257\n",
            "[541,     2] loss: 0.258\n",
            "[542,     2] loss: 0.259\n",
            "[543,     2] loss: 0.257\n",
            "[544,     2] loss: 0.270\n",
            "[545,     2] loss: 0.260\n",
            "[546,     2] loss: 0.257\n",
            "[547,     2] loss: 0.259\n",
            "[548,     2] loss: 0.262\n",
            "[549,     2] loss: 0.259\n",
            "[550,     2] loss: 0.258\n",
            "[551,     2] loss: 0.259\n",
            "[552,     2] loss: 0.257\n",
            "[553,     2] loss: 0.256\n",
            "[554,     2] loss: 0.257\n",
            "[555,     2] loss: 0.257\n",
            "[556,     2] loss: 0.257\n",
            "[557,     2] loss: 0.257\n",
            "[558,     2] loss: 0.263\n",
            "[559,     2] loss: 0.258\n",
            "[560,     2] loss: 0.257\n",
            "[561,     2] loss: 0.258\n",
            "[562,     2] loss: 0.257\n",
            "[563,     2] loss: 0.259\n",
            "[564,     2] loss: 0.259\n",
            "[565,     2] loss: 0.272\n",
            "[566,     2] loss: 0.267\n",
            "[567,     2] loss: 0.257\n",
            "[568,     2] loss: 0.257\n",
            "[569,     2] loss: 0.257\n",
            "[570,     2] loss: 0.261\n",
            "[571,     2] loss: 0.258\n",
            "[572,     2] loss: 0.258\n",
            "[573,     2] loss: 0.256\n",
            "[574,     2] loss: 0.258\n",
            "[575,     2] loss: 0.257\n",
            "[576,     2] loss: 0.256\n",
            "[577,     2] loss: 0.258\n",
            "[578,     2] loss: 0.262\n",
            "[579,     2] loss: 0.259\n",
            "[580,     2] loss: 0.256\n",
            "[581,     2] loss: 0.256\n",
            "[582,     2] loss: 0.256\n",
            "[583,     2] loss: 0.259\n",
            "[584,     2] loss: 0.274\n",
            "[585,     2] loss: 0.264\n",
            "[586,     2] loss: 0.265\n",
            "[587,     2] loss: 0.259\n",
            "[588,     2] loss: 0.256\n",
            "[589,     2] loss: 0.261\n",
            "[590,     2] loss: 0.257\n",
            "[591,     2] loss: 0.259\n",
            "[592,     2] loss: 0.263\n",
            "[593,     2] loss: 0.263\n",
            "[594,     2] loss: 0.259\n",
            "[595,     2] loss: 0.259\n",
            "[596,     2] loss: 0.255\n",
            "[597,     2] loss: 0.258\n",
            "[598,     2] loss: 0.259\n",
            "[599,     2] loss: 0.255\n",
            "[600,     2] loss: 0.257\n",
            "[601,     2] loss: 0.261\n",
            "[602,     2] loss: 0.256\n",
            "[603,     2] loss: 0.258\n",
            "[604,     2] loss: 0.257\n",
            "[605,     2] loss: 0.257\n",
            "[606,     2] loss: 0.262\n",
            "[607,     2] loss: 0.259\n",
            "[608,     2] loss: 0.255\n",
            "[609,     2] loss: 0.267\n",
            "[610,     2] loss: 0.260\n",
            "[611,     2] loss: 0.256\n",
            "[612,     2] loss: 0.262\n",
            "[613,     2] loss: 0.263\n",
            "[614,     2] loss: 0.260\n",
            "[615,     2] loss: 0.258\n",
            "[616,     2] loss: 0.259\n",
            "[617,     2] loss: 0.255\n",
            "[618,     2] loss: 0.260\n",
            "[619,     2] loss: 0.263\n",
            "[620,     2] loss: 0.257\n",
            "[621,     2] loss: 0.257\n",
            "[622,     2] loss: 0.259\n",
            "[623,     2] loss: 0.267\n",
            "[624,     2] loss: 0.260\n",
            "[625,     2] loss: 0.266\n",
            "[626,     2] loss: 0.259\n",
            "[627,     2] loss: 0.255\n",
            "[628,     2] loss: 0.257\n",
            "[629,     2] loss: 0.256\n",
            "[630,     2] loss: 0.256\n",
            "[631,     2] loss: 0.260\n",
            "[632,     2] loss: 0.257\n",
            "[633,     2] loss: 0.256\n",
            "[634,     2] loss: 0.259\n",
            "[635,     2] loss: 0.261\n",
            "[636,     2] loss: 0.258\n",
            "[637,     2] loss: 0.256\n",
            "[638,     2] loss: 0.256\n",
            "[639,     2] loss: 0.256\n",
            "[640,     2] loss: 0.256\n",
            "[641,     2] loss: 0.256\n",
            "[642,     2] loss: 0.263\n",
            "[643,     2] loss: 0.256\n",
            "[644,     2] loss: 0.260\n",
            "[645,     2] loss: 0.259\n",
            "[646,     2] loss: 0.259\n",
            "[647,     2] loss: 0.259\n",
            "[648,     2] loss: 0.256\n",
            "[649,     2] loss: 0.265\n",
            "[650,     2] loss: 0.257\n",
            "[651,     2] loss: 0.256\n",
            "[652,     2] loss: 0.265\n",
            "[653,     2] loss: 0.256\n",
            "[654,     2] loss: 0.258\n",
            "[655,     2] loss: 0.267\n",
            "[656,     2] loss: 0.260\n",
            "[657,     2] loss: 0.260\n",
            "[658,     2] loss: 0.259\n",
            "[659,     2] loss: 0.255\n",
            "[660,     2] loss: 0.258\n",
            "[661,     2] loss: 0.261\n",
            "[662,     2] loss: 0.258\n",
            "[663,     2] loss: 0.259\n",
            "[664,     2] loss: 0.259\n",
            "[665,     2] loss: 0.256\n",
            "[666,     2] loss: 0.257\n",
            "[667,     2] loss: 0.260\n",
            "[668,     2] loss: 0.261\n",
            "[669,     2] loss: 0.260\n",
            "[670,     2] loss: 0.258\n",
            "[671,     2] loss: 0.260\n",
            "[672,     2] loss: 0.259\n",
            "[673,     2] loss: 0.256\n",
            "[674,     2] loss: 0.261\n",
            "[675,     2] loss: 0.259\n",
            "[676,     2] loss: 0.255\n",
            "[677,     2] loss: 0.263\n",
            "[678,     2] loss: 0.257\n",
            "[679,     2] loss: 0.258\n",
            "[680,     2] loss: 0.267\n",
            "[681,     2] loss: 0.255\n",
            "[682,     2] loss: 0.273\n",
            "[683,     2] loss: 0.263\n",
            "[684,     2] loss: 0.258\n",
            "[685,     2] loss: 0.258\n",
            "[686,     2] loss: 0.259\n",
            "[687,     2] loss: 0.259\n",
            "[688,     2] loss: 0.262\n",
            "[689,     2] loss: 0.258\n",
            "[690,     2] loss: 0.258\n",
            "[691,     2] loss: 0.255\n",
            "[692,     2] loss: 0.255\n",
            "[693,     2] loss: 0.255\n",
            "[694,     2] loss: 0.258\n",
            "[695,     2] loss: 0.258\n",
            "[696,     2] loss: 0.256\n",
            "[697,     2] loss: 0.256\n",
            "[698,     2] loss: 0.255\n",
            "[699,     2] loss: 0.255\n",
            "[700,     2] loss: 0.259\n",
            "[701,     2] loss: 0.256\n",
            "[702,     2] loss: 0.264\n",
            "[703,     2] loss: 0.257\n",
            "[704,     2] loss: 0.258\n",
            "[705,     2] loss: 0.258\n",
            "[706,     2] loss: 0.257\n",
            "[707,     2] loss: 0.257\n",
            "[708,     2] loss: 0.257\n",
            "[709,     2] loss: 0.257\n",
            "[710,     2] loss: 0.256\n",
            "[711,     2] loss: 0.256\n",
            "[712,     2] loss: 0.258\n",
            "[713,     2] loss: 0.257\n",
            "[714,     2] loss: 0.258\n",
            "[715,     2] loss: 0.255\n",
            "[716,     2] loss: 0.255\n",
            "[717,     2] loss: 0.256\n",
            "[718,     2] loss: 0.259\n",
            "[719,     2] loss: 0.255\n",
            "[720,     2] loss: 0.258\n",
            "[721,     2] loss: 0.259\n",
            "[722,     2] loss: 0.265\n",
            "[723,     2] loss: 0.256\n",
            "[724,     2] loss: 0.255\n",
            "[725,     2] loss: 0.255\n",
            "[726,     2] loss: 0.255\n",
            "[727,     2] loss: 0.255\n",
            "[728,     2] loss: 0.255\n",
            "[729,     2] loss: 0.255\n",
            "[730,     2] loss: 0.256\n",
            "[731,     2] loss: 0.255\n",
            "[732,     2] loss: 0.256\n",
            "[733,     2] loss: 0.265\n",
            "[734,     2] loss: 0.257\n",
            "[735,     2] loss: 0.259\n",
            "[736,     2] loss: 0.256\n",
            "[737,     2] loss: 0.262\n",
            "[738,     2] loss: 0.258\n",
            "[739,     2] loss: 0.254\n",
            "[740,     2] loss: 0.255\n",
            "[741,     2] loss: 0.260\n",
            "[742,     2] loss: 0.258\n",
            "[743,     2] loss: 0.255\n",
            "[744,     2] loss: 0.255\n",
            "[745,     2] loss: 0.258\n",
            "[746,     2] loss: 0.257\n",
            "[747,     2] loss: 0.256\n",
            "[748,     2] loss: 0.255\n",
            "[749,     2] loss: 0.255\n",
            "[750,     2] loss: 0.261\n",
            "[751,     2] loss: 0.259\n",
            "[752,     2] loss: 0.255\n",
            "[753,     2] loss: 0.259\n",
            "[754,     2] loss: 0.258\n",
            "[755,     2] loss: 0.257\n",
            "[756,     2] loss: 0.261\n",
            "[757,     2] loss: 0.255\n",
            "[758,     2] loss: 0.257\n",
            "[759,     2] loss: 0.257\n",
            "[760,     2] loss: 0.263\n",
            "[761,     2] loss: 0.259\n",
            "[762,     2] loss: 0.258\n",
            "[763,     2] loss: 0.257\n",
            "[764,     2] loss: 0.260\n",
            "[765,     2] loss: 0.255\n",
            "[766,     2] loss: 0.257\n",
            "[767,     2] loss: 0.258\n",
            "[768,     2] loss: 0.256\n",
            "[769,     2] loss: 0.257\n",
            "[770,     2] loss: 0.256\n",
            "[771,     2] loss: 0.255\n",
            "[772,     2] loss: 0.257\n",
            "[773,     2] loss: 0.260\n",
            "[774,     2] loss: 0.257\n",
            "[775,     2] loss: 0.255\n",
            "[776,     2] loss: 0.259\n",
            "[777,     2] loss: 0.261\n",
            "[778,     2] loss: 0.259\n",
            "[779,     2] loss: 0.262\n",
            "[780,     2] loss: 0.259\n",
            "[781,     2] loss: 0.259\n",
            "[782,     2] loss: 0.259\n",
            "[783,     2] loss: 0.256\n",
            "[784,     2] loss: 0.257\n",
            "[785,     2] loss: 0.260\n",
            "[786,     2] loss: 0.256\n",
            "[787,     2] loss: 0.256\n",
            "[788,     2] loss: 0.255\n",
            "[789,     2] loss: 0.263\n",
            "[790,     2] loss: 0.261\n",
            "[791,     2] loss: 0.258\n",
            "[792,     2] loss: 0.259\n",
            "[793,     2] loss: 0.255\n",
            "[794,     2] loss: 0.257\n",
            "[795,     2] loss: 0.258\n",
            "[796,     2] loss: 0.254\n",
            "[797,     2] loss: 0.256\n",
            "[798,     2] loss: 0.256\n",
            "[799,     2] loss: 0.268\n",
            "[800,     2] loss: 0.259\n",
            "[801,     2] loss: 0.254\n",
            "[802,     2] loss: 0.255\n",
            "[803,     2] loss: 0.255\n",
            "[804,     2] loss: 0.259\n",
            "[805,     2] loss: 0.259\n",
            "[806,     2] loss: 0.255\n",
            "[807,     2] loss: 0.255\n",
            "[808,     2] loss: 0.255\n",
            "[809,     2] loss: 0.256\n",
            "[810,     2] loss: 0.255\n",
            "[811,     2] loss: 0.259\n",
            "[812,     2] loss: 0.255\n",
            "[813,     2] loss: 0.256\n",
            "[814,     2] loss: 0.255\n",
            "[815,     2] loss: 0.255\n",
            "[816,     2] loss: 0.255\n",
            "[817,     2] loss: 0.258\n",
            "[818,     2] loss: 0.255\n",
            "[819,     2] loss: 0.258\n",
            "[820,     2] loss: 0.255\n",
            "[821,     2] loss: 0.255\n",
            "[822,     2] loss: 0.258\n",
            "[823,     2] loss: 0.256\n",
            "[824,     2] loss: 0.257\n",
            "[825,     2] loss: 0.257\n",
            "[826,     2] loss: 0.259\n",
            "[827,     2] loss: 0.260\n",
            "[828,     2] loss: 0.256\n",
            "[829,     2] loss: 0.254\n",
            "[830,     2] loss: 0.255\n",
            "[831,     2] loss: 0.256\n",
            "[832,     2] loss: 0.254\n",
            "[833,     2] loss: 0.260\n",
            "[834,     2] loss: 0.256\n",
            "[835,     2] loss: 0.255\n",
            "[836,     2] loss: 0.257\n",
            "[837,     2] loss: 0.257\n",
            "[838,     2] loss: 0.256\n",
            "[839,     2] loss: 0.255\n",
            "[840,     2] loss: 0.255\n",
            "[841,     2] loss: 0.255\n",
            "[842,     2] loss: 0.255\n",
            "[843,     2] loss: 0.256\n",
            "[844,     2] loss: 0.255\n",
            "[845,     2] loss: 0.256\n",
            "[846,     2] loss: 0.258\n",
            "[847,     2] loss: 0.254\n",
            "[848,     2] loss: 0.255\n",
            "[849,     2] loss: 0.255\n",
            "[850,     2] loss: 0.255\n",
            "[851,     2] loss: 0.261\n",
            "[852,     2] loss: 0.256\n",
            "[853,     2] loss: 0.258\n",
            "[854,     2] loss: 0.262\n",
            "[855,     2] loss: 0.255\n",
            "[856,     2] loss: 0.262\n",
            "[857,     2] loss: 0.261\n",
            "[858,     2] loss: 0.263\n",
            "[859,     2] loss: 0.258\n",
            "[860,     2] loss: 0.254\n",
            "[861,     2] loss: 0.255\n",
            "[862,     2] loss: 0.257\n",
            "[863,     2] loss: 0.256\n",
            "[864,     2] loss: 0.254\n",
            "[865,     2] loss: 0.256\n",
            "[866,     2] loss: 0.261\n",
            "[867,     2] loss: 0.254\n",
            "[868,     2] loss: 0.256\n",
            "[869,     2] loss: 0.262\n",
            "[870,     2] loss: 0.255\n",
            "[871,     2] loss: 0.259\n",
            "[872,     2] loss: 0.254\n",
            "[873,     2] loss: 0.254\n",
            "[874,     2] loss: 0.255\n",
            "[875,     2] loss: 0.254\n",
            "[876,     2] loss: 0.262\n",
            "[877,     2] loss: 0.255\n",
            "[878,     2] loss: 0.256\n",
            "[879,     2] loss: 0.254\n",
            "[880,     2] loss: 0.258\n",
            "[881,     2] loss: 0.255\n",
            "[882,     2] loss: 0.270\n",
            "[883,     2] loss: 0.255\n",
            "[884,     2] loss: 0.259\n",
            "[885,     2] loss: 0.256\n",
            "[886,     2] loss: 0.257\n",
            "[887,     2] loss: 0.254\n",
            "[888,     2] loss: 0.259\n",
            "[889,     2] loss: 0.258\n",
            "[890,     2] loss: 0.254\n",
            "[891,     2] loss: 0.260\n",
            "[892,     2] loss: 0.256\n",
            "[893,     2] loss: 0.258\n",
            "[894,     2] loss: 0.257\n",
            "[895,     2] loss: 0.256\n",
            "[896,     2] loss: 0.255\n",
            "[897,     2] loss: 0.259\n",
            "[898,     2] loss: 0.258\n",
            "[899,     2] loss: 0.254\n",
            "[900,     2] loss: 0.262\n",
            "[901,     2] loss: 0.256\n",
            "[902,     2] loss: 0.258\n",
            "[903,     2] loss: 0.264\n",
            "[904,     2] loss: 0.255\n",
            "[905,     2] loss: 0.255\n",
            "[906,     2] loss: 0.255\n",
            "[907,     2] loss: 0.254\n",
            "[908,     2] loss: 0.261\n",
            "[909,     2] loss: 0.254\n",
            "[910,     2] loss: 0.256\n",
            "[911,     2] loss: 0.257\n",
            "[912,     2] loss: 0.257\n",
            "[913,     2] loss: 0.254\n",
            "[914,     2] loss: 0.255\n",
            "[915,     2] loss: 0.259\n",
            "[916,     2] loss: 0.254\n",
            "[917,     2] loss: 0.255\n",
            "[918,     2] loss: 0.256\n",
            "[919,     2] loss: 0.266\n",
            "[920,     2] loss: 0.258\n",
            "[921,     2] loss: 0.256\n",
            "[922,     2] loss: 0.262\n",
            "[923,     2] loss: 0.256\n",
            "[924,     2] loss: 0.254\n",
            "[925,     2] loss: 0.263\n",
            "[926,     2] loss: 0.258\n",
            "[927,     2] loss: 0.253\n",
            "[928,     2] loss: 0.256\n",
            "[929,     2] loss: 0.256\n",
            "[930,     2] loss: 0.254\n",
            "[931,     2] loss: 0.262\n",
            "[932,     2] loss: 0.254\n",
            "[933,     2] loss: 0.260\n",
            "[934,     2] loss: 0.259\n",
            "[935,     2] loss: 0.254\n",
            "[936,     2] loss: 0.256\n",
            "[937,     2] loss: 0.257\n",
            "[938,     2] loss: 0.254\n",
            "[939,     2] loss: 0.255\n",
            "[940,     2] loss: 0.255\n",
            "[941,     2] loss: 0.255\n",
            "[942,     2] loss: 0.254\n",
            "[943,     2] loss: 0.254\n",
            "[944,     2] loss: 0.255\n",
            "[945,     2] loss: 0.255\n",
            "[946,     2] loss: 0.254\n",
            "[947,     2] loss: 0.261\n",
            "[948,     2] loss: 0.259\n",
            "[949,     2] loss: 0.257\n",
            "[950,     2] loss: 0.259\n",
            "[951,     2] loss: 0.267\n",
            "[952,     2] loss: 0.259\n",
            "[953,     2] loss: 0.254\n",
            "[954,     2] loss: 0.255\n",
            "[955,     2] loss: 0.254\n",
            "[956,     2] loss: 0.254\n",
            "[957,     2] loss: 0.254\n",
            "[958,     2] loss: 0.254\n",
            "[959,     2] loss: 0.254\n",
            "[960,     2] loss: 0.255\n",
            "[961,     2] loss: 0.257\n",
            "[962,     2] loss: 0.255\n",
            "[963,     2] loss: 0.258\n",
            "[964,     2] loss: 0.255\n",
            "[965,     2] loss: 0.254\n",
            "[966,     2] loss: 0.255\n",
            "[967,     2] loss: 0.256\n",
            "[968,     2] loss: 0.259\n",
            "[969,     2] loss: 0.258\n",
            "[970,     2] loss: 0.259\n",
            "[971,     2] loss: 0.257\n",
            "[972,     2] loss: 0.257\n",
            "[973,     2] loss: 0.256\n",
            "[974,     2] loss: 0.261\n",
            "[975,     2] loss: 0.255\n",
            "[976,     2] loss: 0.254\n",
            "[977,     2] loss: 0.257\n",
            "[978,     2] loss: 0.255\n",
            "[979,     2] loss: 0.256\n",
            "[980,     2] loss: 0.256\n",
            "[981,     2] loss: 0.258\n",
            "[982,     2] loss: 0.260\n",
            "[983,     2] loss: 0.254\n",
            "[984,     2] loss: 0.254\n",
            "[985,     2] loss: 0.255\n",
            "[986,     2] loss: 0.255\n",
            "[987,     2] loss: 0.255\n",
            "[988,     2] loss: 0.254\n",
            "[989,     2] loss: 0.254\n",
            "[990,     2] loss: 0.254\n",
            "[991,     2] loss: 0.267\n",
            "[992,     2] loss: 0.254\n",
            "[993,     2] loss: 0.256\n",
            "[994,     2] loss: 0.258\n",
            "[995,     2] loss: 0.263\n",
            "[996,     2] loss: 0.259\n",
            "[997,     2] loss: 0.254\n",
            "[998,     2] loss: 0.255\n",
            "[999,     2] loss: 0.267\n",
            "[1000,     2] loss: 0.257\n",
            "[1001,     2] loss: 0.252\n",
            "[1002,     2] loss: 0.257\n",
            "[1003,     2] loss: 0.257\n",
            "[1004,     2] loss: 0.255\n",
            "[1005,     2] loss: 0.255\n",
            "[1006,     2] loss: 0.255\n",
            "[1007,     2] loss: 0.255\n",
            "[1008,     2] loss: 0.253\n",
            "[1009,     2] loss: 0.254\n",
            "[1010,     2] loss: 0.254\n",
            "[1011,     2] loss: 0.255\n",
            "[1012,     2] loss: 0.254\n",
            "[1013,     2] loss: 0.256\n",
            "[1014,     2] loss: 0.254\n",
            "[1015,     2] loss: 0.254\n",
            "[1016,     2] loss: 0.262\n",
            "[1017,     2] loss: 0.260\n",
            "[1018,     2] loss: 0.258\n",
            "[1019,     2] loss: 0.255\n",
            "[1020,     2] loss: 0.254\n",
            "[1021,     2] loss: 0.270\n",
            "[1022,     2] loss: 0.255\n",
            "[1023,     2] loss: 0.254\n",
            "[1024,     2] loss: 0.260\n",
            "[1025,     2] loss: 0.255\n",
            "[1026,     2] loss: 0.257\n",
            "[1027,     2] loss: 0.260\n",
            "[1028,     2] loss: 0.264\n",
            "[1029,     2] loss: 0.256\n",
            "[1030,     2] loss: 0.260\n",
            "[1031,     2] loss: 0.255\n",
            "[1032,     2] loss: 0.254\n",
            "[1033,     2] loss: 0.254\n",
            "[1034,     2] loss: 0.264\n",
            "[1035,     2] loss: 0.253\n",
            "[1036,     2] loss: 0.256\n",
            "[1037,     2] loss: 0.257\n",
            "[1038,     2] loss: 0.254\n",
            "[1039,     2] loss: 0.260\n",
            "[1040,     2] loss: 0.258\n",
            "[1041,     2] loss: 0.255\n",
            "[1042,     2] loss: 0.257\n",
            "[1043,     2] loss: 0.255\n",
            "[1044,     2] loss: 0.253\n",
            "[1045,     2] loss: 0.259\n",
            "[1046,     2] loss: 0.263\n",
            "[1047,     2] loss: 0.256\n",
            "[1048,     2] loss: 0.254\n",
            "[1049,     2] loss: 0.254\n",
            "[1050,     2] loss: 0.255\n",
            "[1051,     2] loss: 0.255\n",
            "[1052,     2] loss: 0.256\n",
            "[1053,     2] loss: 0.255\n",
            "[1054,     2] loss: 0.254\n",
            "[1055,     2] loss: 0.255\n",
            "[1056,     2] loss: 0.254\n",
            "[1057,     2] loss: 0.255\n",
            "[1058,     2] loss: 0.256\n",
            "[1059,     2] loss: 0.253\n",
            "[1060,     2] loss: 0.254\n",
            "[1061,     2] loss: 0.254\n",
            "[1062,     2] loss: 0.254\n",
            "[1063,     2] loss: 0.254\n",
            "[1064,     2] loss: 0.269\n",
            "[1065,     2] loss: 0.258\n",
            "[1066,     2] loss: 0.262\n",
            "[1067,     2] loss: 0.264\n",
            "[1068,     2] loss: 0.256\n",
            "[1069,     2] loss: 0.260\n",
            "[1070,     2] loss: 0.259\n",
            "[1071,     2] loss: 0.254\n",
            "[1072,     2] loss: 0.256\n",
            "[1073,     2] loss: 0.253\n",
            "[1074,     2] loss: 0.260\n",
            "[1075,     2] loss: 0.254\n",
            "[1076,     2] loss: 0.253\n",
            "[1077,     2] loss: 0.257\n",
            "[1078,     2] loss: 0.254\n",
            "[1079,     2] loss: 0.254\n",
            "[1080,     2] loss: 0.255\n",
            "[1081,     2] loss: 0.253\n",
            "[1082,     2] loss: 0.255\n",
            "[1083,     2] loss: 0.261\n",
            "[1084,     2] loss: 0.265\n",
            "[1085,     2] loss: 0.254\n",
            "[1086,     2] loss: 0.262\n",
            "[1087,     2] loss: 0.255\n",
            "[1088,     2] loss: 0.259\n",
            "[1089,     2] loss: 0.280\n",
            "[1090,     2] loss: 0.255\n",
            "[1091,     2] loss: 0.256\n",
            "[1092,     2] loss: 0.254\n",
            "[1093,     2] loss: 0.253\n",
            "[1094,     2] loss: 0.258\n",
            "[1095,     2] loss: 0.254\n",
            "[1096,     2] loss: 0.253\n",
            "[1097,     2] loss: 0.255\n",
            "[1098,     2] loss: 0.255\n",
            "[1099,     2] loss: 0.253\n",
            "[1100,     2] loss: 0.261\n",
            "[1101,     2] loss: 0.255\n",
            "[1102,     2] loss: 0.253\n",
            "[1103,     2] loss: 0.258\n",
            "[1104,     2] loss: 0.254\n",
            "[1105,     2] loss: 0.253\n",
            "[1106,     2] loss: 0.258\n",
            "[1107,     2] loss: 0.257\n",
            "[1108,     2] loss: 0.254\n",
            "[1109,     2] loss: 0.257\n",
            "[1110,     2] loss: 0.256\n",
            "[1111,     2] loss: 0.254\n",
            "[1112,     2] loss: 0.254\n",
            "[1113,     2] loss: 0.254\n",
            "[1114,     2] loss: 0.253\n",
            "[1115,     2] loss: 0.255\n",
            "[1116,     2] loss: 0.255\n",
            "[1117,     2] loss: 0.254\n",
            "[1118,     2] loss: 0.259\n",
            "[1119,     2] loss: 0.254\n",
            "[1120,     2] loss: 0.253\n",
            "[1121,     2] loss: 0.257\n",
            "[1122,     2] loss: 0.257\n",
            "[1123,     2] loss: 0.255\n",
            "[1124,     2] loss: 0.254\n",
            "[1125,     2] loss: 0.253\n",
            "[1126,     2] loss: 0.254\n",
            "[1127,     2] loss: 0.254\n",
            "[1128,     2] loss: 0.254\n",
            "[1129,     2] loss: 0.254\n",
            "[1130,     2] loss: 0.254\n",
            "[1131,     2] loss: 0.257\n",
            "[1132,     2] loss: 0.254\n",
            "[1133,     2] loss: 0.254\n",
            "[1134,     2] loss: 0.254\n",
            "[1135,     2] loss: 0.255\n",
            "[1136,     2] loss: 0.254\n",
            "[1137,     2] loss: 0.255\n",
            "[1138,     2] loss: 0.254\n",
            "[1139,     2] loss: 0.255\n",
            "[1140,     2] loss: 0.254\n",
            "[1141,     2] loss: 0.253\n",
            "[1142,     2] loss: 0.256\n",
            "[1143,     2] loss: 0.257\n",
            "[1144,     2] loss: 0.260\n",
            "[1145,     2] loss: 0.258\n",
            "[1146,     2] loss: 0.254\n",
            "[1147,     2] loss: 0.254\n",
            "[1148,     2] loss: 0.255\n",
            "[1149,     2] loss: 0.253\n",
            "[1150,     2] loss: 0.271\n",
            "[1151,     2] loss: 0.257\n",
            "[1152,     2] loss: 0.253\n",
            "[1153,     2] loss: 0.260\n",
            "[1154,     2] loss: 0.261\n",
            "[1155,     2] loss: 0.259\n",
            "[1156,     2] loss: 0.257\n",
            "[1157,     2] loss: 0.252\n",
            "[1158,     2] loss: 0.255\n",
            "[1159,     2] loss: 0.256\n",
            "[1160,     2] loss: 0.253\n",
            "[1161,     2] loss: 0.255\n",
            "[1162,     2] loss: 0.254\n",
            "[1163,     2] loss: 0.257\n",
            "[1164,     2] loss: 0.254\n",
            "[1165,     2] loss: 0.256\n",
            "[1166,     2] loss: 0.258\n",
            "[1167,     2] loss: 0.254\n",
            "[1168,     2] loss: 0.254\n",
            "[1169,     2] loss: 0.258\n",
            "[1170,     2] loss: 0.257\n",
            "[1171,     2] loss: 0.256\n",
            "[1172,     2] loss: 0.256\n",
            "[1173,     2] loss: 0.263\n",
            "[1174,     2] loss: 0.253\n",
            "[1175,     2] loss: 0.253\n",
            "[1176,     2] loss: 0.256\n",
            "[1177,     2] loss: 0.265\n",
            "[1178,     2] loss: 0.261\n",
            "[1179,     2] loss: 0.253\n",
            "[1180,     2] loss: 0.254\n",
            "[1181,     2] loss: 0.253\n",
            "[1182,     2] loss: 0.259\n",
            "[1183,     2] loss: 0.256\n",
            "[1184,     2] loss: 0.254\n",
            "[1185,     2] loss: 0.259\n",
            "[1186,     2] loss: 0.257\n",
            "[1187,     2] loss: 0.258\n",
            "[1188,     2] loss: 0.257\n",
            "[1189,     2] loss: 0.259\n",
            "[1190,     2] loss: 0.255\n",
            "[1191,     2] loss: 0.253\n",
            "[1192,     2] loss: 0.257\n",
            "[1193,     2] loss: 0.257\n",
            "[1194,     2] loss: 0.252\n",
            "[1195,     2] loss: 0.260\n",
            "[1196,     2] loss: 0.258\n",
            "[1197,     2] loss: 0.272\n",
            "[1198,     2] loss: 0.268\n",
            "[1199,     2] loss: 0.265\n",
            "[1200,     2] loss: 0.261\n",
            "[1201,     2] loss: 0.259\n",
            "[1202,     2] loss: 0.253\n",
            "[1203,     2] loss: 0.255\n",
            "[1204,     2] loss: 0.258\n",
            "[1205,     2] loss: 0.253\n",
            "[1206,     2] loss: 0.256\n",
            "[1207,     2] loss: 0.256\n",
            "[1208,     2] loss: 0.254\n",
            "[1209,     2] loss: 0.253\n",
            "[1210,     2] loss: 0.255\n",
            "[1211,     2] loss: 0.255\n",
            "[1212,     2] loss: 0.253\n",
            "[1213,     2] loss: 0.254\n",
            "[1214,     2] loss: 0.257\n",
            "[1215,     2] loss: 0.255\n",
            "[1216,     2] loss: 0.267\n",
            "[1217,     2] loss: 0.256\n",
            "[1218,     2] loss: 0.255\n",
            "[1219,     2] loss: 0.259\n",
            "[1220,     2] loss: 0.256\n",
            "[1221,     2] loss: 0.257\n",
            "[1222,     2] loss: 0.267\n",
            "[1223,     2] loss: 0.254\n",
            "[1224,     2] loss: 0.255\n",
            "[1225,     2] loss: 0.255\n",
            "[1226,     2] loss: 0.255\n",
            "[1227,     2] loss: 0.253\n",
            "[1228,     2] loss: 0.255\n",
            "[1229,     2] loss: 0.254\n",
            "[1230,     2] loss: 0.254\n",
            "[1231,     2] loss: 0.253\n",
            "[1232,     2] loss: 0.254\n",
            "[1233,     2] loss: 0.254\n",
            "[1234,     2] loss: 0.254\n",
            "[1235,     2] loss: 0.255\n",
            "[1236,     2] loss: 0.254\n",
            "[1237,     2] loss: 0.253\n",
            "[1238,     2] loss: 0.254\n",
            "[1239,     2] loss: 0.254\n",
            "[1240,     2] loss: 0.261\n",
            "[1241,     2] loss: 0.254\n",
            "[1242,     2] loss: 0.259\n",
            "[1243,     2] loss: 0.257\n",
            "[1244,     2] loss: 0.254\n",
            "[1245,     2] loss: 0.255\n",
            "[1246,     2] loss: 0.254\n",
            "[1247,     2] loss: 0.253\n",
            "[1248,     2] loss: 0.254\n",
            "[1249,     2] loss: 0.254\n",
            "[1250,     2] loss: 0.253\n",
            "[1251,     2] loss: 0.253\n",
            "[1252,     2] loss: 0.257\n",
            "[1253,     2] loss: 0.255\n",
            "[1254,     2] loss: 0.254\n",
            "[1255,     2] loss: 0.256\n",
            "[1256,     2] loss: 0.253\n",
            "[1257,     2] loss: 0.264\n",
            "[1258,     2] loss: 0.254\n",
            "[1259,     2] loss: 0.252\n",
            "[1260,     2] loss: 0.257\n",
            "[1261,     2] loss: 0.257\n",
            "[1262,     2] loss: 0.258\n",
            "[1263,     2] loss: 0.255\n",
            "[1264,     2] loss: 0.256\n",
            "[1265,     2] loss: 0.256\n",
            "[1266,     2] loss: 0.255\n",
            "[1267,     2] loss: 0.258\n",
            "[1268,     2] loss: 0.254\n",
            "[1269,     2] loss: 0.253\n",
            "[1270,     2] loss: 0.256\n",
            "[1271,     2] loss: 0.254\n",
            "[1272,     2] loss: 0.257\n",
            "[1273,     2] loss: 0.259\n",
            "[1274,     2] loss: 0.253\n",
            "[1275,     2] loss: 0.259\n",
            "[1276,     2] loss: 0.253\n",
            "[1277,     2] loss: 0.256\n",
            "[1278,     2] loss: 0.254\n",
            "[1279,     2] loss: 0.256\n",
            "[1280,     2] loss: 0.255\n",
            "[1281,     2] loss: 0.258\n",
            "[1282,     2] loss: 0.255\n",
            "[1283,     2] loss: 0.253\n",
            "[1284,     2] loss: 0.254\n",
            "[1285,     2] loss: 0.253\n",
            "[1286,     2] loss: 0.253\n",
            "[1287,     2] loss: 0.253\n",
            "[1288,     2] loss: 0.257\n",
            "[1289,     2] loss: 0.253\n",
            "[1290,     2] loss: 0.253\n",
            "[1291,     2] loss: 0.258\n",
            "[1292,     2] loss: 0.260\n",
            "[1293,     2] loss: 0.255\n",
            "[1294,     2] loss: 0.255\n",
            "[1295,     2] loss: 0.267\n",
            "[1296,     2] loss: 0.259\n",
            "[1297,     2] loss: 0.255\n",
            "[1298,     2] loss: 0.256\n",
            "[1299,     2] loss: 0.264\n",
            "[1300,     2] loss: 0.258\n",
            "[1301,     2] loss: 0.257\n",
            "[1302,     2] loss: 0.255\n",
            "[1303,     2] loss: 0.254\n",
            "[1304,     2] loss: 0.261\n",
            "[1305,     2] loss: 0.255\n",
            "[1306,     2] loss: 0.257\n",
            "[1307,     2] loss: 0.255\n",
            "[1308,     2] loss: 0.254\n",
            "[1309,     2] loss: 0.257\n",
            "[1310,     2] loss: 0.253\n",
            "[1311,     2] loss: 0.256\n",
            "[1312,     2] loss: 0.259\n",
            "[1313,     2] loss: 0.255\n",
            "[1314,     2] loss: 0.252\n",
            "[1315,     2] loss: 0.262\n",
            "[1316,     2] loss: 0.255\n",
            "[1317,     2] loss: 0.257\n",
            "[1318,     2] loss: 0.259\n",
            "[1319,     2] loss: 0.254\n",
            "[1320,     2] loss: 0.253\n",
            "[1321,     2] loss: 0.257\n",
            "[1322,     2] loss: 0.257\n",
            "[1323,     2] loss: 0.254\n",
            "[1324,     2] loss: 0.256\n",
            "[1325,     2] loss: 0.254\n",
            "[1326,     2] loss: 0.254\n",
            "[1327,     2] loss: 0.257\n",
            "[1328,     2] loss: 0.253\n",
            "[1329,     2] loss: 0.254\n",
            "[1330,     2] loss: 0.254\n",
            "[1331,     2] loss: 0.259\n",
            "[1332,     2] loss: 0.264\n",
            "[1333,     2] loss: 0.256\n",
            "[1334,     2] loss: 0.255\n",
            "[1335,     2] loss: 0.255\n",
            "[1336,     2] loss: 0.253\n",
            "[1337,     2] loss: 0.260\n",
            "[1338,     2] loss: 0.255\n",
            "[1339,     2] loss: 0.252\n",
            "[1340,     2] loss: 0.257\n",
            "[1341,     2] loss: 0.255\n",
            "[1342,     2] loss: 0.257\n",
            "[1343,     2] loss: 0.257\n",
            "[1344,     2] loss: 0.253\n",
            "[1345,     2] loss: 0.254\n",
            "[1346,     2] loss: 0.254\n",
            "[1347,     2] loss: 0.254\n",
            "[1348,     2] loss: 0.256\n",
            "[1349,     2] loss: 0.254\n",
            "[1350,     2] loss: 0.257\n",
            "[1351,     2] loss: 0.260\n",
            "[1352,     2] loss: 0.255\n",
            "[1353,     2] loss: 0.253\n",
            "[1354,     2] loss: 0.254\n",
            "[1355,     2] loss: 0.254\n",
            "[1356,     2] loss: 0.255\n",
            "[1357,     2] loss: 0.258\n",
            "[1358,     2] loss: 0.253\n",
            "[1359,     2] loss: 0.254\n",
            "[1360,     2] loss: 0.254\n",
            "[1361,     2] loss: 0.255\n",
            "[1362,     2] loss: 0.258\n",
            "[1363,     2] loss: 0.252\n",
            "[1364,     2] loss: 0.255\n",
            "[1365,     2] loss: 0.255\n",
            "[1366,     2] loss: 0.252\n",
            "[1367,     2] loss: 0.255\n",
            "[1368,     2] loss: 0.256\n",
            "[1369,     2] loss: 0.253\n",
            "[1370,     2] loss: 0.255\n",
            "[1371,     2] loss: 0.253\n",
            "[1372,     2] loss: 0.254\n",
            "[1373,     2] loss: 0.259\n",
            "[1374,     2] loss: 0.253\n",
            "[1375,     2] loss: 0.253\n",
            "[1376,     2] loss: 0.253\n",
            "[1377,     2] loss: 0.255\n",
            "[1378,     2] loss: 0.257\n",
            "[1379,     2] loss: 0.253\n",
            "[1380,     2] loss: 0.254\n",
            "[1381,     2] loss: 0.259\n",
            "[1382,     2] loss: 0.253\n",
            "[1383,     2] loss: 0.255\n",
            "[1384,     2] loss: 0.254\n",
            "[1385,     2] loss: 0.253\n",
            "[1386,     2] loss: 0.255\n",
            "[1387,     2] loss: 0.254\n",
            "[1388,     2] loss: 0.255\n",
            "[1389,     2] loss: 0.254\n",
            "[1390,     2] loss: 0.253\n",
            "[1391,     2] loss: 0.253\n",
            "[1392,     2] loss: 0.256\n",
            "[1393,     2] loss: 0.262\n",
            "[1394,     2] loss: 0.255\n",
            "[1395,     2] loss: 0.253\n",
            "[1396,     2] loss: 0.257\n",
            "[1397,     2] loss: 0.255\n",
            "[1398,     2] loss: 0.253\n",
            "[1399,     2] loss: 0.256\n",
            "[1400,     2] loss: 0.256\n",
            "[1401,     2] loss: 0.254\n",
            "[1402,     2] loss: 0.254\n",
            "[1403,     2] loss: 0.254\n",
            "[1404,     2] loss: 0.255\n",
            "[1405,     2] loss: 0.258\n",
            "[1406,     2] loss: 0.255\n",
            "[1407,     2] loss: 0.253\n",
            "[1408,     2] loss: 0.254\n",
            "[1409,     2] loss: 0.258\n",
            "[1410,     2] loss: 0.254\n",
            "[1411,     2] loss: 0.253\n",
            "[1412,     2] loss: 0.259\n",
            "[1413,     2] loss: 0.253\n",
            "[1414,     2] loss: 0.254\n",
            "[1415,     2] loss: 0.253\n",
            "[1416,     2] loss: 0.262\n",
            "[1417,     2] loss: 0.254\n",
            "[1418,     2] loss: 0.259\n",
            "[1419,     2] loss: 0.255\n",
            "[1420,     2] loss: 0.255\n",
            "[1421,     2] loss: 0.256\n",
            "[1422,     2] loss: 0.257\n",
            "[1423,     2] loss: 0.258\n",
            "[1424,     2] loss: 0.259\n",
            "[1425,     2] loss: 0.254\n",
            "[1426,     2] loss: 0.254\n",
            "[1427,     2] loss: 0.253\n",
            "[1428,     2] loss: 0.253\n",
            "[1429,     2] loss: 0.253\n",
            "[1430,     2] loss: 0.253\n",
            "[1431,     2] loss: 0.253\n",
            "[1432,     2] loss: 0.253\n",
            "[1433,     2] loss: 0.255\n",
            "[1434,     2] loss: 0.255\n",
            "[1435,     2] loss: 0.254\n",
            "[1436,     2] loss: 0.253\n",
            "[1437,     2] loss: 0.254\n",
            "[1438,     2] loss: 0.253\n",
            "[1439,     2] loss: 0.254\n",
            "[1440,     2] loss: 0.255\n",
            "[1441,     2] loss: 0.254\n",
            "[1442,     2] loss: 0.254\n",
            "[1443,     2] loss: 0.253\n",
            "[1444,     2] loss: 0.253\n",
            "[1445,     2] loss: 0.254\n",
            "[1446,     2] loss: 0.254\n",
            "[1447,     2] loss: 0.254\n",
            "[1448,     2] loss: 0.253\n",
            "[1449,     2] loss: 0.257\n",
            "[1450,     2] loss: 0.256\n",
            "[1451,     2] loss: 0.254\n",
            "[1452,     2] loss: 0.254\n",
            "[1453,     2] loss: 0.253\n",
            "[1454,     2] loss: 0.254\n",
            "[1455,     2] loss: 0.253\n",
            "[1456,     2] loss: 0.253\n",
            "[1457,     2] loss: 0.255\n",
            "[1458,     2] loss: 0.254\n",
            "[1459,     2] loss: 0.253\n",
            "[1460,     2] loss: 0.257\n",
            "[1461,     2] loss: 0.253\n",
            "[1462,     2] loss: 0.255\n",
            "[1463,     2] loss: 0.258\n",
            "[1464,     2] loss: 0.254\n",
            "[1465,     2] loss: 0.258\n",
            "[1466,     2] loss: 0.254\n",
            "[1467,     2] loss: 0.253\n",
            "[1468,     2] loss: 0.253\n",
            "[1469,     2] loss: 0.256\n",
            "[1470,     2] loss: 0.253\n",
            "[1471,     2] loss: 0.255\n",
            "[1472,     2] loss: 0.253\n",
            "[1473,     2] loss: 0.256\n",
            "[1474,     2] loss: 0.259\n",
            "[1475,     2] loss: 0.255\n",
            "[1476,     2] loss: 0.254\n",
            "[1477,     2] loss: 0.256\n",
            "[1478,     2] loss: 0.256\n",
            "[1479,     2] loss: 0.253\n",
            "[1480,     2] loss: 0.256\n",
            "[1481,     2] loss: 0.253\n",
            "[1482,     2] loss: 0.253\n",
            "[1483,     2] loss: 0.253\n",
            "[1484,     2] loss: 0.254\n",
            "[1485,     2] loss: 0.257\n",
            "[1486,     2] loss: 0.253\n",
            "[1487,     2] loss: 0.254\n",
            "[1488,     2] loss: 0.254\n",
            "[1489,     2] loss: 0.257\n",
            "[1490,     2] loss: 0.253\n",
            "[1491,     2] loss: 0.254\n",
            "[1492,     2] loss: 0.254\n",
            "[1493,     2] loss: 0.257\n",
            "[1494,     2] loss: 0.255\n",
            "[1495,     2] loss: 0.256\n",
            "[1496,     2] loss: 0.267\n",
            "[1497,     2] loss: 0.256\n",
            "[1498,     2] loss: 0.254\n",
            "[1499,     2] loss: 0.255\n",
            "[1500,     2] loss: 0.254\n",
            "[1501,     2] loss: 0.258\n",
            "[1502,     2] loss: 0.252\n",
            "[1503,     2] loss: 0.254\n",
            "[1504,     2] loss: 0.254\n",
            "[1505,     2] loss: 0.254\n",
            "[1506,     2] loss: 0.253\n",
            "[1507,     2] loss: 0.260\n",
            "[1508,     2] loss: 0.253\n",
            "[1509,     2] loss: 0.259\n",
            "[1510,     2] loss: 0.259\n",
            "[1511,     2] loss: 0.256\n",
            "[1512,     2] loss: 0.254\n",
            "[1513,     2] loss: 0.253\n",
            "[1514,     2] loss: 0.254\n",
            "[1515,     2] loss: 0.254\n",
            "[1516,     2] loss: 0.256\n",
            "[1517,     2] loss: 0.255\n",
            "[1518,     2] loss: 0.258\n",
            "[1519,     2] loss: 0.253\n",
            "[1520,     2] loss: 0.254\n",
            "[1521,     2] loss: 0.264\n",
            "[1522,     2] loss: 0.259\n",
            "[1523,     2] loss: 0.275\n",
            "[1524,     2] loss: 0.259\n",
            "[1525,     2] loss: 0.254\n",
            "[1526,     2] loss: 0.266\n",
            "[1527,     2] loss: 0.253\n",
            "[1528,     2] loss: 0.253\n",
            "[1529,     2] loss: 0.256\n",
            "[1530,     2] loss: 0.255\n",
            "[1531,     2] loss: 0.255\n",
            "[1532,     2] loss: 0.261\n",
            "[1533,     2] loss: 0.253\n",
            "[1534,     2] loss: 0.253\n",
            "[1535,     2] loss: 0.253\n",
            "[1536,     2] loss: 0.254\n",
            "[1537,     2] loss: 0.254\n",
            "[1538,     2] loss: 0.259\n",
            "[1539,     2] loss: 0.259\n",
            "[1540,     2] loss: 0.253\n",
            "[1541,     2] loss: 0.257\n",
            "[1542,     2] loss: 0.257\n",
            "[1543,     2] loss: 0.252\n",
            "[1544,     2] loss: 0.264\n",
            "[1545,     2] loss: 0.260\n",
            "[1546,     2] loss: 0.260\n",
            "[1547,     2] loss: 0.260\n",
            "[1548,     2] loss: 0.252\n",
            "[1549,     2] loss: 0.254\n",
            "[1550,     2] loss: 0.254\n",
            "[1551,     2] loss: 0.253\n",
            "[1552,     2] loss: 0.254\n",
            "[1553,     2] loss: 0.254\n",
            "[1554,     2] loss: 0.254\n",
            "[1555,     2] loss: 0.254\n",
            "[1556,     2] loss: 0.262\n",
            "[1557,     2] loss: 0.256\n",
            "[1558,     2] loss: 0.255\n",
            "[1559,     2] loss: 0.255\n",
            "[1560,     2] loss: 0.253\n",
            "[1561,     2] loss: 0.265\n",
            "[1562,     2] loss: 0.256\n",
            "[1563,     2] loss: 0.253\n",
            "[1564,     2] loss: 0.254\n",
            "[1565,     2] loss: 0.254\n",
            "[1566,     2] loss: 0.254\n",
            "[1567,     2] loss: 0.253\n",
            "[1568,     2] loss: 0.253\n",
            "[1569,     2] loss: 0.253\n",
            "[1570,     2] loss: 0.253\n",
            "[1571,     2] loss: 0.255\n",
            "[1572,     2] loss: 0.256\n",
            "[1573,     2] loss: 0.253\n",
            "[1574,     2] loss: 0.255\n",
            "[1575,     2] loss: 0.254\n",
            "[1576,     2] loss: 0.256\n",
            "[1577,     2] loss: 0.255\n",
            "[1578,     2] loss: 0.253\n",
            "[1579,     2] loss: 0.259\n",
            "[1580,     2] loss: 0.259\n",
            "[1581,     2] loss: 0.253\n",
            "[1582,     2] loss: 0.256\n",
            "[1583,     2] loss: 0.258\n",
            "[1584,     2] loss: 0.253\n",
            "[1585,     2] loss: 0.254\n",
            "[1586,     2] loss: 0.253\n",
            "[1587,     2] loss: 0.256\n",
            "[1588,     2] loss: 0.255\n",
            "[1589,     2] loss: 0.269\n",
            "[1590,     2] loss: 0.259\n",
            "[1591,     2] loss: 0.269\n",
            "[1592,     2] loss: 0.259\n",
            "[1593,     2] loss: 0.256\n",
            "[1594,     2] loss: 0.255\n",
            "[1595,     2] loss: 0.256\n",
            "[1596,     2] loss: 0.259\n",
            "[1597,     2] loss: 0.254\n",
            "[1598,     2] loss: 0.258\n",
            "[1599,     2] loss: 0.259\n",
            "[1600,     2] loss: 0.253\n",
            "[1601,     2] loss: 0.259\n",
            "[1602,     2] loss: 0.257\n",
            "[1603,     2] loss: 0.252\n",
            "[1604,     2] loss: 0.258\n",
            "[1605,     2] loss: 0.257\n",
            "[1606,     2] loss: 0.260\n",
            "[1607,     2] loss: 0.259\n",
            "[1608,     2] loss: 0.258\n",
            "[1609,     2] loss: 0.257\n",
            "[1610,     2] loss: 0.256\n",
            "[1611,     2] loss: 0.256\n",
            "[1612,     2] loss: 0.256\n",
            "[1613,     2] loss: 0.254\n",
            "[1614,     2] loss: 0.258\n",
            "[1615,     2] loss: 0.254\n",
            "[1616,     2] loss: 0.252\n",
            "[1617,     2] loss: 0.258\n",
            "[1618,     2] loss: 0.254\n",
            "[1619,     2] loss: 0.266\n",
            "[1620,     2] loss: 0.263\n",
            "[1621,     2] loss: 0.252\n",
            "[1622,     2] loss: 0.255\n",
            "[1623,     2] loss: 0.254\n",
            "[1624,     2] loss: 0.253\n",
            "[1625,     2] loss: 0.260\n",
            "[1626,     2] loss: 0.253\n",
            "[1627,     2] loss: 0.253\n",
            "[1628,     2] loss: 0.254\n",
            "[1629,     2] loss: 0.255\n",
            "[1630,     2] loss: 0.259\n",
            "[1631,     2] loss: 0.268\n",
            "[1632,     2] loss: 0.260\n",
            "[1633,     2] loss: 0.257\n",
            "[1634,     2] loss: 0.255\n",
            "[1635,     2] loss: 0.252\n",
            "[1636,     2] loss: 0.254\n",
            "[1637,     2] loss: 0.256\n",
            "[1638,     2] loss: 0.252\n",
            "[1639,     2] loss: 0.268\n",
            "[1640,     2] loss: 0.258\n",
            "[1641,     2] loss: 0.254\n",
            "[1642,     2] loss: 0.258\n",
            "[1643,     2] loss: 0.256\n",
            "[1644,     2] loss: 0.257\n",
            "[1645,     2] loss: 0.255\n",
            "[1646,     2] loss: 0.253\n",
            "[1647,     2] loss: 0.257\n",
            "[1648,     2] loss: 0.254\n",
            "[1649,     2] loss: 0.253\n",
            "[1650,     2] loss: 0.271\n",
            "[1651,     2] loss: 0.260\n",
            "[1652,     2] loss: 0.256\n",
            "[1653,     2] loss: 0.254\n",
            "[1654,     2] loss: 0.254\n",
            "[1655,     2] loss: 0.259\n",
            "[1656,     2] loss: 0.259\n",
            "[1657,     2] loss: 0.253\n",
            "[1658,     2] loss: 0.253\n",
            "[1659,     2] loss: 0.253\n",
            "[1660,     2] loss: 0.253\n",
            "[1661,     2] loss: 0.258\n",
            "[1662,     2] loss: 0.260\n",
            "[1663,     2] loss: 0.254\n",
            "[1664,     2] loss: 0.259\n",
            "[1665,     2] loss: 0.254\n",
            "[1666,     2] loss: 0.260\n",
            "[1667,     2] loss: 0.262\n",
            "[1668,     2] loss: 0.253\n",
            "[1669,     2] loss: 0.253\n",
            "[1670,     2] loss: 0.254\n",
            "[1671,     2] loss: 0.256\n",
            "[1672,     2] loss: 0.257\n",
            "[1673,     2] loss: 0.255\n",
            "[1674,     2] loss: 0.253\n",
            "[1675,     2] loss: 0.264\n",
            "[1676,     2] loss: 0.265\n",
            "[1677,     2] loss: 0.251\n",
            "[1678,     2] loss: 0.261\n",
            "[1679,     2] loss: 0.259\n",
            "[1680,     2] loss: 0.259\n",
            "[1681,     2] loss: 0.259\n",
            "[1682,     2] loss: 0.253\n",
            "[1683,     2] loss: 0.253\n",
            "[1684,     2] loss: 0.257\n",
            "[1685,     2] loss: 0.255\n",
            "[1686,     2] loss: 0.254\n",
            "[1687,     2] loss: 0.258\n",
            "[1688,     2] loss: 0.258\n",
            "[1689,     2] loss: 0.255\n",
            "[1690,     2] loss: 0.255\n",
            "[1691,     2] loss: 0.257\n",
            "[1692,     2] loss: 0.252\n",
            "[1693,     2] loss: 0.254\n",
            "[1694,     2] loss: 0.254\n",
            "[1695,     2] loss: 0.253\n",
            "[1696,     2] loss: 0.254\n",
            "[1697,     2] loss: 0.254\n",
            "[1698,     2] loss: 0.254\n",
            "[1699,     2] loss: 0.256\n",
            "[1700,     2] loss: 0.255\n",
            "[1701,     2] loss: 0.257\n",
            "[1702,     2] loss: 0.254\n",
            "[1703,     2] loss: 0.253\n",
            "[1704,     2] loss: 0.255\n",
            "[1705,     2] loss: 0.254\n",
            "[1706,     2] loss: 0.257\n",
            "[1707,     2] loss: 0.254\n",
            "[1708,     2] loss: 0.258\n",
            "[1709,     2] loss: 0.253\n",
            "[1710,     2] loss: 0.255\n",
            "[1711,     2] loss: 0.255\n",
            "[1712,     2] loss: 0.253\n",
            "[1713,     2] loss: 0.257\n",
            "[1714,     2] loss: 0.259\n",
            "[1715,     2] loss: 0.254\n",
            "[1716,     2] loss: 0.262\n",
            "[1717,     2] loss: 0.256\n",
            "[1718,     2] loss: 0.253\n",
            "[1719,     2] loss: 0.255\n",
            "[1720,     2] loss: 0.253\n",
            "[1721,     2] loss: 0.253\n",
            "[1722,     2] loss: 0.253\n",
            "[1723,     2] loss: 0.254\n",
            "[1724,     2] loss: 0.253\n",
            "[1725,     2] loss: 0.253\n",
            "[1726,     2] loss: 0.256\n",
            "[1727,     2] loss: 0.253\n",
            "[1728,     2] loss: 0.253\n",
            "[1729,     2] loss: 0.264\n",
            "[1730,     2] loss: 0.256\n",
            "[1731,     2] loss: 0.259\n",
            "[1732,     2] loss: 0.260\n",
            "[1733,     2] loss: 0.256\n",
            "[1734,     2] loss: 0.254\n",
            "[1735,     2] loss: 0.255\n",
            "[1736,     2] loss: 0.254\n",
            "[1737,     2] loss: 0.253\n",
            "[1738,     2] loss: 0.255\n",
            "[1739,     2] loss: 0.256\n",
            "[1740,     2] loss: 0.257\n",
            "[1741,     2] loss: 0.254\n",
            "[1742,     2] loss: 0.253\n",
            "[1743,     2] loss: 0.253\n",
            "[1744,     2] loss: 0.260\n",
            "[1745,     2] loss: 0.254\n",
            "[1746,     2] loss: 0.253\n",
            "[1747,     2] loss: 0.258\n",
            "[1748,     2] loss: 0.253\n",
            "[1749,     2] loss: 0.263\n",
            "[1750,     2] loss: 0.259\n",
            "[1751,     2] loss: 0.252\n",
            "[1752,     2] loss: 0.257\n",
            "[1753,     2] loss: 0.257\n",
            "[1754,     2] loss: 0.254\n",
            "[1755,     2] loss: 0.257\n",
            "[1756,     2] loss: 0.259\n",
            "[1757,     2] loss: 0.257\n",
            "[1758,     2] loss: 0.259\n",
            "[1759,     2] loss: 0.255\n",
            "[1760,     2] loss: 0.259\n",
            "[1761,     2] loss: 0.263\n",
            "[1762,     2] loss: 0.255\n",
            "[1763,     2] loss: 0.260\n",
            "[1764,     2] loss: 0.253\n",
            "[1765,     2] loss: 0.253\n",
            "[1766,     2] loss: 0.255\n",
            "[1767,     2] loss: 0.253\n",
            "[1768,     2] loss: 0.253\n",
            "[1769,     2] loss: 0.256\n",
            "[1770,     2] loss: 0.253\n",
            "[1771,     2] loss: 0.254\n",
            "[1772,     2] loss: 0.254\n",
            "[1773,     2] loss: 0.253\n",
            "[1774,     2] loss: 0.254\n",
            "[1775,     2] loss: 0.254\n",
            "[1776,     2] loss: 0.254\n",
            "[1777,     2] loss: 0.253\n",
            "[1778,     2] loss: 0.256\n",
            "[1779,     2] loss: 0.255\n",
            "[1780,     2] loss: 0.253\n",
            "[1781,     2] loss: 0.256\n",
            "[1782,     2] loss: 0.258\n",
            "[1783,     2] loss: 0.254\n",
            "[1784,     2] loss: 0.254\n",
            "[1785,     2] loss: 0.256\n",
            "[1786,     2] loss: 0.255\n",
            "[1787,     2] loss: 0.254\n",
            "[1788,     2] loss: 0.255\n",
            "[1789,     2] loss: 0.254\n",
            "[1790,     2] loss: 0.254\n",
            "[1791,     2] loss: 0.254\n",
            "[1792,     2] loss: 0.260\n",
            "[1793,     2] loss: 0.256\n",
            "[1794,     2] loss: 0.254\n",
            "[1795,     2] loss: 0.256\n",
            "[1796,     2] loss: 0.253\n",
            "[1797,     2] loss: 0.252\n",
            "[1798,     2] loss: 0.255\n",
            "[1799,     2] loss: 0.253\n",
            "[1800,     2] loss: 0.253\n",
            "[1801,     2] loss: 0.254\n",
            "[1802,     2] loss: 0.260\n",
            "[1803,     2] loss: 0.252\n",
            "[1804,     2] loss: 0.260\n",
            "[1805,     2] loss: 0.258\n",
            "[1806,     2] loss: 0.255\n",
            "[1807,     2] loss: 0.258\n",
            "[1808,     2] loss: 0.258\n",
            "[1809,     2] loss: 0.254\n",
            "[1810,     2] loss: 0.255\n",
            "[1811,     2] loss: 0.255\n",
            "[1812,     2] loss: 0.255\n",
            "[1813,     2] loss: 0.254\n",
            "[1814,     2] loss: 0.259\n",
            "[1815,     2] loss: 0.253\n",
            "[1816,     2] loss: 0.253\n",
            "[1817,     2] loss: 0.255\n",
            "[1818,     2] loss: 0.255\n",
            "[1819,     2] loss: 0.255\n",
            "[1820,     2] loss: 0.255\n",
            "[1821,     2] loss: 0.253\n",
            "[1822,     2] loss: 0.253\n",
            "[1823,     2] loss: 0.253\n",
            "[1824,     2] loss: 0.255\n",
            "[1825,     2] loss: 0.253\n",
            "[1826,     2] loss: 0.260\n",
            "[1827,     2] loss: 0.258\n",
            "[1828,     2] loss: 0.253\n",
            "[1829,     2] loss: 0.257\n",
            "[1830,     2] loss: 0.255\n",
            "[1831,     2] loss: 0.265\n",
            "[1832,     2] loss: 0.258\n",
            "[1833,     2] loss: 0.263\n",
            "[1834,     2] loss: 0.256\n",
            "[1835,     2] loss: 0.252\n",
            "[1836,     2] loss: 0.255\n",
            "[1837,     2] loss: 0.256\n",
            "[1838,     2] loss: 0.253\n",
            "[1839,     2] loss: 0.254\n",
            "[1840,     2] loss: 0.255\n",
            "[1841,     2] loss: 0.255\n",
            "[1842,     2] loss: 0.252\n",
            "[1843,     2] loss: 0.254\n",
            "[1844,     2] loss: 0.259\n",
            "[1845,     2] loss: 0.253\n",
            "[1846,     2] loss: 0.254\n",
            "[1847,     2] loss: 0.254\n",
            "[1848,     2] loss: 0.254\n",
            "[1849,     2] loss: 0.253\n",
            "[1850,     2] loss: 0.262\n",
            "[1851,     2] loss: 0.254\n",
            "[1852,     2] loss: 0.252\n",
            "[1853,     2] loss: 0.258\n",
            "[1854,     2] loss: 0.259\n",
            "[1855,     2] loss: 0.253\n",
            "[1856,     2] loss: 0.254\n",
            "[1857,     2] loss: 0.256\n",
            "[1858,     2] loss: 0.263\n",
            "[1859,     2] loss: 0.262\n",
            "[1860,     2] loss: 0.255\n",
            "[1861,     2] loss: 0.253\n",
            "[1862,     2] loss: 0.260\n",
            "[1863,     2] loss: 0.255\n",
            "[1864,     2] loss: 0.264\n",
            "[1865,     2] loss: 0.256\n",
            "[1866,     2] loss: 0.252\n",
            "[1867,     2] loss: 0.256\n",
            "[1868,     2] loss: 0.255\n",
            "[1869,     2] loss: 0.253\n",
            "[1870,     2] loss: 0.254\n",
            "[1871,     2] loss: 0.253\n",
            "[1872,     2] loss: 0.254\n",
            "[1873,     2] loss: 0.258\n",
            "[1874,     2] loss: 0.255\n",
            "[1875,     2] loss: 0.258\n",
            "[1876,     2] loss: 0.255\n",
            "[1877,     2] loss: 0.251\n",
            "[1878,     2] loss: 0.255\n",
            "[1879,     2] loss: 0.254\n",
            "[1880,     2] loss: 0.252\n",
            "[1881,     2] loss: 0.253\n",
            "[1882,     2] loss: 0.253\n",
            "[1883,     2] loss: 0.253\n",
            "[1884,     2] loss: 0.254\n",
            "[1885,     2] loss: 0.257\n",
            "[1886,     2] loss: 0.253\n",
            "[1887,     2] loss: 0.253\n",
            "[1888,     2] loss: 0.253\n",
            "[1889,     2] loss: 0.254\n",
            "[1890,     2] loss: 0.258\n",
            "[1891,     2] loss: 0.256\n",
            "[1892,     2] loss: 0.258\n",
            "[1893,     2] loss: 0.259\n",
            "[1894,     2] loss: 0.252\n",
            "[1895,     2] loss: 0.253\n",
            "[1896,     2] loss: 0.259\n",
            "[1897,     2] loss: 0.254\n",
            "[1898,     2] loss: 0.255\n",
            "[1899,     2] loss: 0.259\n",
            "[1900,     2] loss: 0.253\n",
            "[1901,     2] loss: 0.253\n",
            "[1902,     2] loss: 0.267\n",
            "[1903,     2] loss: 0.254\n",
            "[1904,     2] loss: 0.256\n",
            "[1905,     2] loss: 0.263\n",
            "[1906,     2] loss: 0.253\n",
            "[1907,     2] loss: 0.255\n",
            "[1908,     2] loss: 0.262\n",
            "[1909,     2] loss: 0.254\n",
            "[1910,     2] loss: 0.255\n",
            "[1911,     2] loss: 0.257\n",
            "[1912,     2] loss: 0.253\n",
            "[1913,     2] loss: 0.255\n",
            "[1914,     2] loss: 0.255\n",
            "[1915,     2] loss: 0.253\n",
            "[1916,     2] loss: 0.254\n",
            "[1917,     2] loss: 0.254\n",
            "[1918,     2] loss: 0.254\n",
            "[1919,     2] loss: 0.257\n",
            "[1920,     2] loss: 0.255\n",
            "[1921,     2] loss: 0.252\n",
            "[1922,     2] loss: 0.255\n",
            "[1923,     2] loss: 0.255\n",
            "[1924,     2] loss: 0.257\n",
            "[1925,     2] loss: 0.255\n",
            "[1926,     2] loss: 0.257\n",
            "[1927,     2] loss: 0.259\n",
            "[1928,     2] loss: 0.253\n",
            "[1929,     2] loss: 0.254\n",
            "[1930,     2] loss: 0.255\n",
            "[1931,     2] loss: 0.256\n",
            "[1932,     2] loss: 0.253\n",
            "[1933,     2] loss: 0.260\n",
            "[1934,     2] loss: 0.256\n",
            "[1935,     2] loss: 0.254\n",
            "[1936,     2] loss: 0.259\n",
            "[1937,     2] loss: 0.254\n",
            "[1938,     2] loss: 0.257\n",
            "[1939,     2] loss: 0.257\n",
            "[1940,     2] loss: 0.257\n",
            "[1941,     2] loss: 0.257\n",
            "[1942,     2] loss: 0.254\n",
            "[1943,     2] loss: 0.253\n",
            "[1944,     2] loss: 0.256\n",
            "[1945,     2] loss: 0.255\n",
            "[1946,     2] loss: 0.264\n",
            "[1947,     2] loss: 0.260\n",
            "[1948,     2] loss: 0.253\n",
            "[1949,     2] loss: 0.255\n",
            "[1950,     2] loss: 0.264\n",
            "[1951,     2] loss: 0.261\n",
            "[1952,     2] loss: 0.262\n",
            "[1953,     2] loss: 0.258\n",
            "[1954,     2] loss: 0.252\n",
            "[1955,     2] loss: 0.255\n",
            "[1956,     2] loss: 0.254\n",
            "[1957,     2] loss: 0.252\n",
            "[1958,     2] loss: 0.260\n",
            "[1959,     2] loss: 0.253\n",
            "[1960,     2] loss: 0.262\n",
            "[1961,     2] loss: 0.253\n",
            "[1962,     2] loss: 0.256\n",
            "[1963,     2] loss: 0.253\n",
            "[1964,     2] loss: 0.253\n",
            "[1965,     2] loss: 0.253\n",
            "[1966,     2] loss: 0.253\n",
            "[1967,     2] loss: 0.261\n",
            "[1968,     2] loss: 0.254\n",
            "[1969,     2] loss: 0.258\n",
            "[1970,     2] loss: 0.260\n",
            "[1971,     2] loss: 0.265\n",
            "[1972,     2] loss: 0.260\n",
            "[1973,     2] loss: 0.255\n",
            "[1974,     2] loss: 0.256\n",
            "[1975,     2] loss: 0.268\n",
            "[1976,     2] loss: 0.256\n",
            "[1977,     2] loss: 0.269\n",
            "[1978,     2] loss: 0.264\n",
            "[1979,     2] loss: 0.254\n",
            "[1980,     2] loss: 0.262\n",
            "[1981,     2] loss: 0.262\n",
            "[1982,     2] loss: 0.255\n",
            "[1983,     2] loss: 0.256\n",
            "[1984,     2] loss: 0.259\n",
            "[1985,     2] loss: 0.255\n",
            "[1986,     2] loss: 0.253\n",
            "[1987,     2] loss: 0.254\n",
            "[1988,     2] loss: 0.254\n",
            "[1989,     2] loss: 0.255\n",
            "[1990,     2] loss: 0.253\n",
            "[1991,     2] loss: 0.256\n",
            "[1992,     2] loss: 0.255\n",
            "[1993,     2] loss: 0.254\n",
            "[1994,     2] loss: 0.254\n",
            "[1995,     2] loss: 0.253\n",
            "[1996,     2] loss: 0.256\n",
            "[1997,     2] loss: 0.254\n",
            "[1998,     2] loss: 0.253\n",
            "[1999,     2] loss: 0.257\n",
            "[2000,     2] loss: 0.254\n",
            "Finished Training\n"
          ]
        }
      ],
      "source": [
        "nos_epochs = 2000\n",
        "focus_true_pred_true =0\n",
        "focus_false_pred_true =0\n",
        "focus_true_pred_false =0\n",
        "focus_false_pred_false =0\n",
        "\n",
        "argmax_more_than_half = 0\n",
        "argmax_less_than_half =0\n",
        "\n",
        "\n",
        "for epoch in range(nos_epochs):  # loop over the dataset multiple times\n",
        "\n",
        "  focus_true_pred_true =0\n",
        "  focus_false_pred_true =0\n",
        "  focus_true_pred_false =0\n",
        "  focus_false_pred_false =0\n",
        "  \n",
        "  argmax_more_than_half = 0\n",
        "  argmax_less_than_half =0\n",
        "  \n",
        "  running_loss = 0.0\n",
        "  epoch_loss = []\n",
        "  cnt=0\n",
        "\n",
        "  iteration = desired_num // batch\n",
        "  \n",
        "  #training data set\n",
        "  \n",
        "  for i, data in  enumerate(train_loader):\n",
        "    inputs , labels , fore_idx = data\n",
        "    batch = inputs.size(0)\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "    inputs = inputs.double()\n",
        "    # zero the parameter gradients\n",
        "    \n",
        "    optimizer_focus.zero_grad()\n",
        "    optimizer_classify.zero_grad()\n",
        "    \n",
        "    alphas, avg_images = focus_net(inputs)\n",
        "    outputs = classify(avg_images)\n",
        "\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "#     print(outputs)\n",
        "#     print(outputs.shape,labels.shape , torch.argmax(outputs, dim=1))\n",
        "\n",
        "    loss = my_cross_entropy(outputs, labels,alphas) \n",
        "    \n",
        "    loss.backward()\n",
        "    \n",
        "    \n",
        "    optimizer_focus.step()\n",
        "    optimizer_classify.step()\n",
        "\n",
        "    running_loss += loss.item()\n",
        "    mini = 2\n",
        "    if cnt % mini == mini-1:    # print every 40 mini-batches\n",
        "      print('[%d, %5d] loss: %.3f' %(epoch + 1, cnt + 1, running_loss / mini))\n",
        "      epoch_loss.append(running_loss/mini)\n",
        "      running_loss = 0.0\n",
        "    cnt=cnt+1\n",
        "\n",
        "  if(np.mean(epoch_loss) <= 0.01):\n",
        "      break;\n",
        "  #plot_attended_data(train_loader,focus_net,epoch)\n",
        "\n",
        "    \n",
        "print('Finished Training')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "3xPsiBtU-GDn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38d91c64-272b-4ba6-d21e-8635f4d4dae6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('fc1.weight', Parameter containing:\n",
            "tensor([[2.7990, 3.0915]], device='cuda:0', dtype=torch.float64,\n",
            "       requires_grad=True))\n"
          ]
        }
      ],
      "source": [
        "for param in focus_net.named_parameters():\n",
        "    print(param)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "jhvhkEAyeRpt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "378eb849-8d27-432e-f6fb-08df009973aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the train images: 88.000000 %\n",
            "total correct 88\n",
            "total train set images 100\n"
          ]
        }
      ],
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "focus_net.eval()\n",
        "classify.eval()\n",
        "with torch.no_grad():\n",
        "  for data in train_loader:\n",
        "    inputs, labels , fore_idx = data\n",
        "    inputs = inputs.double()\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "    alphas, avg_images = focus_net(inputs)\n",
        "    outputs = classify(avg_images)\n",
        "    #print(outputs.shape)\n",
        "\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network on the train images: %f %%' % ( 100 * correct / total))\n",
        "print(\"total correct\", correct)\n",
        "print(\"total train set images\", total)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "OKcmpKwGeS8M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6522878c-68a5-4f74-a07d-ca1f046f8140"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the test images: 87.500000 %\n",
            "total correct 875\n",
            "total train set images 1000\n"
          ]
        }
      ],
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "  for data in test_loader:\n",
        "    inputs, labels , fore_idx = data\n",
        "    inputs = inputs.double()\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "    alphas, avg_images = focus_net(inputs)\n",
        "    outputs = classify(avg_images)\n",
        "\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network on the test images: %f %%' % ( 100 * correct / total))\n",
        "print(\"total correct\", correct)\n",
        "print(\"total train set images\", total)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "L6ml_ieu7IpB"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "hard_attention_1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
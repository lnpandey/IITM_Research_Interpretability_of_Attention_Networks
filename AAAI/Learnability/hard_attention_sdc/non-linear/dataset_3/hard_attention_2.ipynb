{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0vlCAi2JLSzD"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "PiPNZm1iTgHy"
      },
      "outputs": [],
      "source": [
        "# path=\"/content/drive/MyDrive/Research/Hard_Attention/dataset_2/m_5_size_100/run_\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "SrZgZMlK-GDe"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2_J4Rw2r0SQ",
        "outputId": "30870b50-a78b-49d3-e30f-3d4cf8a25d07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "from tqdm import tqdm\n",
        "%matplotlib inline\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn import functional as F\n",
        "from tqdm import tqdm as tqdm\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "-Dmy2iPWlgnc"
      },
      "outputs": [],
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "writer = SummaryWriter('runs/hard_attention')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6fjud_Fr0Sa"
      },
      "source": [
        "# Generate dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqdXHO0Cr0Sd",
        "outputId": "2f308066-5560-4f0a-e07a-c730ff5f7a68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 500\n",
            "1 500\n",
            "2 500\n",
            "3 500\n",
            "4 500\n",
            "5 500\n",
            "6 500\n",
            "7 500\n",
            "8 500\n",
            "9 500\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(1)\n",
        "\n",
        "y = np.concatenate((np.zeros(500),np.ones(500),np.ones(500)*2,np.ones(500)*3,np.ones(500)*4,\n",
        "                    np.ones(500)*5,np.ones(500)*6,np.ones(500)*7,np.ones(500)*8,np.ones(500)*9))\n",
        "#y = np.random.randint(0,3,6000)\n",
        "idx= []\n",
        "for i in range(10):\n",
        "    print(i,sum(y==i))\n",
        "    idx.append(y==i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ddhXyODwr0Sk"
      },
      "outputs": [],
      "source": [
        "x = np.zeros((5000,2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "DyV3N2DIr0Sp"
      },
      "outputs": [],
      "source": [
        "x = np.zeros((5000,2))\n",
        "\n",
        "\n",
        "np.random.seed(12)\n",
        "x[idx[0],:] = np.random.multivariate_normal(mean = [5,5],cov=[[0.1,0],[0,0.1]],size=sum(idx[0]))\n",
        "x[idx[1],:] = np.random.multivariate_normal(mean = [6,6],cov=[[0.1,0],[0,0.1]],size=sum(idx[1]))\n",
        "x[idx[2],:] = np.random.multivariate_normal(mean = [5.5,6.5],cov=[[0.1,0],[0,0.1]],size=sum(idx[2]))\n",
        "x[idx[3],:] = np.random.multivariate_normal(mean = [-1,0],cov=[[0.1,0],[0,0.1]],size=sum(idx[3]))\n",
        "x[idx[4],:] = np.random.multivariate_normal(mean = [0,2],cov=[[0.1,0],[0,0.1]],size=sum(idx[4]))\n",
        "x[idx[5],:] = np.random.multivariate_normal(mean = [1,0],cov=[[0.1,0],[0,0.1]],size=sum(idx[5]))\n",
        "x[idx[6],:] = np.random.multivariate_normal(mean = [0,-1],cov=[[0.1,0],[0,0.1]],size=sum(idx[6]))\n",
        "x[idx[7],:] = np.random.multivariate_normal(mean = [0,0],cov=[[0.1,0],[0,0.1]],size=sum(idx[7]))\n",
        "x[idx[8],:] = np.random.multivariate_normal(mean = [-0.5,-0.5],cov=[[0.1,0],[0,0.1]],size=sum(idx[8]))\n",
        "x[idx[9],:] = np.random.multivariate_normal(mean = [0.4,0.2],cov=[[0.1,0],[0,0.1]],size=sum(idx[9]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qh1mDScsU07I",
        "outputId": "afa08a42-77e0-4171-c116-935e4b971237"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([5.14957125, 4.78451422]), array([5.59513544, 6.5252764 ]))"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "x[idx[0]][0], x[idx[2]][5] "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Vr5ErQ_wSrV",
        "outputId": "95d8a8bc-a327-4b3a-c0c5-b9058913ac1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5000, 2) (5000,)\n"
          ]
        }
      ],
      "source": [
        "print(x.shape,y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "NG-3RpffwU_i"
      },
      "outputs": [],
      "source": [
        "idx= []\n",
        "for i in range(10):\n",
        "  idx.append(y==i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "id": "hJ8Jm7YUr0St",
        "outputId": "c0ea8a8f-1cfa-420c-ed81-d71248f4a138"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f504ee07dd0>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "for i in range(10):\n",
        "    plt.scatter(x[idx[i],0],x[idx[i],1],label=\"class_\"+str(i))\n",
        "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "fNWgnhUJnWLV"
      },
      "outputs": [],
      "source": [
        "x = ( x -  np.mean(x,axis=0,keepdims=True) ) / np.std(x,axis=0,keepdims=True) \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "id": "8-VLhUfDDeHt",
        "outputId": "c970cd1c-6d86-4a8a-ffd0-a787767ad261"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f504ed89090>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "for i in range(10):\n",
        "    plt.scatter(x[idx[i],0],x[idx[i],1],label=\"class_\"+str(i))\n",
        "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "UfFHcZJOr0Sz"
      },
      "outputs": [],
      "source": [
        "foreground_classes = {'class_0','class_1' }\n",
        "\n",
        "background_classes = {'bg_classes',}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jqbvfbwVr0TN",
        "outputId": "aa57af7e-e1c1-4173-97b6-70698884028e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1500/1500 [00:00<00:00, 3167.82it/s]\n"
          ]
        }
      ],
      "source": [
        "desired_num = 1500\n",
        "mosaic_list_of_images =[]\n",
        "mosaic_label = []\n",
        "fore_idx=[]\n",
        "m = 5\n",
        "for j in tqdm(range(desired_num)):\n",
        "    np.random.seed(j)\n",
        "    fg_class  = np.random.randint(0,3)\n",
        "    fg_idx = np.random.randint(0,m)\n",
        "    a = []\n",
        "    for i in range(m):\n",
        "        if i == fg_idx:\n",
        "            b = np.random.choice(np.where(idx[fg_class]==True)[0],size=1)\n",
        "            a.append(x[b])\n",
        "#             print(\"foreground \"+str(fg_class)+\" present at \" + str(fg_idx))\n",
        "        else:\n",
        "            bg_class = np.random.randint(3,10)\n",
        "            b = np.random.choice(np.where(idx[bg_class]==True)[0],size=1)\n",
        "            a.append(x[b])\n",
        "#             print(\"background \"+str(bg_class)+\" present at \" + str(i))\n",
        "    a = np.concatenate(a,axis=0)\n",
        "    mosaic_list_of_images.append(np.reshape(a,(m,2)))\n",
        "    mosaic_label.append(fg_class)\n",
        "    fore_idx.append(fg_idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "BOsFmWfMr0TR"
      },
      "outputs": [],
      "source": [
        "# mosaic_list_of_images = np.concatenate(mosaic_list_of_images,axis=1).T\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2aIPMgLXNiXW",
        "outputId": "ae5b1fb5-4f87-4c3e-d2a7-fbc174c6c9eb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1500, array([[ 1.29420953,  0.98948331],\n",
              "        [-0.62723678, -1.08186056],\n",
              "        [-0.85061921,  0.02673107],\n",
              "        [-0.83096682, -0.93025336],\n",
              "        [-0.94404393, -0.55234797]]), (5, 2))"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "len(mosaic_list_of_images), mosaic_list_of_images[0],mosaic_list_of_images[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "iPoIwbMHx44n"
      },
      "outputs": [],
      "source": [
        "class MosaicDataset(Dataset):\n",
        "  \"\"\"MosaicDataset dataset.\"\"\"\n",
        "\n",
        "  def __init__(self, mosaic_list_of_images, mosaic_label, fore_idx):\n",
        "    \"\"\"\n",
        "      Args:\n",
        "        csv_file (string): Path to the csv file with annotations.\n",
        "        root_dir (string): Directory with all the images.\n",
        "        transform (callable, optional): Optional transform to be applied\n",
        "            on a sample.\n",
        "    \"\"\"\n",
        "    self.mosaic = mosaic_list_of_images\n",
        "    self.label = mosaic_label\n",
        "    self.fore_idx = fore_idx\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.label)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.mosaic[idx] , self.label[idx], self.fore_idx[idx]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "fOPAJQJeW8Ah"
      },
      "outputs": [],
      "source": [
        "batch = 50\n",
        "msd1 = MosaicDataset(mosaic_list_of_images[0:500], mosaic_label[0:500] , fore_idx[0:500])\n",
        "train_loader = DataLoader( msd1 ,batch_size= batch ,shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "aWBIcyvGApLt"
      },
      "outputs": [],
      "source": [
        "data,_,_=iter(train_loader).next()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cauJIvKEAxKM",
        "outputId": "eff1fcec-37a4-4a44-fff2-0475bf9a8092"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([50, 5, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "qjNiQgxZW8bA"
      },
      "outputs": [],
      "source": [
        "batch = 250\n",
        "msd2 = MosaicDataset(mosaic_list_of_images[500:], mosaic_label[500:] , fore_idx[500:])\n",
        "test_loader = DataLoader( msd2 ,batch_size= batch ,shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "yda1E5ApiKpH"
      },
      "outputs": [],
      "source": [
        "class Focus(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Focus, self).__init__()\n",
        "        self.fc1 = nn.Linear(2,1, bias=False)\n",
        "        torch.nn.init.xavier_normal_(self.fc1.weight)\n",
        "        #self.fc2 = nn.Linear(64, 1, bias=False)\n",
        "        #torch.nn.init.xavier_normal_(self.fc2.weight)\n",
        "\n",
        "    def forward(self,z):\n",
        "        #print(\"data\",z)\n",
        "        batch = z.size(0)\n",
        "        patches = z.size(1)\n",
        "        z = z.view(batch,patches,2*1)\n",
        "        alp1,ft1 = self.helper(z)\n",
        "\n",
        "        alpha = F.softmax(alp1,dim=1)\n",
        "        #print(self.training)\n",
        "        \n",
        "        if self.training:\n",
        "            alpha =alpha[:,:,0]\n",
        "            y = ft1 \n",
        "            return alpha,y\n",
        "        else:\n",
        "            #alpha_cumsum = torch.cumsum(alpha, dim = 1)\n",
        "            #print(alpha_cumsum)\n",
        "            #len_batch = alpha_cumsum.size(0)\n",
        "            #patches = alpha_cumsum.size(1)\n",
        "            #rand_prob = torch.rand(len_batch,patches, 1).to(device)\n",
        "            #alpha_relu = F.relu(rand_prob-alpha_cumsum)\n",
        "            #print(alpha_relu)\n",
        "            #alpha_index = torch.count_nonzero(alpha_relu,dim=1)\n",
        "            #alpha_hard = F.one_hot(alpha_index,num_classes=patches)\n",
        "            #print(alpha_hard)\n",
        "            #alpha_hard = torch.transpose(alpha_hard,dim0=1,dim1=2)\n",
        "            #print(ft1,\"alpha_hard\",alpha_hard) \n",
        "            #y = torch.sum(alpha_hard*ft1,dim=1)\n",
        "            #print(alpha,alpha.shape)\n",
        "         \n",
        "        \n",
        "            index = torch.argmax(alpha,dim=1)\n",
        "            hard_alpha = torch.nn.functional.one_hot(index[:,0], patches)\n",
        "            y = torch.sum(hard_alpha[:,:,None]*ft1,dim=1)\n",
        "            alpha = alpha[:,:,0]\n",
        "            return alpha,y\n",
        "    \n",
        "    def helper(self, x):\n",
        "        x1 = x\n",
        "        x = self.fc1(x)\n",
        "        return x,x1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "0dYXnywAD-4l"
      },
      "outputs": [],
      "source": [
        "class Classification(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Classification, self).__init__()\n",
        "    self.fc1 = nn.Linear(2, 50)\n",
        "    self.fc2 = nn.Linear(50,3)\n",
        "    torch.nn.init.xavier_normal_(self.fc1.weight)\n",
        "    torch.nn.init.zeros_(self.fc1.bias)\n",
        "    torch.nn.init.xavier_normal_(self.fc2.weight)\n",
        "    torch.nn.init.zeros_(self.fc2.bias)\n",
        "  def forward(self, x):\n",
        "    #print(x.shape)\n",
        "    #x = x.view(-1, 1)\n",
        "    #print(x.shape)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = self.fc2(x)\n",
        "    # print(x.shape)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "lSa6O9f6XNf4"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(12)\n",
        "focus_net = Focus().double()\n",
        "focus_net = focus_net.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "36k3H2G-XO9A"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(12)\n",
        "classify = Classification().double()\n",
        "classify = classify.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "bK78aII8-GDl"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "optimizer_classify = optim.Adam(classify.parameters(), lr=0.1 ) #, momentum=0.9)\n",
        "optimizer_focus = optim.Adam(focus_net.parameters(), lr=0.1 ) #, momentum=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "h0mjWiFG-GDl"
      },
      "outputs": [],
      "source": [
        "def my_cross_entropy(output,target,alpha):\n",
        "    criterion = nn.CrossEntropyLoss(reduce=False)\n",
        "    \n",
        "    batch = output.size(0)\n",
        "    #print(batch)\n",
        "    patches = output.size(1)\n",
        "    classes = output.size(2)\n",
        "    \n",
        "    \n",
        "    \n",
        "    output = torch.reshape(output,(batch*patches,classes))\n",
        "    \n",
        "    \n",
        "    target = target.repeat_interleave(patches)\n",
        "    \n",
        "    loss = criterion(output,target)\n",
        "    \n",
        "    #print(loss,loss.shape)\n",
        "    loss = torch.reshape(loss,(batch,patches))\n",
        "    #print(loss.size())\n",
        "    final_loss = torch.sum(torch.mul(loss,alpha),dim=1)\n",
        "    #print(final_loss.shape)\n",
        "    final_loss = torch.mean(final_loss,dim=0)\n",
        "    \n",
        "    \n",
        "    \n",
        "    #print(final_loss)\n",
        "    return final_loss\n",
        "    \n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "pjD2VZuV9Ed4"
      },
      "outputs": [],
      "source": [
        "col1=[]\n",
        "col2=[]\n",
        "col3=[]\n",
        "col4=[]\n",
        "col5=[]\n",
        "col6=[]\n",
        "col7=[]\n",
        "col8=[]\n",
        "col9=[]\n",
        "col10=[]\n",
        "col11=[]\n",
        "col12=[]\n",
        "col13=[]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "eEVrBg7d-GDl"
      },
      "outputs": [],
      "source": [
        "def plot_attended_data(trainloader,net,epoch):\n",
        "    attd_data =[]\n",
        "    lbls = []\n",
        "    for data in trainloader:\n",
        "        inputs, labels , fore_idx = data\n",
        "        inputs = inputs.double()\n",
        "        inputs, labels , fore_idx = inputs.to(device),labels.to(device), fore_idx.to(device)\n",
        "        alphas, avg_images = focus_net(inputs)\n",
        "        attd_data.append(avg_images.numpy())\n",
        "        lbls.append(labels)\n",
        "    attd_data = np.concatenate(attd_data,axis=0)\n",
        "    lbls = np.concatenate(lbls,axis=0)\n",
        "    plt.figure(figsize=(6,8))\n",
        "    plt.scatter(attd_data[:,0],attd_data[:,1],c=lbls)\n",
        "    plt.title(\"EPOCH_\"+str(epoch))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "uALi25pmzQHV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5a807de-c969-4de7-a8e6-984c107d6ead"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1.1012, device='cuda:0', dtype=torch.float64)\n",
            "tensor(1.1132, device='cuda:0', dtype=torch.float64)\n",
            "tensor(1.0935, device='cuda:0', dtype=torch.float64)\n",
            "tensor(1.0927, device='cuda:0', dtype=torch.float64)\n",
            "tensor(1.1068, device='cuda:0', dtype=torch.float64)\n",
            "tensor(1.1048, device='cuda:0', dtype=torch.float64)\n",
            "tensor(1.1054, device='cuda:0', dtype=torch.float64)\n",
            "tensor(1.0836, device='cuda:0', dtype=torch.float64)\n",
            "tensor(1.1204, device='cuda:0', dtype=torch.float64)\n",
            "tensor(1.1172, device='cuda:0', dtype=torch.float64)\n",
            "====================================================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        }
      ],
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "count = 0\n",
        "flag = 1\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "  for data in train_loader:\n",
        "    inputs, labels , fore_idx = data\n",
        "    inputs = inputs.double()\n",
        "    inputs, labels , fore_idx = inputs.to(device),labels.to(device), fore_idx.to(device)\n",
        "    alphas, avg_images = focus_net(inputs)\n",
        "    outputs = classify(avg_images)\n",
        "    loss = my_cross_entropy(outputs,labels,alphas)\n",
        "    print(loss)\n",
        "    # print(outputs.shape)\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "\n",
        "print(\"=\"*100)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "4vmNprlPzTjP"
      },
      "outputs": [],
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "count = 0\n",
        "flag = 1\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "  for data in test_loader:\n",
        "    inputs, labels , fore_idx = data\n",
        "    inputs = inputs.double()\n",
        "    inputs, labels , fore_idx = inputs.to(device),labels.to(device), fore_idx.to(device)\n",
        "    alphas, avg_images = focus_net(inputs)\n",
        "    outputs = classify(avg_images)\n",
        "\n",
        "    _, predicted = torch.max(outputs.data, 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "_nvicAzw-GDm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f65e7ec-b811-476b-fbf3-44a2374406b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('fc1.weight', Parameter containing:\n",
            "tensor([[-0.4664, -0.4478]], device='cuda:0', dtype=torch.float64,\n",
            "       requires_grad=True))\n"
          ]
        }
      ],
      "source": [
        "for param in focus_net.named_parameters():\n",
        "    print(param)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "Yl41sE8vFERk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11b287d1-7fd1-44b0-8418-b943b5186d2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,     2] loss: 1.229\n",
            "[1,     4] loss: 1.124\n",
            "[1,     6] loss: 1.107\n",
            "[1,     8] loss: 1.082\n",
            "[1,    10] loss: 1.117\n",
            "[2,     2] loss: 1.112\n",
            "[2,     4] loss: 1.093\n",
            "[2,     6] loss: 1.064\n",
            "[2,     8] loss: 1.058\n",
            "[2,    10] loss: 0.999\n",
            "[3,     2] loss: 0.910\n",
            "[3,     4] loss: 0.810\n",
            "[3,     6] loss: 0.747\n",
            "[3,     8] loss: 0.639\n",
            "[3,    10] loss: 0.592\n",
            "[4,     2] loss: 0.675\n",
            "[4,     4] loss: 0.478\n",
            "[4,     6] loss: 0.553\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "[1001,     4] loss: 0.308\n",
            "[1001,     6] loss: 0.218\n",
            "[1001,     8] loss: 0.308\n",
            "[1001,    10] loss: 0.380\n",
            "[1002,     2] loss: 0.256\n",
            "[1002,     4] loss: 0.331\n",
            "[1002,     6] loss: 0.216\n",
            "[1002,     8] loss: 0.271\n",
            "[1002,    10] loss: 0.286\n",
            "[1003,     2] loss: 0.243\n",
            "[1003,     4] loss: 0.281\n",
            "[1003,     6] loss: 0.216\n",
            "[1003,     8] loss: 0.278\n",
            "[1003,    10] loss: 0.342\n",
            "[1004,     2] loss: 0.367\n",
            "[1004,     4] loss: 0.243\n",
            "[1004,     6] loss: 0.238\n",
            "[1004,     8] loss: 0.260\n",
            "[1004,    10] loss: 0.305\n",
            "[1005,     2] loss: 0.294\n",
            "[1005,     4] loss: 0.264\n",
            "[1005,     6] loss: 0.260\n",
            "[1005,     8] loss: 0.191\n",
            "[1005,    10] loss: 0.371\n",
            "[1006,     2] loss: 0.354\n",
            "[1006,     4] loss: 0.306\n",
            "[1006,     6] loss: 0.238\n",
            "[1006,     8] loss: 0.357\n",
            "[1006,    10] loss: 0.264\n",
            "[1007,     2] loss: 0.326\n",
            "[1007,     4] loss: 0.235\n",
            "[1007,     6] loss: 0.307\n",
            "[1007,     8] loss: 0.312\n",
            "[1007,    10] loss: 0.301\n",
            "[1008,     2] loss: 0.366\n",
            "[1008,     4] loss: 0.276\n",
            "[1008,     6] loss: 0.326\n",
            "[1008,     8] loss: 0.277\n",
            "[1008,    10] loss: 0.285\n",
            "[1009,     2] loss: 0.246\n",
            "[1009,     4] loss: 0.391\n",
            "[1009,     6] loss: 0.236\n",
            "[1009,     8] loss: 0.250\n",
            "[1009,    10] loss: 0.287\n",
            "[1010,     2] loss: 0.258\n",
            "[1010,     4] loss: 0.278\n",
            "[1010,     6] loss: 0.259\n",
            "[1010,     8] loss: 0.291\n",
            "[1010,    10] loss: 0.336\n",
            "[1011,     2] loss: 0.302\n",
            "[1011,     4] loss: 0.341\n",
            "[1011,     6] loss: 0.286\n",
            "[1011,     8] loss: 0.282\n",
            "[1011,    10] loss: 0.385\n",
            "[1012,     2] loss: 0.360\n",
            "[1012,     4] loss: 0.275\n",
            "[1012,     6] loss: 0.335\n",
            "[1012,     8] loss: 0.300\n",
            "[1012,    10] loss: 0.377\n",
            "[1013,     2] loss: 0.373\n",
            "[1013,     4] loss: 0.253\n",
            "[1013,     6] loss: 0.259\n",
            "[1013,     8] loss: 0.264\n",
            "[1013,    10] loss: 0.318\n",
            "[1014,     2] loss: 0.223\n",
            "[1014,     4] loss: 0.407\n",
            "[1014,     6] loss: 0.304\n",
            "[1014,     8] loss: 0.286\n",
            "[1014,    10] loss: 0.394\n",
            "[1015,     2] loss: 0.286\n",
            "[1015,     4] loss: 0.310\n",
            "[1015,     6] loss: 0.263\n",
            "[1015,     8] loss: 0.295\n",
            "[1015,    10] loss: 0.321\n",
            "[1016,     2] loss: 0.285\n",
            "[1016,     4] loss: 0.350\n",
            "[1016,     6] loss: 0.301\n",
            "[1016,     8] loss: 0.256\n",
            "[1016,    10] loss: 0.301\n",
            "[1017,     2] loss: 0.237\n",
            "[1017,     4] loss: 0.284\n",
            "[1017,     6] loss: 0.355\n",
            "[1017,     8] loss: 0.220\n",
            "[1017,    10] loss: 0.288\n",
            "[1018,     2] loss: 0.193\n",
            "[1018,     4] loss: 0.366\n",
            "[1018,     6] loss: 0.269\n",
            "[1018,     8] loss: 0.323\n",
            "[1018,    10] loss: 0.341\n",
            "[1019,     2] loss: 0.306\n",
            "[1019,     4] loss: 0.219\n",
            "[1019,     6] loss: 0.321\n",
            "[1019,     8] loss: 0.214\n",
            "[1019,    10] loss: 0.305\n",
            "[1020,     2] loss: 0.299\n",
            "[1020,     4] loss: 0.353\n",
            "[1020,     6] loss: 0.301\n",
            "[1020,     8] loss: 0.201\n",
            "[1020,    10] loss: 0.279\n",
            "[1021,     2] loss: 0.368\n",
            "[1021,     4] loss: 0.246\n",
            "[1021,     6] loss: 0.338\n",
            "[1021,     8] loss: 0.338\n",
            "[1021,    10] loss: 0.219\n",
            "[1022,     2] loss: 0.226\n",
            "[1022,     4] loss: 0.263\n",
            "[1022,     6] loss: 0.273\n",
            "[1022,     8] loss: 0.301\n",
            "[1022,    10] loss: 0.359\n",
            "[1023,     2] loss: 0.282\n",
            "[1023,     4] loss: 0.285\n",
            "[1023,     6] loss: 0.279\n",
            "[1023,     8] loss: 0.266\n",
            "[1023,    10] loss: 0.284\n",
            "[1024,     2] loss: 0.247\n",
            "[1024,     4] loss: 0.226\n",
            "[1024,     6] loss: 0.268\n",
            "[1024,     8] loss: 0.363\n",
            "[1024,    10] loss: 0.373\n",
            "[1025,     2] loss: 0.359\n",
            "[1025,     4] loss: 0.188\n",
            "[1025,     6] loss: 0.293\n",
            "[1025,     8] loss: 0.281\n",
            "[1025,    10] loss: 0.304\n",
            "[1026,     2] loss: 0.312\n",
            "[1026,     4] loss: 0.291\n",
            "[1026,     6] loss: 0.265\n",
            "[1026,     8] loss: 0.211\n",
            "[1026,    10] loss: 0.348\n",
            "[1027,     2] loss: 0.294\n",
            "[1027,     4] loss: 0.249\n",
            "[1027,     6] loss: 0.231\n",
            "[1027,     8] loss: 0.280\n",
            "[1027,    10] loss: 0.392\n",
            "[1028,     2] loss: 0.206\n",
            "[1028,     4] loss: 0.282\n",
            "[1028,     6] loss: 0.222\n",
            "[1028,     8] loss: 0.317\n",
            "[1028,    10] loss: 0.356\n",
            "[1029,     2] loss: 0.363\n",
            "[1029,     4] loss: 0.215\n",
            "[1029,     6] loss: 0.347\n",
            "[1029,     8] loss: 0.235\n",
            "[1029,    10] loss: 0.254\n",
            "[1030,     2] loss: 0.311\n",
            "[1030,     4] loss: 0.282\n",
            "[1030,     6] loss: 0.227\n",
            "[1030,     8] loss: 0.442\n",
            "[1030,    10] loss: 0.324\n",
            "[1031,     2] loss: 0.309\n",
            "[1031,     4] loss: 0.282\n",
            "[1031,     6] loss: 0.362\n",
            "[1031,     8] loss: 0.363\n",
            "[1031,    10] loss: 0.175\n",
            "[1032,     2] loss: 0.266\n",
            "[1032,     4] loss: 0.280\n",
            "[1032,     6] loss: 0.307\n",
            "[1032,     8] loss: 0.265\n",
            "[1032,    10] loss: 0.275\n",
            "[1033,     2] loss: 0.214\n",
            "[1033,     4] loss: 0.377\n",
            "[1033,     6] loss: 0.288\n",
            "[1033,     8] loss: 0.277\n",
            "[1033,    10] loss: 0.234\n",
            "[1034,     2] loss: 0.275\n",
            "[1034,     4] loss: 0.179\n",
            "[1034,     6] loss: 0.306\n",
            "[1034,     8] loss: 0.277\n",
            "[1034,    10] loss: 0.331\n",
            "[1035,     2] loss: 0.305\n",
            "[1035,     4] loss: 0.253\n",
            "[1035,     6] loss: 0.293\n",
            "[1035,     8] loss: 0.448\n",
            "[1035,    10] loss: 0.252\n",
            "[1036,     2] loss: 0.316\n",
            "[1036,     4] loss: 0.260\n",
            "[1036,     6] loss: 0.251\n",
            "[1036,     8] loss: 0.303\n",
            "[1036,    10] loss: 0.292\n",
            "[1037,     2] loss: 0.215\n",
            "[1037,     4] loss: 0.357\n",
            "[1037,     6] loss: 0.321\n",
            "[1037,     8] loss: 0.244\n",
            "[1037,    10] loss: 0.406\n",
            "[1038,     2] loss: 0.189\n",
            "[1038,     4] loss: 0.347\n",
            "[1038,     6] loss: 0.330\n",
            "[1038,     8] loss: 0.380\n",
            "[1038,    10] loss: 0.258\n",
            "[1039,     2] loss: 0.339\n",
            "[1039,     4] loss: 0.272\n",
            "[1039,     6] loss: 0.311\n",
            "[1039,     8] loss: 0.352\n",
            "[1039,    10] loss: 0.294\n",
            "[1040,     2] loss: 0.342\n",
            "[1040,     4] loss: 0.266\n",
            "[1040,     6] loss: 0.271\n",
            "[1040,     8] loss: 0.295\n",
            "[1040,    10] loss: 0.299\n",
            "[1041,     2] loss: 0.256\n",
            "[1041,     4] loss: 0.353\n",
            "[1041,     6] loss: 0.295\n",
            "[1041,     8] loss: 0.232\n",
            "[1041,    10] loss: 0.229\n",
            "[1042,     2] loss: 0.342\n",
            "[1042,     4] loss: 0.227\n",
            "[1042,     6] loss: 0.356\n",
            "[1042,     8] loss: 0.328\n",
            "[1042,    10] loss: 0.215\n",
            "[1043,     2] loss: 0.287\n",
            "[1043,     4] loss: 0.305\n",
            "[1043,     6] loss: 0.245\n",
            "[1043,     8] loss: 0.376\n",
            "[1043,    10] loss: 0.258\n",
            "[1044,     2] loss: 0.190\n",
            "[1044,     4] loss: 0.345\n",
            "[1044,     6] loss: 0.257\n",
            "[1044,     8] loss: 0.356\n",
            "[1044,    10] loss: 0.260\n",
            "[1045,     2] loss: 0.280\n",
            "[1045,     4] loss: 0.272\n",
            "[1045,     6] loss: 0.342\n",
            "[1045,     8] loss: 0.200\n",
            "[1045,    10] loss: 0.343\n",
            "[1046,     2] loss: 0.326\n",
            "[1046,     4] loss: 0.333\n",
            "[1046,     6] loss: 0.324\n",
            "[1046,     8] loss: 0.172\n",
            "[1046,    10] loss: 0.296\n",
            "[1047,     2] loss: 0.264\n",
            "[1047,     4] loss: 0.218\n",
            "[1047,     6] loss: 0.340\n",
            "[1047,     8] loss: 0.298\n",
            "[1047,    10] loss: 0.301\n",
            "[1048,     2] loss: 0.254\n",
            "[1048,     4] loss: 0.280\n",
            "[1048,     6] loss: 0.291\n",
            "[1048,     8] loss: 0.312\n",
            "[1048,    10] loss: 0.267\n",
            "[1049,     2] loss: 0.315\n",
            "[1049,     4] loss: 0.248\n",
            "[1049,     6] loss: 0.285\n",
            "[1049,     8] loss: 0.267\n",
            "[1049,    10] loss: 0.273\n",
            "[1050,     2] loss: 0.334\n",
            "[1050,     4] loss: 0.196\n",
            "[1050,     6] loss: 0.255\n",
            "[1050,     8] loss: 0.232\n",
            "[1050,    10] loss: 0.348\n",
            "[1051,     2] loss: 0.348\n",
            "[1051,     4] loss: 0.239\n",
            "[1051,     6] loss: 0.309\n",
            "[1051,     8] loss: 0.221\n",
            "[1051,    10] loss: 0.238\n",
            "[1052,     2] loss: 0.244\n",
            "[1052,     4] loss: 0.276\n",
            "[1052,     6] loss: 0.268\n",
            "[1052,     8] loss: 0.303\n",
            "[1052,    10] loss: 0.260\n",
            "[1053,     2] loss: 0.300\n",
            "[1053,     4] loss: 0.300\n",
            "[1053,     6] loss: 0.292\n",
            "[1053,     8] loss: 0.341\n",
            "[1053,    10] loss: 0.239\n",
            "[1054,     2] loss: 0.310\n",
            "[1054,     4] loss: 0.330\n",
            "[1054,     6] loss: 0.255\n",
            "[1054,     8] loss: 0.199\n",
            "[1054,    10] loss: 0.321\n",
            "[1055,     2] loss: 0.292\n",
            "[1055,     4] loss: 0.279\n",
            "[1055,     6] loss: 0.183\n",
            "[1055,     8] loss: 0.281\n",
            "[1055,    10] loss: 0.316\n",
            "[1056,     2] loss: 0.281\n",
            "[1056,     4] loss: 0.234\n",
            "[1056,     6] loss: 0.266\n",
            "[1056,     8] loss: 0.327\n",
            "[1056,    10] loss: 0.319\n",
            "[1057,     2] loss: 0.241\n",
            "[1057,     4] loss: 0.310\n",
            "[1057,     6] loss: 0.290\n",
            "[1057,     8] loss: 0.236\n",
            "[1057,    10] loss: 0.289\n",
            "[1058,     2] loss: 0.287\n",
            "[1058,     4] loss: 0.280\n",
            "[1058,     6] loss: 0.229\n",
            "[1058,     8] loss: 0.276\n",
            "[1058,    10] loss: 0.356\n",
            "[1059,     2] loss: 0.233\n",
            "[1059,     4] loss: 0.272\n",
            "[1059,     6] loss: 0.324\n",
            "[1059,     8] loss: 0.274\n",
            "[1059,    10] loss: 0.248\n",
            "[1060,     2] loss: 0.198\n",
            "[1060,     4] loss: 0.320\n",
            "[1060,     6] loss: 0.318\n",
            "[1060,     8] loss: 0.261\n",
            "[1060,    10] loss: 0.404\n",
            "[1061,     2] loss: 0.231\n",
            "[1061,     4] loss: 0.314\n",
            "[1061,     6] loss: 0.408\n",
            "[1061,     8] loss: 0.261\n",
            "[1061,    10] loss: 0.379\n",
            "[1062,     2] loss: 0.291\n",
            "[1062,     4] loss: 0.244\n",
            "[1062,     6] loss: 0.365\n",
            "[1062,     8] loss: 0.323\n",
            "[1062,    10] loss: 0.308\n",
            "[1063,     2] loss: 0.325\n",
            "[1063,     4] loss: 0.263\n",
            "[1063,     6] loss: 0.365\n",
            "[1063,     8] loss: 0.305\n",
            "[1063,    10] loss: 0.333\n",
            "[1064,     2] loss: 0.244\n",
            "[1064,     4] loss: 0.271\n",
            "[1064,     6] loss: 0.304\n",
            "[1064,     8] loss: 0.251\n",
            "[1064,    10] loss: 0.270\n",
            "[1065,     2] loss: 0.358\n",
            "[1065,     4] loss: 0.232\n",
            "[1065,     6] loss: 0.376\n",
            "[1065,     8] loss: 0.165\n",
            "[1065,    10] loss: 0.302\n",
            "[1066,     2] loss: 0.279\n",
            "[1066,     4] loss: 0.238\n",
            "[1066,     6] loss: 0.309\n",
            "[1066,     8] loss: 0.313\n",
            "[1066,    10] loss: 0.257\n",
            "[1067,     2] loss: 0.206\n",
            "[1067,     4] loss: 0.281\n",
            "[1067,     6] loss: 0.222\n",
            "[1067,     8] loss: 0.345\n",
            "[1067,    10] loss: 0.296\n",
            "[1068,     2] loss: 0.214\n",
            "[1068,     4] loss: 0.275\n",
            "[1068,     6] loss: 0.394\n",
            "[1068,     8] loss: 0.302\n",
            "[1068,    10] loss: 0.216\n",
            "[1069,     2] loss: 0.283\n",
            "[1069,     4] loss: 0.338\n",
            "[1069,     6] loss: 0.259\n",
            "[1069,     8] loss: 0.283\n",
            "[1069,    10] loss: 0.369\n",
            "[1070,     2] loss: 0.234\n",
            "[1070,     4] loss: 0.352\n",
            "[1070,     6] loss: 0.424\n",
            "[1070,     8] loss: 0.250\n",
            "[1070,    10] loss: 0.261\n",
            "[1071,     2] loss: 0.300\n",
            "[1071,     4] loss: 0.300\n",
            "[1071,     6] loss: 0.326\n",
            "[1071,     8] loss: 0.256\n",
            "[1071,    10] loss: 0.270\n",
            "[1072,     2] loss: 0.370\n",
            "[1072,     4] loss: 0.271\n",
            "[1072,     6] loss: 0.233\n",
            "[1072,     8] loss: 0.310\n",
            "[1072,    10] loss: 0.263\n",
            "[1073,     2] loss: 0.337\n",
            "[1073,     4] loss: 0.275\n",
            "[1073,     6] loss: 0.250\n",
            "[1073,     8] loss: 0.238\n",
            "[1073,    10] loss: 0.368\n",
            "[1074,     2] loss: 0.332\n",
            "[1074,     4] loss: 0.249\n",
            "[1074,     6] loss: 0.264\n",
            "[1074,     8] loss: 0.322\n",
            "[1074,    10] loss: 0.367\n",
            "[1075,     2] loss: 0.269\n",
            "[1075,     4] loss: 0.292\n",
            "[1075,     6] loss: 0.202\n",
            "[1075,     8] loss: 0.347\n",
            "[1075,    10] loss: 0.310\n",
            "[1076,     2] loss: 0.374\n",
            "[1076,     4] loss: 0.315\n",
            "[1076,     6] loss: 0.307\n",
            "[1076,     8] loss: 0.163\n",
            "[1076,    10] loss: 0.250\n",
            "[1077,     2] loss: 0.253\n",
            "[1077,     4] loss: 0.306\n",
            "[1077,     6] loss: 0.273\n",
            "[1077,     8] loss: 0.299\n",
            "[1077,    10] loss: 0.298\n",
            "[1078,     2] loss: 0.371\n",
            "[1078,     4] loss: 0.262\n",
            "[1078,     6] loss: 0.252\n",
            "[1078,     8] loss: 0.302\n",
            "[1078,    10] loss: 0.241\n",
            "[1079,     2] loss: 0.299\n",
            "[1079,     4] loss: 0.334\n",
            "[1079,     6] loss: 0.229\n",
            "[1079,     8] loss: 0.304\n",
            "[1079,    10] loss: 0.254\n",
            "[1080,     2] loss: 0.327\n",
            "[1080,     4] loss: 0.250\n",
            "[1080,     6] loss: 0.280\n",
            "[1080,     8] loss: 0.281\n",
            "[1080,    10] loss: 0.267\n",
            "[1081,     2] loss: 0.263\n",
            "[1081,     4] loss: 0.269\n",
            "[1081,     6] loss: 0.339\n",
            "[1081,     8] loss: 0.275\n",
            "[1081,    10] loss: 0.258\n",
            "[1082,     2] loss: 0.338\n",
            "[1082,     4] loss: 0.218\n",
            "[1082,     6] loss: 0.249\n",
            "[1082,     8] loss: 0.341\n",
            "[1082,    10] loss: 0.202\n",
            "[1083,     2] loss: 0.256\n",
            "[1083,     4] loss: 0.286\n",
            "[1083,     6] loss: 0.224\n",
            "[1083,     8] loss: 0.234\n",
            "[1083,    10] loss: 0.339\n",
            "[1084,     2] loss: 0.304\n",
            "[1084,     4] loss: 0.280\n",
            "[1084,     6] loss: 0.297\n",
            "[1084,     8] loss: 0.220\n",
            "[1084,    10] loss: 0.301\n",
            "[1085,     2] loss: 0.365\n",
            "[1085,     4] loss: 0.268\n",
            "[1085,     6] loss: 0.272\n",
            "[1085,     8] loss: 0.144\n",
            "[1085,    10] loss: 0.348\n",
            "[1086,     2] loss: 0.235\n",
            "[1086,     4] loss: 0.392\n",
            "[1086,     6] loss: 0.295\n",
            "[1086,     8] loss: 0.280\n",
            "[1086,    10] loss: 0.213\n",
            "[1087,     2] loss: 0.300\n",
            "[1087,     4] loss: 0.279\n",
            "[1087,     6] loss: 0.276\n",
            "[1087,     8] loss: 0.210\n",
            "[1087,    10] loss: 0.358\n",
            "[1088,     2] loss: 0.313\n",
            "[1088,     4] loss: 0.239\n",
            "[1088,     6] loss: 0.293\n",
            "[1088,     8] loss: 0.312\n",
            "[1088,    10] loss: 0.226\n",
            "[1089,     2] loss: 0.278\n",
            "[1089,     4] loss: 0.244\n",
            "[1089,     6] loss: 0.273\n",
            "[1089,     8] loss: 0.298\n",
            "[1089,    10] loss: 0.253\n",
            "[1090,     2] loss: 0.266\n",
            "[1090,     4] loss: 0.281\n",
            "[1090,     6] loss: 0.286\n",
            "[1090,     8] loss: 0.186\n",
            "[1090,    10] loss: 0.325\n",
            "[1091,     2] loss: 0.282\n",
            "[1091,     4] loss: 0.309\n",
            "[1091,     6] loss: 0.363\n",
            "[1091,     8] loss: 0.277\n",
            "[1091,    10] loss: 0.231\n",
            "[1092,     2] loss: 0.273\n",
            "[1092,     4] loss: 0.292\n",
            "[1092,     6] loss: 0.293\n",
            "[1092,     8] loss: 0.325\n",
            "[1092,    10] loss: 0.268\n",
            "[1093,     2] loss: 0.287\n",
            "[1093,     4] loss: 0.231\n",
            "[1093,     6] loss: 0.252\n",
            "[1093,     8] loss: 0.257\n",
            "[1093,    10] loss: 0.339\n",
            "[1094,     2] loss: 0.306\n",
            "[1094,     4] loss: 0.293\n",
            "[1094,     6] loss: 0.272\n",
            "[1094,     8] loss: 0.236\n",
            "[1094,    10] loss: 0.281\n",
            "[1095,     2] loss: 0.388\n",
            "[1095,     4] loss: 0.399\n",
            "[1095,     6] loss: 0.173\n",
            "[1095,     8] loss: 0.234\n",
            "[1095,    10] loss: 0.378\n",
            "[1096,     2] loss: 0.207\n",
            "[1096,     4] loss: 0.344\n",
            "[1096,     6] loss: 0.380\n",
            "[1096,     8] loss: 0.323\n",
            "[1096,    10] loss: 0.230\n",
            "[1097,     2] loss: 0.289\n",
            "[1097,     4] loss: 0.360\n",
            "[1097,     6] loss: 0.297\n",
            "[1097,     8] loss: 0.248\n",
            "[1097,    10] loss: 0.263\n",
            "[1098,     2] loss: 0.315\n",
            "[1098,     4] loss: 0.338\n",
            "[1098,     6] loss: 0.247\n",
            "[1098,     8] loss: 0.298\n",
            "[1098,    10] loss: 0.253\n",
            "[1099,     2] loss: 0.281\n",
            "[1099,     4] loss: 0.284\n",
            "[1099,     6] loss: 0.209\n",
            "[1099,     8] loss: 0.262\n",
            "[1099,    10] loss: 0.403\n",
            "[1100,     2] loss: 0.228\n",
            "[1100,     4] loss: 0.267\n",
            "[1100,     6] loss: 0.285\n",
            "[1100,     8] loss: 0.268\n",
            "[1100,    10] loss: 0.367\n",
            "[1101,     2] loss: 0.220\n",
            "[1101,     4] loss: 0.277\n",
            "[1101,     6] loss: 0.418\n",
            "[1101,     8] loss: 0.262\n",
            "[1101,    10] loss: 0.295\n",
            "[1102,     2] loss: 0.300\n",
            "[1102,     4] loss: 0.290\n",
            "[1102,     6] loss: 0.283\n",
            "[1102,     8] loss: 0.269\n",
            "[1102,    10] loss: 0.309\n",
            "[1103,     2] loss: 0.316\n",
            "[1103,     4] loss: 0.269\n",
            "[1103,     6] loss: 0.232\n",
            "[1103,     8] loss: 0.305\n",
            "[1103,    10] loss: 0.257\n",
            "[1104,     2] loss: 0.213\n",
            "[1104,     4] loss: 0.252\n",
            "[1104,     6] loss: 0.208\n",
            "[1104,     8] loss: 0.351\n",
            "[1104,    10] loss: 0.342\n",
            "[1105,     2] loss: 0.255\n",
            "[1105,     4] loss: 0.363\n",
            "[1105,     6] loss: 0.263\n",
            "[1105,     8] loss: 0.247\n",
            "[1105,    10] loss: 0.264\n",
            "[1106,     2] loss: 0.317\n",
            "[1106,     4] loss: 0.263\n",
            "[1106,     6] loss: 0.310\n",
            "[1106,     8] loss: 0.274\n",
            "[1106,    10] loss: 0.216\n",
            "[1107,     2] loss: 0.227\n",
            "[1107,     4] loss: 0.304\n",
            "[1107,     6] loss: 0.363\n",
            "[1107,     8] loss: 0.228\n",
            "[1107,    10] loss: 0.307\n",
            "[1108,     2] loss: 0.240\n",
            "[1108,     4] loss: 0.277\n",
            "[1108,     6] loss: 0.256\n",
            "[1108,     8] loss: 0.309\n",
            "[1108,    10] loss: 0.321\n",
            "[1109,     2] loss: 0.193\n",
            "[1109,     4] loss: 0.372\n",
            "[1109,     6] loss: 0.253\n",
            "[1109,     8] loss: 0.240\n",
            "[1109,    10] loss: 0.430\n",
            "[1110,     2] loss: 0.274\n",
            "[1110,     4] loss: 0.399\n",
            "[1110,     6] loss: 0.393\n",
            "[1110,     8] loss: 0.271\n",
            "[1110,    10] loss: 0.341\n",
            "[1111,     2] loss: 0.328\n",
            "[1111,     4] loss: 0.280\n",
            "[1111,     6] loss: 0.290\n",
            "[1111,     8] loss: 0.335\n",
            "[1111,    10] loss: 0.344\n",
            "[1112,     2] loss: 0.312\n",
            "[1112,     4] loss: 0.263\n",
            "[1112,     6] loss: 0.330\n",
            "[1112,     8] loss: 0.320\n",
            "[1112,    10] loss: 0.262\n",
            "[1113,     2] loss: 0.287\n",
            "[1113,     4] loss: 0.317\n",
            "[1113,     6] loss: 0.303\n",
            "[1113,     8] loss: 0.284\n",
            "[1113,    10] loss: 0.216\n",
            "[1114,     2] loss: 0.264\n",
            "[1114,     4] loss: 0.285\n",
            "[1114,     6] loss: 0.269\n",
            "[1114,     8] loss: 0.329\n",
            "[1114,    10] loss: 0.277\n",
            "[1115,     2] loss: 0.283\n",
            "[1115,     4] loss: 0.276\n",
            "[1115,     6] loss: 0.405\n",
            "[1115,     8] loss: 0.351\n",
            "[1115,    10] loss: 0.282\n",
            "[1116,     2] loss: 0.306\n",
            "[1116,     4] loss: 0.188\n",
            "[1116,     6] loss: 0.335\n",
            "[1116,     8] loss: 0.231\n",
            "[1116,    10] loss: 0.295\n",
            "[1117,     2] loss: 0.301\n",
            "[1117,     4] loss: 0.300\n",
            "[1117,     6] loss: 0.226\n",
            "[1117,     8] loss: 0.276\n",
            "[1117,    10] loss: 0.281\n",
            "[1118,     2] loss: 0.296\n",
            "[1118,     4] loss: 0.302\n",
            "[1118,     6] loss: 0.278\n",
            "[1118,     8] loss: 0.227\n",
            "[1118,    10] loss: 0.237\n",
            "[1119,     2] loss: 0.277\n",
            "[1119,     4] loss: 0.266\n",
            "[1119,     6] loss: 0.321\n",
            "[1119,     8] loss: 0.283\n",
            "[1119,    10] loss: 0.231\n",
            "[1120,     2] loss: 0.280\n",
            "[1120,     4] loss: 0.243\n",
            "[1120,     6] loss: 0.283\n",
            "[1120,     8] loss: 0.246\n",
            "[1120,    10] loss: 0.339\n",
            "[1121,     2] loss: 0.286\n",
            "[1121,     4] loss: 0.375\n",
            "[1121,     6] loss: 0.201\n",
            "[1121,     8] loss: 0.245\n",
            "[1121,    10] loss: 0.398\n",
            "[1122,     2] loss: 0.266\n",
            "[1122,     4] loss: 0.341\n",
            "[1122,     6] loss: 0.297\n",
            "[1122,     8] loss: 0.272\n",
            "[1122,    10] loss: 0.411\n",
            "[1123,     2] loss: 0.315\n",
            "[1123,     4] loss: 0.286\n",
            "[1123,     6] loss: 0.359\n",
            "[1123,     8] loss: 0.213\n",
            "[1123,    10] loss: 0.331\n",
            "[1124,     2] loss: 0.283\n",
            "[1124,     4] loss: 0.257\n",
            "[1124,     6] loss: 0.259\n",
            "[1124,     8] loss: 0.220\n",
            "[1124,    10] loss: 0.383\n",
            "[1125,     2] loss: 0.256\n",
            "[1125,     4] loss: 0.221\n",
            "[1125,     6] loss: 0.374\n",
            "[1125,     8] loss: 0.292\n",
            "[1125,    10] loss: 0.268\n",
            "[1126,     2] loss: 0.280\n",
            "[1126,     4] loss: 0.310\n",
            "[1126,     6] loss: 0.297\n",
            "[1126,     8] loss: 0.250\n",
            "[1126,    10] loss: 0.277\n",
            "[1127,     2] loss: 0.248\n",
            "[1127,     4] loss: 0.278\n",
            "[1127,     6] loss: 0.293\n",
            "[1127,     8] loss: 0.339\n",
            "[1127,    10] loss: 0.214\n",
            "[1128,     2] loss: 0.360\n",
            "[1128,     4] loss: 0.274\n",
            "[1128,     6] loss: 0.224\n",
            "[1128,     8] loss: 0.299\n",
            "[1128,    10] loss: 0.277\n",
            "[1129,     2] loss: 0.358\n",
            "[1129,     4] loss: 0.226\n",
            "[1129,     6] loss: 0.328\n",
            "[1129,     8] loss: 0.303\n",
            "[1129,    10] loss: 0.309\n",
            "[1130,     2] loss: 0.254\n",
            "[1130,     4] loss: 0.348\n",
            "[1130,     6] loss: 0.418\n",
            "[1130,     8] loss: 0.266\n",
            "[1130,    10] loss: 0.389\n",
            "[1131,     2] loss: 0.270\n",
            "[1131,     4] loss: 0.278\n",
            "[1131,     6] loss: 0.286\n",
            "[1131,     8] loss: 0.424\n",
            "[1131,    10] loss: 0.229\n",
            "[1132,     2] loss: 0.202\n",
            "[1132,     4] loss: 0.378\n",
            "[1132,     6] loss: 0.339\n",
            "[1132,     8] loss: 0.314\n",
            "[1132,    10] loss: 0.261\n",
            "[1133,     2] loss: 0.197\n",
            "[1133,     4] loss: 0.226\n",
            "[1133,     6] loss: 0.365\n",
            "[1133,     8] loss: 0.352\n",
            "[1133,    10] loss: 0.260\n",
            "[1134,     2] loss: 0.265\n",
            "[1134,     4] loss: 0.236\n",
            "[1134,     6] loss: 0.382\n",
            "[1134,     8] loss: 0.270\n",
            "[1134,    10] loss: 0.279\n",
            "[1135,     2] loss: 0.290\n",
            "[1135,     4] loss: 0.278\n",
            "[1135,     6] loss: 0.260\n",
            "[1135,     8] loss: 0.291\n",
            "[1135,    10] loss: 0.236\n",
            "[1136,     2] loss: 0.193\n",
            "[1136,     4] loss: 0.233\n",
            "[1136,     6] loss: 0.385\n",
            "[1136,     8] loss: 0.237\n",
            "[1136,    10] loss: 0.301\n",
            "[1137,     2] loss: 0.244\n",
            "[1137,     4] loss: 0.288\n",
            "[1137,     6] loss: 0.276\n",
            "[1137,     8] loss: 0.278\n",
            "[1137,    10] loss: 0.281\n",
            "[1138,     2] loss: 0.224\n",
            "[1138,     4] loss: 0.304\n",
            "[1138,     6] loss: 0.367\n",
            "[1138,     8] loss: 0.226\n",
            "[1138,    10] loss: 0.269\n",
            "[1139,     2] loss: 0.333\n",
            "[1139,     4] loss: 0.304\n",
            "[1139,     6] loss: 0.256\n",
            "[1139,     8] loss: 0.270\n",
            "[1139,    10] loss: 0.266\n",
            "[1140,     2] loss: 0.209\n",
            "[1140,     4] loss: 0.298\n",
            "[1140,     6] loss: 0.353\n",
            "[1140,     8] loss: 0.255\n",
            "[1140,    10] loss: 0.312\n",
            "[1141,     2] loss: 0.221\n",
            "[1141,     4] loss: 0.327\n",
            "[1141,     6] loss: 0.301\n",
            "[1141,     8] loss: 0.305\n",
            "[1141,    10] loss: 0.310\n",
            "[1142,     2] loss: 0.277\n",
            "[1142,     4] loss: 0.281\n",
            "[1142,     6] loss: 0.362\n",
            "[1142,     8] loss: 0.286\n",
            "[1142,    10] loss: 0.299\n",
            "[1143,     2] loss: 0.344\n",
            "[1143,     4] loss: 0.350\n",
            "[1143,     6] loss: 0.303\n",
            "[1143,     8] loss: 0.293\n",
            "[1143,    10] loss: 0.286\n",
            "[1144,     2] loss: 0.230\n",
            "[1144,     4] loss: 0.339\n",
            "[1144,     6] loss: 0.275\n",
            "[1144,     8] loss: 0.333\n",
            "[1144,    10] loss: 0.229\n",
            "[1145,     2] loss: 0.252\n",
            "[1145,     4] loss: 0.317\n",
            "[1145,     6] loss: 0.264\n",
            "[1145,     8] loss: 0.263\n",
            "[1145,    10] loss: 0.278\n",
            "[1146,     2] loss: 0.197\n",
            "[1146,     4] loss: 0.226\n",
            "[1146,     6] loss: 0.395\n",
            "[1146,     8] loss: 0.314\n",
            "[1146,    10] loss: 0.369\n",
            "[1147,     2] loss: 0.324\n",
            "[1147,     4] loss: 0.292\n",
            "[1147,     6] loss: 0.270\n",
            "[1147,     8] loss: 0.238\n",
            "[1147,    10] loss: 0.239\n",
            "[1148,     2] loss: 0.258\n",
            "[1148,     4] loss: 0.254\n",
            "[1148,     6] loss: 0.238\n",
            "[1148,     8] loss: 0.278\n",
            "[1148,    10] loss: 0.326\n",
            "[1149,     2] loss: 0.296\n",
            "[1149,     4] loss: 0.258\n",
            "[1149,     6] loss: 0.272\n",
            "[1149,     8] loss: 0.219\n",
            "[1149,    10] loss: 0.319\n",
            "[1150,     2] loss: 0.308\n",
            "[1150,     4] loss: 0.273\n",
            "[1150,     6] loss: 0.267\n",
            "[1150,     8] loss: 0.290\n",
            "[1150,    10] loss: 0.246\n",
            "[1151,     2] loss: 0.333\n",
            "[1151,     4] loss: 0.324\n",
            "[1151,     6] loss: 0.273\n",
            "[1151,     8] loss: 0.280\n",
            "[1151,    10] loss: 0.205\n",
            "[1152,     2] loss: 0.256\n",
            "[1152,     4] loss: 0.259\n",
            "[1152,     6] loss: 0.332\n",
            "[1152,     8] loss: 0.279\n",
            "[1152,    10] loss: 0.252\n",
            "[1153,     2] loss: 0.331\n",
            "[1153,     4] loss: 0.346\n",
            "[1153,     6] loss: 0.238\n",
            "[1153,     8] loss: 0.266\n",
            "[1153,    10] loss: 0.234\n",
            "[1154,     2] loss: 0.330\n",
            "[1154,     4] loss: 0.260\n",
            "[1154,     6] loss: 0.304\n",
            "[1154,     8] loss: 0.212\n",
            "[1154,    10] loss: 0.538\n",
            "[1155,     2] loss: 0.365\n",
            "[1155,     4] loss: 0.236\n",
            "[1155,     6] loss: 0.407\n",
            "[1155,     8] loss: 0.280\n",
            "[1155,    10] loss: 0.215\n",
            "[1156,     2] loss: 0.224\n",
            "[1156,     4] loss: 0.334\n",
            "[1156,     6] loss: 0.307\n",
            "[1156,     8] loss: 0.373\n",
            "[1156,    10] loss: 0.346\n",
            "[1157,     2] loss: 0.261\n",
            "[1157,     4] loss: 0.355\n",
            "[1157,     6] loss: 0.259\n",
            "[1157,     8] loss: 0.288\n",
            "[1157,    10] loss: 0.339\n",
            "[1158,     2] loss: 0.247\n",
            "[1158,     4] loss: 0.326\n",
            "[1158,     6] loss: 0.272\n",
            "[1158,     8] loss: 0.297\n",
            "[1158,    10] loss: 0.251\n",
            "[1159,     2] loss: 0.257\n",
            "[1159,     4] loss: 0.322\n",
            "[1159,     6] loss: 0.209\n",
            "[1159,     8] loss: 0.220\n",
            "[1159,    10] loss: 0.447\n",
            "[1160,     2] loss: 0.255\n",
            "[1160,     4] loss: 0.347\n",
            "[1160,     6] loss: 0.248\n",
            "[1160,     8] loss: 0.221\n",
            "[1160,    10] loss: 0.362\n",
            "[1161,     2] loss: 0.264\n",
            "[1161,     4] loss: 0.397\n",
            "[1161,     6] loss: 0.277\n",
            "[1161,     8] loss: 0.330\n",
            "[1161,    10] loss: 0.219\n",
            "[1162,     2] loss: 0.299\n",
            "[1162,     4] loss: 0.212\n",
            "[1162,     6] loss: 0.232\n",
            "[1162,     8] loss: 0.306\n",
            "[1162,    10] loss: 0.293\n",
            "[1163,     2] loss: 0.344\n",
            "[1163,     4] loss: 0.312\n",
            "[1163,     6] loss: 0.217\n",
            "[1163,     8] loss: 0.220\n",
            "[1163,    10] loss: 0.369\n",
            "[1164,     2] loss: 0.196\n",
            "[1164,     4] loss: 0.252\n",
            "[1164,     6] loss: 0.247\n",
            "[1164,     8] loss: 0.341\n",
            "[1164,    10] loss: 0.315\n",
            "[1165,     2] loss: 0.245\n",
            "[1165,     4] loss: 0.331\n",
            "[1165,     6] loss: 0.260\n",
            "[1165,     8] loss: 0.279\n",
            "[1165,    10] loss: 0.254\n",
            "[1166,     2] loss: 0.315\n",
            "[1166,     4] loss: 0.287\n",
            "[1166,     6] loss: 0.272\n",
            "[1166,     8] loss: 0.241\n",
            "[1166,    10] loss: 0.280\n",
            "[1167,     2] loss: 0.353\n",
            "[1167,     4] loss: 0.196\n",
            "[1167,     6] loss: 0.263\n",
            "[1167,     8] loss: 0.318\n",
            "[1167,    10] loss: 0.277\n",
            "[1168,     2] loss: 0.257\n",
            "[1168,     4] loss: 0.312\n",
            "[1168,     6] loss: 0.243\n",
            "[1168,     8] loss: 0.343\n",
            "[1168,    10] loss: 0.223\n",
            "[1169,     2] loss: 0.259\n",
            "[1169,     4] loss: 0.253\n",
            "[1169,     6] loss: 0.333\n",
            "[1169,     8] loss: 0.272\n",
            "[1169,    10] loss: 0.275\n",
            "[1170,     2] loss: 0.332\n",
            "[1170,     4] loss: 0.315\n",
            "[1170,     6] loss: 0.213\n",
            "[1170,     8] loss: 0.241\n",
            "[1170,    10] loss: 0.390\n",
            "[1171,     2] loss: 0.224\n",
            "[1171,     4] loss: 0.420\n",
            "[1171,     6] loss: 0.311\n",
            "[1171,     8] loss: 0.429\n",
            "[1171,    10] loss: 0.232\n",
            "[1172,     2] loss: 0.281\n",
            "[1172,     4] loss: 0.320\n",
            "[1172,     6] loss: 0.397\n",
            "[1172,     8] loss: 0.322\n",
            "[1172,    10] loss: 0.311\n",
            "[1173,     2] loss: 0.310\n",
            "[1173,     4] loss: 0.370\n",
            "[1173,     6] loss: 0.336\n",
            "[1173,     8] loss: 0.326\n",
            "[1173,    10] loss: 0.256\n",
            "[1174,     2] loss: 0.286\n",
            "[1174,     4] loss: 0.247\n",
            "[1174,     6] loss: 0.335\n",
            "[1174,     8] loss: 0.304\n",
            "[1174,    10] loss: 0.247\n",
            "[1175,     2] loss: 0.254\n",
            "[1175,     4] loss: 0.226\n",
            "[1175,     6] loss: 0.337\n",
            "[1175,     8] loss: 0.281\n",
            "[1175,    10] loss: 0.335\n",
            "[1176,     2] loss: 0.201\n",
            "[1176,     4] loss: 0.319\n",
            "[1176,     6] loss: 0.273\n",
            "[1176,     8] loss: 0.338\n",
            "[1176,    10] loss: 0.273\n",
            "[1177,     2] loss: 0.216\n",
            "[1177,     4] loss: 0.200\n",
            "[1177,     6] loss: 0.257\n",
            "[1177,     8] loss: 0.298\n",
            "[1177,    10] loss: 0.419\n",
            "[1178,     2] loss: 0.211\n",
            "[1178,     4] loss: 0.327\n",
            "[1178,     6] loss: 0.219\n",
            "[1178,     8] loss: 0.329\n",
            "[1178,    10] loss: 0.281\n",
            "[1179,     2] loss: 0.269\n",
            "[1179,     4] loss: 0.284\n",
            "[1179,     6] loss: 0.259\n",
            "[1179,     8] loss: 0.371\n",
            "[1179,    10] loss: 0.322\n",
            "[1180,     2] loss: 0.306\n",
            "[1180,     4] loss: 0.344\n",
            "[1180,     6] loss: 0.267\n",
            "[1180,     8] loss: 0.260\n",
            "[1180,    10] loss: 0.234\n",
            "[1181,     2] loss: 0.215\n",
            "[1181,     4] loss: 0.346\n",
            "[1181,     6] loss: 0.248\n",
            "[1181,     8] loss: 0.296\n",
            "[1181,    10] loss: 0.273\n",
            "[1182,     2] loss: 0.254\n",
            "[1182,     4] loss: 0.301\n",
            "[1182,     6] loss: 0.237\n",
            "[1182,     8] loss: 0.294\n",
            "[1182,    10] loss: 0.262\n",
            "[1183,     2] loss: 0.233\n",
            "[1183,     4] loss: 0.283\n",
            "[1183,     6] loss: 0.312\n",
            "[1183,     8] loss: 0.285\n",
            "[1183,    10] loss: 0.286\n",
            "[1184,     2] loss: 0.286\n",
            "[1184,     4] loss: 0.339\n",
            "[1184,     6] loss: 0.327\n",
            "[1184,     8] loss: 0.268\n",
            "[1184,    10] loss: 0.256\n",
            "[1185,     2] loss: 0.247\n",
            "[1185,     4] loss: 0.315\n",
            "[1185,     6] loss: 0.309\n",
            "[1185,     8] loss: 0.253\n",
            "[1185,    10] loss: 0.262\n",
            "[1186,     2] loss: 0.308\n",
            "[1186,     4] loss: 0.270\n",
            "[1186,     6] loss: 0.256\n",
            "[1186,     8] loss: 0.266\n",
            "[1186,    10] loss: 0.262\n",
            "[1187,     2] loss: 0.290\n",
            "[1187,     4] loss: 0.348\n",
            "[1187,     6] loss: 0.201\n",
            "[1187,     8] loss: 0.190\n",
            "[1187,    10] loss: 0.359\n",
            "[1188,     2] loss: 0.318\n",
            "[1188,     4] loss: 0.300\n",
            "[1188,     6] loss: 0.242\n",
            "[1188,     8] loss: 0.249\n",
            "[1188,    10] loss: 0.313\n",
            "[1189,     2] loss: 0.279\n",
            "[1189,     4] loss: 0.251\n",
            "[1189,     6] loss: 0.270\n",
            "[1189,     8] loss: 0.317\n",
            "[1189,    10] loss: 0.267\n",
            "[1190,     2] loss: 0.226\n",
            "[1190,     4] loss: 0.264\n",
            "[1190,     6] loss: 0.302\n",
            "[1190,     8] loss: 0.308\n",
            "[1190,    10] loss: 0.339\n",
            "[1191,     2] loss: 0.253\n",
            "[1191,     4] loss: 0.253\n",
            "[1191,     6] loss: 0.317\n",
            "[1191,     8] loss: 0.201\n",
            "[1191,    10] loss: 0.340\n",
            "[1192,     2] loss: 0.303\n",
            "[1192,     4] loss: 0.229\n",
            "[1192,     6] loss: 0.250\n",
            "[1192,     8] loss: 0.322\n",
            "[1192,    10] loss: 0.300\n",
            "[1193,     2] loss: 0.380\n",
            "[1193,     4] loss: 0.215\n",
            "[1193,     6] loss: 0.298\n",
            "[1193,     8] loss: 0.245\n",
            "[1193,    10] loss: 0.234\n",
            "[1194,     2] loss: 0.230\n",
            "[1194,     4] loss: 0.329\n",
            "[1194,     6] loss: 0.307\n",
            "[1194,     8] loss: 0.252\n",
            "[1194,    10] loss: 0.415\n",
            "[1195,     2] loss: 0.224\n",
            "[1195,     4] loss: 0.270\n",
            "[1195,     6] loss: 0.322\n",
            "[1195,     8] loss: 0.249\n",
            "[1195,    10] loss: 0.403\n",
            "[1196,     2] loss: 0.308\n",
            "[1196,     4] loss: 0.244\n",
            "[1196,     6] loss: 0.262\n",
            "[1196,     8] loss: 0.336\n",
            "[1196,    10] loss: 0.278\n",
            "[1197,     2] loss: 0.349\n",
            "[1197,     4] loss: 0.218\n",
            "[1197,     6] loss: 0.349\n",
            "[1197,     8] loss: 0.212\n",
            "[1197,    10] loss: 0.252\n",
            "[1198,     2] loss: 0.276\n",
            "[1198,     4] loss: 0.253\n",
            "[1198,     6] loss: 0.271\n",
            "[1198,     8] loss: 0.301\n",
            "[1198,    10] loss: 0.258\n",
            "[1199,     2] loss: 0.255\n",
            "[1199,     4] loss: 0.277\n",
            "[1199,     6] loss: 0.279\n",
            "[1199,     8] loss: 0.317\n",
            "[1199,    10] loss: 0.267\n",
            "[1200,     2] loss: 0.244\n",
            "[1200,     4] loss: 0.226\n",
            "[1200,     6] loss: 0.349\n",
            "[1200,     8] loss: 0.249\n",
            "[1200,    10] loss: 0.314\n",
            "[1201,     2] loss: 0.240\n",
            "[1201,     4] loss: 0.202\n",
            "[1201,     6] loss: 0.254\n",
            "[1201,     8] loss: 0.299\n",
            "[1201,    10] loss: 0.352\n",
            "[1202,     2] loss: 0.208\n",
            "[1202,     4] loss: 0.331\n",
            "[1202,     6] loss: 0.362\n",
            "[1202,     8] loss: 0.202\n",
            "[1202,    10] loss: 0.287\n",
            "[1203,     2] loss: 0.199\n",
            "[1203,     4] loss: 0.379\n",
            "[1203,     6] loss: 0.295\n",
            "[1203,     8] loss: 0.243\n",
            "[1203,    10] loss: 0.308\n",
            "[1204,     2] loss: 0.262\n",
            "[1204,     4] loss: 0.285\n",
            "[1204,     6] loss: 0.345\n",
            "[1204,     8] loss: 0.342\n",
            "[1204,    10] loss: 0.245\n",
            "[1205,     2] loss: 0.284\n",
            "[1205,     4] loss: 0.361\n",
            "[1205,     6] loss: 0.314\n",
            "[1205,     8] loss: 0.275\n",
            "[1205,    10] loss: 0.207\n",
            "[1206,     2] loss: 0.270\n",
            "[1206,     4] loss: 0.266\n",
            "[1206,     6] loss: 0.411\n",
            "[1206,     8] loss: 0.199\n",
            "[1206,    10] loss: 0.299\n",
            "[1207,     2] loss: 0.389\n",
            "[1207,     4] loss: 0.183\n",
            "[1207,     6] loss: 0.302\n",
            "[1207,     8] loss: 0.307\n",
            "[1207,    10] loss: 0.276\n",
            "[1208,     2] loss: 0.258\n",
            "[1208,     4] loss: 0.294\n",
            "[1208,     6] loss: 0.276\n",
            "[1208,     8] loss: 0.373\n",
            "[1208,    10] loss: 0.216\n",
            "[1209,     2] loss: 0.240\n",
            "[1209,     4] loss: 0.251\n",
            "[1209,     6] loss: 0.271\n",
            "[1209,     8] loss: 0.250\n",
            "[1209,    10] loss: 0.351\n",
            "[1210,     2] loss: 0.231\n",
            "[1210,     4] loss: 0.315\n",
            "[1210,     6] loss: 0.303\n",
            "[1210,     8] loss: 0.255\n",
            "[1210,    10] loss: 0.312\n",
            "[1211,     2] loss: 0.197\n",
            "[1211,     4] loss: 0.334\n",
            "[1211,     6] loss: 0.399\n",
            "[1211,     8] loss: 0.328\n",
            "[1211,    10] loss: 0.276\n",
            "[1212,     2] loss: 0.269\n",
            "[1212,     4] loss: 0.359\n",
            "[1212,     6] loss: 0.317\n",
            "[1212,     8] loss: 0.285\n",
            "[1212,    10] loss: 0.309\n",
            "[1213,     2] loss: 0.340\n",
            "[1213,     4] loss: 0.257\n",
            "[1213,     6] loss: 0.254\n",
            "[1213,     8] loss: 0.322\n",
            "[1213,    10] loss: 0.225\n",
            "[1214,     2] loss: 0.254\n",
            "[1214,     4] loss: 0.261\n",
            "[1214,     6] loss: 0.270\n",
            "[1214,     8] loss: 0.246\n",
            "[1214,    10] loss: 0.344\n",
            "[1215,     2] loss: 0.320\n",
            "[1215,     4] loss: 0.252\n",
            "[1215,     6] loss: 0.235\n",
            "[1215,     8] loss: 0.303\n",
            "[1215,    10] loss: 0.245\n",
            "[1216,     2] loss: 0.213\n",
            "[1216,     4] loss: 0.297\n",
            "[1216,     6] loss: 0.270\n",
            "[1216,     8] loss: 0.350\n",
            "[1216,    10] loss: 0.221\n",
            "[1217,     2] loss: 0.275\n",
            "[1217,     4] loss: 0.223\n",
            "[1217,     6] loss: 0.289\n",
            "[1217,     8] loss: 0.267\n",
            "[1217,    10] loss: 0.279\n",
            "[1218,     2] loss: 0.233\n",
            "[1218,     4] loss: 0.269\n",
            "[1218,     6] loss: 0.257\n",
            "[1218,     8] loss: 0.284\n",
            "[1218,    10] loss: 0.327\n",
            "[1219,     2] loss: 0.238\n",
            "[1219,     4] loss: 0.253\n",
            "[1219,     6] loss: 0.294\n",
            "[1219,     8] loss: 0.350\n",
            "[1219,    10] loss: 0.300\n",
            "[1220,     2] loss: 0.320\n",
            "[1220,     4] loss: 0.248\n",
            "[1220,     6] loss: 0.266\n",
            "[1220,     8] loss: 0.263\n",
            "[1220,    10] loss: 0.270\n",
            "[1221,     2] loss: 0.265\n",
            "[1221,     4] loss: 0.314\n",
            "[1221,     6] loss: 0.294\n",
            "[1221,     8] loss: 0.198\n",
            "[1221,    10] loss: 0.266\n",
            "[1222,     2] loss: 0.275\n",
            "[1222,     4] loss: 0.257\n",
            "[1222,     6] loss: 0.325\n",
            "[1222,     8] loss: 0.242\n",
            "[1222,    10] loss: 0.268\n",
            "[1223,     2] loss: 0.310\n",
            "[1223,     4] loss: 0.285\n",
            "[1223,     6] loss: 0.191\n",
            "[1223,     8] loss: 0.329\n",
            "[1223,    10] loss: 0.270\n",
            "[1224,     2] loss: 0.247\n",
            "[1224,     4] loss: 0.240\n",
            "[1224,     6] loss: 0.316\n",
            "[1224,     8] loss: 0.242\n",
            "[1224,    10] loss: 0.346\n",
            "[1225,     2] loss: 0.266\n",
            "[1225,     4] loss: 0.400\n",
            "[1225,     6] loss: 0.240\n",
            "[1225,     8] loss: 0.295\n",
            "[1225,    10] loss: 0.199\n",
            "[1226,     2] loss: 0.257\n",
            "[1226,     4] loss: 0.373\n",
            "[1226,     6] loss: 0.278\n",
            "[1226,     8] loss: 0.281\n",
            "[1226,    10] loss: 0.305\n",
            "[1227,     2] loss: 0.347\n",
            "[1227,     4] loss: 0.266\n",
            "[1227,     6] loss: 0.282\n",
            "[1227,     8] loss: 0.200\n",
            "[1227,    10] loss: 0.305\n",
            "[1228,     2] loss: 0.257\n",
            "[1228,     4] loss: 0.310\n",
            "[1228,     6] loss: 0.302\n",
            "[1228,     8] loss: 0.275\n",
            "[1228,    10] loss: 0.265\n",
            "[1229,     2] loss: 0.316\n",
            "[1229,     4] loss: 0.230\n",
            "[1229,     6] loss: 0.283\n",
            "[1229,     8] loss: 0.302\n",
            "[1229,    10] loss: 0.353\n",
            "[1230,     2] loss: 0.288\n",
            "[1230,     4] loss: 0.286\n",
            "[1230,     6] loss: 0.388\n",
            "[1230,     8] loss: 0.229\n",
            "[1230,    10] loss: 0.310\n",
            "[1231,     2] loss: 0.292\n",
            "[1231,     4] loss: 0.345\n",
            "[1231,     6] loss: 0.392\n",
            "[1231,     8] loss: 0.242\n",
            "[1231,    10] loss: 0.337\n",
            "[1232,     2] loss: 0.309\n",
            "[1232,     4] loss: 0.253\n",
            "[1232,     6] loss: 0.245\n",
            "[1232,     8] loss: 0.297\n",
            "[1232,    10] loss: 0.354\n",
            "[1233,     2] loss: 0.302\n",
            "[1233,     4] loss: 0.285\n",
            "[1233,     6] loss: 0.293\n",
            "[1233,     8] loss: 0.299\n",
            "[1233,    10] loss: 0.253\n",
            "[1234,     2] loss: 0.246\n",
            "[1234,     4] loss: 0.346\n",
            "[1234,     6] loss: 0.318\n",
            "[1234,     8] loss: 0.194\n",
            "[1234,    10] loss: 0.255\n",
            "[1235,     2] loss: 0.219\n",
            "[1235,     4] loss: 0.250\n",
            "[1235,     6] loss: 0.284\n",
            "[1235,     8] loss: 0.302\n",
            "[1235,    10] loss: 0.319\n",
            "[1236,     2] loss: 0.319\n",
            "[1236,     4] loss: 0.301\n",
            "[1236,     6] loss: 0.245\n",
            "[1236,     8] loss: 0.240\n",
            "[1236,    10] loss: 0.257\n",
            "[1237,     2] loss: 0.228\n",
            "[1237,     4] loss: 0.305\n",
            "[1237,     6] loss: 0.234\n",
            "[1237,     8] loss: 0.255\n",
            "[1237,    10] loss: 0.382\n",
            "[1238,     2] loss: 0.332\n",
            "[1238,     4] loss: 0.192\n",
            "[1238,     6] loss: 0.420\n",
            "[1238,     8] loss: 0.379\n",
            "[1238,    10] loss: 0.278\n",
            "[1239,     2] loss: 0.568\n",
            "[1239,     4] loss: 0.322\n",
            "[1239,     6] loss: 0.275\n",
            "[1239,     8] loss: 0.195\n",
            "[1239,    10] loss: 0.328\n",
            "[1240,     2] loss: 0.276\n",
            "[1240,     4] loss: 0.410\n",
            "[1240,     6] loss: 0.219\n",
            "[1240,     8] loss: 0.249\n",
            "[1240,    10] loss: 0.295\n",
            "[1241,     2] loss: 0.222\n",
            "[1241,     4] loss: 0.264\n",
            "[1241,     6] loss: 0.456\n",
            "[1241,     8] loss: 0.344\n",
            "[1241,    10] loss: 0.246\n",
            "[1242,     2] loss: 0.374\n",
            "[1242,     4] loss: 0.267\n",
            "[1242,     6] loss: 0.215\n",
            "[1242,     8] loss: 0.362\n",
            "[1242,    10] loss: 0.295\n",
            "[1243,     2] loss: 0.306\n",
            "[1243,     4] loss: 0.193\n",
            "[1243,     6] loss: 0.223\n",
            "[1243,     8] loss: 0.360\n",
            "[1243,    10] loss: 0.313\n",
            "[1244,     2] loss: 0.286\n",
            "[1244,     4] loss: 0.208\n",
            "[1244,     6] loss: 0.356\n",
            "[1244,     8] loss: 0.224\n",
            "[1244,    10] loss: 0.282\n",
            "[1245,     2] loss: 0.340\n",
            "[1245,     4] loss: 0.235\n",
            "[1245,     6] loss: 0.229\n",
            "[1245,     8] loss: 0.361\n",
            "[1245,    10] loss: 0.288\n",
            "[1246,     2] loss: 0.416\n",
            "[1246,     4] loss: 0.391\n",
            "[1246,     6] loss: 0.263\n",
            "[1246,     8] loss: 0.197\n",
            "[1246,    10] loss: 0.274\n",
            "[1247,     2] loss: 0.275\n",
            "[1247,     4] loss: 0.208\n",
            "[1247,     6] loss: 0.359\n",
            "[1247,     8] loss: 0.399\n",
            "[1247,    10] loss: 0.308\n",
            "[1248,     2] loss: 0.287\n",
            "[1248,     4] loss: 0.239\n",
            "[1248,     6] loss: 0.346\n",
            "[1248,     8] loss: 0.411\n",
            "[1248,    10] loss: 0.348\n",
            "[1249,     2] loss: 0.439\n",
            "[1249,     4] loss: 0.361\n",
            "[1249,     6] loss: 0.234\n",
            "[1249,     8] loss: 0.337\n",
            "[1249,    10] loss: 0.271\n",
            "[1250,     2] loss: 0.331\n",
            "[1250,     4] loss: 0.216\n",
            "[1250,     6] loss: 0.285\n",
            "[1250,     8] loss: 0.315\n",
            "[1250,    10] loss: 0.215\n",
            "[1251,     2] loss: 0.200\n",
            "[1251,     4] loss: 0.314\n",
            "[1251,     6] loss: 0.291\n",
            "[1251,     8] loss: 0.359\n",
            "[1251,    10] loss: 0.317\n",
            "[1252,     2] loss: 0.258\n",
            "[1252,     4] loss: 0.366\n",
            "[1252,     6] loss: 0.186\n",
            "[1252,     8] loss: 0.316\n",
            "[1252,    10] loss: 0.311\n",
            "[1253,     2] loss: 0.201\n",
            "[1253,     4] loss: 0.234\n",
            "[1253,     6] loss: 0.357\n",
            "[1253,     8] loss: 0.292\n",
            "[1253,    10] loss: 0.395\n",
            "[1254,     2] loss: 0.394\n",
            "[1254,     4] loss: 0.235\n",
            "[1254,     6] loss: 0.265\n",
            "[1254,     8] loss: 0.350\n",
            "[1254,    10] loss: 0.280\n",
            "[1255,     2] loss: 0.199\n",
            "[1255,     4] loss: 0.334\n",
            "[1255,     6] loss: 0.311\n",
            "[1255,     8] loss: 0.314\n",
            "[1255,    10] loss: 0.270\n",
            "[1256,     2] loss: 0.284\n",
            "[1256,     4] loss: 0.379\n",
            "[1256,     6] loss: 0.337\n",
            "[1256,     8] loss: 0.225\n",
            "[1256,    10] loss: 0.349\n",
            "[1257,     2] loss: 0.363\n",
            "[1257,     4] loss: 0.353\n",
            "[1257,     6] loss: 0.303\n",
            "[1257,     8] loss: 0.255\n",
            "[1257,    10] loss: 0.332\n",
            "[1258,     2] loss: 0.297\n",
            "[1258,     4] loss: 0.264\n",
            "[1258,     6] loss: 0.282\n",
            "[1258,     8] loss: 0.233\n",
            "[1258,    10] loss: 0.314\n",
            "[1259,     2] loss: 0.262\n",
            "[1259,     4] loss: 0.261\n",
            "[1259,     6] loss: 0.319\n",
            "[1259,     8] loss: 0.267\n",
            "[1259,    10] loss: 0.290\n",
            "[1260,     2] loss: 0.317\n",
            "[1260,     4] loss: 0.324\n",
            "[1260,     6] loss: 0.293\n",
            "[1260,     8] loss: 0.251\n",
            "[1260,    10] loss: 0.219\n",
            "[1261,     2] loss: 0.306\n",
            "[1261,     4] loss: 0.264\n",
            "[1261,     6] loss: 0.285\n",
            "[1261,     8] loss: 0.260\n",
            "[1261,    10] loss: 0.308\n",
            "[1262,     2] loss: 0.227\n",
            "[1262,     4] loss: 0.269\n",
            "[1262,     6] loss: 0.299\n",
            "[1262,     8] loss: 0.277\n",
            "[1262,    10] loss: 0.298\n",
            "[1263,     2] loss: 0.220\n",
            "[1263,     4] loss: 0.181\n",
            "[1263,     6] loss: 0.366\n",
            "[1263,     8] loss: 0.390\n",
            "[1263,    10] loss: 0.221\n",
            "[1264,     2] loss: 0.213\n",
            "[1264,     4] loss: 0.214\n",
            "[1264,     6] loss: 0.448\n",
            "[1264,     8] loss: 0.296\n",
            "[1264,    10] loss: 0.312\n",
            "[1265,     2] loss: 0.329\n",
            "[1265,     4] loss: 0.353\n",
            "[1265,     6] loss: 0.313\n",
            "[1265,     8] loss: 0.286\n",
            "[1265,    10] loss: 0.193\n",
            "[1266,     2] loss: 0.345\n",
            "[1266,     4] loss: 0.269\n",
            "[1266,     6] loss: 0.231\n",
            "[1266,     8] loss: 0.203\n",
            "[1266,    10] loss: 0.331\n",
            "[1267,     2] loss: 0.321\n",
            "[1267,     4] loss: 0.283\n",
            "[1267,     6] loss: 0.210\n",
            "[1267,     8] loss: 0.279\n",
            "[1267,    10] loss: 0.263\n",
            "[1268,     2] loss: 0.313\n",
            "[1268,     4] loss: 0.244\n",
            "[1268,     6] loss: 0.289\n",
            "[1268,     8] loss: 0.264\n",
            "[1268,    10] loss: 0.246\n",
            "[1269,     2] loss: 0.249\n",
            "[1269,     4] loss: 0.214\n",
            "[1269,     6] loss: 0.216\n",
            "[1269,     8] loss: 0.335\n",
            "[1269,    10] loss: 0.357\n",
            "[1270,     2] loss: 0.296\n",
            "[1270,     4] loss: 0.310\n",
            "[1270,     6] loss: 0.304\n",
            "[1270,     8] loss: 0.263\n",
            "[1270,    10] loss: 0.211\n",
            "[1271,     2] loss: 0.258\n",
            "[1271,     4] loss: 0.388\n",
            "[1271,     6] loss: 0.278\n",
            "[1271,     8] loss: 0.242\n",
            "[1271,    10] loss: 0.212\n",
            "[1272,     2] loss: 0.257\n",
            "[1272,     4] loss: 0.271\n",
            "[1272,     6] loss: 0.247\n",
            "[1272,     8] loss: 0.295\n",
            "[1272,    10] loss: 0.295\n",
            "[1273,     2] loss: 0.277\n",
            "[1273,     4] loss: 0.296\n",
            "[1273,     6] loss: 0.343\n",
            "[1273,     8] loss: 0.285\n",
            "[1273,    10] loss: 0.263\n",
            "[1274,     2] loss: 0.266\n",
            "[1274,     4] loss: 0.354\n",
            "[1274,     6] loss: 0.183\n",
            "[1274,     8] loss: 0.224\n",
            "[1274,    10] loss: 0.317\n",
            "[1275,     2] loss: 0.298\n",
            "[1275,     4] loss: 0.281\n",
            "[1275,     6] loss: 0.293\n",
            "[1275,     8] loss: 0.247\n",
            "[1275,    10] loss: 0.233\n",
            "[1276,     2] loss: 0.284\n",
            "[1276,     4] loss: 0.312\n",
            "[1276,     6] loss: 0.257\n",
            "[1276,     8] loss: 0.352\n",
            "[1276,    10] loss: 0.272\n",
            "[1277,     2] loss: 0.366\n",
            "[1277,     4] loss: 0.291\n",
            "[1277,     6] loss: 0.221\n",
            "[1277,     8] loss: 0.234\n",
            "[1277,    10] loss: 0.257\n",
            "[1278,     2] loss: 0.302\n",
            "[1278,     4] loss: 0.336\n",
            "[1278,     6] loss: 0.211\n",
            "[1278,     8] loss: 0.369\n",
            "[1278,    10] loss: 0.296\n",
            "[1279,     2] loss: 0.217\n",
            "[1279,     4] loss: 0.374\n",
            "[1279,     6] loss: 0.345\n",
            "[1279,     8] loss: 0.205\n",
            "[1279,    10] loss: 0.377\n",
            "[1280,     2] loss: 0.271\n",
            "[1280,     4] loss: 0.235\n",
            "[1280,     6] loss: 0.198\n",
            "[1280,     8] loss: 0.363\n",
            "[1280,    10] loss: 0.309\n",
            "[1281,     2] loss: 0.282\n",
            "[1281,     4] loss: 0.265\n",
            "[1281,     6] loss: 0.299\n",
            "[1281,     8] loss: 0.312\n",
            "[1281,    10] loss: 0.212\n",
            "[1282,     2] loss: 0.325\n",
            "[1282,     4] loss: 0.244\n",
            "[1282,     6] loss: 0.228\n",
            "[1282,     8] loss: 0.346\n",
            "[1282,    10] loss: 0.224\n",
            "[1283,     2] loss: 0.276\n",
            "[1283,     4] loss: 0.219\n",
            "[1283,     6] loss: 0.213\n",
            "[1283,     8] loss: 0.325\n",
            "[1283,    10] loss: 0.370\n",
            "[1284,     2] loss: 0.263\n",
            "[1284,     4] loss: 0.334\n",
            "[1284,     6] loss: 0.246\n",
            "[1284,     8] loss: 0.266\n",
            "[1284,    10] loss: 0.323\n",
            "[1285,     2] loss: 0.311\n",
            "[1285,     4] loss: 0.322\n",
            "[1285,     6] loss: 0.289\n",
            "[1285,     8] loss: 0.244\n",
            "[1285,    10] loss: 0.244\n",
            "[1286,     2] loss: 0.313\n",
            "[1286,     4] loss: 0.378\n",
            "[1286,     6] loss: 0.236\n",
            "[1286,     8] loss: 0.278\n",
            "[1286,    10] loss: 0.213\n",
            "[1287,     2] loss: 0.283\n",
            "[1287,     4] loss: 0.288\n",
            "[1287,     6] loss: 0.213\n",
            "[1287,     8] loss: 0.263\n",
            "[1287,    10] loss: 0.349\n",
            "[1288,     2] loss: 0.204\n",
            "[1288,     4] loss: 0.368\n",
            "[1288,     6] loss: 0.301\n",
            "[1288,     8] loss: 0.263\n",
            "[1288,    10] loss: 0.293\n",
            "[1289,     2] loss: 0.282\n",
            "[1289,     4] loss: 0.312\n",
            "[1289,     6] loss: 0.275\n",
            "[1289,     8] loss: 0.277\n",
            "[1289,    10] loss: 0.240\n",
            "[1290,     2] loss: 0.249\n",
            "[1290,     4] loss: 0.267\n",
            "[1290,     6] loss: 0.301\n",
            "[1290,     8] loss: 0.355\n",
            "[1290,    10] loss: 0.249\n",
            "[1291,     2] loss: 0.493\n",
            "[1291,     4] loss: 0.211\n",
            "[1291,     6] loss: 0.207\n",
            "[1291,     8] loss: 0.331\n",
            "[1291,    10] loss: 0.241\n",
            "[1292,     2] loss: 0.289\n",
            "[1292,     4] loss: 0.303\n",
            "[1292,     6] loss: 0.336\n",
            "[1292,     8] loss: 0.287\n",
            "[1292,    10] loss: 0.254\n",
            "[1293,     2] loss: 0.351\n",
            "[1293,     4] loss: 0.229\n",
            "[1293,     6] loss: 0.300\n",
            "[1293,     8] loss: 0.322\n",
            "[1293,    10] loss: 0.245\n",
            "[1294,     2] loss: 0.320\n",
            "[1294,     4] loss: 0.272\n",
            "[1294,     6] loss: 0.222\n",
            "[1294,     8] loss: 0.283\n",
            "[1294,    10] loss: 0.292\n",
            "[1295,     2] loss: 0.261\n",
            "[1295,     4] loss: 0.262\n",
            "[1295,     6] loss: 0.235\n",
            "[1295,     8] loss: 0.243\n",
            "[1295,    10] loss: 0.338\n",
            "[1296,     2] loss: 0.275\n",
            "[1296,     4] loss: 0.315\n",
            "[1296,     6] loss: 0.268\n",
            "[1296,     8] loss: 0.244\n",
            "[1296,    10] loss: 0.241\n",
            "[1297,     2] loss: 0.354\n",
            "[1297,     4] loss: 0.293\n",
            "[1297,     6] loss: 0.272\n",
            "[1297,     8] loss: 0.267\n",
            "[1297,    10] loss: 0.236\n",
            "[1298,     2] loss: 0.253\n",
            "[1298,     4] loss: 0.239\n",
            "[1298,     6] loss: 0.361\n",
            "[1298,     8] loss: 0.283\n",
            "[1298,    10] loss: 0.286\n",
            "[1299,     2] loss: 0.292\n",
            "[1299,     4] loss: 0.265\n",
            "[1299,     6] loss: 0.281\n",
            "[1299,     8] loss: 0.267\n",
            "[1299,    10] loss: 0.294\n",
            "[1300,     2] loss: 0.303\n",
            "[1300,     4] loss: 0.292\n",
            "[1300,     6] loss: 0.252\n",
            "[1300,     8] loss: 0.258\n",
            "[1300,    10] loss: 0.288\n",
            "[1301,     2] loss: 0.299\n",
            "[1301,     4] loss: 0.226\n",
            "[1301,     6] loss: 0.347\n",
            "[1301,     8] loss: 0.288\n",
            "[1301,    10] loss: 0.231\n",
            "[1302,     2] loss: 0.318\n",
            "[1302,     4] loss: 0.265\n",
            "[1302,     6] loss: 0.165\n",
            "[1302,     8] loss: 0.362\n",
            "[1302,    10] loss: 0.291\n",
            "[1303,     2] loss: 0.285\n",
            "[1303,     4] loss: 0.300\n",
            "[1303,     6] loss: 0.319\n",
            "[1303,     8] loss: 0.267\n",
            "[1303,    10] loss: 0.256\n",
            "[1304,     2] loss: 0.322\n",
            "[1304,     4] loss: 0.340\n",
            "[1304,     6] loss: 0.273\n",
            "[1304,     8] loss: 0.221\n",
            "[1304,    10] loss: 0.304\n",
            "[1305,     2] loss: 0.335\n",
            "[1305,     4] loss: 0.188\n",
            "[1305,     6] loss: 0.298\n",
            "[1305,     8] loss: 0.295\n",
            "[1305,    10] loss: 0.250\n",
            "[1306,     2] loss: 0.264\n",
            "[1306,     4] loss: 0.260\n",
            "[1306,     6] loss: 0.264\n",
            "[1306,     8] loss: 0.290\n",
            "[1306,    10] loss: 0.323\n",
            "[1307,     2] loss: 0.278\n",
            "[1307,     4] loss: 0.245\n",
            "[1307,     6] loss: 0.253\n",
            "[1307,     8] loss: 0.281\n",
            "[1307,    10] loss: 0.290\n",
            "[1308,     2] loss: 0.301\n",
            "[1308,     4] loss: 0.298\n",
            "[1308,     6] loss: 0.205\n",
            "[1308,     8] loss: 0.295\n",
            "[1308,    10] loss: 0.265\n",
            "[1309,     2] loss: 0.303\n",
            "[1309,     4] loss: 0.301\n",
            "[1309,     6] loss: 0.216\n",
            "[1309,     8] loss: 0.311\n",
            "[1309,    10] loss: 0.227\n",
            "[1310,     2] loss: 0.266\n",
            "[1310,     4] loss: 0.298\n",
            "[1310,     6] loss: 0.351\n",
            "[1310,     8] loss: 0.258\n",
            "[1310,    10] loss: 0.255\n",
            "[1311,     2] loss: 0.339\n",
            "[1311,     4] loss: 0.234\n",
            "[1311,     6] loss: 0.335\n",
            "[1311,     8] loss: 0.262\n",
            "[1311,    10] loss: 0.367\n",
            "[1312,     2] loss: 0.381\n",
            "[1312,     4] loss: 0.245\n",
            "[1312,     6] loss: 0.354\n",
            "[1312,     8] loss: 0.361\n",
            "[1312,    10] loss: 0.258\n",
            "[1313,     2] loss: 0.291\n",
            "[1313,     4] loss: 0.293\n",
            "[1313,     6] loss: 0.369\n",
            "[1313,     8] loss: 0.229\n",
            "[1313,    10] loss: 0.268\n",
            "[1314,     2] loss: 0.335\n",
            "[1314,     4] loss: 0.261\n",
            "[1314,     6] loss: 0.255\n",
            "[1314,     8] loss: 0.256\n",
            "[1314,    10] loss: 0.268\n",
            "[1315,     2] loss: 0.288\n",
            "[1315,     4] loss: 0.303\n",
            "[1315,     6] loss: 0.245\n",
            "[1315,     8] loss: 0.250\n",
            "[1315,    10] loss: 0.310\n",
            "[1316,     2] loss: 0.327\n",
            "[1316,     4] loss: 0.262\n",
            "[1316,     6] loss: 0.345\n",
            "[1316,     8] loss: 0.176\n",
            "[1316,    10] loss: 0.289\n",
            "[1317,     2] loss: 0.269\n",
            "[1317,     4] loss: 0.337\n",
            "[1317,     6] loss: 0.296\n",
            "[1317,     8] loss: 0.258\n",
            "[1317,    10] loss: 0.302\n",
            "[1318,     2] loss: 0.223\n",
            "[1318,     4] loss: 0.270\n",
            "[1318,     6] loss: 0.288\n",
            "[1318,     8] loss: 0.297\n",
            "[1318,    10] loss: 0.374\n",
            "[1319,     2] loss: 0.245\n",
            "[1319,     4] loss: 0.209\n",
            "[1319,     6] loss: 0.304\n",
            "[1319,     8] loss: 0.296\n",
            "[1319,    10] loss: 0.378\n",
            "[1320,     2] loss: 0.355\n",
            "[1320,     4] loss: 0.279\n",
            "[1320,     6] loss: 0.267\n",
            "[1320,     8] loss: 0.330\n",
            "[1320,    10] loss: 0.339\n",
            "[1321,     2] loss: 0.207\n",
            "[1321,     4] loss: 0.416\n",
            "[1321,     6] loss: 0.266\n",
            "[1321,     8] loss: 0.372\n",
            "[1321,    10] loss: 0.253\n",
            "[1322,     2] loss: 0.321\n",
            "[1322,     4] loss: 0.254\n",
            "[1322,     6] loss: 0.230\n",
            "[1322,     8] loss: 0.302\n",
            "[1322,    10] loss: 0.271\n",
            "[1323,     2] loss: 0.268\n",
            "[1323,     4] loss: 0.273\n",
            "[1323,     6] loss: 0.299\n",
            "[1323,     8] loss: 0.343\n",
            "[1323,    10] loss: 0.162\n",
            "[1324,     2] loss: 0.290\n",
            "[1324,     4] loss: 0.250\n",
            "[1324,     6] loss: 0.286\n",
            "[1324,     8] loss: 0.269\n",
            "[1324,    10] loss: 0.252\n",
            "[1325,     2] loss: 0.298\n",
            "[1325,     4] loss: 0.243\n",
            "[1325,     6] loss: 0.256\n",
            "[1325,     8] loss: 0.307\n",
            "[1325,    10] loss: 0.327\n",
            "[1326,     2] loss: 0.294\n",
            "[1326,     4] loss: 0.224\n",
            "[1326,     6] loss: 0.265\n",
            "[1326,     8] loss: 0.337\n",
            "[1326,    10] loss: 0.284\n",
            "[1327,     2] loss: 0.366\n",
            "[1327,     4] loss: 0.278\n",
            "[1327,     6] loss: 0.259\n",
            "[1327,     8] loss: 0.243\n",
            "[1327,    10] loss: 0.258\n",
            "[1328,     2] loss: 0.306\n",
            "[1328,     4] loss: 0.321\n",
            "[1328,     6] loss: 0.257\n",
            "[1328,     8] loss: 0.268\n",
            "[1328,    10] loss: 0.220\n",
            "[1329,     2] loss: 0.231\n",
            "[1329,     4] loss: 0.266\n",
            "[1329,     6] loss: 0.362\n",
            "[1329,     8] loss: 0.308\n",
            "[1329,    10] loss: 0.252\n",
            "[1330,     2] loss: 0.318\n",
            "[1330,     4] loss: 0.271\n",
            "[1330,     6] loss: 0.248\n",
            "[1330,     8] loss: 0.247\n",
            "[1330,    10] loss: 0.379\n",
            "[1331,     2] loss: 0.236\n",
            "[1331,     4] loss: 0.262\n",
            "[1331,     6] loss: 0.299\n",
            "[1331,     8] loss: 0.299\n",
            "[1331,    10] loss: 0.321\n",
            "[1332,     2] loss: 0.218\n",
            "[1332,     4] loss: 0.299\n",
            "[1332,     6] loss: 0.302\n",
            "[1332,     8] loss: 0.347\n",
            "[1332,    10] loss: 0.253\n",
            "[1333,     2] loss: 0.276\n",
            "[1333,     4] loss: 0.297\n",
            "[1333,     6] loss: 0.366\n",
            "[1333,     8] loss: 0.231\n",
            "[1333,    10] loss: 0.315\n",
            "[1334,     2] loss: 0.355\n",
            "[1334,     4] loss: 0.277\n",
            "[1334,     6] loss: 0.272\n",
            "[1334,     8] loss: 0.225\n",
            "[1334,    10] loss: 0.305\n",
            "[1335,     2] loss: 0.242\n",
            "[1335,     4] loss: 0.275\n",
            "[1335,     6] loss: 0.367\n",
            "[1335,     8] loss: 0.243\n",
            "[1335,    10] loss: 0.302\n",
            "[1336,     2] loss: 0.198\n",
            "[1336,     4] loss: 0.262\n",
            "[1336,     6] loss: 0.321\n",
            "[1336,     8] loss: 0.223\n",
            "[1336,    10] loss: 0.399\n",
            "[1337,     2] loss: 0.257\n",
            "[1337,     4] loss: 0.386\n",
            "[1337,     6] loss: 0.217\n",
            "[1337,     8] loss: 0.205\n",
            "[1337,    10] loss: 0.308\n",
            "[1338,     2] loss: 0.290\n",
            "[1338,     4] loss: 0.303\n",
            "[1338,     6] loss: 0.214\n",
            "[1338,     8] loss: 0.329\n",
            "[1338,    10] loss: 0.271\n",
            "[1339,     2] loss: 0.228\n",
            "[1339,     4] loss: 0.313\n",
            "[1339,     6] loss: 0.259\n",
            "[1339,     8] loss: 0.266\n",
            "[1339,    10] loss: 0.318\n",
            "[1340,     2] loss: 0.195\n",
            "[1340,     4] loss: 0.322\n",
            "[1340,     6] loss: 0.359\n",
            "[1340,     8] loss: 0.316\n",
            "[1340,    10] loss: 0.252\n",
            "[1341,     2] loss: 0.273\n",
            "[1341,     4] loss: 0.329\n",
            "[1341,     6] loss: 0.236\n",
            "[1341,     8] loss: 0.263\n",
            "[1341,    10] loss: 0.280\n",
            "[1342,     2] loss: 0.306\n",
            "[1342,     4] loss: 0.230\n",
            "[1342,     6] loss: 0.369\n",
            "[1342,     8] loss: 0.270\n",
            "[1342,    10] loss: 0.211\n",
            "[1343,     2] loss: 0.318\n",
            "[1343,     4] loss: 0.329\n",
            "[1343,     6] loss: 0.262\n",
            "[1343,     8] loss: 0.229\n",
            "[1343,    10] loss: 0.230\n",
            "[1344,     2] loss: 0.365\n",
            "[1344,     4] loss: 0.267\n",
            "[1344,     6] loss: 0.282\n",
            "[1344,     8] loss: 0.170\n",
            "[1344,    10] loss: 0.285\n",
            "[1345,     2] loss: 0.306\n",
            "[1345,     4] loss: 0.214\n",
            "[1345,     6] loss: 0.311\n",
            "[1345,     8] loss: 0.236\n",
            "[1345,    10] loss: 0.296\n",
            "[1346,     2] loss: 0.254\n",
            "[1346,     4] loss: 0.335\n",
            "[1346,     6] loss: 0.288\n",
            "[1346,     8] loss: 0.296\n",
            "[1346,    10] loss: 0.177\n",
            "[1347,     2] loss: 0.254\n",
            "[1347,     4] loss: 0.358\n",
            "[1347,     6] loss: 0.296\n",
            "[1347,     8] loss: 0.291\n",
            "[1347,    10] loss: 0.206\n",
            "[1348,     2] loss: 0.211\n",
            "[1348,     4] loss: 0.234\n",
            "[1348,     6] loss: 0.410\n",
            "[1348,     8] loss: 0.309\n",
            "[1348,    10] loss: 0.288\n",
            "[1349,     2] loss: 0.268\n",
            "[1349,     4] loss: 0.287\n",
            "[1349,     6] loss: 0.300\n",
            "[1349,     8] loss: 0.287\n",
            "[1349,    10] loss: 0.329\n",
            "[1350,     2] loss: 0.247\n",
            "[1350,     4] loss: 0.167\n",
            "[1350,     6] loss: 0.215\n",
            "[1350,     8] loss: 0.269\n",
            "[1350,    10] loss: 0.447\n",
            "[1351,     2] loss: 0.275\n",
            "[1351,     4] loss: 0.306\n",
            "[1351,     6] loss: 0.264\n",
            "[1351,     8] loss: 0.186\n",
            "[1351,    10] loss: 0.354\n",
            "[1352,     2] loss: 0.256\n",
            "[1352,     4] loss: 0.266\n",
            "[1352,     6] loss: 0.321\n",
            "[1352,     8] loss: 0.332\n",
            "[1352,    10] loss: 0.256\n",
            "[1353,     2] loss: 0.305\n",
            "[1353,     4] loss: 0.304\n",
            "[1353,     6] loss: 0.300\n",
            "[1353,     8] loss: 0.185\n",
            "[1353,    10] loss: 0.263\n",
            "[1354,     2] loss: 0.257\n",
            "[1354,     4] loss: 0.259\n",
            "[1354,     6] loss: 0.296\n",
            "[1354,     8] loss: 0.293\n",
            "[1354,    10] loss: 0.290\n",
            "[1355,     2] loss: 0.306\n",
            "[1355,     4] loss: 0.321\n",
            "[1355,     6] loss: 0.258\n",
            "[1355,     8] loss: 0.323\n",
            "[1355,    10] loss: 0.237\n",
            "[1356,     2] loss: 0.284\n",
            "[1356,     4] loss: 0.191\n",
            "[1356,     6] loss: 0.303\n",
            "[1356,     8] loss: 0.271\n",
            "[1356,    10] loss: 0.318\n",
            "[1357,     2] loss: 0.329\n",
            "[1357,     4] loss: 0.242\n",
            "[1357,     6] loss: 0.191\n",
            "[1357,     8] loss: 0.282\n",
            "[1357,    10] loss: 0.305\n",
            "[1358,     2] loss: 0.329\n",
            "[1358,     4] loss: 0.264\n",
            "[1358,     6] loss: 0.259\n",
            "[1358,     8] loss: 0.315\n",
            "[1358,    10] loss: 0.199\n",
            "[1359,     2] loss: 0.298\n",
            "[1359,     4] loss: 0.234\n",
            "[1359,     6] loss: 0.251\n",
            "[1359,     8] loss: 0.299\n",
            "[1359,    10] loss: 0.329\n",
            "[1360,     2] loss: 0.390\n",
            "[1360,     4] loss: 0.270\n",
            "[1360,     6] loss: 0.261\n",
            "[1360,     8] loss: 0.231\n",
            "[1360,    10] loss: 0.285\n",
            "[1361,     2] loss: 0.228\n",
            "[1361,     4] loss: 0.345\n",
            "[1361,     6] loss: 0.262\n",
            "[1361,     8] loss: 0.270\n",
            "[1361,    10] loss: 0.428\n",
            "[1362,     2] loss: 0.305\n",
            "[1362,     4] loss: 0.341\n",
            "[1362,     6] loss: 0.200\n",
            "[1362,     8] loss: 0.278\n",
            "[1362,    10] loss: 0.271\n",
            "[1363,     2] loss: 0.261\n",
            "[1363,     4] loss: 0.302\n",
            "[1363,     6] loss: 0.215\n",
            "[1363,     8] loss: 0.296\n",
            "[1363,    10] loss: 0.335\n",
            "[1364,     2] loss: 0.331\n",
            "[1364,     4] loss: 0.236\n",
            "[1364,     6] loss: 0.253\n",
            "[1364,     8] loss: 0.313\n",
            "[1364,    10] loss: 0.295\n",
            "[1365,     2] loss: 0.315\n",
            "[1365,     4] loss: 0.276\n",
            "[1365,     6] loss: 0.388\n",
            "[1365,     8] loss: 0.197\n",
            "[1365,    10] loss: 0.209\n",
            "[1366,     2] loss: 0.299\n",
            "[1366,     4] loss: 0.200\n",
            "[1366,     6] loss: 0.265\n",
            "[1366,     8] loss: 0.239\n",
            "[1366,    10] loss: 0.400\n",
            "[1367,     2] loss: 0.289\n",
            "[1367,     4] loss: 0.255\n",
            "[1367,     6] loss: 0.458\n",
            "[1367,     8] loss: 0.223\n",
            "[1367,    10] loss: 0.373\n",
            "[1368,     2] loss: 0.434\n",
            "[1368,     4] loss: 0.277\n",
            "[1368,     6] loss: 0.365\n",
            "[1368,     8] loss: 0.290\n",
            "[1368,    10] loss: 0.326\n",
            "[1369,     2] loss: 0.323\n",
            "[1369,     4] loss: 0.290\n",
            "[1369,     6] loss: 0.220\n",
            "[1369,     8] loss: 0.342\n",
            "[1369,    10] loss: 0.364\n",
            "[1370,     2] loss: 0.277\n",
            "[1370,     4] loss: 0.301\n",
            "[1370,     6] loss: 0.304\n",
            "[1370,     8] loss: 0.405\n",
            "[1370,    10] loss: 0.299\n",
            "[1371,     2] loss: 0.253\n",
            "[1371,     4] loss: 0.257\n",
            "[1371,     6] loss: 0.230\n",
            "[1371,     8] loss: 0.333\n",
            "[1371,    10] loss: 0.373\n",
            "[1372,     2] loss: 0.206\n",
            "[1372,     4] loss: 0.273\n",
            "[1372,     6] loss: 0.313\n",
            "[1372,     8] loss: 0.301\n",
            "[1372,    10] loss: 0.251\n",
            "[1373,     2] loss: 0.238\n",
            "[1373,     4] loss: 0.223\n",
            "[1373,     6] loss: 0.270\n",
            "[1373,     8] loss: 0.333\n",
            "[1373,    10] loss: 0.364\n",
            "[1374,     2] loss: 0.317\n",
            "[1374,     4] loss: 0.280\n",
            "[1374,     6] loss: 0.353\n",
            "[1374,     8] loss: 0.295\n",
            "[1374,    10] loss: 0.251\n",
            "[1375,     2] loss: 0.236\n",
            "[1375,     4] loss: 0.286\n",
            "[1375,     6] loss: 0.358\n",
            "[1375,     8] loss: 0.247\n",
            "[1375,    10] loss: 0.351\n",
            "[1376,     2] loss: 0.416\n",
            "[1376,     4] loss: 0.371\n",
            "[1376,     6] loss: 0.363\n",
            "[1376,     8] loss: 0.216\n",
            "[1376,    10] loss: 0.293\n",
            "[1377,     2] loss: 0.295\n",
            "[1377,     4] loss: 0.315\n",
            "[1377,     6] loss: 0.280\n",
            "[1377,     8] loss: 0.255\n",
            "[1377,    10] loss: 0.309\n",
            "[1378,     2] loss: 0.390\n",
            "[1378,     4] loss: 0.250\n",
            "[1378,     6] loss: 0.241\n",
            "[1378,     8] loss: 0.246\n",
            "[1378,    10] loss: 0.289\n",
            "[1379,     2] loss: 0.239\n",
            "[1379,     4] loss: 0.295\n",
            "[1379,     6] loss: 0.334\n",
            "[1379,     8] loss: 0.304\n",
            "[1379,    10] loss: 0.233\n",
            "[1380,     2] loss: 0.257\n",
            "[1380,     4] loss: 0.301\n",
            "[1380,     6] loss: 0.208\n",
            "[1380,     8] loss: 0.357\n",
            "[1380,    10] loss: 0.245\n",
            "[1381,     2] loss: 0.315\n",
            "[1381,     4] loss: 0.290\n",
            "[1381,     6] loss: 0.324\n",
            "[1381,     8] loss: 0.240\n",
            "[1381,    10] loss: 0.190\n",
            "[1382,     2] loss: 0.380\n",
            "[1382,     4] loss: 0.229\n",
            "[1382,     6] loss: 0.346\n",
            "[1382,     8] loss: 0.260\n",
            "[1382,    10] loss: 0.211\n",
            "[1383,     2] loss: 0.380\n",
            "[1383,     4] loss: 0.322\n",
            "[1383,     6] loss: 0.275\n",
            "[1383,     8] loss: 0.186\n",
            "[1383,    10] loss: 0.277\n",
            "[1384,     2] loss: 0.222\n",
            "[1384,     4] loss: 0.333\n",
            "[1384,     6] loss: 0.216\n",
            "[1384,     8] loss: 0.323\n",
            "[1384,    10] loss: 0.278\n",
            "[1385,     2] loss: 0.303\n",
            "[1385,     4] loss: 0.222\n",
            "[1385,     6] loss: 0.233\n",
            "[1385,     8] loss: 0.287\n",
            "[1385,    10] loss: 0.320\n",
            "[1386,     2] loss: 0.161\n",
            "[1386,     4] loss: 0.383\n",
            "[1386,     6] loss: 0.259\n",
            "[1386,     8] loss: 0.333\n",
            "[1386,    10] loss: 0.305\n",
            "[1387,     2] loss: 0.301\n",
            "[1387,     4] loss: 0.189\n",
            "[1387,     6] loss: 0.236\n",
            "[1387,     8] loss: 0.335\n",
            "[1387,    10] loss: 0.285\n",
            "[1388,     2] loss: 0.302\n",
            "[1388,     4] loss: 0.246\n",
            "[1388,     6] loss: 0.254\n",
            "[1388,     8] loss: 0.306\n",
            "[1388,    10] loss: 0.296\n",
            "[1389,     2] loss: 0.227\n",
            "[1389,     4] loss: 0.279\n",
            "[1389,     6] loss: 0.278\n",
            "[1389,     8] loss: 0.345\n",
            "[1389,    10] loss: 0.285\n",
            "[1390,     2] loss: 0.275\n",
            "[1390,     4] loss: 0.339\n",
            "[1390,     6] loss: 0.357\n",
            "[1390,     8] loss: 0.204\n",
            "[1390,    10] loss: 0.221\n",
            "[1391,     2] loss: 0.338\n",
            "[1391,     4] loss: 0.164\n",
            "[1391,     6] loss: 0.242\n",
            "[1391,     8] loss: 0.257\n",
            "[1391,    10] loss: 0.356\n",
            "[1392,     2] loss: 0.287\n",
            "[1392,     4] loss: 0.337\n",
            "[1392,     6] loss: 0.351\n",
            "[1392,     8] loss: 0.281\n",
            "[1392,    10] loss: 0.257\n",
            "[1393,     2] loss: 0.293\n",
            "[1393,     4] loss: 0.372\n",
            "[1393,     6] loss: 0.300\n",
            "[1393,     8] loss: 0.242\n",
            "[1393,    10] loss: 0.267\n",
            "[1394,     2] loss: 0.308\n",
            "[1394,     4] loss: 0.374\n",
            "[1394,     6] loss: 0.238\n",
            "[1394,     8] loss: 0.284\n",
            "[1394,    10] loss: 0.295\n",
            "[1395,     2] loss: 0.458\n",
            "[1395,     4] loss: 0.268\n",
            "[1395,     6] loss: 0.375\n",
            "[1395,     8] loss: 0.355\n",
            "[1395,    10] loss: 0.311\n",
            "[1396,     2] loss: 0.334\n",
            "[1396,     4] loss: 0.381\n",
            "[1396,     6] loss: 0.317\n",
            "[1396,     8] loss: 0.230\n",
            "[1396,    10] loss: 0.319\n",
            "[1397,     2] loss: 0.221\n",
            "[1397,     4] loss: 0.484\n",
            "[1397,     6] loss: 0.301\n",
            "[1397,     8] loss: 0.273\n",
            "[1397,    10] loss: 0.372\n",
            "[1398,     2] loss: 0.349\n",
            "[1398,     4] loss: 0.348\n",
            "[1398,     6] loss: 0.271\n",
            "[1398,     8] loss: 0.247\n",
            "[1398,    10] loss: 0.247\n",
            "[1399,     2] loss: 0.389\n",
            "[1399,     4] loss: 0.316\n",
            "[1399,     6] loss: 0.313\n",
            "[1399,     8] loss: 0.280\n",
            "[1399,    10] loss: 0.251\n",
            "[1400,     2] loss: 0.232\n",
            "[1400,     4] loss: 0.285\n",
            "[1400,     6] loss: 0.221\n",
            "[1400,     8] loss: 0.374\n",
            "[1400,    10] loss: 0.271\n",
            "[1401,     2] loss: 0.305\n",
            "[1401,     4] loss: 0.308\n",
            "[1401,     6] loss: 0.306\n",
            "[1401,     8] loss: 0.258\n",
            "[1401,    10] loss: 0.286\n",
            "[1402,     2] loss: 0.346\n",
            "[1402,     4] loss: 0.293\n",
            "[1402,     6] loss: 0.286\n",
            "[1402,     8] loss: 0.237\n",
            "[1402,    10] loss: 0.259\n",
            "[1403,     2] loss: 0.332\n",
            "[1403,     4] loss: 0.230\n",
            "[1403,     6] loss: 0.202\n",
            "[1403,     8] loss: 0.320\n",
            "[1403,    10] loss: 0.355\n",
            "[1404,     2] loss: 0.415\n",
            "[1404,     4] loss: 0.221\n",
            "[1404,     6] loss: 0.273\n",
            "[1404,     8] loss: 0.200\n",
            "[1404,    10] loss: 0.317\n",
            "[1405,     2] loss: 0.346\n",
            "[1405,     4] loss: 0.252\n",
            "[1405,     6] loss: 0.361\n",
            "[1405,     8] loss: 0.245\n",
            "[1405,    10] loss: 0.200\n",
            "[1406,     2] loss: 0.263\n",
            "[1406,     4] loss: 0.203\n",
            "[1406,     6] loss: 0.362\n",
            "[1406,     8] loss: 0.256\n",
            "[1406,    10] loss: 0.339\n",
            "[1407,     2] loss: 0.293\n",
            "[1407,     4] loss: 0.258\n",
            "[1407,     6] loss: 0.248\n",
            "[1407,     8] loss: 0.353\n",
            "[1407,    10] loss: 0.194\n",
            "[1408,     2] loss: 0.291\n",
            "[1408,     4] loss: 0.210\n",
            "[1408,     6] loss: 0.316\n",
            "[1408,     8] loss: 0.255\n",
            "[1408,    10] loss: 0.294\n",
            "[1409,     2] loss: 0.225\n",
            "[1409,     4] loss: 0.286\n",
            "[1409,     6] loss: 0.320\n",
            "[1409,     8] loss: 0.300\n",
            "[1409,    10] loss: 0.271\n",
            "[1410,     2] loss: 0.214\n",
            "[1410,     4] loss: 0.281\n",
            "[1410,     6] loss: 0.317\n",
            "[1410,     8] loss: 0.280\n",
            "[1410,    10] loss: 0.327\n",
            "[1411,     2] loss: 0.236\n",
            "[1411,     4] loss: 0.310\n",
            "[1411,     6] loss: 0.320\n",
            "[1411,     8] loss: 0.302\n",
            "[1411,    10] loss: 0.333\n",
            "[1412,     2] loss: 0.274\n",
            "[1412,     4] loss: 0.383\n",
            "[1412,     6] loss: 0.286\n",
            "[1412,     8] loss: 0.284\n",
            "[1412,    10] loss: 0.291\n",
            "[1413,     2] loss: 0.311\n",
            "[1413,     4] loss: 0.234\n",
            "[1413,     6] loss: 0.352\n",
            "[1413,     8] loss: 0.262\n",
            "[1413,    10] loss: 0.280\n",
            "[1414,     2] loss: 0.360\n",
            "[1414,     4] loss: 0.222\n",
            "[1414,     6] loss: 0.293\n",
            "[1414,     8] loss: 0.273\n",
            "[1414,    10] loss: 0.228\n",
            "[1415,     2] loss: 0.214\n",
            "[1415,     4] loss: 0.227\n",
            "[1415,     6] loss: 0.319\n",
            "[1415,     8] loss: 0.332\n",
            "[1415,    10] loss: 0.295\n",
            "[1416,     2] loss: 0.307\n",
            "[1416,     4] loss: 0.274\n",
            "[1416,     6] loss: 0.328\n",
            "[1416,     8] loss: 0.277\n",
            "[1416,    10] loss: 0.243\n",
            "[1417,     2] loss: 0.352\n",
            "[1417,     4] loss: 0.263\n",
            "[1417,     6] loss: 0.250\n",
            "[1417,     8] loss: 0.291\n",
            "[1417,    10] loss: 0.258\n",
            "[1418,     2] loss: 0.266\n",
            "[1418,     4] loss: 0.295\n",
            "[1418,     6] loss: 0.332\n",
            "[1418,     8] loss: 0.244\n",
            "[1418,    10] loss: 0.286\n",
            "[1419,     2] loss: 0.248\n",
            "[1419,     4] loss: 0.298\n",
            "[1419,     6] loss: 0.318\n",
            "[1419,     8] loss: 0.246\n",
            "[1419,    10] loss: 0.362\n",
            "[1420,     2] loss: 0.259\n",
            "[1420,     4] loss: 0.333\n",
            "[1420,     6] loss: 0.447\n",
            "[1420,     8] loss: 0.369\n",
            "[1420,    10] loss: 0.243\n",
            "[1421,     2] loss: 0.234\n",
            "[1421,     4] loss: 0.424\n",
            "[1421,     6] loss: 0.303\n",
            "[1421,     8] loss: 0.290\n",
            "[1421,    10] loss: 0.412\n",
            "[1422,     2] loss: 0.310\n",
            "[1422,     4] loss: 0.312\n",
            "[1422,     6] loss: 0.423\n",
            "[1422,     8] loss: 0.239\n",
            "[1422,    10] loss: 0.292\n",
            "[1423,     2] loss: 0.240\n",
            "[1423,     4] loss: 0.279\n",
            "[1423,     6] loss: 0.329\n",
            "[1423,     8] loss: 0.354\n",
            "[1423,    10] loss: 0.304\n",
            "[1424,     2] loss: 0.311\n",
            "[1424,     4] loss: 0.295\n",
            "[1424,     6] loss: 0.351\n",
            "[1424,     8] loss: 0.286\n",
            "[1424,    10] loss: 0.207\n",
            "[1425,     2] loss: 0.293\n",
            "[1425,     4] loss: 0.240\n",
            "[1425,     6] loss: 0.339\n",
            "[1425,     8] loss: 0.252\n",
            "[1425,    10] loss: 0.266\n",
            "[1426,     2] loss: 0.378\n",
            "[1426,     4] loss: 0.216\n",
            "[1426,     6] loss: 0.294\n",
            "[1426,     8] loss: 0.226\n",
            "[1426,    10] loss: 0.329\n",
            "[1427,     2] loss: 0.269\n",
            "[1427,     4] loss: 0.338\n",
            "[1427,     6] loss: 0.256\n",
            "[1427,     8] loss: 0.346\n",
            "[1427,    10] loss: 0.199\n",
            "[1428,     2] loss: 0.263\n",
            "[1428,     4] loss: 0.293\n",
            "[1428,     6] loss: 0.278\n",
            "[1428,     8] loss: 0.247\n",
            "[1428,    10] loss: 0.304\n",
            "[1429,     2] loss: 0.373\n",
            "[1429,     4] loss: 0.209\n",
            "[1429,     6] loss: 0.242\n",
            "[1429,     8] loss: 0.245\n",
            "[1429,    10] loss: 0.286\n",
            "[1430,     2] loss: 0.284\n",
            "[1430,     4] loss: 0.321\n",
            "[1430,     6] loss: 0.290\n",
            "[1430,     8] loss: 0.290\n",
            "[1430,    10] loss: 0.284\n",
            "[1431,     2] loss: 0.213\n",
            "[1431,     4] loss: 0.351\n",
            "[1431,     6] loss: 0.260\n",
            "[1431,     8] loss: 0.236\n",
            "[1431,    10] loss: 0.300\n",
            "[1432,     2] loss: 0.271\n",
            "[1432,     4] loss: 0.264\n",
            "[1432,     6] loss: 0.281\n",
            "[1432,     8] loss: 0.228\n",
            "[1432,    10] loss: 0.324\n",
            "[1433,     2] loss: 0.215\n",
            "[1433,     4] loss: 0.230\n",
            "[1433,     6] loss: 0.300\n",
            "[1433,     8] loss: 0.287\n",
            "[1433,    10] loss: 0.352\n",
            "[1434,     2] loss: 0.260\n",
            "[1434,     4] loss: 0.167\n",
            "[1434,     6] loss: 0.354\n",
            "[1434,     8] loss: 0.320\n",
            "[1434,    10] loss: 0.286\n",
            "[1435,     2] loss: 0.328\n",
            "[1435,     4] loss: 0.319\n",
            "[1435,     6] loss: 0.297\n",
            "[1435,     8] loss: 0.171\n",
            "[1435,    10] loss: 0.288\n",
            "[1436,     2] loss: 0.271\n",
            "[1436,     4] loss: 0.208\n",
            "[1436,     6] loss: 0.342\n",
            "[1436,     8] loss: 0.374\n",
            "[1436,    10] loss: 0.245\n",
            "[1437,     2] loss: 0.297\n",
            "[1437,     4] loss: 0.295\n",
            "[1437,     6] loss: 0.275\n",
            "[1437,     8] loss: 0.290\n",
            "[1437,    10] loss: 0.346\n",
            "[1438,     2] loss: 0.346\n",
            "[1438,     4] loss: 0.228\n",
            "[1438,     6] loss: 0.248\n",
            "[1438,     8] loss: 0.243\n",
            "[1438,    10] loss: 0.346\n",
            "[1439,     2] loss: 0.260\n",
            "[1439,     4] loss: 0.253\n",
            "[1439,     6] loss: 0.210\n",
            "[1439,     8] loss: 0.377\n",
            "[1439,    10] loss: 0.328\n",
            "[1440,     2] loss: 0.181\n",
            "[1440,     4] loss: 0.352\n",
            "[1440,     6] loss: 0.248\n",
            "[1440,     8] loss: 0.280\n",
            "[1440,    10] loss: 0.305\n",
            "[1441,     2] loss: 0.312\n",
            "[1441,     4] loss: 0.276\n",
            "[1441,     6] loss: 0.342\n",
            "[1441,     8] loss: 0.326\n",
            "[1441,    10] loss: 0.242\n",
            "[1442,     2] loss: 0.314\n",
            "[1442,     4] loss: 0.368\n",
            "[1442,     6] loss: 0.366\n",
            "[1442,     8] loss: 0.264\n",
            "[1442,    10] loss: 0.323\n",
            "[1443,     2] loss: 0.257\n",
            "[1443,     4] loss: 0.396\n",
            "[1443,     6] loss: 0.333\n",
            "[1443,     8] loss: 0.294\n",
            "[1443,    10] loss: 0.354\n",
            "[1444,     2] loss: 0.291\n",
            "[1444,     4] loss: 0.340\n",
            "[1444,     6] loss: 0.300\n",
            "[1444,     8] loss: 0.204\n",
            "[1444,    10] loss: 0.229\n",
            "[1445,     2] loss: 0.245\n",
            "[1445,     4] loss: 0.348\n",
            "[1445,     6] loss: 0.322\n",
            "[1445,     8] loss: 0.265\n",
            "[1445,    10] loss: 0.269\n",
            "[1446,     2] loss: 0.305\n",
            "[1446,     4] loss: 0.382\n",
            "[1446,     6] loss: 0.213\n",
            "[1446,     8] loss: 0.263\n",
            "[1446,    10] loss: 0.323\n",
            "[1447,     2] loss: 0.279\n",
            "[1447,     4] loss: 0.276\n",
            "[1447,     6] loss: 0.325\n",
            "[1447,     8] loss: 0.297\n",
            "[1447,    10] loss: 0.208\n",
            "[1448,     2] loss: 0.279\n",
            "[1448,     4] loss: 0.310\n",
            "[1448,     6] loss: 0.228\n",
            "[1448,     8] loss: 0.261\n",
            "[1448,    10] loss: 0.270\n",
            "[1449,     2] loss: 0.357\n",
            "[1449,     4] loss: 0.318\n",
            "[1449,     6] loss: 0.204\n",
            "[1449,     8] loss: 0.288\n",
            "[1449,    10] loss: 0.198\n",
            "[1450,     2] loss: 0.268\n",
            "[1450,     4] loss: 0.272\n",
            "[1450,     6] loss: 0.423\n",
            "[1450,     8] loss: 0.226\n",
            "[1450,    10] loss: 0.307\n",
            "[1451,     2] loss: 0.296\n",
            "[1451,     4] loss: 0.231\n",
            "[1451,     6] loss: 0.361\n",
            "[1451,     8] loss: 0.257\n",
            "[1451,    10] loss: 0.276\n",
            "[1452,     2] loss: 0.265\n",
            "[1452,     4] loss: 0.278\n",
            "[1452,     6] loss: 0.324\n",
            "[1452,     8] loss: 0.249\n",
            "[1452,    10] loss: 0.352\n",
            "[1453,     2] loss: 0.300\n",
            "[1453,     4] loss: 0.248\n",
            "[1453,     6] loss: 0.322\n",
            "[1453,     8] loss: 0.226\n",
            "[1453,    10] loss: 0.339\n",
            "[1454,     2] loss: 0.253\n",
            "[1454,     4] loss: 0.267\n",
            "[1454,     6] loss: 0.305\n",
            "[1454,     8] loss: 0.303\n",
            "[1454,    10] loss: 0.230\n",
            "[1455,     2] loss: 0.184\n",
            "[1455,     4] loss: 0.382\n",
            "[1455,     6] loss: 0.246\n",
            "[1455,     8] loss: 0.247\n",
            "[1455,    10] loss: 0.347\n",
            "[1456,     2] loss: 0.224\n",
            "[1456,     4] loss: 0.315\n",
            "[1456,     6] loss: 0.243\n",
            "[1456,     8] loss: 0.245\n",
            "[1456,    10] loss: 0.330\n",
            "[1457,     2] loss: 0.258\n",
            "[1457,     4] loss: 0.210\n",
            "[1457,     6] loss: 0.349\n",
            "[1457,     8] loss: 0.337\n",
            "[1457,    10] loss: 0.234\n",
            "[1458,     2] loss: 0.343\n",
            "[1458,     4] loss: 0.260\n",
            "[1458,     6] loss: 0.256\n",
            "[1458,     8] loss: 0.224\n",
            "[1458,    10] loss: 0.305\n",
            "[1459,     2] loss: 0.259\n",
            "[1459,     4] loss: 0.246\n",
            "[1459,     6] loss: 0.297\n",
            "[1459,     8] loss: 0.287\n",
            "[1459,    10] loss: 0.347\n",
            "[1460,     2] loss: 0.258\n",
            "[1460,     4] loss: 0.331\n",
            "[1460,     6] loss: 0.243\n",
            "[1460,     8] loss: 0.272\n",
            "[1460,    10] loss: 0.272\n",
            "[1461,     2] loss: 0.308\n",
            "[1461,     4] loss: 0.247\n",
            "[1461,     6] loss: 0.272\n",
            "[1461,     8] loss: 0.326\n",
            "[1461,    10] loss: 0.315\n",
            "[1462,     2] loss: 0.246\n",
            "[1462,     4] loss: 0.484\n",
            "[1462,     6] loss: 0.312\n",
            "[1462,     8] loss: 0.302\n",
            "[1462,    10] loss: 0.247\n",
            "[1463,     2] loss: 0.262\n",
            "[1463,     4] loss: 0.245\n",
            "[1463,     6] loss: 0.295\n",
            "[1463,     8] loss: 0.279\n",
            "[1463,    10] loss: 0.307\n",
            "[1464,     2] loss: 0.323\n",
            "[1464,     4] loss: 0.299\n",
            "[1464,     6] loss: 0.199\n",
            "[1464,     8] loss: 0.232\n",
            "[1464,    10] loss: 0.317\n",
            "[1465,     2] loss: 0.243\n",
            "[1465,     4] loss: 0.320\n",
            "[1465,     6] loss: 0.295\n",
            "[1465,     8] loss: 0.267\n",
            "[1465,    10] loss: 0.259\n",
            "[1466,     2] loss: 0.289\n",
            "[1466,     4] loss: 0.294\n",
            "[1466,     6] loss: 0.318\n",
            "[1466,     8] loss: 0.213\n",
            "[1466,    10] loss: 0.340\n",
            "[1467,     2] loss: 0.263\n",
            "[1467,     4] loss: 0.305\n",
            "[1467,     6] loss: 0.246\n",
            "[1467,     8] loss: 0.312\n",
            "[1467,    10] loss: 0.309\n",
            "[1468,     2] loss: 0.241\n",
            "[1468,     4] loss: 0.267\n",
            "[1468,     6] loss: 0.274\n",
            "[1468,     8] loss: 0.360\n",
            "[1468,    10] loss: 0.320\n",
            "[1469,     2] loss: 0.321\n",
            "[1469,     4] loss: 0.213\n",
            "[1469,     6] loss: 0.326\n",
            "[1469,     8] loss: 0.393\n",
            "[1469,    10] loss: 0.244\n",
            "[1470,     2] loss: 0.301\n",
            "[1470,     4] loss: 0.308\n",
            "[1470,     6] loss: 0.244\n",
            "[1470,     8] loss: 0.254\n",
            "[1470,    10] loss: 0.282\n",
            "[1471,     2] loss: 0.259\n",
            "[1471,     4] loss: 0.306\n",
            "[1471,     6] loss: 0.229\n",
            "[1471,     8] loss: 0.328\n",
            "[1471,    10] loss: 0.245\n",
            "[1472,     2] loss: 0.351\n",
            "[1472,     4] loss: 0.311\n",
            "[1472,     6] loss: 0.290\n",
            "[1472,     8] loss: 0.229\n",
            "[1472,    10] loss: 0.259\n",
            "[1473,     2] loss: 0.254\n",
            "[1473,     4] loss: 0.336\n",
            "[1473,     6] loss: 0.224\n",
            "[1473,     8] loss: 0.344\n",
            "[1473,    10] loss: 0.241\n",
            "[1474,     2] loss: 0.260\n",
            "[1474,     4] loss: 0.259\n",
            "[1474,     6] loss: 0.339\n",
            "[1474,     8] loss: 0.264\n",
            "[1474,    10] loss: 0.263\n",
            "[1475,     2] loss: 0.289\n",
            "[1475,     4] loss: 0.214\n",
            "[1475,     6] loss: 0.345\n",
            "[1475,     8] loss: 0.332\n",
            "[1475,    10] loss: 0.212\n",
            "[1476,     2] loss: 0.267\n",
            "[1476,     4] loss: 0.335\n",
            "[1476,     6] loss: 0.202\n",
            "[1476,     8] loss: 0.318\n",
            "[1476,    10] loss: 0.327\n",
            "[1477,     2] loss: 0.310\n",
            "[1477,     4] loss: 0.296\n",
            "[1477,     6] loss: 0.343\n",
            "[1477,     8] loss: 0.277\n",
            "[1477,    10] loss: 0.273\n",
            "[1478,     2] loss: 0.312\n",
            "[1478,     4] loss: 0.395\n",
            "[1478,     6] loss: 0.239\n",
            "[1478,     8] loss: 0.279\n",
            "[1478,    10] loss: 0.306\n",
            "[1479,     2] loss: 0.359\n",
            "[1479,     4] loss: 0.308\n",
            "[1479,     6] loss: 0.357\n",
            "[1479,     8] loss: 0.310\n",
            "[1479,    10] loss: 0.315\n",
            "[1480,     2] loss: 0.315\n",
            "[1480,     4] loss: 0.229\n",
            "[1480,     6] loss: 0.404\n",
            "[1480,     8] loss: 0.375\n",
            "[1480,    10] loss: 0.297\n",
            "[1481,     2] loss: 0.357\n",
            "[1481,     4] loss: 0.293\n",
            "[1481,     6] loss: 0.236\n",
            "[1481,     8] loss: 0.300\n",
            "[1481,    10] loss: 0.248\n",
            "[1482,     2] loss: 0.273\n",
            "[1482,     4] loss: 0.289\n",
            "[1482,     6] loss: 0.216\n",
            "[1482,     8] loss: 0.240\n",
            "[1482,    10] loss: 0.359\n",
            "[1483,     2] loss: 0.290\n",
            "[1483,     4] loss: 0.251\n",
            "[1483,     6] loss: 0.265\n",
            "[1483,     8] loss: 0.304\n",
            "[1483,    10] loss: 0.298\n",
            "[1484,     2] loss: 0.334\n",
            "[1484,     4] loss: 0.264\n",
            "[1484,     6] loss: 0.219\n",
            "[1484,     8] loss: 0.434\n",
            "[1484,    10] loss: 0.264\n",
            "[1485,     2] loss: 0.241\n",
            "[1485,     4] loss: 0.327\n",
            "[1485,     6] loss: 0.247\n",
            "[1485,     8] loss: 0.350\n",
            "[1485,    10] loss: 0.320\n",
            "[1486,     2] loss: 0.248\n",
            "[1486,     4] loss: 0.298\n",
            "[1486,     6] loss: 0.335\n",
            "[1486,     8] loss: 0.269\n",
            "[1486,    10] loss: 0.310\n",
            "[1487,     2] loss: 0.293\n",
            "[1487,     4] loss: 0.206\n",
            "[1487,     6] loss: 0.201\n",
            "[1487,     8] loss: 0.403\n",
            "[1487,    10] loss: 0.310\n",
            "[1488,     2] loss: 0.375\n",
            "[1488,     4] loss: 0.263\n",
            "[1488,     6] loss: 0.377\n",
            "[1488,     8] loss: 0.204\n",
            "[1488,    10] loss: 0.393\n",
            "[1489,     2] loss: 0.255\n",
            "[1489,     4] loss: 0.313\n",
            "[1489,     6] loss: 0.424\n",
            "[1489,     8] loss: 0.284\n",
            "[1489,    10] loss: 0.352\n",
            "[1490,     2] loss: 0.346\n",
            "[1490,     4] loss: 0.275\n",
            "[1490,     6] loss: 0.396\n",
            "[1490,     8] loss: 0.340\n",
            "[1490,    10] loss: 0.304\n",
            "[1491,     2] loss: 0.353\n",
            "[1491,     4] loss: 0.320\n",
            "[1491,     6] loss: 0.386\n",
            "[1491,     8] loss: 0.264\n",
            "[1491,    10] loss: 0.347\n",
            "[1492,     2] loss: 0.183\n",
            "[1492,     4] loss: 0.310\n",
            "[1492,     6] loss: 0.363\n",
            "[1492,     8] loss: 0.298\n",
            "[1492,    10] loss: 0.324\n",
            "[1493,     2] loss: 0.230\n",
            "[1493,     4] loss: 0.307\n",
            "[1493,     6] loss: 0.296\n",
            "[1493,     8] loss: 0.274\n",
            "[1493,    10] loss: 0.323\n",
            "[1494,     2] loss: 0.321\n",
            "[1494,     4] loss: 0.284\n",
            "[1494,     6] loss: 0.254\n",
            "[1494,     8] loss: 0.240\n",
            "[1494,    10] loss: 0.291\n",
            "[1495,     2] loss: 0.317\n",
            "[1495,     4] loss: 0.261\n",
            "[1495,     6] loss: 0.248\n",
            "[1495,     8] loss: 0.255\n",
            "[1495,    10] loss: 0.355\n",
            "[1496,     2] loss: 0.289\n",
            "[1496,     4] loss: 0.226\n",
            "[1496,     6] loss: 0.372\n",
            "[1496,     8] loss: 0.289\n",
            "[1496,    10] loss: 0.218\n",
            "[1497,     2] loss: 0.204\n",
            "[1497,     4] loss: 0.342\n",
            "[1497,     6] loss: 0.226\n",
            "[1497,     8] loss: 0.340\n",
            "[1497,    10] loss: 0.348\n",
            "[1498,     2] loss: 0.261\n",
            "[1498,     4] loss: 0.310\n",
            "[1498,     6] loss: 0.255\n",
            "[1498,     8] loss: 0.254\n",
            "[1498,    10] loss: 0.323\n",
            "[1499,     2] loss: 0.253\n",
            "[1499,     4] loss: 0.279\n",
            "[1499,     6] loss: 0.301\n",
            "[1499,     8] loss: 0.263\n",
            "[1499,    10] loss: 0.280\n",
            "[1500,     2] loss: 0.259\n",
            "[1500,     4] loss: 0.249\n",
            "[1500,     6] loss: 0.318\n",
            "[1500,     8] loss: 0.262\n",
            "[1500,    10] loss: 0.286\n",
            "[1501,     2] loss: 0.291\n",
            "[1501,     4] loss: 0.196\n",
            "[1501,     6] loss: 0.362\n",
            "[1501,     8] loss: 0.332\n",
            "[1501,    10] loss: 0.282\n",
            "[1502,     2] loss: 0.284\n",
            "[1502,     4] loss: 0.286\n",
            "[1502,     6] loss: 0.281\n",
            "[1502,     8] loss: 0.308\n",
            "[1502,    10] loss: 0.292\n",
            "[1503,     2] loss: 0.236\n",
            "[1503,     4] loss: 0.362\n",
            "[1503,     6] loss: 0.278\n",
            "[1503,     8] loss: 0.293\n",
            "[1503,    10] loss: 0.246\n",
            "[1504,     2] loss: 0.288\n",
            "[1504,     4] loss: 0.364\n",
            "[1504,     6] loss: 0.219\n",
            "[1504,     8] loss: 0.252\n",
            "[1504,    10] loss: 0.240\n",
            "[1505,     2] loss: 0.279\n",
            "[1505,     4] loss: 0.271\n",
            "[1505,     6] loss: 0.276\n",
            "[1505,     8] loss: 0.269\n",
            "[1505,    10] loss: 0.297\n",
            "[1506,     2] loss: 0.310\n",
            "[1506,     4] loss: 0.259\n",
            "[1506,     6] loss: 0.247\n",
            "[1506,     8] loss: 0.338\n",
            "[1506,    10] loss: 0.259\n",
            "[1507,     2] loss: 0.358\n",
            "[1507,     4] loss: 0.427\n",
            "[1507,     6] loss: 0.253\n",
            "[1507,     8] loss: 0.252\n",
            "[1507,    10] loss: 0.243\n",
            "[1508,     2] loss: 0.252\n",
            "[1508,     4] loss: 0.305\n",
            "[1508,     6] loss: 0.273\n",
            "[1508,     8] loss: 0.241\n",
            "[1508,    10] loss: 0.328\n",
            "[1509,     2] loss: 0.273\n",
            "[1509,     4] loss: 0.336\n",
            "[1509,     6] loss: 0.315\n",
            "[1509,     8] loss: 0.252\n",
            "[1509,    10] loss: 0.341\n",
            "[1510,     2] loss: 0.263\n",
            "[1510,     4] loss: 0.422\n",
            "[1510,     6] loss: 0.218\n",
            "[1510,     8] loss: 0.273\n",
            "[1510,    10] loss: 0.274\n",
            "[1511,     2] loss: 0.231\n",
            "[1511,     4] loss: 0.229\n",
            "[1511,     6] loss: 0.316\n",
            "[1511,     8] loss: 0.323\n",
            "[1511,    10] loss: 0.311\n",
            "[1512,     2] loss: 0.321\n",
            "[1512,     4] loss: 0.294\n",
            "[1512,     6] loss: 0.351\n",
            "[1512,     8] loss: 0.248\n",
            "[1512,    10] loss: 0.235\n",
            "[1513,     2] loss: 0.274\n",
            "[1513,     4] loss: 0.338\n",
            "[1513,     6] loss: 0.309\n",
            "[1513,     8] loss: 0.303\n",
            "[1513,    10] loss: 0.323\n",
            "[1514,     2] loss: 0.351\n",
            "[1514,     4] loss: 0.287\n",
            "[1514,     6] loss: 0.282\n",
            "[1514,     8] loss: 0.258\n",
            "[1514,    10] loss: 0.297\n",
            "[1515,     2] loss: 0.319\n",
            "[1515,     4] loss: 0.279\n",
            "[1515,     6] loss: 0.249\n",
            "[1515,     8] loss: 0.320\n",
            "[1515,    10] loss: 0.267\n",
            "[1516,     2] loss: 0.316\n",
            "[1516,     4] loss: 0.287\n",
            "[1516,     6] loss: 0.237\n",
            "[1516,     8] loss: 0.327\n",
            "[1516,    10] loss: 0.253\n",
            "[1517,     2] loss: 0.264\n",
            "[1517,     4] loss: 0.197\n",
            "[1517,     6] loss: 0.374\n",
            "[1517,     8] loss: 0.221\n",
            "[1517,    10] loss: 0.302\n",
            "[1518,     2] loss: 0.212\n",
            "[1518,     4] loss: 0.372\n",
            "[1518,     6] loss: 0.230\n",
            "[1518,     8] loss: 0.298\n",
            "[1518,    10] loss: 0.319\n",
            "[1519,     2] loss: 0.253\n",
            "[1519,     4] loss: 0.340\n",
            "[1519,     6] loss: 0.369\n",
            "[1519,     8] loss: 0.239\n",
            "[1519,    10] loss: 0.298\n",
            "[1520,     2] loss: 0.330\n",
            "[1520,     4] loss: 0.306\n",
            "[1520,     6] loss: 0.220\n",
            "[1520,     8] loss: 0.242\n",
            "[1520,    10] loss: 0.297\n",
            "[1521,     2] loss: 0.204\n",
            "[1521,     4] loss: 0.286\n",
            "[1521,     6] loss: 0.261\n",
            "[1521,     8] loss: 0.314\n",
            "[1521,    10] loss: 0.331\n",
            "[1522,     2] loss: 0.235\n",
            "[1522,     4] loss: 0.295\n",
            "[1522,     6] loss: 0.363\n",
            "[1522,     8] loss: 0.311\n",
            "[1522,    10] loss: 0.180\n",
            "[1523,     2] loss: 0.296\n",
            "[1523,     4] loss: 0.270\n",
            "[1523,     6] loss: 0.265\n",
            "[1523,     8] loss: 0.241\n",
            "[1523,    10] loss: 0.348\n",
            "[1524,     2] loss: 0.289\n",
            "[1524,     4] loss: 0.227\n",
            "[1524,     6] loss: 0.310\n",
            "[1524,     8] loss: 0.364\n",
            "[1524,    10] loss: 0.298\n",
            "[1525,     2] loss: 0.263\n",
            "[1525,     4] loss: 0.274\n",
            "[1525,     6] loss: 0.242\n",
            "[1525,     8] loss: 0.260\n",
            "[1525,    10] loss: 0.326\n",
            "[1526,     2] loss: 0.231\n",
            "[1526,     4] loss: 0.268\n",
            "[1526,     6] loss: 0.322\n",
            "[1526,     8] loss: 0.223\n",
            "[1526,    10] loss: 0.346\n",
            "[1527,     2] loss: 0.262\n",
            "[1527,     4] loss: 0.338\n",
            "[1527,     6] loss: 0.248\n",
            "[1527,     8] loss: 0.261\n",
            "[1527,    10] loss: 0.292\n",
            "[1528,     2] loss: 0.281\n",
            "[1528,     4] loss: 0.291\n",
            "[1528,     6] loss: 0.209\n",
            "[1528,     8] loss: 0.243\n",
            "[1528,    10] loss: 0.333\n",
            "[1529,     2] loss: 0.267\n",
            "[1529,     4] loss: 0.328\n",
            "[1529,     6] loss: 0.343\n",
            "[1529,     8] loss: 0.263\n",
            "[1529,    10] loss: 0.194\n",
            "[1530,     2] loss: 0.245\n",
            "[1530,     4] loss: 0.251\n",
            "[1530,     6] loss: 0.365\n",
            "[1530,     8] loss: 0.212\n",
            "[1530,    10] loss: 0.275\n",
            "[1531,     2] loss: 0.256\n",
            "[1531,     4] loss: 0.307\n",
            "[1531,     6] loss: 0.305\n",
            "[1531,     8] loss: 0.210\n",
            "[1531,    10] loss: 0.306\n",
            "[1532,     2] loss: 0.301\n",
            "[1532,     4] loss: 0.275\n",
            "[1532,     6] loss: 0.286\n",
            "[1532,     8] loss: 0.292\n",
            "[1532,    10] loss: 0.220\n",
            "[1533,     2] loss: 0.369\n",
            "[1533,     4] loss: 0.327\n",
            "[1533,     6] loss: 0.335\n",
            "[1533,     8] loss: 0.245\n",
            "[1533,    10] loss: 0.254\n",
            "[1534,     2] loss: 0.219\n",
            "[1534,     4] loss: 0.255\n",
            "[1534,     6] loss: 0.268\n",
            "[1534,     8] loss: 0.406\n",
            "[1534,    10] loss: 0.336\n",
            "[1535,     2] loss: 0.322\n",
            "[1535,     4] loss: 0.285\n",
            "[1535,     6] loss: 0.330\n",
            "[1535,     8] loss: 0.201\n",
            "[1535,    10] loss: 0.301\n",
            "[1536,     2] loss: 0.375\n",
            "[1536,     4] loss: 0.239\n",
            "[1536,     6] loss: 0.286\n",
            "[1536,     8] loss: 0.231\n",
            "[1536,    10] loss: 0.274\n",
            "[1537,     2] loss: 0.299\n",
            "[1537,     4] loss: 0.231\n",
            "[1537,     6] loss: 0.288\n",
            "[1537,     8] loss: 0.302\n",
            "[1537,    10] loss: 0.244\n",
            "[1538,     2] loss: 0.214\n",
            "[1538,     4] loss: 0.294\n",
            "[1538,     6] loss: 0.259\n",
            "[1538,     8] loss: 0.302\n",
            "[1538,    10] loss: 0.287\n",
            "[1539,     2] loss: 0.314\n",
            "[1539,     4] loss: 0.246\n",
            "[1539,     6] loss: 0.256\n",
            "[1539,     8] loss: 0.336\n",
            "[1539,    10] loss: 0.258\n",
            "[1540,     2] loss: 0.274\n",
            "[1540,     4] loss: 0.294\n",
            "[1540,     6] loss: 0.355\n",
            "[1540,     8] loss: 0.191\n",
            "[1540,    10] loss: 0.293\n",
            "[1541,     2] loss: 0.296\n",
            "[1541,     4] loss: 0.224\n",
            "[1541,     6] loss: 0.293\n",
            "[1541,     8] loss: 0.398\n",
            "[1541,    10] loss: 0.186\n",
            "[1542,     2] loss: 0.282\n",
            "[1542,     4] loss: 0.279\n",
            "[1542,     6] loss: 0.233\n",
            "[1542,     8] loss: 0.201\n",
            "[1542,    10] loss: 0.347\n",
            "[1543,     2] loss: 0.239\n",
            "[1543,     4] loss: 0.318\n",
            "[1543,     6] loss: 0.317\n",
            "[1543,     8] loss: 0.196\n",
            "[1543,    10] loss: 0.265\n",
            "[1544,     2] loss: 0.302\n",
            "[1544,     4] loss: 0.293\n",
            "[1544,     6] loss: 0.216\n",
            "[1544,     8] loss: 0.312\n",
            "[1544,    10] loss: 0.245\n",
            "[1545,     2] loss: 0.254\n",
            "[1545,     4] loss: 0.293\n",
            "[1545,     6] loss: 0.326\n",
            "[1545,     8] loss: 0.221\n",
            "[1545,    10] loss: 0.320\n",
            "[1546,     2] loss: 0.247\n",
            "[1546,     4] loss: 0.351\n",
            "[1546,     6] loss: 0.278\n",
            "[1546,     8] loss: 0.300\n",
            "[1546,    10] loss: 0.225\n",
            "[1547,     2] loss: 0.294\n",
            "[1547,     4] loss: 0.302\n",
            "[1547,     6] loss: 0.269\n",
            "[1547,     8] loss: 0.250\n",
            "[1547,    10] loss: 0.232\n",
            "[1548,     2] loss: 0.305\n",
            "[1548,     4] loss: 0.325\n",
            "[1548,     6] loss: 0.262\n",
            "[1548,     8] loss: 0.162\n",
            "[1548,    10] loss: 0.308\n",
            "[1549,     2] loss: 0.193\n",
            "[1549,     4] loss: 0.334\n",
            "[1549,     6] loss: 0.325\n",
            "[1549,     8] loss: 0.276\n",
            "[1549,    10] loss: 0.239\n",
            "[1550,     2] loss: 0.344\n",
            "[1550,     4] loss: 0.228\n",
            "[1550,     6] loss: 0.251\n",
            "[1550,     8] loss: 0.246\n",
            "[1550,    10] loss: 0.323\n",
            "[1551,     2] loss: 0.294\n",
            "[1551,     4] loss: 0.306\n",
            "[1551,     6] loss: 0.300\n",
            "[1551,     8] loss: 0.384\n",
            "[1551,    10] loss: 0.252\n",
            "[1552,     2] loss: 0.275\n",
            "[1552,     4] loss: 0.398\n",
            "[1552,     6] loss: 0.282\n",
            "[1552,     8] loss: 0.379\n",
            "[1552,    10] loss: 0.359\n",
            "[1553,     2] loss: 0.271\n",
            "[1553,     4] loss: 0.371\n",
            "[1553,     6] loss: 0.384\n",
            "[1553,     8] loss: 0.265\n",
            "[1553,    10] loss: 0.304\n",
            "[1554,     2] loss: 0.295\n",
            "[1554,     4] loss: 0.275\n",
            "[1554,     6] loss: 0.276\n",
            "[1554,     8] loss: 0.307\n",
            "[1554,    10] loss: 0.323\n",
            "[1555,     2] loss: 0.295\n",
            "[1555,     4] loss: 0.252\n",
            "[1555,     6] loss: 0.406\n",
            "[1555,     8] loss: 0.232\n",
            "[1555,    10] loss: 0.253\n",
            "[1556,     2] loss: 0.257\n",
            "[1556,     4] loss: 0.347\n",
            "[1556,     6] loss: 0.233\n",
            "[1556,     8] loss: 0.272\n",
            "[1556,    10] loss: 0.318\n",
            "[1557,     2] loss: 0.335\n",
            "[1557,     4] loss: 0.380\n",
            "[1557,     6] loss: 0.298\n",
            "[1557,     8] loss: 0.211\n",
            "[1557,    10] loss: 0.239\n",
            "[1558,     2] loss: 0.263\n",
            "[1558,     4] loss: 0.267\n",
            "[1558,     6] loss: 0.259\n",
            "[1558,     8] loss: 0.293\n",
            "[1558,    10] loss: 0.324\n",
            "[1559,     2] loss: 0.359\n",
            "[1559,     4] loss: 0.181\n",
            "[1559,     6] loss: 0.308\n",
            "[1559,     8] loss: 0.316\n",
            "[1559,    10] loss: 0.250\n",
            "[1560,     2] loss: 0.323\n",
            "[1560,     4] loss: 0.301\n",
            "[1560,     6] loss: 0.238\n",
            "[1560,     8] loss: 0.265\n",
            "[1560,    10] loss: 0.274\n",
            "[1561,     2] loss: 0.235\n",
            "[1561,     4] loss: 0.271\n",
            "[1561,     6] loss: 0.287\n",
            "[1561,     8] loss: 0.304\n",
            "[1561,    10] loss: 0.291\n",
            "[1562,     2] loss: 0.378\n",
            "[1562,     4] loss: 0.256\n",
            "[1562,     6] loss: 0.260\n",
            "[1562,     8] loss: 0.205\n",
            "[1562,    10] loss: 0.278\n",
            "[1563,     2] loss: 0.258\n",
            "[1563,     4] loss: 0.312\n",
            "[1563,     6] loss: 0.279\n",
            "[1563,     8] loss: 0.304\n",
            "[1563,    10] loss: 0.291\n",
            "[1564,     2] loss: 0.280\n",
            "[1564,     4] loss: 0.245\n",
            "[1564,     6] loss: 0.332\n",
            "[1564,     8] loss: 0.321\n",
            "[1564,    10] loss: 0.225\n",
            "[1565,     2] loss: 0.381\n",
            "[1565,     4] loss: 0.274\n",
            "[1565,     6] loss: 0.269\n",
            "[1565,     8] loss: 0.227\n",
            "[1565,    10] loss: 0.237\n",
            "[1566,     2] loss: 0.288\n",
            "[1566,     4] loss: 0.256\n",
            "[1566,     6] loss: 0.296\n",
            "[1566,     8] loss: 0.239\n",
            "[1566,    10] loss: 0.330\n",
            "[1567,     2] loss: 0.323\n",
            "[1567,     4] loss: 0.289\n",
            "[1567,     6] loss: 0.264\n",
            "[1567,     8] loss: 0.290\n",
            "[1567,    10] loss: 0.304\n",
            "[1568,     2] loss: 0.254\n",
            "[1568,     4] loss: 0.220\n",
            "[1568,     6] loss: 0.308\n",
            "[1568,     8] loss: 0.301\n",
            "[1568,    10] loss: 0.331\n",
            "[1569,     2] loss: 0.212\n",
            "[1569,     4] loss: 0.268\n",
            "[1569,     6] loss: 0.331\n",
            "[1569,     8] loss: 0.267\n",
            "[1569,    10] loss: 0.278\n",
            "[1570,     2] loss: 0.213\n",
            "[1570,     4] loss: 0.400\n",
            "[1570,     6] loss: 0.321\n",
            "[1570,     8] loss: 0.222\n",
            "[1570,    10] loss: 0.250\n",
            "[1571,     2] loss: 0.259\n",
            "[1571,     4] loss: 0.219\n",
            "[1571,     6] loss: 0.293\n",
            "[1571,     8] loss: 0.326\n",
            "[1571,    10] loss: 0.278\n",
            "[1572,     2] loss: 0.297\n",
            "[1572,     4] loss: 0.232\n",
            "[1572,     6] loss: 0.265\n",
            "[1572,     8] loss: 0.241\n",
            "[1572,    10] loss: 0.337\n",
            "[1573,     2] loss: 0.336\n",
            "[1573,     4] loss: 0.249\n",
            "[1573,     6] loss: 0.277\n",
            "[1573,     8] loss: 0.287\n",
            "[1573,    10] loss: 0.288\n",
            "[1574,     2] loss: 0.277\n",
            "[1574,     4] loss: 0.244\n",
            "[1574,     6] loss: 0.319\n",
            "[1574,     8] loss: 0.372\n",
            "[1574,    10] loss: 0.268\n",
            "[1575,     2] loss: 0.319\n",
            "[1575,     4] loss: 0.284\n",
            "[1575,     6] loss: 0.325\n",
            "[1575,     8] loss: 0.188\n",
            "[1575,    10] loss: 0.277\n",
            "[1576,     2] loss: 0.265\n",
            "[1576,     4] loss: 0.324\n",
            "[1576,     6] loss: 0.245\n",
            "[1576,     8] loss: 0.236\n",
            "[1576,    10] loss: 0.287\n",
            "[1577,     2] loss: 0.233\n",
            "[1577,     4] loss: 0.292\n",
            "[1577,     6] loss: 0.302\n",
            "[1577,     8] loss: 0.323\n",
            "[1577,    10] loss: 0.221\n",
            "[1578,     2] loss: 0.240\n",
            "[1578,     4] loss: 0.391\n",
            "[1578,     6] loss: 0.250\n",
            "[1578,     8] loss: 0.272\n",
            "[1578,    10] loss: 0.183\n",
            "[1579,     2] loss: 0.372\n",
            "[1579,     4] loss: 0.290\n",
            "[1579,     6] loss: 0.243\n",
            "[1579,     8] loss: 0.253\n",
            "[1579,    10] loss: 0.297\n",
            "[1580,     2] loss: 0.207\n",
            "[1580,     4] loss: 0.258\n",
            "[1580,     6] loss: 0.365\n",
            "[1580,     8] loss: 0.317\n",
            "[1580,    10] loss: 0.230\n",
            "[1581,     2] loss: 0.289\n",
            "[1581,     4] loss: 0.272\n",
            "[1581,     6] loss: 0.233\n",
            "[1581,     8] loss: 0.284\n",
            "[1581,    10] loss: 0.319\n",
            "[1582,     2] loss: 0.239\n",
            "[1582,     4] loss: 0.275\n",
            "[1582,     6] loss: 0.256\n",
            "[1582,     8] loss: 0.284\n",
            "[1582,    10] loss: 0.336\n",
            "[1583,     2] loss: 0.259\n",
            "[1583,     4] loss: 0.301\n",
            "[1583,     6] loss: 0.321\n",
            "[1583,     8] loss: 0.285\n",
            "[1583,    10] loss: 0.188\n",
            "[1584,     2] loss: 0.333\n",
            "[1584,     4] loss: 0.237\n",
            "[1584,     6] loss: 0.233\n",
            "[1584,     8] loss: 0.208\n",
            "[1584,    10] loss: 0.323\n",
            "[1585,     2] loss: 0.348\n",
            "[1585,     4] loss: 0.291\n",
            "[1585,     6] loss: 0.229\n",
            "[1585,     8] loss: 0.338\n",
            "[1585,    10] loss: 0.317\n",
            "[1586,     2] loss: 0.243\n",
            "[1586,     4] loss: 0.251\n",
            "[1586,     6] loss: 0.265\n",
            "[1586,     8] loss: 0.325\n",
            "[1586,    10] loss: 0.307\n",
            "[1587,     2] loss: 0.341\n",
            "[1587,     4] loss: 0.301\n",
            "[1587,     6] loss: 0.215\n",
            "[1587,     8] loss: 0.323\n",
            "[1587,    10] loss: 0.296\n",
            "[1588,     2] loss: 0.447\n",
            "[1588,     4] loss: 0.245\n",
            "[1588,     6] loss: 0.206\n",
            "[1588,     8] loss: 0.266\n",
            "[1588,    10] loss: 0.279\n",
            "[1589,     2] loss: 0.265\n",
            "[1589,     4] loss: 0.212\n",
            "[1589,     6] loss: 0.368\n",
            "[1589,     8] loss: 0.278\n",
            "[1589,    10] loss: 0.383\n",
            "[1590,     2] loss: 0.259\n",
            "[1590,     4] loss: 0.261\n",
            "[1590,     6] loss: 0.240\n",
            "[1590,     8] loss: 0.300\n",
            "[1590,    10] loss: 0.344\n",
            "[1591,     2] loss: 0.292\n",
            "[1591,     4] loss: 0.266\n",
            "[1591,     6] loss: 0.214\n",
            "[1591,     8] loss: 0.320\n",
            "[1591,    10] loss: 0.320\n",
            "[1592,     2] loss: 0.319\n",
            "[1592,     4] loss: 0.359\n",
            "[1592,     6] loss: 0.338\n",
            "[1592,     8] loss: 0.246\n",
            "[1592,    10] loss: 0.195\n",
            "[1593,     2] loss: 0.287\n",
            "[1593,     4] loss: 0.365\n",
            "[1593,     6] loss: 0.282\n",
            "[1593,     8] loss: 0.298\n",
            "[1593,    10] loss: 0.204\n",
            "[1594,     2] loss: 0.289\n",
            "[1594,     4] loss: 0.212\n",
            "[1594,     6] loss: 0.287\n",
            "[1594,     8] loss: 0.289\n",
            "[1594,    10] loss: 0.329\n",
            "[1595,     2] loss: 0.265\n",
            "[1595,     4] loss: 0.245\n",
            "[1595,     6] loss: 0.270\n",
            "[1595,     8] loss: 0.277\n",
            "[1595,    10] loss: 0.331\n",
            "[1596,     2] loss: 0.291\n",
            "[1596,     4] loss: 0.249\n",
            "[1596,     6] loss: 0.287\n",
            "[1596,     8] loss: 0.270\n",
            "[1596,    10] loss: 0.298\n",
            "[1597,     2] loss: 0.269\n",
            "[1597,     4] loss: 0.296\n",
            "[1597,     6] loss: 0.305\n",
            "[1597,     8] loss: 0.318\n",
            "[1597,    10] loss: 0.225\n",
            "[1598,     2] loss: 0.231\n",
            "[1598,     4] loss: 0.350\n",
            "[1598,     6] loss: 0.257\n",
            "[1598,     8] loss: 0.288\n",
            "[1598,    10] loss: 0.303\n",
            "[1599,     2] loss: 0.211\n",
            "[1599,     4] loss: 0.362\n",
            "[1599,     6] loss: 0.211\n",
            "[1599,     8] loss: 0.279\n",
            "[1599,    10] loss: 0.271\n",
            "[1600,     2] loss: 0.306\n",
            "[1600,     4] loss: 0.309\n",
            "[1600,     6] loss: 0.288\n",
            "[1600,     8] loss: 0.193\n",
            "[1600,    10] loss: 0.253\n",
            "[1601,     2] loss: 0.356\n",
            "[1601,     4] loss: 0.226\n",
            "[1601,     6] loss: 0.240\n",
            "[1601,     8] loss: 0.302\n",
            "[1601,    10] loss: 0.313\n",
            "[1602,     2] loss: 0.371\n",
            "[1602,     4] loss: 0.330\n",
            "[1602,     6] loss: 0.289\n",
            "[1602,     8] loss: 0.223\n",
            "[1602,    10] loss: 0.298\n",
            "[1603,     2] loss: 0.301\n",
            "[1603,     4] loss: 0.339\n",
            "[1603,     6] loss: 0.304\n",
            "[1603,     8] loss: 0.230\n",
            "[1603,    10] loss: 0.268\n",
            "[1604,     2] loss: 0.323\n",
            "[1604,     4] loss: 0.283\n",
            "[1604,     6] loss: 0.277\n",
            "[1604,     8] loss: 0.257\n",
            "[1604,    10] loss: 0.402\n",
            "[1605,     2] loss: 0.227\n",
            "[1605,     4] loss: 0.274\n",
            "[1605,     6] loss: 0.225\n",
            "[1605,     8] loss: 0.281\n",
            "[1605,    10] loss: 0.404\n",
            "[1606,     2] loss: 0.248\n",
            "[1606,     4] loss: 0.303\n",
            "[1606,     6] loss: 0.229\n",
            "[1606,     8] loss: 0.306\n",
            "[1606,    10] loss: 0.317\n",
            "[1607,     2] loss: 0.265\n",
            "[1607,     4] loss: 0.295\n",
            "[1607,     6] loss: 0.338\n",
            "[1607,     8] loss: 0.216\n",
            "[1607,    10] loss: 0.336\n",
            "[1608,     2] loss: 0.281\n",
            "[1608,     4] loss: 0.213\n",
            "[1608,     6] loss: 0.316\n",
            "[1608,     8] loss: 0.344\n",
            "[1608,    10] loss: 0.233\n",
            "[1609,     2] loss: 0.242\n",
            "[1609,     4] loss: 0.308\n",
            "[1609,     6] loss: 0.277\n",
            "[1609,     8] loss: 0.265\n",
            "[1609,    10] loss: 0.423\n",
            "[1610,     2] loss: 0.339\n",
            "[1610,     4] loss: 0.199\n",
            "[1610,     6] loss: 0.252\n",
            "[1610,     8] loss: 0.361\n",
            "[1610,    10] loss: 0.328\n",
            "[1611,     2] loss: 0.394\n",
            "[1611,     4] loss: 0.245\n",
            "[1611,     6] loss: 0.274\n",
            "[1611,     8] loss: 0.253\n",
            "[1611,    10] loss: 0.248\n",
            "[1612,     2] loss: 0.348\n",
            "[1612,     4] loss: 0.237\n",
            "[1612,     6] loss: 0.207\n",
            "[1612,     8] loss: 0.273\n",
            "[1612,    10] loss: 0.274\n",
            "[1613,     2] loss: 0.294\n",
            "[1613,     4] loss: 0.226\n",
            "[1613,     6] loss: 0.296\n",
            "[1613,     8] loss: 0.278\n",
            "[1613,    10] loss: 0.282\n",
            "[1614,     2] loss: 0.276\n",
            "[1614,     4] loss: 0.264\n",
            "[1614,     6] loss: 0.194\n",
            "[1614,     8] loss: 0.295\n",
            "[1614,    10] loss: 0.338\n",
            "[1615,     2] loss: 0.299\n",
            "[1615,     4] loss: 0.299\n",
            "[1615,     6] loss: 0.247\n",
            "[1615,     8] loss: 0.244\n",
            "[1615,    10] loss: 0.307\n",
            "[1616,     2] loss: 0.333\n",
            "[1616,     4] loss: 0.263\n",
            "[1616,     6] loss: 0.295\n",
            "[1616,     8] loss: 0.317\n",
            "[1616,    10] loss: 0.158\n",
            "[1617,     2] loss: 0.308\n",
            "[1617,     4] loss: 0.246\n",
            "[1617,     6] loss: 0.282\n",
            "[1617,     8] loss: 0.230\n",
            "[1617,    10] loss: 0.308\n",
            "[1618,     2] loss: 0.338\n",
            "[1618,     4] loss: 0.326\n",
            "[1618,     6] loss: 0.250\n",
            "[1618,     8] loss: 0.234\n",
            "[1618,    10] loss: 0.283\n",
            "[1619,     2] loss: 0.356\n",
            "[1619,     4] loss: 0.282\n",
            "[1619,     6] loss: 0.241\n",
            "[1619,     8] loss: 0.217\n",
            "[1619,    10] loss: 0.266\n",
            "[1620,     2] loss: 0.316\n",
            "[1620,     4] loss: 0.322\n",
            "[1620,     6] loss: 0.268\n",
            "[1620,     8] loss: 0.311\n",
            "[1620,    10] loss: 0.182\n",
            "[1621,     2] loss: 0.270\n",
            "[1621,     4] loss: 0.254\n",
            "[1621,     6] loss: 0.353\n",
            "[1621,     8] loss: 0.284\n",
            "[1621,    10] loss: 0.322\n",
            "[1622,     2] loss: 0.351\n",
            "[1622,     4] loss: 0.291\n",
            "[1622,     6] loss: 0.257\n",
            "[1622,     8] loss: 0.244\n",
            "[1622,    10] loss: 0.224\n",
            "[1623,     2] loss: 0.426\n",
            "[1623,     4] loss: 0.260\n",
            "[1623,     6] loss: 0.227\n",
            "[1623,     8] loss: 0.296\n",
            "[1623,    10] loss: 0.347\n",
            "[1624,     2] loss: 0.271\n",
            "[1624,     4] loss: 0.309\n",
            "[1624,     6] loss: 0.328\n",
            "[1624,     8] loss: 0.260\n",
            "[1624,    10] loss: 0.240\n",
            "[1625,     2] loss: 0.283\n",
            "[1625,     4] loss: 0.265\n",
            "[1625,     6] loss: 0.291\n",
            "[1625,     8] loss: 0.263\n",
            "[1625,    10] loss: 0.306\n",
            "[1626,     2] loss: 0.262\n",
            "[1626,     4] loss: 0.385\n",
            "[1626,     6] loss: 0.220\n",
            "[1626,     8] loss: 0.234\n",
            "[1626,    10] loss: 0.261\n",
            "[1627,     2] loss: 0.262\n",
            "[1627,     4] loss: 0.261\n",
            "[1627,     6] loss: 0.283\n",
            "[1627,     8] loss: 0.240\n",
            "[1627,    10] loss: 0.404\n",
            "[1628,     2] loss: 0.340\n",
            "[1628,     4] loss: 0.222\n",
            "[1628,     6] loss: 0.262\n",
            "[1628,     8] loss: 0.382\n",
            "[1628,    10] loss: 0.317\n",
            "[1629,     2] loss: 0.259\n",
            "[1629,     4] loss: 0.304\n",
            "[1629,     6] loss: 0.287\n",
            "[1629,     8] loss: 0.397\n",
            "[1629,    10] loss: 0.243\n",
            "[1630,     2] loss: 0.283\n",
            "[1630,     4] loss: 0.277\n",
            "[1630,     6] loss: 0.365\n",
            "[1630,     8] loss: 0.291\n",
            "[1630,    10] loss: 0.276\n",
            "[1631,     2] loss: 0.405\n",
            "[1631,     4] loss: 0.247\n",
            "[1631,     6] loss: 0.314\n",
            "[1631,     8] loss: 0.332\n",
            "[1631,    10] loss: 0.299\n",
            "[1632,     2] loss: 0.278\n",
            "[1632,     4] loss: 0.356\n",
            "[1632,     6] loss: 0.308\n",
            "[1632,     8] loss: 0.263\n",
            "[1632,    10] loss: 0.278\n",
            "[1633,     2] loss: 0.244\n",
            "[1633,     4] loss: 0.309\n",
            "[1633,     6] loss: 0.330\n",
            "[1633,     8] loss: 0.270\n",
            "[1633,    10] loss: 0.263\n",
            "[1634,     2] loss: 0.287\n",
            "[1634,     4] loss: 0.310\n",
            "[1634,     6] loss: 0.294\n",
            "[1634,     8] loss: 0.227\n",
            "[1634,    10] loss: 0.353\n",
            "[1635,     2] loss: 0.301\n",
            "[1635,     4] loss: 0.338\n",
            "[1635,     6] loss: 0.180\n",
            "[1635,     8] loss: 0.290\n",
            "[1635,    10] loss: 0.230\n",
            "[1636,     2] loss: 0.285\n",
            "[1636,     4] loss: 0.225\n",
            "[1636,     6] loss: 0.306\n",
            "[1636,     8] loss: 0.239\n",
            "[1636,    10] loss: 0.325\n",
            "[1637,     2] loss: 0.259\n",
            "[1637,     4] loss: 0.177\n",
            "[1637,     6] loss: 0.325\n",
            "[1637,     8] loss: 0.273\n",
            "[1637,    10] loss: 0.369\n",
            "[1638,     2] loss: 0.263\n",
            "[1638,     4] loss: 0.339\n",
            "[1638,     6] loss: 0.333\n",
            "[1638,     8] loss: 0.206\n",
            "[1638,    10] loss: 0.230\n",
            "[1639,     2] loss: 0.253\n",
            "[1639,     4] loss: 0.245\n",
            "[1639,     6] loss: 0.326\n",
            "[1639,     8] loss: 0.331\n",
            "[1639,    10] loss: 0.251\n",
            "[1640,     2] loss: 0.354\n",
            "[1640,     4] loss: 0.313\n",
            "[1640,     6] loss: 0.266\n",
            "[1640,     8] loss: 0.350\n",
            "[1640,    10] loss: 0.195\n",
            "[1641,     2] loss: 0.355\n",
            "[1641,     4] loss: 0.234\n",
            "[1641,     6] loss: 0.280\n",
            "[1641,     8] loss: 0.309\n",
            "[1641,    10] loss: 0.286\n",
            "[1642,     2] loss: 0.311\n",
            "[1642,     4] loss: 0.253\n",
            "[1642,     6] loss: 0.339\n",
            "[1642,     8] loss: 0.292\n",
            "[1642,    10] loss: 0.210\n",
            "[1643,     2] loss: 0.226\n",
            "[1643,     4] loss: 0.307\n",
            "[1643,     6] loss: 0.234\n",
            "[1643,     8] loss: 0.328\n",
            "[1643,    10] loss: 0.287\n",
            "[1644,     2] loss: 0.248\n",
            "[1644,     4] loss: 0.349\n",
            "[1644,     6] loss: 0.231\n",
            "[1644,     8] loss: 0.318\n",
            "[1644,    10] loss: 0.260\n",
            "[1645,     2] loss: 0.375\n",
            "[1645,     4] loss: 0.279\n",
            "[1645,     6] loss: 0.229\n",
            "[1645,     8] loss: 0.233\n",
            "[1645,    10] loss: 0.250\n",
            "[1646,     2] loss: 0.317\n",
            "[1646,     4] loss: 0.266\n",
            "[1646,     6] loss: 0.247\n",
            "[1646,     8] loss: 0.281\n",
            "[1646,    10] loss: 0.254\n",
            "[1647,     2] loss: 0.341\n",
            "[1647,     4] loss: 0.365\n",
            "[1647,     6] loss: 0.223\n",
            "[1647,     8] loss: 0.239\n",
            "[1647,    10] loss: 0.231\n",
            "[1648,     2] loss: 0.266\n",
            "[1648,     4] loss: 0.229\n",
            "[1648,     6] loss: 0.330\n",
            "[1648,     8] loss: 0.221\n",
            "[1648,    10] loss: 0.283\n",
            "[1649,     2] loss: 0.284\n",
            "[1649,     4] loss: 0.295\n",
            "[1649,     6] loss: 0.280\n",
            "[1649,     8] loss: 0.251\n",
            "[1649,    10] loss: 0.289\n",
            "[1650,     2] loss: 0.349\n",
            "[1650,     4] loss: 0.229\n",
            "[1650,     6] loss: 0.187\n",
            "[1650,     8] loss: 0.290\n",
            "[1650,    10] loss: 0.308\n",
            "[1651,     2] loss: 0.215\n",
            "[1651,     4] loss: 0.288\n",
            "[1651,     6] loss: 0.283\n",
            "[1651,     8] loss: 0.322\n",
            "[1651,    10] loss: 0.296\n",
            "[1652,     2] loss: 0.266\n",
            "[1652,     4] loss: 0.277\n",
            "[1652,     6] loss: 0.209\n",
            "[1652,     8] loss: 0.326\n",
            "[1652,    10] loss: 0.265\n",
            "[1653,     2] loss: 0.284\n",
            "[1653,     4] loss: 0.272\n",
            "[1653,     6] loss: 0.232\n",
            "[1653,     8] loss: 0.313\n",
            "[1653,    10] loss: 0.275\n",
            "[1654,     2] loss: 0.254\n",
            "[1654,     4] loss: 0.331\n",
            "[1654,     6] loss: 0.259\n",
            "[1654,     8] loss: 0.271\n",
            "[1654,    10] loss: 0.303\n",
            "[1655,     2] loss: 0.296\n",
            "[1655,     4] loss: 0.197\n",
            "[1655,     6] loss: 0.341\n",
            "[1655,     8] loss: 0.341\n",
            "[1655,    10] loss: 0.208\n",
            "[1656,     2] loss: 0.299\n",
            "[1656,     4] loss: 0.273\n",
            "[1656,     6] loss: 0.195\n",
            "[1656,     8] loss: 0.355\n",
            "[1656,    10] loss: 0.320\n",
            "[1657,     2] loss: 0.345\n",
            "[1657,     4] loss: 0.289\n",
            "[1657,     6] loss: 0.206\n",
            "[1657,     8] loss: 0.303\n",
            "[1657,    10] loss: 0.287\n",
            "[1658,     2] loss: 0.336\n",
            "[1658,     4] loss: 0.244\n",
            "[1658,     6] loss: 0.228\n",
            "[1658,     8] loss: 0.287\n",
            "[1658,    10] loss: 0.305\n",
            "[1659,     2] loss: 0.339\n",
            "[1659,     4] loss: 0.357\n",
            "[1659,     6] loss: 0.246\n",
            "[1659,     8] loss: 0.296\n",
            "[1659,    10] loss: 0.154\n",
            "[1660,     2] loss: 0.245\n",
            "[1660,     4] loss: 0.231\n",
            "[1660,     6] loss: 0.306\n",
            "[1660,     8] loss: 0.306\n",
            "[1660,    10] loss: 0.313\n",
            "[1661,     2] loss: 0.260\n",
            "[1661,     4] loss: 0.340\n",
            "[1661,     6] loss: 0.243\n",
            "[1661,     8] loss: 0.304\n",
            "[1661,    10] loss: 0.253\n",
            "[1662,     2] loss: 0.235\n",
            "[1662,     4] loss: 0.314\n",
            "[1662,     6] loss: 0.314\n",
            "[1662,     8] loss: 0.336\n",
            "[1662,    10] loss: 0.227\n",
            "[1663,     2] loss: 0.374\n",
            "[1663,     4] loss: 0.248\n",
            "[1663,     6] loss: 0.296\n",
            "[1663,     8] loss: 0.203\n",
            "[1663,    10] loss: 0.357\n",
            "[1664,     2] loss: 0.238\n",
            "[1664,     4] loss: 0.237\n",
            "[1664,     6] loss: 0.333\n",
            "[1664,     8] loss: 0.292\n",
            "[1664,    10] loss: 0.334\n",
            "[1665,     2] loss: 0.232\n",
            "[1665,     4] loss: 0.301\n",
            "[1665,     6] loss: 0.291\n",
            "[1665,     8] loss: 0.344\n",
            "[1665,    10] loss: 0.247\n",
            "[1666,     2] loss: 0.309\n",
            "[1666,     4] loss: 0.341\n",
            "[1666,     6] loss: 0.308\n",
            "[1666,     8] loss: 0.194\n",
            "[1666,    10] loss: 0.235\n",
            "[1667,     2] loss: 0.350\n",
            "[1667,     4] loss: 0.194\n",
            "[1667,     6] loss: 0.286\n",
            "[1667,     8] loss: 0.304\n",
            "[1667,    10] loss: 0.263\n",
            "[1668,     2] loss: 0.293\n",
            "[1668,     4] loss: 0.291\n",
            "[1668,     6] loss: 0.183\n",
            "[1668,     8] loss: 0.313\n",
            "[1668,    10] loss: 0.320\n",
            "[1669,     2] loss: 0.390\n",
            "[1669,     4] loss: 0.273\n",
            "[1669,     6] loss: 0.170\n",
            "[1669,     8] loss: 0.263\n",
            "[1669,    10] loss: 0.259\n",
            "[1670,     2] loss: 0.212\n",
            "[1670,     4] loss: 0.355\n",
            "[1670,     6] loss: 0.199\n",
            "[1670,     8] loss: 0.294\n",
            "[1670,    10] loss: 0.316\n",
            "[1671,     2] loss: 0.296\n",
            "[1671,     4] loss: 0.359\n",
            "[1671,     6] loss: 0.178\n",
            "[1671,     8] loss: 0.246\n",
            "[1671,    10] loss: 0.301\n",
            "[1672,     2] loss: 0.231\n",
            "[1672,     4] loss: 0.273\n",
            "[1672,     6] loss: 0.367\n",
            "[1672,     8] loss: 0.306\n",
            "[1672,    10] loss: 0.226\n",
            "[1673,     2] loss: 0.252\n",
            "[1673,     4] loss: 0.236\n",
            "[1673,     6] loss: 0.313\n",
            "[1673,     8] loss: 0.327\n",
            "[1673,    10] loss: 0.296\n",
            "[1674,     2] loss: 0.267\n",
            "[1674,     4] loss: 0.225\n",
            "[1674,     6] loss: 0.370\n",
            "[1674,     8] loss: 0.259\n",
            "[1674,    10] loss: 0.284\n",
            "[1675,     2] loss: 0.324\n",
            "[1675,     4] loss: 0.202\n",
            "[1675,     6] loss: 0.286\n",
            "[1675,     8] loss: 0.237\n",
            "[1675,    10] loss: 0.299\n",
            "[1676,     2] loss: 0.293\n",
            "[1676,     4] loss: 0.255\n",
            "[1676,     6] loss: 0.276\n",
            "[1676,     8] loss: 0.270\n",
            "[1676,    10] loss: 0.265\n",
            "[1677,     2] loss: 0.300\n",
            "[1677,     4] loss: 0.323\n",
            "[1677,     6] loss: 0.303\n",
            "[1677,     8] loss: 0.210\n",
            "[1677,    10] loss: 0.308\n",
            "[1678,     2] loss: 0.247\n",
            "[1678,     4] loss: 0.269\n",
            "[1678,     6] loss: 0.321\n",
            "[1678,     8] loss: 0.414\n",
            "[1678,    10] loss: 0.230\n",
            "[1679,     2] loss: 0.318\n",
            "[1679,     4] loss: 0.295\n",
            "[1679,     6] loss: 0.376\n",
            "[1679,     8] loss: 0.269\n",
            "[1679,    10] loss: 0.162\n",
            "[1680,     2] loss: 0.271\n",
            "[1680,     4] loss: 0.277\n",
            "[1680,     6] loss: 0.290\n",
            "[1680,     8] loss: 0.240\n",
            "[1680,    10] loss: 0.362\n",
            "[1681,     2] loss: 0.369\n",
            "[1681,     4] loss: 0.262\n",
            "[1681,     6] loss: 0.355\n",
            "[1681,     8] loss: 0.221\n",
            "[1681,    10] loss: 0.268\n",
            "[1682,     2] loss: 0.296\n",
            "[1682,     4] loss: 0.251\n",
            "[1682,     6] loss: 0.356\n",
            "[1682,     8] loss: 0.272\n",
            "[1682,    10] loss: 0.236\n",
            "[1683,     2] loss: 0.334\n",
            "[1683,     4] loss: 0.205\n",
            "[1683,     6] loss: 0.307\n",
            "[1683,     8] loss: 0.296\n",
            "[1683,    10] loss: 0.308\n",
            "[1684,     2] loss: 0.318\n",
            "[1684,     4] loss: 0.290\n",
            "[1684,     6] loss: 0.312\n",
            "[1684,     8] loss: 0.258\n",
            "[1684,    10] loss: 0.343\n",
            "[1685,     2] loss: 0.291\n",
            "[1685,     4] loss: 0.390\n",
            "[1685,     6] loss: 0.315\n",
            "[1685,     8] loss: 0.213\n",
            "[1685,    10] loss: 0.250\n",
            "[1686,     2] loss: 0.275\n",
            "[1686,     4] loss: 0.298\n",
            "[1686,     6] loss: 0.227\n",
            "[1686,     8] loss: 0.309\n",
            "[1686,    10] loss: 0.318\n",
            "[1687,     2] loss: 0.252\n",
            "[1687,     4] loss: 0.311\n",
            "[1687,     6] loss: 0.212\n",
            "[1687,     8] loss: 0.220\n",
            "[1687,    10] loss: 0.417\n",
            "[1688,     2] loss: 0.297\n",
            "[1688,     4] loss: 0.163\n",
            "[1688,     6] loss: 0.300\n",
            "[1688,     8] loss: 0.358\n",
            "[1688,    10] loss: 0.397\n",
            "[1689,     2] loss: 0.291\n",
            "[1689,     4] loss: 0.289\n",
            "[1689,     6] loss: 0.331\n",
            "[1689,     8] loss: 0.272\n",
            "[1689,    10] loss: 0.283\n",
            "[1690,     2] loss: 0.257\n",
            "[1690,     4] loss: 0.329\n",
            "[1690,     6] loss: 0.327\n",
            "[1690,     8] loss: 0.319\n",
            "[1690,    10] loss: 0.288\n",
            "[1691,     2] loss: 0.302\n",
            "[1691,     4] loss: 0.261\n",
            "[1691,     6] loss: 0.301\n",
            "[1691,     8] loss: 0.218\n",
            "[1691,    10] loss: 0.322\n",
            "[1692,     2] loss: 0.270\n",
            "[1692,     4] loss: 0.215\n",
            "[1692,     6] loss: 0.332\n",
            "[1692,     8] loss: 0.334\n",
            "[1692,    10] loss: 0.308\n",
            "[1693,     2] loss: 0.299\n",
            "[1693,     4] loss: 0.284\n",
            "[1693,     6] loss: 0.293\n",
            "[1693,     8] loss: 0.273\n",
            "[1693,    10] loss: 0.277\n",
            "[1694,     2] loss: 0.256\n",
            "[1694,     4] loss: 0.275\n",
            "[1694,     6] loss: 0.336\n",
            "[1694,     8] loss: 0.314\n",
            "[1694,    10] loss: 0.223\n",
            "[1695,     2] loss: 0.290\n",
            "[1695,     4] loss: 0.209\n",
            "[1695,     6] loss: 0.355\n",
            "[1695,     8] loss: 0.254\n",
            "[1695,    10] loss: 0.257\n",
            "[1696,     2] loss: 0.370\n",
            "[1696,     4] loss: 0.288\n",
            "[1696,     6] loss: 0.344\n",
            "[1696,     8] loss: 0.259\n",
            "[1696,    10] loss: 0.237\n",
            "[1697,     2] loss: 0.298\n",
            "[1697,     4] loss: 0.330\n",
            "[1697,     6] loss: 0.312\n",
            "[1697,     8] loss: 0.279\n",
            "[1697,    10] loss: 0.287\n",
            "[1698,     2] loss: 0.404\n",
            "[1698,     4] loss: 0.259\n",
            "[1698,     6] loss: 0.285\n",
            "[1698,     8] loss: 0.283\n",
            "[1698,    10] loss: 0.362\n",
            "[1699,     2] loss: 0.367\n",
            "[1699,     4] loss: 0.230\n",
            "[1699,     6] loss: 0.255\n",
            "[1699,     8] loss: 0.294\n",
            "[1699,    10] loss: 0.300\n",
            "[1700,     2] loss: 0.347\n",
            "[1700,     4] loss: 0.224\n",
            "[1700,     6] loss: 0.198\n",
            "[1700,     8] loss: 0.290\n",
            "[1700,    10] loss: 0.366\n",
            "[1701,     2] loss: 0.199\n",
            "[1701,     4] loss: 0.317\n",
            "[1701,     6] loss: 0.247\n",
            "[1701,     8] loss: 0.368\n",
            "[1701,    10] loss: 0.253\n",
            "[1702,     2] loss: 0.222\n",
            "[1702,     4] loss: 0.322\n",
            "[1702,     6] loss: 0.252\n",
            "[1702,     8] loss: 0.219\n",
            "[1702,    10] loss: 0.327\n",
            "[1703,     2] loss: 0.460\n",
            "[1703,     4] loss: 0.218\n",
            "[1703,     6] loss: 0.248\n",
            "[1703,     8] loss: 0.241\n",
            "[1703,    10] loss: 0.243\n",
            "[1704,     2] loss: 0.297\n",
            "[1704,     4] loss: 0.318\n",
            "[1704,     6] loss: 0.234\n",
            "[1704,     8] loss: 0.222\n",
            "[1704,    10] loss: 0.277\n",
            "[1705,     2] loss: 0.223\n",
            "[1705,     4] loss: 0.265\n",
            "[1705,     6] loss: 0.313\n",
            "[1705,     8] loss: 0.260\n",
            "[1705,    10] loss: 0.301\n",
            "[1706,     2] loss: 0.267\n",
            "[1706,     4] loss: 0.322\n",
            "[1706,     6] loss: 0.249\n",
            "[1706,     8] loss: 0.368\n",
            "[1706,    10] loss: 0.169\n",
            "[1707,     2] loss: 0.200\n",
            "[1707,     4] loss: 0.214\n",
            "[1707,     6] loss: 0.281\n",
            "[1707,     8] loss: 0.329\n",
            "[1707,    10] loss: 0.348\n",
            "[1708,     2] loss: 0.245\n",
            "[1708,     4] loss: 0.256\n",
            "[1708,     6] loss: 0.409\n",
            "[1708,     8] loss: 0.292\n",
            "[1708,    10] loss: 0.190\n",
            "[1709,     2] loss: 0.185\n",
            "[1709,     4] loss: 0.253\n",
            "[1709,     6] loss: 0.348\n",
            "[1709,     8] loss: 0.323\n",
            "[1709,    10] loss: 0.287\n",
            "[1710,     2] loss: 0.197\n",
            "[1710,     4] loss: 0.409\n",
            "[1710,     6] loss: 0.258\n",
            "[1710,     8] loss: 0.371\n",
            "[1710,    10] loss: 0.238\n",
            "[1711,     2] loss: 0.328\n",
            "[1711,     4] loss: 0.387\n",
            "[1711,     6] loss: 0.275\n",
            "[1711,     8] loss: 0.373\n",
            "[1711,    10] loss: 0.184\n",
            "[1712,     2] loss: 0.391\n",
            "[1712,     4] loss: 0.378\n",
            "[1712,     6] loss: 0.326\n",
            "[1712,     8] loss: 0.390\n",
            "[1712,    10] loss: 0.319\n",
            "[1713,     2] loss: 0.351\n",
            "[1713,     4] loss: 0.309\n",
            "[1713,     6] loss: 0.262\n",
            "[1713,     8] loss: 0.342\n",
            "[1713,    10] loss: 0.249\n",
            "[1714,     2] loss: 0.227\n",
            "[1714,     4] loss: 0.318\n",
            "[1714,     6] loss: 0.295\n",
            "[1714,     8] loss: 0.281\n",
            "[1714,    10] loss: 0.257\n",
            "[1715,     2] loss: 0.209\n",
            "[1715,     4] loss: 0.348\n",
            "[1715,     6] loss: 0.270\n",
            "[1715,     8] loss: 0.309\n",
            "[1715,    10] loss: 0.267\n",
            "[1716,     2] loss: 0.305\n",
            "[1716,     4] loss: 0.247\n",
            "[1716,     6] loss: 0.281\n",
            "[1716,     8] loss: 0.310\n",
            "[1716,    10] loss: 0.325\n",
            "[1717,     2] loss: 0.243\n",
            "[1717,     4] loss: 0.470\n",
            "[1717,     6] loss: 0.301\n",
            "[1717,     8] loss: 0.236\n",
            "[1717,    10] loss: 0.267\n",
            "[1718,     2] loss: 0.324\n",
            "[1718,     4] loss: 0.227\n",
            "[1718,     6] loss: 0.246\n",
            "[1718,     8] loss: 0.319\n",
            "[1718,    10] loss: 0.262\n",
            "[1719,     2] loss: 0.213\n",
            "[1719,     4] loss: 0.287\n",
            "[1719,     6] loss: 0.331\n",
            "[1719,     8] loss: 0.306\n",
            "[1719,    10] loss: 0.288\n",
            "[1720,     2] loss: 0.273\n",
            "[1720,     4] loss: 0.271\n",
            "[1720,     6] loss: 0.333\n",
            "[1720,     8] loss: 0.292\n",
            "[1720,    10] loss: 0.299\n",
            "[1721,     2] loss: 0.301\n",
            "[1721,     4] loss: 0.237\n",
            "[1721,     6] loss: 0.315\n",
            "[1721,     8] loss: 0.239\n",
            "[1721,    10] loss: 0.332\n",
            "[1722,     2] loss: 0.314\n",
            "[1722,     4] loss: 0.209\n",
            "[1722,     6] loss: 0.294\n",
            "[1722,     8] loss: 0.263\n",
            "[1722,    10] loss: 0.254\n",
            "[1723,     2] loss: 0.278\n",
            "[1723,     4] loss: 0.328\n",
            "[1723,     6] loss: 0.222\n",
            "[1723,     8] loss: 0.343\n",
            "[1723,    10] loss: 0.183\n",
            "[1724,     2] loss: 0.262\n",
            "[1724,     4] loss: 0.218\n",
            "[1724,     6] loss: 0.344\n",
            "[1724,     8] loss: 0.278\n",
            "[1724,    10] loss: 0.239\n",
            "[1725,     2] loss: 0.293\n",
            "[1725,     4] loss: 0.216\n",
            "[1725,     6] loss: 0.321\n",
            "[1725,     8] loss: 0.249\n",
            "[1725,    10] loss: 0.275\n",
            "[1726,     2] loss: 0.263\n",
            "[1726,     4] loss: 0.244\n",
            "[1726,     6] loss: 0.318\n",
            "[1726,     8] loss: 0.306\n",
            "[1726,    10] loss: 0.257\n",
            "[1727,     2] loss: 0.278\n",
            "[1727,     4] loss: 0.253\n",
            "[1727,     6] loss: 0.296\n",
            "[1727,     8] loss: 0.340\n",
            "[1727,    10] loss: 0.268\n",
            "[1728,     2] loss: 0.301\n",
            "[1728,     4] loss: 0.267\n",
            "[1728,     6] loss: 0.250\n",
            "[1728,     8] loss: 0.306\n",
            "[1728,    10] loss: 0.254\n",
            "[1729,     2] loss: 0.234\n",
            "[1729,     4] loss: 0.221\n",
            "[1729,     6] loss: 0.364\n",
            "[1729,     8] loss: 0.365\n",
            "[1729,    10] loss: 0.293\n",
            "[1730,     2] loss: 0.287\n",
            "[1730,     4] loss: 0.239\n",
            "[1730,     6] loss: 0.315\n",
            "[1730,     8] loss: 0.272\n",
            "[1730,    10] loss: 0.274\n",
            "[1731,     2] loss: 0.298\n",
            "[1731,     4] loss: 0.230\n",
            "[1731,     6] loss: 0.298\n",
            "[1731,     8] loss: 0.305\n",
            "[1731,    10] loss: 0.298\n",
            "[1732,     2] loss: 0.273\n",
            "[1732,     4] loss: 0.402\n",
            "[1732,     6] loss: 0.313\n",
            "[1732,     8] loss: 0.276\n",
            "[1732,    10] loss: 0.206\n",
            "[1733,     2] loss: 0.340\n",
            "[1733,     4] loss: 0.240\n",
            "[1733,     6] loss: 0.277\n",
            "[1733,     8] loss: 0.272\n",
            "[1733,    10] loss: 0.254\n",
            "[1734,     2] loss: 0.332\n",
            "[1734,     4] loss: 0.242\n",
            "[1734,     6] loss: 0.241\n",
            "[1734,     8] loss: 0.272\n",
            "[1734,    10] loss: 0.255\n",
            "[1735,     2] loss: 0.302\n",
            "[1735,     4] loss: 0.321\n",
            "[1735,     6] loss: 0.287\n",
            "[1735,     8] loss: 0.214\n",
            "[1735,    10] loss: 0.264\n",
            "[1736,     2] loss: 0.262\n",
            "[1736,     4] loss: 0.248\n",
            "[1736,     6] loss: 0.288\n",
            "[1736,     8] loss: 0.273\n",
            "[1736,    10] loss: 0.273\n",
            "[1737,     2] loss: 0.261\n",
            "[1737,     4] loss: 0.308\n",
            "[1737,     6] loss: 0.298\n",
            "[1737,     8] loss: 0.335\n",
            "[1737,    10] loss: 0.148\n",
            "[1738,     2] loss: 0.280\n",
            "[1738,     4] loss: 0.241\n",
            "[1738,     6] loss: 0.256\n",
            "[1738,     8] loss: 0.299\n",
            "[1738,    10] loss: 0.291\n",
            "[1739,     2] loss: 0.238\n",
            "[1739,     4] loss: 0.278\n",
            "[1739,     6] loss: 0.243\n",
            "[1739,     8] loss: 0.299\n",
            "[1739,    10] loss: 0.302\n",
            "[1740,     2] loss: 0.303\n",
            "[1740,     4] loss: 0.355\n",
            "[1740,     6] loss: 0.177\n",
            "[1740,     8] loss: 0.340\n",
            "[1740,    10] loss: 0.249\n",
            "[1741,     2] loss: 0.326\n",
            "[1741,     4] loss: 0.303\n",
            "[1741,     6] loss: 0.294\n",
            "[1741,     8] loss: 0.160\n",
            "[1741,    10] loss: 0.314\n",
            "[1742,     2] loss: 0.214\n",
            "[1742,     4] loss: 0.211\n",
            "[1742,     6] loss: 0.294\n",
            "[1742,     8] loss: 0.321\n",
            "[1742,    10] loss: 0.340\n",
            "[1743,     2] loss: 0.273\n",
            "[1743,     4] loss: 0.367\n",
            "[1743,     6] loss: 0.226\n",
            "[1743,     8] loss: 0.202\n",
            "[1743,    10] loss: 0.330\n",
            "[1744,     2] loss: 0.255\n",
            "[1744,     4] loss: 0.328\n",
            "[1744,     6] loss: 0.258\n",
            "[1744,     8] loss: 0.295\n",
            "[1744,    10] loss: 0.274\n",
            "[1745,     2] loss: 0.259\n",
            "[1745,     4] loss: 0.255\n",
            "[1745,     6] loss: 0.370\n",
            "[1745,     8] loss: 0.313\n",
            "[1745,    10] loss: 0.213\n",
            "[1746,     2] loss: 0.250\n",
            "[1746,     4] loss: 0.320\n",
            "[1746,     6] loss: 0.263\n",
            "[1746,     8] loss: 0.254\n",
            "[1746,    10] loss: 0.307\n",
            "[1747,     2] loss: 0.285\n",
            "[1747,     4] loss: 0.357\n",
            "[1747,     6] loss: 0.293\n",
            "[1747,     8] loss: 0.247\n",
            "[1747,    10] loss: 0.343\n",
            "[1748,     2] loss: 0.371\n",
            "[1748,     4] loss: 0.231\n",
            "[1748,     6] loss: 0.254\n",
            "[1748,     8] loss: 0.222\n",
            "[1748,    10] loss: 0.302\n",
            "[1749,     2] loss: 0.254\n",
            "[1749,     4] loss: 0.400\n",
            "[1749,     6] loss: 0.294\n",
            "[1749,     8] loss: 0.233\n",
            "[1749,    10] loss: 0.342\n",
            "[1750,     2] loss: 0.324\n",
            "[1750,     4] loss: 0.328\n",
            "[1750,     6] loss: 0.315\n",
            "[1750,     8] loss: 0.339\n",
            "[1750,    10] loss: 0.202\n",
            "[1751,     2] loss: 0.277\n",
            "[1751,     4] loss: 0.294\n",
            "[1751,     6] loss: 0.292\n",
            "[1751,     8] loss: 0.499\n",
            "[1751,    10] loss: 0.252\n",
            "[1752,     2] loss: 0.274\n",
            "[1752,     4] loss: 0.377\n",
            "[1752,     6] loss: 0.219\n",
            "[1752,     8] loss: 0.295\n",
            "[1752,    10] loss: 0.317\n",
            "[1753,     2] loss: 0.262\n",
            "[1753,     4] loss: 0.222\n",
            "[1753,     6] loss: 0.258\n",
            "[1753,     8] loss: 0.339\n",
            "[1753,    10] loss: 0.296\n",
            "[1754,     2] loss: 0.299\n",
            "[1754,     4] loss: 0.236\n",
            "[1754,     6] loss: 0.305\n",
            "[1754,     8] loss: 0.273\n",
            "[1754,    10] loss: 0.310\n",
            "[1755,     2] loss: 0.325\n",
            "[1755,     4] loss: 0.337\n",
            "[1755,     6] loss: 0.213\n",
            "[1755,     8] loss: 0.250\n",
            "[1755,    10] loss: 0.281\n",
            "[1756,     2] loss: 0.272\n",
            "[1756,     4] loss: 0.240\n",
            "[1756,     6] loss: 0.366\n",
            "[1756,     8] loss: 0.279\n",
            "[1756,    10] loss: 0.261\n",
            "[1757,     2] loss: 0.396\n",
            "[1757,     4] loss: 0.212\n",
            "[1757,     6] loss: 0.307\n",
            "[1757,     8] loss: 0.275\n",
            "[1757,    10] loss: 0.289\n",
            "[1758,     2] loss: 0.279\n",
            "[1758,     4] loss: 0.338\n",
            "[1758,     6] loss: 0.318\n",
            "[1758,     8] loss: 0.211\n",
            "[1758,    10] loss: 0.326\n",
            "[1759,     2] loss: 0.339\n",
            "[1759,     4] loss: 0.297\n",
            "[1759,     6] loss: 0.289\n",
            "[1759,     8] loss: 0.210\n",
            "[1759,    10] loss: 0.258\n",
            "[1760,     2] loss: 0.338\n",
            "[1760,     4] loss: 0.272\n",
            "[1760,     6] loss: 0.258\n",
            "[1760,     8] loss: 0.269\n",
            "[1760,    10] loss: 0.233\n",
            "[1761,     2] loss: 0.344\n",
            "[1761,     4] loss: 0.225\n",
            "[1761,     6] loss: 0.286\n",
            "[1761,     8] loss: 0.271\n",
            "[1761,    10] loss: 0.260\n",
            "[1762,     2] loss: 0.423\n",
            "[1762,     4] loss: 0.197\n",
            "[1762,     6] loss: 0.240\n",
            "[1762,     8] loss: 0.336\n",
            "[1762,    10] loss: 0.216\n",
            "[1763,     2] loss: 0.287\n",
            "[1763,     4] loss: 0.384\n",
            "[1763,     6] loss: 0.240\n",
            "[1763,     8] loss: 0.249\n",
            "[1763,    10] loss: 0.328\n",
            "[1764,     2] loss: 0.290\n",
            "[1764,     4] loss: 0.256\n",
            "[1764,     6] loss: 0.278\n",
            "[1764,     8] loss: 0.274\n",
            "[1764,    10] loss: 0.272\n",
            "[1765,     2] loss: 0.279\n",
            "[1765,     4] loss: 0.291\n",
            "[1765,     6] loss: 0.318\n",
            "[1765,     8] loss: 0.279\n",
            "[1765,    10] loss: 0.251\n",
            "[1766,     2] loss: 0.358\n",
            "[1766,     4] loss: 0.252\n",
            "[1766,     6] loss: 0.274\n",
            "[1766,     8] loss: 0.290\n",
            "[1766,    10] loss: 0.272\n",
            "[1767,     2] loss: 0.351\n",
            "[1767,     4] loss: 0.334\n",
            "[1767,     6] loss: 0.330\n",
            "[1767,     8] loss: 0.313\n",
            "[1767,    10] loss: 0.344\n",
            "[1768,     2] loss: 0.370\n",
            "[1768,     4] loss: 0.258\n",
            "[1768,     6] loss: 0.283\n",
            "[1768,     8] loss: 0.540\n",
            "[1768,    10] loss: 0.251\n",
            "[1769,     2] loss: 0.527\n",
            "[1769,     4] loss: 0.296\n",
            "[1769,     6] loss: 0.263\n",
            "[1769,     8] loss: 0.348\n",
            "[1769,    10] loss: 0.319\n",
            "[1770,     2] loss: 0.216\n",
            "[1770,     4] loss: 0.521\n",
            "[1770,     6] loss: 0.382\n",
            "[1770,     8] loss: 0.286\n",
            "[1770,    10] loss: 0.333\n",
            "[1771,     2] loss: 0.279\n",
            "[1771,     4] loss: 0.263\n",
            "[1771,     6] loss: 0.278\n",
            "[1771,     8] loss: 0.353\n",
            "[1771,    10] loss: 0.275\n",
            "[1772,     2] loss: 0.306\n",
            "[1772,     4] loss: 0.390\n",
            "[1772,     6] loss: 0.251\n",
            "[1772,     8] loss: 0.247\n",
            "[1772,    10] loss: 0.291\n",
            "[1773,     2] loss: 0.249\n",
            "[1773,     4] loss: 0.237\n",
            "[1773,     6] loss: 0.327\n",
            "[1773,     8] loss: 0.257\n",
            "[1773,    10] loss: 0.323\n",
            "[1774,     2] loss: 0.269\n",
            "[1774,     4] loss: 0.307\n",
            "[1774,     6] loss: 0.313\n",
            "[1774,     8] loss: 0.259\n",
            "[1774,    10] loss: 0.239\n",
            "[1775,     2] loss: 0.255\n",
            "[1775,     4] loss: 0.276\n",
            "[1775,     6] loss: 0.339\n",
            "[1775,     8] loss: 0.266\n",
            "[1775,    10] loss: 0.272\n",
            "[1776,     2] loss: 0.297\n",
            "[1776,     4] loss: 0.331\n",
            "[1776,     6] loss: 0.300\n",
            "[1776,     8] loss: 0.243\n",
            "[1776,    10] loss: 0.252\n",
            "[1777,     2] loss: 0.318\n",
            "[1777,     4] loss: 0.336\n",
            "[1777,     6] loss: 0.347\n",
            "[1777,     8] loss: 0.286\n",
            "[1777,    10] loss: 0.215\n",
            "[1778,     2] loss: 0.312\n",
            "[1778,     4] loss: 0.375\n",
            "[1778,     6] loss: 0.234\n",
            "[1778,     8] loss: 0.236\n",
            "[1778,    10] loss: 0.245\n",
            "[1779,     2] loss: 0.169\n",
            "[1779,     4] loss: 0.282\n",
            "[1779,     6] loss: 0.319\n",
            "[1779,     8] loss: 0.275\n",
            "[1779,    10] loss: 0.339\n",
            "[1780,     2] loss: 0.232\n",
            "[1780,     4] loss: 0.311\n",
            "[1780,     6] loss: 0.206\n",
            "[1780,     8] loss: 0.285\n",
            "[1780,    10] loss: 0.324\n",
            "[1781,     2] loss: 0.252\n",
            "[1781,     4] loss: 0.364\n",
            "[1781,     6] loss: 0.208\n",
            "[1781,     8] loss: 0.344\n",
            "[1781,    10] loss: 0.235\n",
            "[1782,     2] loss: 0.268\n",
            "[1782,     4] loss: 0.310\n",
            "[1782,     6] loss: 0.225\n",
            "[1782,     8] loss: 0.275\n",
            "[1782,    10] loss: 0.286\n",
            "[1783,     2] loss: 0.223\n",
            "[1783,     4] loss: 0.194\n",
            "[1783,     6] loss: 0.291\n",
            "[1783,     8] loss: 0.324\n",
            "[1783,    10] loss: 0.327\n",
            "[1784,     2] loss: 0.266\n",
            "[1784,     4] loss: 0.229\n",
            "[1784,     6] loss: 0.251\n",
            "[1784,     8] loss: 0.299\n",
            "[1784,    10] loss: 0.440\n",
            "[1785,     2] loss: 0.323\n",
            "[1785,     4] loss: 0.299\n",
            "[1785,     6] loss: 0.280\n",
            "[1785,     8] loss: 0.243\n",
            "[1785,    10] loss: 0.249\n",
            "[1786,     2] loss: 0.209\n",
            "[1786,     4] loss: 0.321\n",
            "[1786,     6] loss: 0.428\n",
            "[1786,     8] loss: 0.306\n",
            "[1786,    10] loss: 0.234\n",
            "[1787,     2] loss: 0.275\n",
            "[1787,     4] loss: 0.241\n",
            "[1787,     6] loss: 0.336\n",
            "[1787,     8] loss: 0.210\n",
            "[1787,    10] loss: 0.313\n",
            "[1788,     2] loss: 0.288\n",
            "[1788,     4] loss: 0.339\n",
            "[1788,     6] loss: 0.236\n",
            "[1788,     8] loss: 0.289\n",
            "[1788,    10] loss: 0.279\n",
            "[1789,     2] loss: 0.264\n",
            "[1789,     4] loss: 0.352\n",
            "[1789,     6] loss: 0.265\n",
            "[1789,     8] loss: 0.303\n",
            "[1789,    10] loss: 0.253\n",
            "[1790,     2] loss: 0.283\n",
            "[1790,     4] loss: 0.274\n",
            "[1790,     6] loss: 0.286\n",
            "[1790,     8] loss: 0.238\n",
            "[1790,    10] loss: 0.294\n",
            "[1791,     2] loss: 0.222\n",
            "[1791,     4] loss: 0.352\n",
            "[1791,     6] loss: 0.347\n",
            "[1791,     8] loss: 0.191\n",
            "[1791,    10] loss: 0.390\n",
            "[1792,     2] loss: 0.248\n",
            "[1792,     4] loss: 0.201\n",
            "[1792,     6] loss: 0.350\n",
            "[1792,     8] loss: 0.263\n",
            "[1792,    10] loss: 0.354\n",
            "[1793,     2] loss: 0.254\n",
            "[1793,     4] loss: 0.327\n",
            "[1793,     6] loss: 0.304\n",
            "[1793,     8] loss: 0.282\n",
            "[1793,    10] loss: 0.470\n",
            "[1794,     2] loss: 0.351\n",
            "[1794,     4] loss: 0.248\n",
            "[1794,     6] loss: 0.358\n",
            "[1794,     8] loss: 0.252\n",
            "[1794,    10] loss: 0.284\n",
            "[1795,     2] loss: 0.337\n",
            "[1795,     4] loss: 0.246\n",
            "[1795,     6] loss: 0.301\n",
            "[1795,     8] loss: 0.214\n",
            "[1795,    10] loss: 0.239\n",
            "[1796,     2] loss: 0.289\n",
            "[1796,     4] loss: 0.194\n",
            "[1796,     6] loss: 0.262\n",
            "[1796,     8] loss: 0.299\n",
            "[1796,    10] loss: 0.294\n",
            "[1797,     2] loss: 0.246\n",
            "[1797,     4] loss: 0.318\n",
            "[1797,     6] loss: 0.255\n",
            "[1797,     8] loss: 0.220\n",
            "[1797,    10] loss: 0.314\n",
            "[1798,     2] loss: 0.274\n",
            "[1798,     4] loss: 0.285\n",
            "[1798,     6] loss: 0.281\n",
            "[1798,     8] loss: 0.226\n",
            "[1798,    10] loss: 0.288\n",
            "[1799,     2] loss: 0.255\n",
            "[1799,     4] loss: 0.288\n",
            "[1799,     6] loss: 0.322\n",
            "[1799,     8] loss: 0.275\n",
            "[1799,    10] loss: 0.297\n",
            "[1800,     2] loss: 0.240\n",
            "[1800,     4] loss: 0.319\n",
            "[1800,     6] loss: 0.286\n",
            "[1800,     8] loss: 0.270\n",
            "[1800,    10] loss: 0.271\n",
            "[1801,     2] loss: 0.375\n",
            "[1801,     4] loss: 0.266\n",
            "[1801,     6] loss: 0.201\n",
            "[1801,     8] loss: 0.253\n",
            "[1801,    10] loss: 0.298\n",
            "[1802,     2] loss: 0.262\n",
            "[1802,     4] loss: 0.250\n",
            "[1802,     6] loss: 0.293\n",
            "[1802,     8] loss: 0.317\n",
            "[1802,    10] loss: 0.260\n",
            "[1803,     2] loss: 0.270\n",
            "[1803,     4] loss: 0.232\n",
            "[1803,     6] loss: 0.263\n",
            "[1803,     8] loss: 0.261\n",
            "[1803,    10] loss: 0.332\n",
            "[1804,     2] loss: 0.259\n",
            "[1804,     4] loss: 0.230\n",
            "[1804,     6] loss: 0.306\n",
            "[1804,     8] loss: 0.317\n",
            "[1804,    10] loss: 0.306\n",
            "[1805,     2] loss: 0.304\n",
            "[1805,     4] loss: 0.339\n",
            "[1805,     6] loss: 0.257\n",
            "[1805,     8] loss: 0.192\n",
            "[1805,    10] loss: 0.380\n",
            "[1806,     2] loss: 0.209\n",
            "[1806,     4] loss: 0.398\n",
            "[1806,     6] loss: 0.281\n",
            "[1806,     8] loss: 0.253\n",
            "[1806,    10] loss: 0.248\n",
            "[1807,     2] loss: 0.291\n",
            "[1807,     4] loss: 0.444\n",
            "[1807,     6] loss: 0.318\n",
            "[1807,     8] loss: 0.239\n",
            "[1807,    10] loss: 0.365\n",
            "[1808,     2] loss: 0.356\n",
            "[1808,     4] loss: 0.289\n",
            "[1808,     6] loss: 0.209\n",
            "[1808,     8] loss: 0.266\n",
            "[1808,    10] loss: 0.358\n",
            "[1809,     2] loss: 0.269\n",
            "[1809,     4] loss: 0.253\n",
            "[1809,     6] loss: 0.223\n",
            "[1809,     8] loss: 0.258\n",
            "[1809,    10] loss: 0.401\n",
            "[1810,     2] loss: 0.259\n",
            "[1810,     4] loss: 0.333\n",
            "[1810,     6] loss: 0.312\n",
            "[1810,     8] loss: 0.277\n",
            "[1810,    10] loss: 0.235\n",
            "[1811,     2] loss: 0.291\n",
            "[1811,     4] loss: 0.261\n",
            "[1811,     6] loss: 0.318\n",
            "[1811,     8] loss: 0.232\n",
            "[1811,    10] loss: 0.285\n",
            "[1812,     2] loss: 0.319\n",
            "[1812,     4] loss: 0.363\n",
            "[1812,     6] loss: 0.274\n",
            "[1812,     8] loss: 0.227\n",
            "[1812,    10] loss: 0.288\n",
            "[1813,     2] loss: 0.263\n",
            "[1813,     4] loss: 0.148\n",
            "[1813,     6] loss: 0.319\n",
            "[1813,     8] loss: 0.304\n",
            "[1813,    10] loss: 0.348\n",
            "[1814,     2] loss: 0.283\n",
            "[1814,     4] loss: 0.244\n",
            "[1814,     6] loss: 0.409\n",
            "[1814,     8] loss: 0.317\n",
            "[1814,    10] loss: 0.238\n",
            "[1815,     2] loss: 0.394\n",
            "[1815,     4] loss: 0.243\n",
            "[1815,     6] loss: 0.256\n",
            "[1815,     8] loss: 0.357\n",
            "[1815,    10] loss: 0.256\n",
            "[1816,     2] loss: 0.251\n",
            "[1816,     4] loss: 0.330\n",
            "[1816,     6] loss: 0.322\n",
            "[1816,     8] loss: 0.209\n",
            "[1816,    10] loss: 0.310\n",
            "[1817,     2] loss: 0.208\n",
            "[1817,     4] loss: 0.289\n",
            "[1817,     6] loss: 0.207\n",
            "[1817,     8] loss: 0.329\n",
            "[1817,    10] loss: 0.280\n",
            "[1818,     2] loss: 0.340\n",
            "[1818,     4] loss: 0.157\n",
            "[1818,     6] loss: 0.286\n",
            "[1818,     8] loss: 0.298\n",
            "[1818,    10] loss: 0.294\n",
            "[1819,     2] loss: 0.209\n",
            "[1819,     4] loss: 0.395\n",
            "[1819,     6] loss: 0.309\n",
            "[1819,     8] loss: 0.378\n",
            "[1819,    10] loss: 0.259\n",
            "[1820,     2] loss: 0.218\n",
            "[1820,     4] loss: 0.383\n",
            "[1820,     6] loss: 0.270\n",
            "[1820,     8] loss: 0.278\n",
            "[1820,    10] loss: 0.445\n",
            "[1821,     2] loss: 0.157\n",
            "[1821,     4] loss: 0.324\n",
            "[1821,     6] loss: 0.354\n",
            "[1821,     8] loss: 0.308\n",
            "[1821,    10] loss: 0.372\n",
            "[1822,     2] loss: 0.361\n",
            "[1822,     4] loss: 0.278\n",
            "[1822,     6] loss: 0.205\n",
            "[1822,     8] loss: 0.256\n",
            "[1822,    10] loss: 0.286\n",
            "[1823,     2] loss: 0.331\n",
            "[1823,     4] loss: 0.283\n",
            "[1823,     6] loss: 0.258\n",
            "[1823,     8] loss: 0.272\n",
            "[1823,    10] loss: 0.243\n",
            "[1824,     2] loss: 0.283\n",
            "[1824,     4] loss: 0.265\n",
            "[1824,     6] loss: 0.342\n",
            "[1824,     8] loss: 0.174\n",
            "[1824,    10] loss: 0.292\n",
            "[1825,     2] loss: 0.338\n",
            "[1825,     4] loss: 0.262\n",
            "[1825,     6] loss: 0.313\n",
            "[1825,     8] loss: 0.323\n",
            "[1825,    10] loss: 0.211\n",
            "[1826,     2] loss: 0.385\n",
            "[1826,     4] loss: 0.270\n",
            "[1826,     6] loss: 0.328\n",
            "[1826,     8] loss: 0.306\n",
            "[1826,    10] loss: 0.304\n",
            "[1827,     2] loss: 0.303\n",
            "[1827,     4] loss: 0.338\n",
            "[1827,     6] loss: 0.288\n",
            "[1827,     8] loss: 0.275\n",
            "[1827,    10] loss: 0.364\n",
            "[1828,     2] loss: 0.274\n",
            "[1828,     4] loss: 0.302\n",
            "[1828,     6] loss: 0.315\n",
            "[1828,     8] loss: 0.287\n",
            "[1828,    10] loss: 0.218\n",
            "[1829,     2] loss: 0.282\n",
            "[1829,     4] loss: 0.325\n",
            "[1829,     6] loss: 0.230\n",
            "[1829,     8] loss: 0.260\n",
            "[1829,    10] loss: 0.274\n",
            "[1830,     2] loss: 0.331\n",
            "[1830,     4] loss: 0.285\n",
            "[1830,     6] loss: 0.318\n",
            "[1830,     8] loss: 0.237\n",
            "[1830,    10] loss: 0.203\n",
            "[1831,     2] loss: 0.323\n",
            "[1831,     4] loss: 0.243\n",
            "[1831,     6] loss: 0.368\n",
            "[1831,     8] loss: 0.295\n",
            "[1831,    10] loss: 0.235\n",
            "[1832,     2] loss: 0.271\n",
            "[1832,     4] loss: 0.327\n",
            "[1832,     6] loss: 0.247\n",
            "[1832,     8] loss: 0.250\n",
            "[1832,    10] loss: 0.354\n",
            "[1833,     2] loss: 0.251\n",
            "[1833,     4] loss: 0.285\n",
            "[1833,     6] loss: 0.297\n",
            "[1833,     8] loss: 0.304\n",
            "[1833,    10] loss: 0.257\n",
            "[1834,     2] loss: 0.270\n",
            "[1834,     4] loss: 0.266\n",
            "[1834,     6] loss: 0.396\n",
            "[1834,     8] loss: 0.332\n",
            "[1834,    10] loss: 0.281\n",
            "[1835,     2] loss: 0.291\n",
            "[1835,     4] loss: 0.316\n",
            "[1835,     6] loss: 0.247\n",
            "[1835,     8] loss: 0.385\n",
            "[1835,    10] loss: 0.212\n",
            "[1836,     2] loss: 0.290\n",
            "[1836,     4] loss: 0.313\n",
            "[1836,     6] loss: 0.224\n",
            "[1836,     8] loss: 0.330\n",
            "[1836,    10] loss: 0.270\n",
            "[1837,     2] loss: 0.339\n",
            "[1837,     4] loss: 0.314\n",
            "[1837,     6] loss: 0.287\n",
            "[1837,     8] loss: 0.209\n",
            "[1837,    10] loss: 0.270\n",
            "[1838,     2] loss: 0.261\n",
            "[1838,     4] loss: 0.296\n",
            "[1838,     6] loss: 0.272\n",
            "[1838,     8] loss: 0.308\n",
            "[1838,    10] loss: 0.280\n",
            "[1839,     2] loss: 0.279\n",
            "[1839,     4] loss: 0.253\n",
            "[1839,     6] loss: 0.352\n",
            "[1839,     8] loss: 0.252\n",
            "[1839,    10] loss: 0.245\n",
            "[1840,     2] loss: 0.316\n",
            "[1840,     4] loss: 0.254\n",
            "[1840,     6] loss: 0.291\n",
            "[1840,     8] loss: 0.282\n",
            "[1840,    10] loss: 0.325\n",
            "[1841,     2] loss: 0.268\n",
            "[1841,     4] loss: 0.233\n",
            "[1841,     6] loss: 0.320\n",
            "[1841,     8] loss: 0.213\n",
            "[1841,    10] loss: 0.343\n",
            "[1842,     2] loss: 0.216\n",
            "[1842,     4] loss: 0.298\n",
            "[1842,     6] loss: 0.222\n",
            "[1842,     8] loss: 0.272\n",
            "[1842,    10] loss: 0.358\n",
            "[1843,     2] loss: 0.232\n",
            "[1843,     4] loss: 0.316\n",
            "[1843,     6] loss: 0.365\n",
            "[1843,     8] loss: 0.256\n",
            "[1843,    10] loss: 0.253\n",
            "[1844,     2] loss: 0.232\n",
            "[1844,     4] loss: 0.257\n",
            "[1844,     6] loss: 0.313\n",
            "[1844,     8] loss: 0.191\n",
            "[1844,    10] loss: 0.423\n",
            "[1845,     2] loss: 0.250\n",
            "[1845,     4] loss: 0.294\n",
            "[1845,     6] loss: 0.309\n",
            "[1845,     8] loss: 0.285\n",
            "[1845,    10] loss: 0.223\n",
            "[1846,     2] loss: 0.272\n",
            "[1846,     4] loss: 0.342\n",
            "[1846,     6] loss: 0.175\n",
            "[1846,     8] loss: 0.246\n",
            "[1846,    10] loss: 0.368\n",
            "[1847,     2] loss: 0.279\n",
            "[1847,     4] loss: 0.242\n",
            "[1847,     6] loss: 0.329\n",
            "[1847,     8] loss: 0.254\n",
            "[1847,    10] loss: 0.324\n",
            "[1848,     2] loss: 0.268\n",
            "[1848,     4] loss: 0.232\n",
            "[1848,     6] loss: 0.239\n",
            "[1848,     8] loss: 0.301\n",
            "[1848,    10] loss: 0.319\n",
            "[1849,     2] loss: 0.379\n",
            "[1849,     4] loss: 0.362\n",
            "[1849,     6] loss: 0.297\n",
            "[1849,     8] loss: 0.243\n",
            "[1849,    10] loss: 0.318\n",
            "[1850,     2] loss: 0.212\n",
            "[1850,     4] loss: 0.354\n",
            "[1850,     6] loss: 0.320\n",
            "[1850,     8] loss: 0.295\n",
            "[1850,    10] loss: 0.306\n",
            "[1851,     2] loss: 0.287\n",
            "[1851,     4] loss: 0.369\n",
            "[1851,     6] loss: 0.306\n",
            "[1851,     8] loss: 0.261\n",
            "[1851,    10] loss: 0.308\n",
            "[1852,     2] loss: 0.287\n",
            "[1852,     4] loss: 0.319\n",
            "[1852,     6] loss: 0.223\n",
            "[1852,     8] loss: 0.209\n",
            "[1852,    10] loss: 0.339\n",
            "[1853,     2] loss: 0.248\n",
            "[1853,     4] loss: 0.262\n",
            "[1853,     6] loss: 0.351\n",
            "[1853,     8] loss: 0.335\n",
            "[1853,    10] loss: 0.262\n",
            "[1854,     2] loss: 0.289\n",
            "[1854,     4] loss: 0.325\n",
            "[1854,     6] loss: 0.264\n",
            "[1854,     8] loss: 0.300\n",
            "[1854,    10] loss: 0.257\n",
            "[1855,     2] loss: 0.259\n",
            "[1855,     4] loss: 0.304\n",
            "[1855,     6] loss: 0.248\n",
            "[1855,     8] loss: 0.337\n",
            "[1855,    10] loss: 0.205\n",
            "[1856,     2] loss: 0.288\n",
            "[1856,     4] loss: 0.299\n",
            "[1856,     6] loss: 0.331\n",
            "[1856,     8] loss: 0.193\n",
            "[1856,    10] loss: 0.274\n",
            "[1857,     2] loss: 0.270\n",
            "[1857,     4] loss: 0.310\n",
            "[1857,     6] loss: 0.253\n",
            "[1857,     8] loss: 0.222\n",
            "[1857,    10] loss: 0.320\n",
            "[1858,     2] loss: 0.274\n",
            "[1858,     4] loss: 0.335\n",
            "[1858,     6] loss: 0.297\n",
            "[1858,     8] loss: 0.281\n",
            "[1858,    10] loss: 0.425\n",
            "[1859,     2] loss: 0.265\n",
            "[1859,     4] loss: 0.298\n",
            "[1859,     6] loss: 0.278\n",
            "[1859,     8] loss: 0.295\n",
            "[1859,    10] loss: 0.308\n",
            "[1860,     2] loss: 0.401\n",
            "[1860,     4] loss: 0.434\n",
            "[1860,     6] loss: 0.264\n",
            "[1860,     8] loss: 0.468\n",
            "[1860,    10] loss: 0.255\n",
            "[1861,     2] loss: 0.233\n",
            "[1861,     4] loss: 0.257\n",
            "[1861,     6] loss: 0.340\n",
            "[1861,     8] loss: 0.357\n",
            "[1861,    10] loss: 0.221\n",
            "[1862,     2] loss: 0.331\n",
            "[1862,     4] loss: 0.295\n",
            "[1862,     6] loss: 0.240\n",
            "[1862,     8] loss: 0.326\n",
            "[1862,    10] loss: 0.271\n",
            "[1863,     2] loss: 0.278\n",
            "[1863,     4] loss: 0.358\n",
            "[1863,     6] loss: 0.176\n",
            "[1863,     8] loss: 0.290\n",
            "[1863,    10] loss: 0.287\n",
            "[1864,     2] loss: 0.275\n",
            "[1864,     4] loss: 0.251\n",
            "[1864,     6] loss: 0.265\n",
            "[1864,     8] loss: 0.257\n",
            "[1864,    10] loss: 0.322\n",
            "[1865,     2] loss: 0.284\n",
            "[1865,     4] loss: 0.264\n",
            "[1865,     6] loss: 0.373\n",
            "[1865,     8] loss: 0.258\n",
            "[1865,    10] loss: 0.426\n",
            "[1866,     2] loss: 0.302\n",
            "[1866,     4] loss: 0.303\n",
            "[1866,     6] loss: 0.353\n",
            "[1866,     8] loss: 0.281\n",
            "[1866,    10] loss: 0.328\n",
            "[1867,     2] loss: 0.242\n",
            "[1867,     4] loss: 0.311\n",
            "[1867,     6] loss: 0.253\n",
            "[1867,     8] loss: 0.336\n",
            "[1867,    10] loss: 0.310\n",
            "[1868,     2] loss: 0.288\n",
            "[1868,     4] loss: 0.249\n",
            "[1868,     6] loss: 0.296\n",
            "[1868,     8] loss: 0.315\n",
            "[1868,    10] loss: 0.345\n",
            "[1869,     2] loss: 0.222\n",
            "[1869,     4] loss: 0.475\n",
            "[1869,     6] loss: 0.212\n",
            "[1869,     8] loss: 0.289\n",
            "[1869,    10] loss: 0.314\n",
            "[1870,     2] loss: 0.236\n",
            "[1870,     4] loss: 0.294\n",
            "[1870,     6] loss: 0.326\n",
            "[1870,     8] loss: 0.300\n",
            "[1870,    10] loss: 0.245\n",
            "[1871,     2] loss: 0.270\n",
            "[1871,     4] loss: 0.281\n",
            "[1871,     6] loss: 0.250\n",
            "[1871,     8] loss: 0.286\n",
            "[1871,    10] loss: 0.299\n",
            "[1872,     2] loss: 0.243\n",
            "[1872,     4] loss: 0.274\n",
            "[1872,     6] loss: 0.278\n",
            "[1872,     8] loss: 0.309\n",
            "[1872,    10] loss: 0.328\n",
            "[1873,     2] loss: 0.283\n",
            "[1873,     4] loss: 0.276\n",
            "[1873,     6] loss: 0.297\n",
            "[1873,     8] loss: 0.227\n",
            "[1873,    10] loss: 0.367\n",
            "[1874,     2] loss: 0.379\n",
            "[1874,     4] loss: 0.280\n",
            "[1874,     6] loss: 0.301\n",
            "[1874,     8] loss: 0.229\n",
            "[1874,    10] loss: 0.235\n",
            "[1875,     2] loss: 0.339\n",
            "[1875,     4] loss: 0.251\n",
            "[1875,     6] loss: 0.320\n",
            "[1875,     8] loss: 0.236\n",
            "[1875,    10] loss: 0.268\n",
            "[1876,     2] loss: 0.314\n",
            "[1876,     4] loss: 0.334\n",
            "[1876,     6] loss: 0.253\n",
            "[1876,     8] loss: 0.263\n",
            "[1876,    10] loss: 0.233\n",
            "[1877,     2] loss: 0.243\n",
            "[1877,     4] loss: 0.313\n",
            "[1877,     6] loss: 0.252\n",
            "[1877,     8] loss: 0.304\n",
            "[1877,    10] loss: 0.250\n",
            "[1878,     2] loss: 0.297\n",
            "[1878,     4] loss: 0.272\n",
            "[1878,     6] loss: 0.249\n",
            "[1878,     8] loss: 0.404\n",
            "[1878,    10] loss: 0.259\n",
            "[1879,     2] loss: 0.268\n",
            "[1879,     4] loss: 0.245\n",
            "[1879,     6] loss: 0.364\n",
            "[1879,     8] loss: 0.239\n",
            "[1879,    10] loss: 0.295\n",
            "[1880,     2] loss: 0.353\n",
            "[1880,     4] loss: 0.235\n",
            "[1880,     6] loss: 0.241\n",
            "[1880,     8] loss: 0.325\n",
            "[1880,    10] loss: 0.248\n",
            "[1881,     2] loss: 0.200\n",
            "[1881,     4] loss: 0.398\n",
            "[1881,     6] loss: 0.328\n",
            "[1881,     8] loss: 0.293\n",
            "[1881,    10] loss: 0.223\n",
            "[1882,     2] loss: 0.234\n",
            "[1882,     4] loss: 0.337\n",
            "[1882,     6] loss: 0.285\n",
            "[1882,     8] loss: 0.397\n",
            "[1882,    10] loss: 0.284\n",
            "[1883,     2] loss: 0.262\n",
            "[1883,     4] loss: 0.327\n",
            "[1883,     6] loss: 0.300\n",
            "[1883,     8] loss: 0.365\n",
            "[1883,    10] loss: 0.226\n",
            "[1884,     2] loss: 0.347\n",
            "[1884,     4] loss: 0.287\n",
            "[1884,     6] loss: 0.272\n",
            "[1884,     8] loss: 0.284\n",
            "[1884,    10] loss: 0.289\n",
            "[1885,     2] loss: 0.266\n",
            "[1885,     4] loss: 0.273\n",
            "[1885,     6] loss: 0.412\n",
            "[1885,     8] loss: 0.204\n",
            "[1885,    10] loss: 0.244\n",
            "[1886,     2] loss: 0.229\n",
            "[1886,     4] loss: 0.298\n",
            "[1886,     6] loss: 0.342\n",
            "[1886,     8] loss: 0.238\n",
            "[1886,    10] loss: 0.254\n",
            "[1887,     2] loss: 0.326\n",
            "[1887,     4] loss: 0.211\n",
            "[1887,     6] loss: 0.279\n",
            "[1887,     8] loss: 0.262\n",
            "[1887,    10] loss: 0.321\n",
            "[1888,     2] loss: 0.231\n",
            "[1888,     4] loss: 0.460\n",
            "[1888,     6] loss: 0.274\n",
            "[1888,     8] loss: 0.280\n",
            "[1888,    10] loss: 0.265\n",
            "[1889,     2] loss: 0.268\n",
            "[1889,     4] loss: 0.261\n",
            "[1889,     6] loss: 0.297\n",
            "[1889,     8] loss: 0.331\n",
            "[1889,    10] loss: 0.251\n",
            "[1890,     2] loss: 0.207\n",
            "[1890,     4] loss: 0.360\n",
            "[1890,     6] loss: 0.218\n",
            "[1890,     8] loss: 0.327\n",
            "[1890,    10] loss: 0.310\n",
            "[1891,     2] loss: 0.277\n",
            "[1891,     4] loss: 0.236\n",
            "[1891,     6] loss: 0.348\n",
            "[1891,     8] loss: 0.292\n",
            "[1891,    10] loss: 0.323\n",
            "[1892,     2] loss: 0.269\n",
            "[1892,     4] loss: 0.253\n",
            "[1892,     6] loss: 0.303\n",
            "[1892,     8] loss: 0.206\n",
            "[1892,    10] loss: 0.446\n",
            "[1893,     2] loss: 0.216\n",
            "[1893,     4] loss: 0.315\n",
            "[1893,     6] loss: 0.324\n",
            "[1893,     8] loss: 0.376\n",
            "[1893,    10] loss: 0.229\n",
            "[1894,     2] loss: 0.306\n",
            "[1894,     4] loss: 0.291\n",
            "[1894,     6] loss: 0.242\n",
            "[1894,     8] loss: 0.321\n",
            "[1894,    10] loss: 0.299\n",
            "[1895,     2] loss: 0.306\n",
            "[1895,     4] loss: 0.300\n",
            "[1895,     6] loss: 0.236\n",
            "[1895,     8] loss: 0.296\n",
            "[1895,    10] loss: 0.229\n",
            "[1896,     2] loss: 0.379\n",
            "[1896,     4] loss: 0.211\n",
            "[1896,     6] loss: 0.232\n",
            "[1896,     8] loss: 0.274\n",
            "[1896,    10] loss: 0.276\n",
            "[1897,     2] loss: 0.278\n",
            "[1897,     4] loss: 0.296\n",
            "[1897,     6] loss: 0.280\n",
            "[1897,     8] loss: 0.272\n",
            "[1897,    10] loss: 0.260\n",
            "[1898,     2] loss: 0.288\n",
            "[1898,     4] loss: 0.241\n",
            "[1898,     6] loss: 0.293\n",
            "[1898,     8] loss: 0.390\n",
            "[1898,    10] loss: 0.254\n",
            "[1899,     2] loss: 0.294\n",
            "[1899,     4] loss: 0.356\n",
            "[1899,     6] loss: 0.347\n",
            "[1899,     8] loss: 0.328\n",
            "[1899,    10] loss: 0.245\n",
            "[1900,     2] loss: 0.229\n",
            "[1900,     4] loss: 0.325\n",
            "[1900,     6] loss: 0.257\n",
            "[1900,     8] loss: 0.380\n",
            "[1900,    10] loss: 0.231\n",
            "[1901,     2] loss: 0.230\n",
            "[1901,     4] loss: 0.287\n",
            "[1901,     6] loss: 0.322\n",
            "[1901,     8] loss: 0.253\n",
            "[1901,    10] loss: 0.251\n",
            "[1902,     2] loss: 0.275\n",
            "[1902,     4] loss: 0.232\n",
            "[1902,     6] loss: 0.272\n",
            "[1902,     8] loss: 0.293\n",
            "[1902,    10] loss: 0.303\n",
            "[1903,     2] loss: 0.337\n",
            "[1903,     4] loss: 0.271\n",
            "[1903,     6] loss: 0.209\n",
            "[1903,     8] loss: 0.264\n",
            "[1903,    10] loss: 0.284\n",
            "[1904,     2] loss: 0.315\n",
            "[1904,     4] loss: 0.376\n",
            "[1904,     6] loss: 0.301\n",
            "[1904,     8] loss: 0.193\n",
            "[1904,    10] loss: 0.324\n",
            "[1905,     2] loss: 0.268\n",
            "[1905,     4] loss: 0.334\n",
            "[1905,     6] loss: 0.340\n",
            "[1905,     8] loss: 0.269\n",
            "[1905,    10] loss: 0.239\n",
            "[1906,     2] loss: 0.339\n",
            "[1906,     4] loss: 0.252\n",
            "[1906,     6] loss: 0.249\n",
            "[1906,     8] loss: 0.261\n",
            "[1906,    10] loss: 0.262\n",
            "[1907,     2] loss: 0.338\n",
            "[1907,     4] loss: 0.251\n",
            "[1907,     6] loss: 0.253\n",
            "[1907,     8] loss: 0.291\n",
            "[1907,    10] loss: 0.242\n",
            "[1908,     2] loss: 0.229\n",
            "[1908,     4] loss: 0.236\n",
            "[1908,     6] loss: 0.274\n",
            "[1908,     8] loss: 0.416\n",
            "[1908,    10] loss: 0.358\n",
            "[1909,     2] loss: 0.390\n",
            "[1909,     4] loss: 0.242\n",
            "[1909,     6] loss: 0.266\n",
            "[1909,     8] loss: 0.352\n",
            "[1909,    10] loss: 0.221\n",
            "[1910,     2] loss: 0.212\n",
            "[1910,     4] loss: 0.310\n",
            "[1910,     6] loss: 0.319\n",
            "[1910,     8] loss: 0.318\n",
            "[1910,    10] loss: 0.215\n",
            "[1911,     2] loss: 0.282\n",
            "[1911,     4] loss: 0.297\n",
            "[1911,     6] loss: 0.316\n",
            "[1911,     8] loss: 0.284\n",
            "[1911,    10] loss: 0.233\n",
            "[1912,     2] loss: 0.383\n",
            "[1912,     4] loss: 0.236\n",
            "[1912,     6] loss: 0.251\n",
            "[1912,     8] loss: 0.272\n",
            "[1912,    10] loss: 0.286\n",
            "[1913,     2] loss: 0.204\n",
            "[1913,     4] loss: 0.250\n",
            "[1913,     6] loss: 0.363\n",
            "[1913,     8] loss: 0.295\n",
            "[1913,    10] loss: 0.306\n",
            "[1914,     2] loss: 0.243\n",
            "[1914,     4] loss: 0.246\n",
            "[1914,     6] loss: 0.299\n",
            "[1914,     8] loss: 0.246\n",
            "[1914,    10] loss: 0.310\n",
            "[1915,     2] loss: 0.259\n",
            "[1915,     4] loss: 0.404\n",
            "[1915,     6] loss: 0.304\n",
            "[1915,     8] loss: 0.184\n",
            "[1915,    10] loss: 0.219\n",
            "[1916,     2] loss: 0.256\n",
            "[1916,     4] loss: 0.278\n",
            "[1916,     6] loss: 0.238\n",
            "[1916,     8] loss: 0.271\n",
            "[1916,    10] loss: 0.336\n",
            "[1917,     2] loss: 0.237\n",
            "[1917,     4] loss: 0.255\n",
            "[1917,     6] loss: 0.348\n",
            "[1917,     8] loss: 0.248\n",
            "[1917,    10] loss: 0.289\n",
            "[1918,     2] loss: 0.251\n",
            "[1918,     4] loss: 0.324\n",
            "[1918,     6] loss: 0.242\n",
            "[1918,     8] loss: 0.317\n",
            "[1918,    10] loss: 0.294\n",
            "[1919,     2] loss: 0.318\n",
            "[1919,     4] loss: 0.363\n",
            "[1919,     6] loss: 0.268\n",
            "[1919,     8] loss: 0.251\n",
            "[1919,    10] loss: 0.202\n",
            "[1920,     2] loss: 0.285\n",
            "[1920,     4] loss: 0.293\n",
            "[1920,     6] loss: 0.256\n",
            "[1920,     8] loss: 0.281\n",
            "[1920,    10] loss: 0.225\n",
            "[1921,     2] loss: 0.263\n",
            "[1921,     4] loss: 0.262\n",
            "[1921,     6] loss: 0.285\n",
            "[1921,     8] loss: 0.347\n",
            "[1921,    10] loss: 0.192\n",
            "[1922,     2] loss: 0.189\n",
            "[1922,     4] loss: 0.302\n",
            "[1922,     6] loss: 0.257\n",
            "[1922,     8] loss: 0.296\n",
            "[1922,    10] loss: 0.317\n",
            "[1923,     2] loss: 0.258\n",
            "[1923,     4] loss: 0.319\n",
            "[1923,     6] loss: 0.255\n",
            "[1923,     8] loss: 0.355\n",
            "[1923,    10] loss: 0.279\n",
            "[1924,     2] loss: 0.276\n",
            "[1924,     4] loss: 0.229\n",
            "[1924,     6] loss: 0.303\n",
            "[1924,     8] loss: 0.294\n",
            "[1924,    10] loss: 0.274\n",
            "[1925,     2] loss: 0.249\n",
            "[1925,     4] loss: 0.330\n",
            "[1925,     6] loss: 0.354\n",
            "[1925,     8] loss: 0.228\n",
            "[1925,    10] loss: 0.287\n",
            "[1926,     2] loss: 0.282\n",
            "[1926,     4] loss: 0.200\n",
            "[1926,     6] loss: 0.394\n",
            "[1926,     8] loss: 0.308\n",
            "[1926,    10] loss: 0.216\n",
            "[1927,     2] loss: 0.287\n",
            "[1927,     4] loss: 0.240\n",
            "[1927,     6] loss: 0.277\n",
            "[1927,     8] loss: 0.293\n",
            "[1927,    10] loss: 0.312\n",
            "[1928,     2] loss: 0.312\n",
            "[1928,     4] loss: 0.304\n",
            "[1928,     6] loss: 0.266\n",
            "[1928,     8] loss: 0.326\n",
            "[1928,    10] loss: 0.257\n",
            "[1929,     2] loss: 0.314\n",
            "[1929,     4] loss: 0.267\n",
            "[1929,     6] loss: 0.252\n",
            "[1929,     8] loss: 0.267\n",
            "[1929,    10] loss: 0.330\n",
            "[1930,     2] loss: 0.219\n",
            "[1930,     4] loss: 0.353\n",
            "[1930,     6] loss: 0.322\n",
            "[1930,     8] loss: 0.212\n",
            "[1930,    10] loss: 0.364\n",
            "[1931,     2] loss: 0.319\n",
            "[1931,     4] loss: 0.247\n",
            "[1931,     6] loss: 0.244\n",
            "[1931,     8] loss: 0.349\n",
            "[1931,    10] loss: 0.234\n",
            "[1932,     2] loss: 0.279\n",
            "[1932,     4] loss: 0.230\n",
            "[1932,     6] loss: 0.346\n",
            "[1932,     8] loss: 0.219\n",
            "[1932,    10] loss: 0.376\n",
            "[1933,     2] loss: 0.296\n",
            "[1933,     4] loss: 0.263\n",
            "[1933,     6] loss: 0.233\n",
            "[1933,     8] loss: 0.278\n",
            "[1933,    10] loss: 0.266\n",
            "[1934,     2] loss: 0.312\n",
            "[1934,     4] loss: 0.366\n",
            "[1934,     6] loss: 0.240\n",
            "[1934,     8] loss: 0.345\n",
            "[1934,    10] loss: 0.276\n",
            "[1935,     2] loss: 0.236\n",
            "[1935,     4] loss: 0.343\n",
            "[1935,     6] loss: 0.234\n",
            "[1935,     8] loss: 0.317\n",
            "[1935,    10] loss: 0.256\n",
            "[1936,     2] loss: 0.291\n",
            "[1936,     4] loss: 0.383\n",
            "[1936,     6] loss: 0.354\n",
            "[1936,     8] loss: 0.130\n",
            "[1936,    10] loss: 0.368\n",
            "[1937,     2] loss: 0.324\n",
            "[1937,     4] loss: 0.324\n",
            "[1937,     6] loss: 0.204\n",
            "[1937,     8] loss: 0.335\n",
            "[1937,    10] loss: 0.241\n",
            "[1938,     2] loss: 0.251\n",
            "[1938,     4] loss: 0.246\n",
            "[1938,     6] loss: 0.316\n",
            "[1938,     8] loss: 0.350\n",
            "[1938,    10] loss: 0.292\n",
            "[1939,     2] loss: 0.347\n",
            "[1939,     4] loss: 0.305\n",
            "[1939,     6] loss: 0.287\n",
            "[1939,     8] loss: 0.255\n",
            "[1939,    10] loss: 0.208\n",
            "[1940,     2] loss: 0.289\n",
            "[1940,     4] loss: 0.353\n",
            "[1940,     6] loss: 0.276\n",
            "[1940,     8] loss: 0.228\n",
            "[1940,    10] loss: 0.319\n",
            "[1941,     2] loss: 0.301\n",
            "[1941,     4] loss: 0.298\n",
            "[1941,     6] loss: 0.297\n",
            "[1941,     8] loss: 0.213\n",
            "[1941,    10] loss: 0.242\n",
            "[1942,     2] loss: 0.248\n",
            "[1942,     4] loss: 0.355\n",
            "[1942,     6] loss: 0.252\n",
            "[1942,     8] loss: 0.310\n",
            "[1942,    10] loss: 0.241\n",
            "[1943,     2] loss: 0.273\n",
            "[1943,     4] loss: 0.342\n",
            "[1943,     6] loss: 0.254\n",
            "[1943,     8] loss: 0.193\n",
            "[1943,    10] loss: 0.296\n",
            "[1944,     2] loss: 0.267\n",
            "[1944,     4] loss: 0.277\n",
            "[1944,     6] loss: 0.261\n",
            "[1944,     8] loss: 0.249\n",
            "[1944,    10] loss: 0.348\n",
            "[1945,     2] loss: 0.215\n",
            "[1945,     4] loss: 0.237\n",
            "[1945,     6] loss: 0.343\n",
            "[1945,     8] loss: 0.320\n",
            "[1945,    10] loss: 0.309\n",
            "[1946,     2] loss: 0.253\n",
            "[1946,     4] loss: 0.258\n",
            "[1946,     6] loss: 0.326\n",
            "[1946,     8] loss: 0.288\n",
            "[1946,    10] loss: 0.312\n",
            "[1947,     2] loss: 0.227\n",
            "[1947,     4] loss: 0.434\n",
            "[1947,     6] loss: 0.241\n",
            "[1947,     8] loss: 0.314\n",
            "[1947,    10] loss: 0.286\n",
            "[1948,     2] loss: 0.303\n",
            "[1948,     4] loss: 0.209\n",
            "[1948,     6] loss: 0.270\n",
            "[1948,     8] loss: 0.272\n",
            "[1948,    10] loss: 0.362\n",
            "[1949,     2] loss: 0.287\n",
            "[1949,     4] loss: 0.228\n",
            "[1949,     6] loss: 0.233\n",
            "[1949,     8] loss: 0.267\n",
            "[1949,    10] loss: 0.364\n",
            "[1950,     2] loss: 0.333\n",
            "[1950,     4] loss: 0.262\n",
            "[1950,     6] loss: 0.337\n",
            "[1950,     8] loss: 0.212\n",
            "[1950,    10] loss: 0.294\n",
            "[1951,     2] loss: 0.250\n",
            "[1951,     4] loss: 0.202\n",
            "[1951,     6] loss: 0.262\n",
            "[1951,     8] loss: 0.390\n",
            "[1951,    10] loss: 0.281\n",
            "[1952,     2] loss: 0.322\n",
            "[1952,     4] loss: 0.236\n",
            "[1952,     6] loss: 0.293\n",
            "[1952,     8] loss: 0.285\n",
            "[1952,    10] loss: 0.288\n",
            "[1953,     2] loss: 0.285\n",
            "[1953,     4] loss: 0.284\n",
            "[1953,     6] loss: 0.232\n",
            "[1953,     8] loss: 0.249\n",
            "[1953,    10] loss: 0.306\n",
            "[1954,     2] loss: 0.289\n",
            "[1954,     4] loss: 0.284\n",
            "[1954,     6] loss: 0.217\n",
            "[1954,     8] loss: 0.279\n",
            "[1954,    10] loss: 0.295\n",
            "[1955,     2] loss: 0.313\n",
            "[1955,     4] loss: 0.199\n",
            "[1955,     6] loss: 0.299\n",
            "[1955,     8] loss: 0.293\n",
            "[1955,    10] loss: 0.251\n",
            "[1956,     2] loss: 0.253\n",
            "[1956,     4] loss: 0.404\n",
            "[1956,     6] loss: 0.263\n",
            "[1956,     8] loss: 0.227\n",
            "[1956,    10] loss: 0.224\n",
            "[1957,     2] loss: 0.313\n",
            "[1957,     4] loss: 0.258\n",
            "[1957,     6] loss: 0.258\n",
            "[1957,     8] loss: 0.317\n",
            "[1957,    10] loss: 0.289\n",
            "[1958,     2] loss: 0.266\n",
            "[1958,     4] loss: 0.374\n",
            "[1958,     6] loss: 0.215\n",
            "[1958,     8] loss: 0.224\n",
            "[1958,    10] loss: 0.307\n",
            "[1959,     2] loss: 0.204\n",
            "[1959,     4] loss: 0.271\n",
            "[1959,     6] loss: 0.364\n",
            "[1959,     8] loss: 0.271\n",
            "[1959,    10] loss: 0.268\n",
            "[1960,     2] loss: 0.350\n",
            "[1960,     4] loss: 0.245\n",
            "[1960,     6] loss: 0.234\n",
            "[1960,     8] loss: 0.315\n",
            "[1960,    10] loss: 0.329\n",
            "[1961,     2] loss: 0.279\n",
            "[1961,     4] loss: 0.355\n",
            "[1961,     6] loss: 0.230\n",
            "[1961,     8] loss: 0.290\n",
            "[1961,    10] loss: 0.280\n",
            "[1962,     2] loss: 0.208\n",
            "[1962,     4] loss: 0.348\n",
            "[1962,     6] loss: 0.253\n",
            "[1962,     8] loss: 0.253\n",
            "[1962,    10] loss: 0.316\n",
            "[1963,     2] loss: 0.272\n",
            "[1963,     4] loss: 0.268\n",
            "[1963,     6] loss: 0.311\n",
            "[1963,     8] loss: 0.256\n",
            "[1963,    10] loss: 0.297\n",
            "[1964,     2] loss: 0.295\n",
            "[1964,     4] loss: 0.296\n",
            "[1964,     6] loss: 0.286\n",
            "[1964,     8] loss: 0.262\n",
            "[1964,    10] loss: 0.303\n",
            "[1965,     2] loss: 0.253\n",
            "[1965,     4] loss: 0.171\n",
            "[1965,     6] loss: 0.374\n",
            "[1965,     8] loss: 0.273\n",
            "[1965,    10] loss: 0.280\n",
            "[1966,     2] loss: 0.260\n",
            "[1966,     4] loss: 0.212\n",
            "[1966,     6] loss: 0.337\n",
            "[1966,     8] loss: 0.246\n",
            "[1966,    10] loss: 0.337\n",
            "[1967,     2] loss: 0.222\n",
            "[1967,     4] loss: 0.322\n",
            "[1967,     6] loss: 0.209\n",
            "[1967,     8] loss: 0.305\n",
            "[1967,    10] loss: 0.291\n",
            "[1968,     2] loss: 0.213\n",
            "[1968,     4] loss: 0.222\n",
            "[1968,     6] loss: 0.281\n",
            "[1968,     8] loss: 0.226\n",
            "[1968,    10] loss: 0.421\n",
            "[1969,     2] loss: 0.238\n",
            "[1969,     4] loss: 0.231\n",
            "[1969,     6] loss: 0.230\n",
            "[1969,     8] loss: 0.292\n",
            "[1969,    10] loss: 0.367\n",
            "[1970,     2] loss: 0.286\n",
            "[1970,     4] loss: 0.363\n",
            "[1970,     6] loss: 0.226\n",
            "[1970,     8] loss: 0.228\n",
            "[1970,    10] loss: 0.273\n",
            "[1971,     2] loss: 0.247\n",
            "[1971,     4] loss: 0.262\n",
            "[1971,     6] loss: 0.288\n",
            "[1971,     8] loss: 0.266\n",
            "[1971,    10] loss: 0.302\n",
            "[1972,     2] loss: 0.260\n",
            "[1972,     4] loss: 0.227\n",
            "[1972,     6] loss: 0.324\n",
            "[1972,     8] loss: 0.325\n",
            "[1972,    10] loss: 0.265\n",
            "[1973,     2] loss: 0.267\n",
            "[1973,     4] loss: 0.299\n",
            "[1973,     6] loss: 0.329\n",
            "[1973,     8] loss: 0.248\n",
            "[1973,    10] loss: 0.279\n",
            "[1974,     2] loss: 0.348\n",
            "[1974,     4] loss: 0.234\n",
            "[1974,     6] loss: 0.230\n",
            "[1974,     8] loss: 0.321\n",
            "[1974,    10] loss: 0.295\n",
            "[1975,     2] loss: 0.215\n",
            "[1975,     4] loss: 0.405\n",
            "[1975,     6] loss: 0.204\n",
            "[1975,     8] loss: 0.298\n",
            "[1975,    10] loss: 0.287\n",
            "[1976,     2] loss: 0.273\n",
            "[1976,     4] loss: 0.252\n",
            "[1976,     6] loss: 0.286\n",
            "[1976,     8] loss: 0.251\n",
            "[1976,    10] loss: 0.297\n",
            "[1977,     2] loss: 0.241\n",
            "[1977,     4] loss: 0.197\n",
            "[1977,     6] loss: 0.270\n",
            "[1977,     8] loss: 0.320\n",
            "[1977,    10] loss: 0.310\n",
            "[1978,     2] loss: 0.167\n",
            "[1978,     4] loss: 0.245\n",
            "[1978,     6] loss: 0.294\n",
            "[1978,     8] loss: 0.366\n",
            "[1978,    10] loss: 0.278\n",
            "[1979,     2] loss: 0.311\n",
            "[1979,     4] loss: 0.221\n",
            "[1979,     6] loss: 0.320\n",
            "[1979,     8] loss: 0.258\n",
            "[1979,    10] loss: 0.250\n",
            "[1980,     2] loss: 0.261\n",
            "[1980,     4] loss: 0.259\n",
            "[1980,     6] loss: 0.273\n",
            "[1980,     8] loss: 0.353\n",
            "[1980,    10] loss: 0.302\n",
            "[1981,     2] loss: 0.316\n",
            "[1981,     4] loss: 0.285\n",
            "[1981,     6] loss: 0.313\n",
            "[1981,     8] loss: 0.254\n",
            "[1981,    10] loss: 0.286\n",
            "[1982,     2] loss: 0.220\n",
            "[1982,     4] loss: 0.354\n",
            "[1982,     6] loss: 0.250\n",
            "[1982,     8] loss: 0.251\n",
            "[1982,    10] loss: 0.326\n",
            "[1983,     2] loss: 0.320\n",
            "[1983,     4] loss: 0.310\n",
            "[1983,     6] loss: 0.190\n",
            "[1983,     8] loss: 0.304\n",
            "[1983,    10] loss: 0.332\n",
            "[1984,     2] loss: 0.324\n",
            "[1984,     4] loss: 0.299\n",
            "[1984,     6] loss: 0.312\n",
            "[1984,     8] loss: 0.288\n",
            "[1984,    10] loss: 0.281\n",
            "[1985,     2] loss: 0.280\n",
            "[1985,     4] loss: 0.368\n",
            "[1985,     6] loss: 0.241\n",
            "[1985,     8] loss: 0.484\n",
            "[1985,    10] loss: 0.226\n",
            "[1986,     2] loss: 0.335\n",
            "[1986,     4] loss: 0.288\n",
            "[1986,     6] loss: 0.334\n",
            "[1986,     8] loss: 0.242\n",
            "[1986,    10] loss: 0.274\n",
            "[1987,     2] loss: 0.291\n",
            "[1987,     4] loss: 0.303\n",
            "[1987,     6] loss: 0.295\n",
            "[1987,     8] loss: 0.237\n",
            "[1987,    10] loss: 0.244\n",
            "[1988,     2] loss: 0.220\n",
            "[1988,     4] loss: 0.218\n",
            "[1988,     6] loss: 0.312\n",
            "[1988,     8] loss: 0.330\n",
            "[1988,    10] loss: 0.339\n",
            "[1989,     2] loss: 0.304\n",
            "[1989,     4] loss: 0.282\n",
            "[1989,     6] loss: 0.318\n",
            "[1989,     8] loss: 0.273\n",
            "[1989,    10] loss: 0.231\n",
            "[1990,     2] loss: 0.266\n",
            "[1990,     4] loss: 0.267\n",
            "[1990,     6] loss: 0.315\n",
            "[1990,     8] loss: 0.255\n",
            "[1990,    10] loss: 0.247\n",
            "[1991,     2] loss: 0.295\n",
            "[1991,     4] loss: 0.216\n",
            "[1991,     6] loss: 0.247\n",
            "[1991,     8] loss: 0.259\n",
            "[1991,    10] loss: 0.355\n",
            "[1992,     2] loss: 0.276\n",
            "[1992,     4] loss: 0.263\n",
            "[1992,     6] loss: 0.306\n",
            "[1992,     8] loss: 0.265\n",
            "[1992,    10] loss: 0.286\n",
            "[1993,     2] loss: 0.328\n",
            "[1993,     4] loss: 0.380\n",
            "[1993,     6] loss: 0.244\n",
            "[1993,     8] loss: 0.221\n",
            "[1993,    10] loss: 0.312\n",
            "[1994,     2] loss: 0.293\n",
            "[1994,     4] loss: 0.326\n",
            "[1994,     6] loss: 0.364\n",
            "[1994,     8] loss: 0.191\n",
            "[1994,    10] loss: 0.298\n",
            "[1995,     2] loss: 0.355\n",
            "[1995,     4] loss: 0.247\n",
            "[1995,     6] loss: 0.246\n",
            "[1995,     8] loss: 0.276\n",
            "[1995,    10] loss: 0.262\n",
            "[1996,     2] loss: 0.308\n",
            "[1996,     4] loss: 0.263\n",
            "[1996,     6] loss: 0.324\n",
            "[1996,     8] loss: 0.227\n",
            "[1996,    10] loss: 0.296\n",
            "[1997,     2] loss: 0.220\n",
            "[1997,     4] loss: 0.345\n",
            "[1997,     6] loss: 0.275\n",
            "[1997,     8] loss: 0.229\n",
            "[1997,    10] loss: 0.372\n",
            "[1998,     2] loss: 0.306\n",
            "[1998,     4] loss: 0.416\n",
            "[1998,     6] loss: 0.237\n",
            "[1998,     8] loss: 0.287\n",
            "[1998,    10] loss: 0.270\n",
            "[1999,     2] loss: 0.250\n",
            "[1999,     4] loss: 0.373\n",
            "[1999,     6] loss: 0.222\n",
            "[1999,     8] loss: 0.286\n",
            "[1999,    10] loss: 0.379\n",
            "[2000,     2] loss: 0.309\n",
            "[2000,     4] loss: 0.339\n",
            "[2000,     6] loss: 0.214\n",
            "[2000,     8] loss: 0.247\n",
            "[2000,    10] loss: 0.266\n",
            "Finished Training\n"
          ]
        }
      ],
      "source": [
        "nos_epochs = 2000\n",
        "focus_true_pred_true =0\n",
        "focus_false_pred_true =0\n",
        "focus_true_pred_false =0\n",
        "focus_false_pred_false =0\n",
        "\n",
        "argmax_more_than_half = 0\n",
        "argmax_less_than_half =0\n",
        "\n",
        "\n",
        "for epoch in range(nos_epochs):  # loop over the dataset multiple times\n",
        "\n",
        "  focus_true_pred_true =0\n",
        "  focus_false_pred_true =0\n",
        "  focus_true_pred_false =0\n",
        "  focus_false_pred_false =0\n",
        "  \n",
        "  argmax_more_than_half = 0\n",
        "  argmax_less_than_half =0\n",
        "  \n",
        "  running_loss = 0.0\n",
        "  epoch_loss = []\n",
        "  cnt=0\n",
        "\n",
        "  iteration = desired_num // batch\n",
        "  \n",
        "  #training data set\n",
        "  \n",
        "  for i, data in  enumerate(train_loader):\n",
        "    inputs , labels , fore_idx = data\n",
        "    batch = inputs.size(0)\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "    inputs = inputs.double()\n",
        "    # zero the parameter gradients\n",
        "    \n",
        "    optimizer_focus.zero_grad()\n",
        "    optimizer_classify.zero_grad()\n",
        "    \n",
        "    alphas, avg_images = focus_net(inputs)\n",
        "    outputs = classify(avg_images)\n",
        "\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "#     print(outputs)\n",
        "#     print(outputs.shape,labels.shape , torch.argmax(outputs, dim=1))\n",
        "\n",
        "    loss = my_cross_entropy(outputs, labels,alphas) \n",
        "    \n",
        "    loss.backward()\n",
        "    \n",
        "    \n",
        "    optimizer_focus.step()\n",
        "    optimizer_classify.step()\n",
        "\n",
        "    running_loss += loss.item()\n",
        "    mini = 2\n",
        "    if cnt % mini == mini-1:    # print every 40 mini-batches\n",
        "      print('[%d, %5d] loss: %.3f' %(epoch + 1, cnt + 1, running_loss / mini))\n",
        "      epoch_loss.append(running_loss/mini)\n",
        "      running_loss = 0.0\n",
        "    cnt=cnt+1\n",
        "\n",
        "  if(np.mean(epoch_loss) <= 0.01):\n",
        "      break;\n",
        "  #plot_attended_data(train_loader,focus_net,epoch)\n",
        "\n",
        "    \n",
        "print('Finished Training')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3xPsiBtU-GDn"
      },
      "outputs": [],
      "source": [
        "for param in focus_net.named_parameters():\n",
        "    print(param)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "jhvhkEAyeRpt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd29215b-97a6-404f-dbe7-d96e1abe7879"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the train images: 86.400000 %\n",
            "total correct 432\n",
            "total train set images 500\n"
          ]
        }
      ],
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "focus_net.eval()\n",
        "classify.eval()\n",
        "with torch.no_grad():\n",
        "  for data in train_loader:\n",
        "    inputs, labels , fore_idx = data\n",
        "    inputs = inputs.double()\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "    alphas, avg_images = focus_net(inputs)\n",
        "    outputs = classify(avg_images)\n",
        "    #print(outputs.shape)\n",
        "\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network on the train images: %f %%' % ( 100 * correct / total))\n",
        "print(\"total correct\", correct)\n",
        "print(\"total train set images\", total)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "OKcmpKwGeS8M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15390e37-8074-4bce-82fc-43f605c4f409"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the test images: 89.100000 %\n",
            "total correct 891\n",
            "total train set images 1000\n"
          ]
        }
      ],
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "  for data in test_loader:\n",
        "    inputs, labels , fore_idx = data\n",
        "    inputs = inputs.double()\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "    alphas, avg_images = focus_net(inputs)\n",
        "    outputs = classify(avg_images)\n",
        "\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network on the test images: %f %%' % ( 100 * correct / total))\n",
        "print(\"total correct\", correct)\n",
        "print(\"total train set images\", total)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "7Mp_KJ1F8QIz"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "hard_attention_2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
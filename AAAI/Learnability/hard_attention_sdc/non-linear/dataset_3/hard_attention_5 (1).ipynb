{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0vlCAi2JLSzD"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "PiPNZm1iTgHy"
      },
      "outputs": [],
      "source": [
        "# path=\"/content/drive/MyDrive/Research/Hard_Attention/dataset_2/m_5_size_100/run_\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "SrZgZMlK-GDe"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2_J4Rw2r0SQ",
        "outputId": "a18062a3-07dd-43f2-8b9a-99bbd8571734"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "from tqdm import tqdm\n",
        "%matplotlib inline\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn import functional as F\n",
        "from tqdm import tqdm as tqdm\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "-Dmy2iPWlgnc"
      },
      "outputs": [],
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "writer = SummaryWriter('runs/hard_attention')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6fjud_Fr0Sa"
      },
      "source": [
        "# Generate dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqdXHO0Cr0Sd",
        "outputId": "8cb2bcb4-9ab9-4890-dd38-c08eb712c9c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 500\n",
            "1 500\n",
            "2 500\n",
            "3 500\n",
            "4 500\n",
            "5 500\n",
            "6 500\n",
            "7 500\n",
            "8 500\n",
            "9 500\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(1)\n",
        "\n",
        "y = np.concatenate((np.zeros(500),np.ones(500),np.ones(500)*2,np.ones(500)*3,np.ones(500)*4,\n",
        "                    np.ones(500)*5,np.ones(500)*6,np.ones(500)*7,np.ones(500)*8,np.ones(500)*9))\n",
        "#y = np.random.randint(0,3,6000)\n",
        "idx= []\n",
        "for i in range(10):\n",
        "    print(i,sum(y==i))\n",
        "    idx.append(y==i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ddhXyODwr0Sk"
      },
      "outputs": [],
      "source": [
        "x = np.zeros((5000,2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "DyV3N2DIr0Sp"
      },
      "outputs": [],
      "source": [
        "x = np.zeros((5000,2))\n",
        "\n",
        "\n",
        "np.random.seed(12)\n",
        "x[idx[0],:] = np.random.multivariate_normal(mean = [5,5],cov=[[0.1,0],[0,0.1]],size=sum(idx[0]))\n",
        "x[idx[1],:] = np.random.multivariate_normal(mean = [6,6],cov=[[0.1,0],[0,0.1]],size=sum(idx[1]))\n",
        "x[idx[2],:] = np.random.multivariate_normal(mean = [5.5,6.5],cov=[[0.1,0],[0,0.1]],size=sum(idx[2]))\n",
        "x[idx[3],:] = np.random.multivariate_normal(mean = [-1,0],cov=[[0.1,0],[0,0.1]],size=sum(idx[3]))\n",
        "x[idx[4],:] = np.random.multivariate_normal(mean = [0,2],cov=[[0.1,0],[0,0.1]],size=sum(idx[4]))\n",
        "x[idx[5],:] = np.random.multivariate_normal(mean = [1,0],cov=[[0.1,0],[0,0.1]],size=sum(idx[5]))\n",
        "x[idx[6],:] = np.random.multivariate_normal(mean = [0,-1],cov=[[0.1,0],[0,0.1]],size=sum(idx[6]))\n",
        "x[idx[7],:] = np.random.multivariate_normal(mean = [0,0],cov=[[0.1,0],[0,0.1]],size=sum(idx[7]))\n",
        "x[idx[8],:] = np.random.multivariate_normal(mean = [-0.5,-0.5],cov=[[0.1,0],[0,0.1]],size=sum(idx[8]))\n",
        "x[idx[9],:] = np.random.multivariate_normal(mean = [0.4,0.2],cov=[[0.1,0],[0,0.1]],size=sum(idx[9]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qh1mDScsU07I",
        "outputId": "7ad9891b-dd0a-43f8-b95e-a4c05fdf7ea4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([5.14957125, 4.78451422]), array([5.59513544, 6.5252764 ]))"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "x[idx[0]][0], x[idx[2]][5] "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Vr5ErQ_wSrV",
        "outputId": "249fbdba-90d4-47ce-b1c9-b9214fa5f30e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5000, 2) (5000,)\n"
          ]
        }
      ],
      "source": [
        "print(x.shape,y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "NG-3RpffwU_i"
      },
      "outputs": [],
      "source": [
        "idx= []\n",
        "for i in range(10):\n",
        "  idx.append(y==i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "id": "hJ8Jm7YUr0St",
        "outputId": "1ac4d728-2414-4dd5-f89e-f1378908b1d8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f75b2ba3c50>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "for i in range(10):\n",
        "    plt.scatter(x[idx[i],0],x[idx[i],1],label=\"class_\"+str(i))\n",
        "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "fNWgnhUJnWLV"
      },
      "outputs": [],
      "source": [
        "x = ( x -  np.mean(x,axis=0,keepdims=True) ) / np.std(x,axis=0,keepdims=True) \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "id": "8-VLhUfDDeHt",
        "outputId": "d9c9fcc1-3a71-40c0-b696-5d4f4887bfb9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f75b2b26d50>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "for i in range(10):\n",
        "    plt.scatter(x[idx[i],0],x[idx[i],1],label=\"class_\"+str(i))\n",
        "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "UfFHcZJOr0Sz"
      },
      "outputs": [],
      "source": [
        "foreground_classes = {'class_0','class_1' }\n",
        "\n",
        "background_classes = {'bg_classes',}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jqbvfbwVr0TN",
        "outputId": "c00111b0-f12a-405d-91cc-d249c9187799"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1500/1500 [00:09<00:00, 164.13it/s]\n"
          ]
        }
      ],
      "source": [
        "desired_num = 1500\n",
        "mosaic_list_of_images =[]\n",
        "mosaic_label = []\n",
        "fore_idx=[]\n",
        "m = 100\n",
        "for j in tqdm(range(desired_num)):\n",
        "    np.random.seed(j)\n",
        "    fg_class  = np.random.randint(0,3)\n",
        "    fg_idx = np.random.randint(0,m)\n",
        "    a = []\n",
        "    for i in range(m):\n",
        "        if i == fg_idx:\n",
        "            b = np.random.choice(np.where(idx[fg_class]==True)[0],size=1)\n",
        "            a.append(x[b])\n",
        "#             print(\"foreground \"+str(fg_class)+\" present at \" + str(fg_idx))\n",
        "        else:\n",
        "            bg_class = np.random.randint(3,10)\n",
        "            b = np.random.choice(np.where(idx[bg_class]==True)[0],size=1)\n",
        "            a.append(x[b])\n",
        "#             print(\"background \"+str(bg_class)+\" present at \" + str(i))\n",
        "    a = np.concatenate(a,axis=0)\n",
        "    mosaic_list_of_images.append(np.reshape(a,(m,2)))\n",
        "    mosaic_label.append(fg_class)\n",
        "    fore_idx.append(fg_idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "BOsFmWfMr0TR"
      },
      "outputs": [],
      "source": [
        "# mosaic_list_of_images = np.concatenate(mosaic_list_of_images,axis=1).T\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2aIPMgLXNiXW",
        "outputId": "714c3ada-6398-43bb-8c53-e6e8a28da99c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1500, array([[-0.59356559, -0.77464016],\n",
              "        [-0.57790426, -1.12507041],\n",
              "        [-0.50120586, -1.10130896],\n",
              "        [-0.85061921,  0.02673107],\n",
              "        [-0.83096682, -0.93025336],\n",
              "        [-0.94404393, -0.55234797],\n",
              "        [-0.52791954, -0.53956741],\n",
              "        [-1.0399314 , -0.53913493],\n",
              "        [-0.34157373, -0.44551072],\n",
              "        [-0.58416129, -0.66457382],\n",
              "        [-0.6128801 , -0.54003195],\n",
              "        [-0.84543321, -0.07104247],\n",
              "        [-0.48987901, -0.03513917],\n",
              "        [-1.13264329, -0.80000207],\n",
              "        [-0.61164105, -0.51718797],\n",
              "        [-0.92553537, -0.5648187 ],\n",
              "        [-0.6441374 , -0.64752999],\n",
              "        [-0.97349118, -0.62245347],\n",
              "        [-0.55912811, -0.74892406],\n",
              "        [-0.84205538,  0.04121291],\n",
              "        [-0.63315146, -0.90570363],\n",
              "        [-0.49828168, -1.19502388],\n",
              "        [-0.99393076, -0.84662554],\n",
              "        [-0.76422982,  0.1050219 ],\n",
              "        [-1.14436649, -0.5901262 ],\n",
              "        [-0.33898115, -0.51546809],\n",
              "        [-0.56105709, -1.1784382 ],\n",
              "        [-0.2349192 , -0.49210668],\n",
              "        [-0.59454857, -0.64324514],\n",
              "        [-1.02738585, -0.85038529],\n",
              "        [-0.55900395, -0.62308578],\n",
              "        [-0.77808883, -0.90695913],\n",
              "        [-0.90618834, -0.60920565],\n",
              "        [-0.87300088,  0.08893332],\n",
              "        [-0.66138395, -0.71667011],\n",
              "        [-0.16949063, -0.52294961],\n",
              "        [-1.1152323 , -0.71334209],\n",
              "        [-0.85296998, -0.12438692],\n",
              "        [-0.77984854,  0.13431457],\n",
              "        [-0.57307774, -1.04636054],\n",
              "        [-0.61102274, -0.8560358 ],\n",
              "        [-0.07940189, -0.59240209],\n",
              "        [-1.14347108, -0.59603963],\n",
              "        [-0.74884699, -1.06508981],\n",
              "        [-0.69251057, -0.65843084],\n",
              "        [-0.04688941, -0.65668226],\n",
              "        [-0.6788178 , -1.0464939 ],\n",
              "        [ 1.30021251,  1.25367363],\n",
              "        [-0.50393525, -0.56235554],\n",
              "        [-0.97922958, -0.83849599],\n",
              "        [-0.77423827, -0.66379713],\n",
              "        [-0.69555404, -0.59413445],\n",
              "        [-0.27017013, -0.48354413],\n",
              "        [-0.36581825, -0.56451181],\n",
              "        [-0.16236426, -0.59085027],\n",
              "        [-0.7374356 , -0.82000073],\n",
              "        [-0.83730057, -0.67327211],\n",
              "        [-0.42665165, -0.22604898],\n",
              "        [-0.73446034,  0.07753272],\n",
              "        [-0.90428461, -0.74282701],\n",
              "        [-0.8582542 , -0.85419213],\n",
              "        [-1.03920702, -0.57171763],\n",
              "        [-0.39121803, -0.59436828],\n",
              "        [-0.25107692, -0.66621535],\n",
              "        [-0.92856027, -0.9592829 ],\n",
              "        [-0.50301169, -0.5868036 ],\n",
              "        [-0.78573745, -0.89223065],\n",
              "        [-0.86269098, -0.91968234],\n",
              "        [-0.97229548, -0.38749838],\n",
              "        [-0.09158111, -0.61997212],\n",
              "        [-0.21161723, -0.6453793 ],\n",
              "        [-0.61034487, -0.94023254],\n",
              "        [-0.69849898,  0.05346573],\n",
              "        [-0.29950854, -0.6432871 ],\n",
              "        [-0.67791062, -0.67889471],\n",
              "        [-0.51012638, -1.1196605 ],\n",
              "        [-0.68342237, -0.67897899],\n",
              "        [-0.59366004, -0.57871903],\n",
              "        [-0.1217509 , -0.65169581],\n",
              "        [-0.40985284, -0.65017012],\n",
              "        [-0.34157373, -0.44551072],\n",
              "        [-0.64630124, -0.54203429],\n",
              "        [-0.2431525 , -0.5822302 ],\n",
              "        [-0.90613668, -0.74560364],\n",
              "        [-0.20542971, -0.96849251],\n",
              "        [-0.97908737, -0.69761633],\n",
              "        [-0.43574254, -0.55814058],\n",
              "        [-0.47477102, -0.83498565],\n",
              "        [-0.36138074, -0.59645773],\n",
              "        [-0.50959095, -0.98751415],\n",
              "        [-0.69555404, -0.59413445],\n",
              "        [-0.96387833, -0.86926566],\n",
              "        [-0.92333841, -0.56867678],\n",
              "        [-0.24852997, -0.61628934],\n",
              "        [-0.45993452, -0.59222465],\n",
              "        [-0.63357305, -1.05785171],\n",
              "        [-1.0067395 , -0.61399099],\n",
              "        [-1.21661468, -0.8286601 ],\n",
              "        [-1.14785005, -0.7849356 ],\n",
              "        [ 0.04147136, -0.68086308]]), (100, 2))"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "len(mosaic_list_of_images), mosaic_list_of_images[0],mosaic_list_of_images[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "iPoIwbMHx44n"
      },
      "outputs": [],
      "source": [
        "class MosaicDataset(Dataset):\n",
        "  \"\"\"MosaicDataset dataset.\"\"\"\n",
        "\n",
        "  def __init__(self, mosaic_list_of_images, mosaic_label, fore_idx):\n",
        "    \"\"\"\n",
        "      Args:\n",
        "        csv_file (string): Path to the csv file with annotations.\n",
        "        root_dir (string): Directory with all the images.\n",
        "        transform (callable, optional): Optional transform to be applied\n",
        "            on a sample.\n",
        "    \"\"\"\n",
        "    self.mosaic = mosaic_list_of_images\n",
        "    self.label = mosaic_label\n",
        "    self.fore_idx = fore_idx\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.label)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.mosaic[idx] , self.label[idx], self.fore_idx[idx]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "fOPAJQJeW8Ah"
      },
      "outputs": [],
      "source": [
        "batch = 50\n",
        "msd1 = MosaicDataset(mosaic_list_of_images[0:500], mosaic_label[0:500] , fore_idx[0:500])\n",
        "train_loader = DataLoader( msd1 ,batch_size= batch ,shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "aWBIcyvGApLt"
      },
      "outputs": [],
      "source": [
        "data,_,_=iter(train_loader).next()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cauJIvKEAxKM",
        "outputId": "46b8ea08-0b8e-4540-fd3f-8a3184d5e9ce"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([50, 100, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "qjNiQgxZW8bA"
      },
      "outputs": [],
      "source": [
        "batch = 250\n",
        "msd2 = MosaicDataset(mosaic_list_of_images[500:], mosaic_label[500:] , fore_idx[500:])\n",
        "test_loader = DataLoader( msd2 ,batch_size= batch ,shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "yda1E5ApiKpH"
      },
      "outputs": [],
      "source": [
        "class Focus(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Focus, self).__init__()\n",
        "        self.fc1 = nn.Linear(2,1, bias=False)\n",
        "        torch.nn.init.xavier_normal_(self.fc1.weight)\n",
        "        #self.fc2 = nn.Linear(64, 1, bias=False)\n",
        "        #torch.nn.init.xavier_normal_(self.fc2.weight)\n",
        "\n",
        "    def forward(self,z):\n",
        "        #print(\"data\",z)\n",
        "        batch = z.size(0)\n",
        "        patches = z.size(1)\n",
        "        z = z.view(batch,patches,2*1)\n",
        "        alp1,ft1 = self.helper(z)\n",
        "\n",
        "        alpha = F.softmax(alp1,dim=1)\n",
        "        #print(self.training)\n",
        "        \n",
        "        if self.training:\n",
        "            alpha =alpha[:,:,0]\n",
        "            y = ft1 \n",
        "            return alpha,y\n",
        "        else:\n",
        "            #alpha_cumsum = torch.cumsum(alpha, dim = 1)\n",
        "            #print(alpha_cumsum)\n",
        "            #len_batch = alpha_cumsum.size(0)\n",
        "            #patches = alpha_cumsum.size(1)\n",
        "            #rand_prob = torch.rand(len_batch,patches, 1).to(device)\n",
        "            #alpha_relu = F.relu(rand_prob-alpha_cumsum)\n",
        "            #print(alpha_relu)\n",
        "            #alpha_index = torch.count_nonzero(alpha_relu,dim=1)\n",
        "            #alpha_hard = F.one_hot(alpha_index,num_classes=patches)\n",
        "            #print(alpha_hard)\n",
        "            #alpha_hard = torch.transpose(alpha_hard,dim0=1,dim1=2)\n",
        "            #print(ft1,\"alpha_hard\",alpha_hard) \n",
        "            #y = torch.sum(alpha_hard*ft1,dim=1)\n",
        "            #print(alpha,alpha.shape)\n",
        "         \n",
        "        \n",
        "            index = torch.argmax(alpha,dim=1)\n",
        "            hard_alpha = torch.nn.functional.one_hot(index[:,0], patches)\n",
        "            y = torch.sum(hard_alpha[:,:,None]*ft1,dim=1)\n",
        "            alpha = alpha[:,:,0]\n",
        "            return alpha,y\n",
        "    \n",
        "    def helper(self, x):\n",
        "        x1 = x\n",
        "        x = self.fc1(x)\n",
        "        return x,x1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "0dYXnywAD-4l"
      },
      "outputs": [],
      "source": [
        "class Classification(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Classification, self).__init__()\n",
        "    self.fc1 = nn.Linear(2, 50)\n",
        "    self.fc2 = nn.Linear(50,3)\n",
        "    torch.nn.init.xavier_normal_(self.fc1.weight)\n",
        "    torch.nn.init.zeros_(self.fc1.bias)\n",
        "    torch.nn.init.xavier_normal_(self.fc2.weight)\n",
        "    torch.nn.init.zeros_(self.fc2.bias)\n",
        "  def forward(self, x):\n",
        "    #print(x.shape)\n",
        "    #x = x.view(-1, 1)\n",
        "    #print(x.shape)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = self.fc2(x)\n",
        "    # print(x.shape)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "lSa6O9f6XNf4"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(13)\n",
        "focus_net = Focus().double()\n",
        "focus_net = focus_net.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "36k3H2G-XO9A"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(13)\n",
        "classify = Classification().double()\n",
        "classify = classify.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "bK78aII8-GDl"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "optimizer_classify = optim.Adam(classify.parameters(), lr=0.1 ) #, momentum=0.9)\n",
        "optimizer_focus = optim.Adam(focus_net.parameters(), lr=0.1 ) #, momentum=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "h0mjWiFG-GDl"
      },
      "outputs": [],
      "source": [
        "def my_cross_entropy(output,target,alpha):\n",
        "    criterion = nn.CrossEntropyLoss(reduce=False)\n",
        "    \n",
        "    batch = output.size(0)\n",
        "    #print(batch)\n",
        "    patches = output.size(1)\n",
        "    classes = output.size(2)\n",
        "    \n",
        "    \n",
        "    \n",
        "    output = torch.reshape(output,(batch*patches,classes))\n",
        "    \n",
        "    \n",
        "    target = target.repeat_interleave(patches)\n",
        "    \n",
        "    loss = criterion(output,target)\n",
        "    \n",
        "    #print(loss,loss.shape)\n",
        "    loss = torch.reshape(loss,(batch,patches))\n",
        "    #print(loss.size())\n",
        "    final_loss = torch.sum(torch.mul(loss,alpha),dim=1)\n",
        "    #print(final_loss.shape)\n",
        "    final_loss = torch.mean(final_loss,dim=0)\n",
        "    \n",
        "    \n",
        "    \n",
        "    #print(final_loss)\n",
        "    return final_loss\n",
        "    \n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "pjD2VZuV9Ed4"
      },
      "outputs": [],
      "source": [
        "col1=[]\n",
        "col2=[]\n",
        "col3=[]\n",
        "col4=[]\n",
        "col5=[]\n",
        "col6=[]\n",
        "col7=[]\n",
        "col8=[]\n",
        "col9=[]\n",
        "col10=[]\n",
        "col11=[]\n",
        "col12=[]\n",
        "col13=[]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "eEVrBg7d-GDl"
      },
      "outputs": [],
      "source": [
        "def plot_attended_data(trainloader,net,epoch):\n",
        "    attd_data =[]\n",
        "    lbls = []\n",
        "    for data in trainloader:\n",
        "        inputs, labels , fore_idx = data\n",
        "        inputs = inputs.double()\n",
        "        inputs, labels , fore_idx = inputs.to(device),labels.to(device), fore_idx.to(device)\n",
        "        alphas, avg_images = focus_net(inputs)\n",
        "        attd_data.append(avg_images.numpy())\n",
        "        lbls.append(labels)\n",
        "    attd_data = np.concatenate(attd_data,axis=0)\n",
        "    lbls = np.concatenate(lbls,axis=0)\n",
        "    plt.figure(figsize=(6,8))\n",
        "    plt.scatter(attd_data[:,0],attd_data[:,1],c=lbls)\n",
        "    plt.title(\"EPOCH_\"+str(epoch))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "uALi25pmzQHV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b106ba8-ca65-454b-adb4-f1f2a34b79c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1.1315, device='cuda:0', dtype=torch.float64)\n",
            "tensor(1.1062, device='cuda:0', dtype=torch.float64)\n",
            "tensor(1.1306, device='cuda:0', dtype=torch.float64)\n",
            "tensor(1.0990, device='cuda:0', dtype=torch.float64)\n",
            "tensor(1.1122, device='cuda:0', dtype=torch.float64)\n",
            "tensor(1.0914, device='cuda:0', dtype=torch.float64)\n",
            "tensor(1.0918, device='cuda:0', dtype=torch.float64)\n",
            "tensor(1.1106, device='cuda:0', dtype=torch.float64)\n",
            "tensor(1.1084, device='cuda:0', dtype=torch.float64)\n",
            "tensor(1.1132, device='cuda:0', dtype=torch.float64)\n",
            "====================================================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        }
      ],
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "count = 0\n",
        "flag = 1\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "  for data in train_loader:\n",
        "    inputs, labels , fore_idx = data\n",
        "    inputs = inputs.double()\n",
        "    inputs, labels , fore_idx = inputs.to(device),labels.to(device), fore_idx.to(device)\n",
        "    alphas, avg_images = focus_net(inputs)\n",
        "    outputs = classify(avg_images)\n",
        "    loss = my_cross_entropy(outputs,labels,alphas)\n",
        "    print(loss)\n",
        "    # print(outputs.shape)\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "\n",
        "print(\"=\"*100)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "4vmNprlPzTjP"
      },
      "outputs": [],
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "count = 0\n",
        "flag = 1\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "  for data in test_loader:\n",
        "    inputs, labels , fore_idx = data\n",
        "    inputs = inputs.double()\n",
        "    inputs, labels , fore_idx = inputs.to(device),labels.to(device), fore_idx.to(device)\n",
        "    alphas, avg_images = focus_net(inputs)\n",
        "    outputs = classify(avg_images)\n",
        "\n",
        "    _, predicted = torch.max(outputs.data, 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "_nvicAzw-GDm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a34ce276-ed4a-43de-8925-ed65ab2942be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('fc1.weight', Parameter containing:\n",
            "tensor([[-0.4153,  0.0408]], device='cuda:0', dtype=torch.float64,\n",
            "       requires_grad=True))\n"
          ]
        }
      ],
      "source": [
        "for param in focus_net.named_parameters():\n",
        "    print(param)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "Yl41sE8vFERk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41ce1888-3b4b-4e11-d4ea-4cc7ffdd587a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,    10] loss: 1.204\n",
            "[2,    10] loss: 1.108\n",
            "[3,    10] loss: 1.095\n",
            "[4,    10] loss: 0.950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5,    10] loss: 0.613\n",
            "[6,    10] loss: 0.455\n",
            "[7,    10] loss: 0.418\n",
            "[8,    10] loss: 0.370\n",
            "[9,    10] loss: 0.375\n",
            "[10,    10] loss: 0.293\n",
            "[11,    10] loss: 0.304\n",
            "[12,    10] loss: 0.307\n",
            "[13,    10] loss: 0.299\n",
            "[14,    10] loss: 0.260\n",
            "[15,    10] loss: 0.277\n",
            "[16,    10] loss: 0.337\n",
            "[17,    10] loss: 0.433\n",
            "[18,    10] loss: 0.303\n",
            "[19,    10] loss: 0.259\n",
            "[20,    10] loss: 0.269\n",
            "[21,    10] loss: 0.293\n",
            "[22,    10] loss: 0.297\n",
            "[23,    10] loss: 0.323\n",
            "[24,    10] loss: 0.291\n",
            "[25,    10] loss: 0.318\n",
            "[26,    10] loss: 0.294\n",
            "[27,    10] loss: 0.320\n",
            "[28,    10] loss: 0.291\n",
            "[29,    10] loss: 0.288\n",
            "[30,    10] loss: 0.307\n",
            "[31,    10] loss: 0.407\n",
            "[32,    10] loss: 0.273\n",
            "[33,    10] loss: 0.257\n",
            "[34,    10] loss: 0.258\n",
            "[35,    10] loss: 0.264\n",
            "[36,    10] loss: 0.284\n",
            "[37,    10] loss: 0.363\n",
            "[38,    10] loss: 0.374\n",
            "[39,    10] loss: 0.363\n",
            "[40,    10] loss: 0.339\n",
            "[41,    10] loss: 0.347\n",
            "[42,    10] loss: 0.270\n",
            "[43,    10] loss: 0.274\n",
            "[44,    10] loss: 0.268\n",
            "[45,    10] loss: 0.313\n",
            "[46,    10] loss: 0.299\n",
            "[47,    10] loss: 0.298\n",
            "[48,    10] loss: 0.288\n",
            "[49,    10] loss: 0.303\n",
            "[50,    10] loss: 0.291\n",
            "[51,    10] loss: 0.283\n",
            "[52,    10] loss: 0.259\n",
            "[53,    10] loss: 0.271\n",
            "[54,    10] loss: 0.310\n",
            "[55,    10] loss: 0.338\n",
            "[56,    10] loss: 0.322\n",
            "[57,    10] loss: 0.321\n",
            "[58,    10] loss: 0.282\n",
            "[59,    10] loss: 0.264\n",
            "[60,    10] loss: 0.257\n",
            "[61,    10] loss: 0.269\n",
            "[62,    10] loss: 0.278\n",
            "[63,    10] loss: 0.274\n",
            "[64,    10] loss: 0.269\n",
            "[65,    10] loss: 0.276\n",
            "[66,    10] loss: 0.328\n",
            "[67,    10] loss: 0.312\n",
            "[68,    10] loss: 0.307\n",
            "[69,    10] loss: 0.268\n",
            "[70,    10] loss: 0.281\n",
            "[71,    10] loss: 0.262\n",
            "[72,    10] loss: 0.248\n",
            "[73,    10] loss: 0.275\n",
            "[74,    10] loss: 0.252\n",
            "[75,    10] loss: 0.265\n",
            "[76,    10] loss: 0.331\n",
            "[77,    10] loss: 0.311\n",
            "[78,    10] loss: 0.280\n",
            "[79,    10] loss: 0.273\n",
            "[80,    10] loss: 0.314\n",
            "[81,    10] loss: 0.302\n",
            "[82,    10] loss: 0.310\n",
            "[83,    10] loss: 0.256\n",
            "[84,    10] loss: 0.254\n",
            "[85,    10] loss: 0.268\n",
            "[86,    10] loss: 0.262\n",
            "[87,    10] loss: 0.278\n",
            "[88,    10] loss: 0.254\n",
            "[89,    10] loss: 0.271\n",
            "[90,    10] loss: 0.269\n",
            "[91,    10] loss: 0.261\n",
            "[92,    10] loss: 0.275\n",
            "[93,    10] loss: 0.315\n",
            "[94,    10] loss: 0.280\n",
            "[95,    10] loss: 0.266\n",
            "[96,    10] loss: 0.266\n",
            "[97,    10] loss: 0.261\n",
            "[98,    10] loss: 0.265\n",
            "[99,    10] loss: 0.267\n",
            "[100,    10] loss: 0.284\n",
            "[101,    10] loss: 0.275\n",
            "[102,    10] loss: 0.287\n",
            "[103,    10] loss: 0.278\n",
            "[104,    10] loss: 0.273\n",
            "[105,    10] loss: 0.264\n",
            "[106,    10] loss: 0.306\n",
            "[107,    10] loss: 0.350\n",
            "[108,    10] loss: 0.249\n",
            "[109,    10] loss: 0.275\n",
            "[110,    10] loss: 0.262\n",
            "[111,    10] loss: 0.299\n",
            "[112,    10] loss: 0.259\n",
            "[113,    10] loss: 0.250\n",
            "[114,    10] loss: 0.258\n",
            "[115,    10] loss: 0.288\n",
            "[116,    10] loss: 0.293\n",
            "[117,    10] loss: 0.249\n",
            "[118,    10] loss: 0.255\n",
            "[119,    10] loss: 0.251\n",
            "[120,    10] loss: 0.256\n",
            "[121,    10] loss: 0.250\n",
            "[122,    10] loss: 0.271\n",
            "[123,    10] loss: 0.318\n",
            "[124,    10] loss: 0.301\n",
            "[125,    10] loss: 0.260\n",
            "[126,    10] loss: 0.249\n",
            "[127,    10] loss: 0.262\n",
            "[128,    10] loss: 0.273\n",
            "[129,    10] loss: 0.266\n",
            "[130,    10] loss: 0.274\n",
            "[131,    10] loss: 0.286\n",
            "[132,    10] loss: 0.298\n",
            "[133,    10] loss: 0.254\n",
            "[134,    10] loss: 0.248\n",
            "[135,    10] loss: 0.243\n",
            "[136,    10] loss: 0.246\n",
            "[137,    10] loss: 0.281\n",
            "[138,    10] loss: 0.277\n",
            "[139,    10] loss: 0.290\n",
            "[140,    10] loss: 0.282\n",
            "[141,    10] loss: 0.253\n",
            "[142,    10] loss: 0.265\n",
            "[143,    10] loss: 0.275\n",
            "[144,    10] loss: 0.254\n",
            "[145,    10] loss: 0.317\n",
            "[146,    10] loss: 0.254\n",
            "[147,    10] loss: 0.259\n",
            "[148,    10] loss: 0.248\n",
            "[149,    10] loss: 0.262\n",
            "[150,    10] loss: 0.271\n",
            "[151,    10] loss: 0.311\n",
            "[152,    10] loss: 0.335\n",
            "[153,    10] loss: 0.286\n",
            "[154,    10] loss: 0.262\n",
            "[155,    10] loss: 0.257\n",
            "[156,    10] loss: 0.273\n",
            "[157,    10] loss: 0.317\n",
            "[158,    10] loss: 0.319\n",
            "[159,    10] loss: 0.255\n",
            "[160,    10] loss: 0.245\n",
            "[161,    10] loss: 0.257\n",
            "[162,    10] loss: 0.256\n",
            "[163,    10] loss: 0.258\n",
            "[164,    10] loss: 0.281\n",
            "[165,    10] loss: 0.270\n",
            "[166,    10] loss: 0.248\n",
            "[167,    10] loss: 0.256\n",
            "[168,    10] loss: 0.282\n",
            "[169,    10] loss: 0.287\n",
            "[170,    10] loss: 0.285\n",
            "[171,    10] loss: 0.259\n",
            "[172,    10] loss: 0.302\n",
            "[173,    10] loss: 0.303\n",
            "[174,    10] loss: 0.267\n",
            "[175,    10] loss: 0.259\n",
            "[176,    10] loss: 0.266\n",
            "[177,    10] loss: 0.251\n",
            "[178,    10] loss: 0.271\n",
            "[179,    10] loss: 0.265\n",
            "[180,    10] loss: 0.314\n",
            "[181,    10] loss: 0.314\n",
            "[182,    10] loss: 0.330\n",
            "[183,    10] loss: 0.299\n",
            "[184,    10] loss: 0.329\n",
            "[185,    10] loss: 0.274\n",
            "[186,    10] loss: 0.272\n",
            "[187,    10] loss: 0.282\n",
            "[188,    10] loss: 0.253\n",
            "[189,    10] loss: 0.264\n",
            "[190,    10] loss: 0.248\n",
            "[191,    10] loss: 0.241\n",
            "[192,    10] loss: 0.258\n",
            "[193,    10] loss: 0.345\n",
            "[194,    10] loss: 0.330\n",
            "[195,    10] loss: 0.263\n",
            "[196,    10] loss: 0.264\n",
            "[197,    10] loss: 0.243\n",
            "[198,    10] loss: 0.256\n",
            "[199,    10] loss: 0.248\n",
            "[200,    10] loss: 0.246\n",
            "[201,    10] loss: 0.264\n",
            "[202,    10] loss: 0.289\n",
            "[203,    10] loss: 0.268\n",
            "[204,    10] loss: 0.275\n",
            "[205,    10] loss: 0.266\n",
            "[206,    10] loss: 0.273\n",
            "[207,    10] loss: 0.278\n",
            "[208,    10] loss: 0.292\n",
            "[209,    10] loss: 0.261\n",
            "[210,    10] loss: 0.266\n",
            "[211,    10] loss: 0.249\n",
            "[212,    10] loss: 0.238\n",
            "[213,    10] loss: 0.240\n",
            "[214,    10] loss: 0.263\n",
            "[215,    10] loss: 0.285\n",
            "[216,    10] loss: 0.280\n",
            "[217,    10] loss: 0.307\n",
            "[218,    10] loss: 0.273\n",
            "[219,    10] loss: 0.258\n",
            "[220,    10] loss: 0.267\n",
            "[221,    10] loss: 0.276\n",
            "[222,    10] loss: 0.285\n",
            "[223,    10] loss: 0.290\n",
            "[224,    10] loss: 0.270\n",
            "[225,    10] loss: 0.257\n",
            "[226,    10] loss: 0.277\n",
            "[227,    10] loss: 0.282\n",
            "[228,    10] loss: 0.277\n",
            "[229,    10] loss: 0.258\n",
            "[230,    10] loss: 0.253\n",
            "[231,    10] loss: 0.247\n",
            "[232,    10] loss: 0.290\n",
            "[233,    10] loss: 0.331\n",
            "[234,    10] loss: 0.344\n",
            "[235,    10] loss: 0.334\n",
            "[236,    10] loss: 0.297\n",
            "[237,    10] loss: 0.277\n",
            "[238,    10] loss: 0.256\n",
            "[239,    10] loss: 0.263\n",
            "[240,    10] loss: 0.256\n",
            "[241,    10] loss: 0.250\n",
            "[242,    10] loss: 0.261\n",
            "[243,    10] loss: 0.276\n",
            "[244,    10] loss: 0.272\n",
            "[245,    10] loss: 0.290\n",
            "[246,    10] loss: 0.282\n",
            "[247,    10] loss: 0.246\n",
            "[248,    10] loss: 0.242\n",
            "[249,    10] loss: 0.255\n",
            "[250,    10] loss: 0.243\n",
            "[251,    10] loss: 0.252\n",
            "[252,    10] loss: 0.269\n",
            "[253,    10] loss: 0.246\n",
            "[254,    10] loss: 0.249\n",
            "[255,    10] loss: 0.251\n",
            "[256,    10] loss: 0.270\n",
            "[257,    10] loss: 0.309\n",
            "[258,    10] loss: 0.271\n",
            "[259,    10] loss: 0.278\n",
            "[260,    10] loss: 0.341\n",
            "[261,    10] loss: 0.310\n",
            "[262,    10] loss: 0.262\n",
            "[263,    10] loss: 0.277\n",
            "[264,    10] loss: 0.268\n",
            "[265,    10] loss: 0.251\n",
            "[266,    10] loss: 0.307\n",
            "[267,    10] loss: 0.273\n",
            "[268,    10] loss: 0.254\n",
            "[269,    10] loss: 0.277\n",
            "[270,    10] loss: 0.253\n",
            "[271,    10] loss: 0.254\n",
            "[272,    10] loss: 0.263\n",
            "[273,    10] loss: 0.293\n",
            "[274,    10] loss: 0.291\n",
            "[275,    10] loss: 0.258\n",
            "[276,    10] loss: 0.317\n",
            "[277,    10] loss: 0.301\n",
            "[278,    10] loss: 0.286\n",
            "[279,    10] loss: 0.272\n",
            "[280,    10] loss: 0.251\n",
            "[281,    10] loss: 0.265\n",
            "[282,    10] loss: 0.267\n",
            "[283,    10] loss: 0.263\n",
            "[284,    10] loss: 0.243\n",
            "[285,    10] loss: 0.246\n",
            "[286,    10] loss: 0.255\n",
            "[287,    10] loss: 0.277\n",
            "[288,    10] loss: 0.299\n",
            "[289,    10] loss: 0.266\n",
            "[290,    10] loss: 0.250\n",
            "[291,    10] loss: 0.243\n",
            "[292,    10] loss: 0.256\n",
            "[293,    10] loss: 0.277\n",
            "[294,    10] loss: 0.258\n",
            "[295,    10] loss: 0.246\n",
            "[296,    10] loss: 0.264\n",
            "[297,    10] loss: 0.308\n",
            "[298,    10] loss: 0.283\n",
            "[299,    10] loss: 0.263\n",
            "[300,    10] loss: 0.253\n",
            "[301,    10] loss: 0.252\n",
            "[302,    10] loss: 0.243\n",
            "[303,    10] loss: 0.245\n",
            "[304,    10] loss: 0.243\n",
            "[305,    10] loss: 0.253\n",
            "[306,    10] loss: 0.264\n",
            "[307,    10] loss: 0.239\n",
            "[308,    10] loss: 0.243\n",
            "[309,    10] loss: 0.249\n",
            "[310,    10] loss: 0.299\n",
            "[311,    10] loss: 0.283\n",
            "[312,    10] loss: 0.246\n",
            "[313,    10] loss: 0.277\n",
            "[314,    10] loss: 0.245\n",
            "[315,    10] loss: 0.248\n",
            "[316,    10] loss: 0.276\n",
            "[317,    10] loss: 0.277\n",
            "[318,    10] loss: 0.296\n",
            "[319,    10] loss: 0.251\n",
            "[320,    10] loss: 0.266\n",
            "[321,    10] loss: 0.249\n",
            "[322,    10] loss: 0.272\n",
            "[323,    10] loss: 0.360\n",
            "[324,    10] loss: 0.296\n",
            "[325,    10] loss: 0.289\n",
            "[326,    10] loss: 0.248\n",
            "[327,    10] loss: 0.270\n",
            "[328,    10] loss: 0.263\n",
            "[329,    10] loss: 0.261\n",
            "[330,    10] loss: 0.291\n",
            "[331,    10] loss: 0.289\n",
            "[332,    10] loss: 0.253\n",
            "[333,    10] loss: 0.248\n",
            "[334,    10] loss: 0.246\n",
            "[335,    10] loss: 0.243\n",
            "[336,    10] loss: 0.240\n",
            "[337,    10] loss: 0.244\n",
            "[338,    10] loss: 0.244\n",
            "[339,    10] loss: 0.242\n",
            "[340,    10] loss: 0.249\n",
            "[341,    10] loss: 0.261\n",
            "[342,    10] loss: 0.254\n",
            "[343,    10] loss: 0.253\n",
            "[344,    10] loss: 0.282\n",
            "[345,    10] loss: 0.263\n",
            "[346,    10] loss: 0.244\n",
            "[347,    10] loss: 0.252\n",
            "[348,    10] loss: 0.280\n",
            "[349,    10] loss: 0.266\n",
            "[350,    10] loss: 0.261\n",
            "[351,    10] loss: 0.248\n",
            "[352,    10] loss: 0.263\n",
            "[353,    10] loss: 0.256\n",
            "[354,    10] loss: 0.287\n",
            "[355,    10] loss: 0.345\n",
            "[356,    10] loss: 0.313\n",
            "[357,    10] loss: 0.273\n",
            "[358,    10] loss: 0.293\n",
            "[359,    10] loss: 0.263\n",
            "[360,    10] loss: 0.274\n",
            "[361,    10] loss: 0.302\n",
            "[362,    10] loss: 0.261\n",
            "[363,    10] loss: 0.313\n",
            "[364,    10] loss: 0.293\n",
            "[365,    10] loss: 0.267\n",
            "[366,    10] loss: 0.272\n",
            "[367,    10] loss: 0.241\n",
            "[368,    10] loss: 0.286\n",
            "[369,    10] loss: 0.314\n",
            "[370,    10] loss: 0.286\n",
            "[371,    10] loss: 0.279\n",
            "[372,    10] loss: 0.262\n",
            "[373,    10] loss: 0.272\n",
            "[374,    10] loss: 0.257\n",
            "[375,    10] loss: 0.258\n",
            "[376,    10] loss: 0.254\n",
            "[377,    10] loss: 0.261\n",
            "[378,    10] loss: 0.262\n",
            "[379,    10] loss: 0.273\n",
            "[380,    10] loss: 0.255\n",
            "[381,    10] loss: 0.258\n",
            "[382,    10] loss: 0.268\n",
            "[383,    10] loss: 0.254\n",
            "[384,    10] loss: 0.294\n",
            "[385,    10] loss: 0.295\n",
            "[386,    10] loss: 0.269\n",
            "[387,    10] loss: 0.262\n",
            "[388,    10] loss: 0.270\n",
            "[389,    10] loss: 0.264\n",
            "[390,    10] loss: 0.258\n",
            "[391,    10] loss: 0.237\n",
            "[392,    10] loss: 0.256\n",
            "[393,    10] loss: 0.257\n",
            "[394,    10] loss: 0.251\n",
            "[395,    10] loss: 0.244\n",
            "[396,    10] loss: 0.268\n",
            "[397,    10] loss: 0.241\n",
            "[398,    10] loss: 0.239\n",
            "[399,    10] loss: 0.246\n",
            "[400,    10] loss: 0.243\n",
            "[401,    10] loss: 0.242\n",
            "[402,    10] loss: 0.265\n",
            "[403,    10] loss: 0.250\n",
            "[404,    10] loss: 0.255\n",
            "[405,    10] loss: 0.256\n",
            "[406,    10] loss: 0.248\n",
            "[407,    10] loss: 0.267\n",
            "[408,    10] loss: 0.251\n",
            "[409,    10] loss: 0.244\n",
            "[410,    10] loss: 0.266\n",
            "[411,    10] loss: 0.240\n",
            "[412,    10] loss: 0.259\n",
            "[413,    10] loss: 0.246\n",
            "[414,    10] loss: 0.262\n",
            "[415,    10] loss: 0.266\n",
            "[416,    10] loss: 0.260\n",
            "[417,    10] loss: 0.251\n",
            "[418,    10] loss: 0.261\n",
            "[419,    10] loss: 0.301\n",
            "[420,    10] loss: 0.292\n",
            "[421,    10] loss: 0.261\n",
            "[422,    10] loss: 0.263\n",
            "[423,    10] loss: 0.279\n",
            "[424,    10] loss: 0.301\n",
            "[425,    10] loss: 0.257\n",
            "[426,    10] loss: 0.248\n",
            "[427,    10] loss: 0.246\n",
            "[428,    10] loss: 0.241\n",
            "[429,    10] loss: 0.240\n",
            "[430,    10] loss: 0.246\n",
            "[431,    10] loss: 0.273\n",
            "[432,    10] loss: 0.287\n",
            "[433,    10] loss: 0.288\n",
            "[434,    10] loss: 0.258\n",
            "[435,    10] loss: 0.307\n",
            "[436,    10] loss: 0.273\n",
            "[437,    10] loss: 0.258\n",
            "[438,    10] loss: 0.253\n",
            "[439,    10] loss: 0.265\n",
            "[440,    10] loss: 0.247\n",
            "[441,    10] loss: 0.268\n",
            "[442,    10] loss: 0.282\n",
            "[443,    10] loss: 0.263\n",
            "[444,    10] loss: 0.247\n",
            "[445,    10] loss: 0.256\n",
            "[446,    10] loss: 0.249\n",
            "[447,    10] loss: 0.249\n",
            "[448,    10] loss: 0.268\n",
            "[449,    10] loss: 0.266\n",
            "[450,    10] loss: 0.249\n",
            "[451,    10] loss: 0.257\n",
            "[452,    10] loss: 0.252\n",
            "[453,    10] loss: 0.264\n",
            "[454,    10] loss: 0.251\n",
            "[455,    10] loss: 0.302\n",
            "[456,    10] loss: 0.279\n",
            "[457,    10] loss: 0.285\n",
            "[458,    10] loss: 0.311\n",
            "[459,    10] loss: 0.282\n",
            "[460,    10] loss: 0.269\n",
            "[461,    10] loss: 0.252\n",
            "[462,    10] loss: 0.276\n",
            "[463,    10] loss: 0.251\n",
            "[464,    10] loss: 0.255\n",
            "[465,    10] loss: 0.285\n",
            "[466,    10] loss: 0.259\n",
            "[467,    10] loss: 0.307\n",
            "[468,    10] loss: 0.256\n",
            "[469,    10] loss: 0.262\n",
            "[470,    10] loss: 0.274\n",
            "[471,    10] loss: 0.270\n",
            "[472,    10] loss: 0.269\n",
            "[473,    10] loss: 0.258\n",
            "[474,    10] loss: 0.258\n",
            "[475,    10] loss: 0.265\n",
            "[476,    10] loss: 0.307\n",
            "[477,    10] loss: 0.347\n",
            "[478,    10] loss: 0.344\n",
            "[479,    10] loss: 0.287\n",
            "[480,    10] loss: 0.304\n",
            "[481,    10] loss: 0.256\n",
            "[482,    10] loss: 0.263\n",
            "[483,    10] loss: 0.244\n",
            "[484,    10] loss: 0.244\n",
            "[485,    10] loss: 0.246\n",
            "[486,    10] loss: 0.258\n",
            "[487,    10] loss: 0.249\n",
            "[488,    10] loss: 0.238\n",
            "[489,    10] loss: 0.257\n",
            "[490,    10] loss: 0.269\n",
            "[491,    10] loss: 0.278\n",
            "[492,    10] loss: 0.247\n",
            "[493,    10] loss: 0.264\n",
            "[494,    10] loss: 0.279\n",
            "[495,    10] loss: 0.265\n",
            "[496,    10] loss: 0.249\n",
            "[497,    10] loss: 0.244\n",
            "[498,    10] loss: 0.247\n",
            "[499,    10] loss: 0.243\n",
            "[500,    10] loss: 0.248\n",
            "[501,    10] loss: 0.255\n",
            "[502,    10] loss: 0.246\n",
            "[503,    10] loss: 0.252\n",
            "[504,    10] loss: 0.264\n",
            "[505,    10] loss: 0.241\n",
            "[506,    10] loss: 0.247\n",
            "[507,    10] loss: 0.242\n",
            "[508,    10] loss: 0.252\n",
            "[509,    10] loss: 0.251\n",
            "[510,    10] loss: 0.254\n",
            "[511,    10] loss: 0.253\n",
            "[512,    10] loss: 0.248\n",
            "[513,    10] loss: 0.255\n",
            "[514,    10] loss: 0.289\n",
            "[515,    10] loss: 0.260\n",
            "[516,    10] loss: 0.273\n",
            "[517,    10] loss: 0.264\n",
            "[518,    10] loss: 0.267\n",
            "[519,    10] loss: 0.285\n",
            "[520,    10] loss: 0.259\n",
            "[521,    10] loss: 0.306\n",
            "[522,    10] loss: 0.356\n",
            "[523,    10] loss: 0.297\n",
            "[524,    10] loss: 0.255\n",
            "[525,    10] loss: 0.253\n",
            "[526,    10] loss: 0.236\n",
            "[527,    10] loss: 0.253\n",
            "[528,    10] loss: 0.238\n",
            "[529,    10] loss: 0.266\n",
            "[530,    10] loss: 0.244\n",
            "[531,    10] loss: 0.255\n",
            "[532,    10] loss: 0.267\n",
            "[533,    10] loss: 0.277\n",
            "[534,    10] loss: 0.264\n",
            "[535,    10] loss: 0.252\n",
            "[536,    10] loss: 0.249\n",
            "[537,    10] loss: 0.287\n",
            "[538,    10] loss: 0.305\n",
            "[539,    10] loss: 0.247\n",
            "[540,    10] loss: 0.266\n",
            "[541,    10] loss: 0.289\n",
            "[542,    10] loss: 0.302\n",
            "[543,    10] loss: 0.347\n",
            "[544,    10] loss: 0.275\n",
            "[545,    10] loss: 0.262\n",
            "[546,    10] loss: 0.272\n",
            "[547,    10] loss: 0.258\n",
            "[548,    10] loss: 0.272\n",
            "[549,    10] loss: 0.250\n",
            "[550,    10] loss: 0.245\n",
            "[551,    10] loss: 0.245\n",
            "[552,    10] loss: 0.261\n",
            "[553,    10] loss: 0.245\n",
            "[554,    10] loss: 0.238\n",
            "[555,    10] loss: 0.242\n",
            "[556,    10] loss: 0.245\n",
            "[557,    10] loss: 0.246\n",
            "[558,    10] loss: 0.251\n",
            "[559,    10] loss: 0.255\n",
            "[560,    10] loss: 0.264\n",
            "[561,    10] loss: 0.284\n",
            "[562,    10] loss: 0.248\n",
            "[563,    10] loss: 0.256\n",
            "[564,    10] loss: 0.252\n",
            "[565,    10] loss: 0.275\n",
            "[566,    10] loss: 0.258\n",
            "[567,    10] loss: 0.265\n",
            "[568,    10] loss: 0.291\n",
            "[569,    10] loss: 0.268\n",
            "[570,    10] loss: 0.272\n",
            "[571,    10] loss: 0.264\n",
            "[572,    10] loss: 0.347\n",
            "[573,    10] loss: 0.335\n",
            "[574,    10] loss: 0.310\n",
            "[575,    10] loss: 0.260\n",
            "[576,    10] loss: 0.269\n",
            "[577,    10] loss: 0.265\n",
            "[578,    10] loss: 0.259\n",
            "[579,    10] loss: 0.248\n",
            "[580,    10] loss: 0.267\n",
            "[581,    10] loss: 0.296\n",
            "[582,    10] loss: 0.248\n",
            "[583,    10] loss: 0.249\n",
            "[584,    10] loss: 0.256\n",
            "[585,    10] loss: 0.262\n",
            "[586,    10] loss: 0.272\n",
            "[587,    10] loss: 0.276\n",
            "[588,    10] loss: 0.242\n",
            "[589,    10] loss: 0.267\n",
            "[590,    10] loss: 0.304\n",
            "[591,    10] loss: 0.294\n",
            "[592,    10] loss: 0.254\n",
            "[593,    10] loss: 0.256\n",
            "[594,    10] loss: 0.246\n",
            "[595,    10] loss: 0.254\n",
            "[596,    10] loss: 0.285\n",
            "[597,    10] loss: 0.275\n",
            "[598,    10] loss: 0.252\n",
            "[599,    10] loss: 0.262\n",
            "[600,    10] loss: 0.242\n",
            "[601,    10] loss: 0.244\n",
            "[602,    10] loss: 0.255\n",
            "[603,    10] loss: 0.269\n",
            "[604,    10] loss: 0.318\n",
            "[605,    10] loss: 0.254\n",
            "[606,    10] loss: 0.270\n",
            "[607,    10] loss: 0.261\n",
            "[608,    10] loss: 0.243\n",
            "[609,    10] loss: 0.275\n",
            "[610,    10] loss: 0.253\n",
            "[611,    10] loss: 0.241\n",
            "[612,    10] loss: 0.243\n",
            "[613,    10] loss: 0.240\n",
            "[614,    10] loss: 0.249\n",
            "[615,    10] loss: 0.257\n",
            "[616,    10] loss: 0.261\n",
            "[617,    10] loss: 0.263\n",
            "[618,    10] loss: 0.271\n",
            "[619,    10] loss: 0.266\n",
            "[620,    10] loss: 0.278\n",
            "[621,    10] loss: 0.274\n",
            "[622,    10] loss: 0.255\n",
            "[623,    10] loss: 0.254\n",
            "[624,    10] loss: 0.247\n",
            "[625,    10] loss: 0.269\n",
            "[626,    10] loss: 0.278\n",
            "[627,    10] loss: 0.268\n",
            "[628,    10] loss: 0.249\n",
            "[629,    10] loss: 0.268\n",
            "[630,    10] loss: 0.277\n",
            "[631,    10] loss: 0.260\n",
            "[632,    10] loss: 0.266\n",
            "[633,    10] loss: 0.276\n",
            "[634,    10] loss: 0.264\n",
            "[635,    10] loss: 0.257\n",
            "[636,    10] loss: 0.251\n",
            "[637,    10] loss: 0.256\n",
            "[638,    10] loss: 0.281\n",
            "[639,    10] loss: 0.260\n",
            "[640,    10] loss: 0.274\n",
            "[641,    10] loss: 0.267\n",
            "[642,    10] loss: 0.286\n",
            "[643,    10] loss: 0.281\n",
            "[644,    10] loss: 0.240\n",
            "[645,    10] loss: 0.245\n",
            "[646,    10] loss: 0.249\n",
            "[647,    10] loss: 0.265\n",
            "[648,    10] loss: 0.290\n",
            "[649,    10] loss: 0.276\n",
            "[650,    10] loss: 0.271\n",
            "[651,    10] loss: 0.271\n",
            "[652,    10] loss: 0.278\n",
            "[653,    10] loss: 0.288\n",
            "[654,    10] loss: 0.250\n",
            "[655,    10] loss: 0.263\n",
            "[656,    10] loss: 0.286\n",
            "[657,    10] loss: 0.295\n",
            "[658,    10] loss: 0.253\n",
            "[659,    10] loss: 0.251\n",
            "[660,    10] loss: 0.288\n",
            "[661,    10] loss: 0.282\n",
            "[662,    10] loss: 0.261\n",
            "[663,    10] loss: 0.257\n",
            "[664,    10] loss: 0.255\n",
            "[665,    10] loss: 0.249\n",
            "[666,    10] loss: 0.280\n",
            "[667,    10] loss: 0.260\n",
            "[668,    10] loss: 0.251\n",
            "[669,    10] loss: 0.244\n",
            "[670,    10] loss: 0.246\n",
            "[671,    10] loss: 0.274\n",
            "[672,    10] loss: 0.260\n",
            "[673,    10] loss: 0.284\n",
            "[674,    10] loss: 0.276\n",
            "[675,    10] loss: 0.259\n",
            "[676,    10] loss: 0.258\n",
            "[677,    10] loss: 0.271\n",
            "[678,    10] loss: 0.280\n",
            "[679,    10] loss: 0.296\n",
            "[680,    10] loss: 0.274\n",
            "[681,    10] loss: 0.290\n",
            "[682,    10] loss: 0.259\n",
            "[683,    10] loss: 0.257\n",
            "[684,    10] loss: 0.253\n",
            "[685,    10] loss: 0.249\n",
            "[686,    10] loss: 0.253\n",
            "[687,    10] loss: 0.250\n",
            "[688,    10] loss: 0.256\n",
            "[689,    10] loss: 0.269\n",
            "[690,    10] loss: 0.252\n",
            "[691,    10] loss: 0.260\n",
            "[692,    10] loss: 0.246\n",
            "[693,    10] loss: 0.249\n",
            "[694,    10] loss: 0.240\n",
            "[695,    10] loss: 0.248\n",
            "[696,    10] loss: 0.244\n",
            "[697,    10] loss: 0.254\n",
            "[698,    10] loss: 0.242\n",
            "[699,    10] loss: 0.245\n",
            "[700,    10] loss: 0.252\n",
            "[701,    10] loss: 0.242\n",
            "[702,    10] loss: 0.250\n",
            "[703,    10] loss: 0.284\n",
            "[704,    10] loss: 0.242\n",
            "[705,    10] loss: 0.241\n",
            "[706,    10] loss: 0.241\n",
            "[707,    10] loss: 0.238\n",
            "[708,    10] loss: 0.245\n",
            "[709,    10] loss: 0.244\n",
            "[710,    10] loss: 0.240\n",
            "[711,    10] loss: 0.252\n",
            "[712,    10] loss: 0.271\n",
            "[713,    10] loss: 0.256\n",
            "[714,    10] loss: 0.264\n",
            "[715,    10] loss: 0.249\n",
            "[716,    10] loss: 0.274\n",
            "[717,    10] loss: 0.245\n",
            "[718,    10] loss: 0.237\n",
            "[719,    10] loss: 0.256\n",
            "[720,    10] loss: 0.240\n",
            "[721,    10] loss: 0.239\n",
            "[722,    10] loss: 0.242\n",
            "[723,    10] loss: 0.253\n",
            "[724,    10] loss: 0.239\n",
            "[725,    10] loss: 0.250\n",
            "[726,    10] loss: 0.241\n",
            "[727,    10] loss: 0.242\n",
            "[728,    10] loss: 0.263\n",
            "[729,    10] loss: 0.272\n",
            "[730,    10] loss: 0.273\n",
            "[731,    10] loss: 0.240\n",
            "[732,    10] loss: 0.258\n",
            "[733,    10] loss: 0.272\n",
            "[734,    10] loss: 0.254\n",
            "[735,    10] loss: 0.259\n",
            "[736,    10] loss: 0.263\n",
            "[737,    10] loss: 0.265\n",
            "[738,    10] loss: 0.258\n",
            "[739,    10] loss: 0.243\n",
            "[740,    10] loss: 0.252\n",
            "[741,    10] loss: 0.258\n",
            "[742,    10] loss: 0.274\n",
            "[743,    10] loss: 0.271\n",
            "[744,    10] loss: 0.260\n",
            "[745,    10] loss: 0.243\n",
            "[746,    10] loss: 0.239\n",
            "[747,    10] loss: 0.258\n",
            "[748,    10] loss: 0.238\n",
            "[749,    10] loss: 0.237\n",
            "[750,    10] loss: 0.246\n",
            "[751,    10] loss: 0.248\n",
            "[752,    10] loss: 0.246\n",
            "[753,    10] loss: 0.246\n",
            "[754,    10] loss: 0.240\n",
            "[755,    10] loss: 0.254\n",
            "[756,    10] loss: 0.243\n",
            "[757,    10] loss: 0.269\n",
            "[758,    10] loss: 0.258\n",
            "[759,    10] loss: 0.260\n",
            "[760,    10] loss: 0.251\n",
            "[761,    10] loss: 0.251\n",
            "[762,    10] loss: 0.245\n",
            "[763,    10] loss: 0.260\n",
            "[764,    10] loss: 0.292\n",
            "[765,    10] loss: 0.298\n",
            "[766,    10] loss: 0.239\n",
            "[767,    10] loss: 0.246\n",
            "[768,    10] loss: 0.246\n",
            "[769,    10] loss: 0.245\n",
            "[770,    10] loss: 0.252\n",
            "[771,    10] loss: 0.245\n",
            "[772,    10] loss: 0.242\n",
            "[773,    10] loss: 0.240\n",
            "[774,    10] loss: 0.259\n",
            "[775,    10] loss: 0.255\n",
            "[776,    10] loss: 0.234\n",
            "[777,    10] loss: 0.248\n",
            "[778,    10] loss: 0.240\n",
            "[779,    10] loss: 0.246\n",
            "[780,    10] loss: 0.250\n",
            "[781,    10] loss: 0.252\n",
            "[782,    10] loss: 0.239\n",
            "[783,    10] loss: 0.260\n",
            "[784,    10] loss: 0.256\n",
            "[785,    10] loss: 0.274\n",
            "[786,    10] loss: 0.285\n",
            "[787,    10] loss: 0.270\n",
            "[788,    10] loss: 0.256\n",
            "[789,    10] loss: 0.241\n",
            "[790,    10] loss: 0.247\n",
            "[791,    10] loss: 0.255\n",
            "[792,    10] loss: 0.247\n",
            "[793,    10] loss: 0.264\n",
            "[794,    10] loss: 0.241\n",
            "[795,    10] loss: 0.256\n",
            "[796,    10] loss: 0.260\n",
            "[797,    10] loss: 0.247\n",
            "[798,    10] loss: 0.246\n",
            "[799,    10] loss: 0.237\n",
            "[800,    10] loss: 0.242\n",
            "[801,    10] loss: 0.244\n",
            "[802,    10] loss: 0.264\n",
            "[803,    10] loss: 0.293\n",
            "[804,    10] loss: 0.260\n",
            "[805,    10] loss: 0.251\n",
            "[806,    10] loss: 0.246\n",
            "[807,    10] loss: 0.249\n",
            "[808,    10] loss: 0.262\n",
            "[809,    10] loss: 0.264\n",
            "[810,    10] loss: 0.253\n",
            "[811,    10] loss: 0.237\n",
            "[812,    10] loss: 0.280\n",
            "[813,    10] loss: 0.269\n",
            "[814,    10] loss: 0.254\n",
            "[815,    10] loss: 0.248\n",
            "[816,    10] loss: 0.237\n",
            "[817,    10] loss: 0.250\n",
            "[818,    10] loss: 0.261\n",
            "[819,    10] loss: 0.247\n",
            "[820,    10] loss: 0.239\n",
            "[821,    10] loss: 0.249\n",
            "[822,    10] loss: 0.259\n",
            "[823,    10] loss: 0.248\n",
            "[824,    10] loss: 0.262\n",
            "[825,    10] loss: 0.251\n",
            "[826,    10] loss: 0.239\n",
            "[827,    10] loss: 0.243\n",
            "[828,    10] loss: 0.248\n",
            "[829,    10] loss: 0.246\n",
            "[830,    10] loss: 0.247\n",
            "[831,    10] loss: 0.267\n",
            "[832,    10] loss: 0.262\n",
            "[833,    10] loss: 0.265\n",
            "[834,    10] loss: 0.258\n",
            "[835,    10] loss: 0.270\n",
            "[836,    10] loss: 0.253\n",
            "[837,    10] loss: 0.242\n",
            "[838,    10] loss: 0.251\n",
            "[839,    10] loss: 0.252\n",
            "[840,    10] loss: 0.240\n",
            "[841,    10] loss: 0.242\n",
            "[842,    10] loss: 0.251\n",
            "[843,    10] loss: 0.280\n",
            "[844,    10] loss: 0.273\n",
            "[845,    10] loss: 0.280\n",
            "[846,    10] loss: 0.258\n",
            "[847,    10] loss: 0.240\n",
            "[848,    10] loss: 0.266\n",
            "[849,    10] loss: 0.237\n",
            "[850,    10] loss: 0.253\n",
            "[851,    10] loss: 0.250\n",
            "[852,    10] loss: 0.244\n",
            "[853,    10] loss: 0.260\n",
            "[854,    10] loss: 0.248\n",
            "[855,    10] loss: 0.247\n",
            "[856,    10] loss: 0.242\n",
            "[857,    10] loss: 0.272\n",
            "[858,    10] loss: 0.247\n",
            "[859,    10] loss: 0.292\n",
            "[860,    10] loss: 0.274\n",
            "[861,    10] loss: 0.284\n",
            "[862,    10] loss: 0.260\n",
            "[863,    10] loss: 0.257\n",
            "[864,    10] loss: 0.246\n",
            "[865,    10] loss: 0.255\n",
            "[866,    10] loss: 0.239\n",
            "[867,    10] loss: 0.251\n",
            "[868,    10] loss: 0.266\n",
            "[869,    10] loss: 0.250\n",
            "[870,    10] loss: 0.244\n",
            "[871,    10] loss: 0.245\n",
            "[872,    10] loss: 0.253\n",
            "[873,    10] loss: 0.251\n",
            "[874,    10] loss: 0.272\n",
            "[875,    10] loss: 0.306\n",
            "[876,    10] loss: 0.306\n",
            "[877,    10] loss: 0.293\n",
            "[878,    10] loss: 0.294\n",
            "[879,    10] loss: 0.280\n",
            "[880,    10] loss: 0.252\n",
            "[881,    10] loss: 0.238\n",
            "[882,    10] loss: 0.244\n",
            "[883,    10] loss: 0.255\n",
            "[884,    10] loss: 0.256\n",
            "[885,    10] loss: 0.280\n",
            "[886,    10] loss: 0.240\n",
            "[887,    10] loss: 0.240\n",
            "[888,    10] loss: 0.245\n",
            "[889,    10] loss: 0.256\n",
            "[890,    10] loss: 0.256\n",
            "[891,    10] loss: 0.262\n",
            "[892,    10] loss: 0.260\n",
            "[893,    10] loss: 0.257\n",
            "[894,    10] loss: 0.255\n",
            "[895,    10] loss: 0.253\n",
            "[896,    10] loss: 0.249\n",
            "[897,    10] loss: 0.237\n",
            "[898,    10] loss: 0.244\n",
            "[899,    10] loss: 0.257\n",
            "[900,    10] loss: 0.272\n",
            "[901,    10] loss: 0.277\n",
            "[902,    10] loss: 0.252\n",
            "[903,    10] loss: 0.241\n",
            "[904,    10] loss: 0.297\n",
            "[905,    10] loss: 0.329\n",
            "[906,    10] loss: 0.274\n",
            "[907,    10] loss: 0.243\n",
            "[908,    10] loss: 0.249\n",
            "[909,    10] loss: 0.257\n",
            "[910,    10] loss: 0.239\n",
            "[911,    10] loss: 0.267\n",
            "[912,    10] loss: 0.278\n",
            "[913,    10] loss: 0.264\n",
            "[914,    10] loss: 0.255\n",
            "[915,    10] loss: 0.244\n",
            "[916,    10] loss: 0.265\n",
            "[917,    10] loss: 0.245\n",
            "[918,    10] loss: 0.257\n",
            "[919,    10] loss: 0.248\n",
            "[920,    10] loss: 0.267\n",
            "[921,    10] loss: 0.282\n",
            "[922,    10] loss: 0.257\n",
            "[923,    10] loss: 0.260\n",
            "[924,    10] loss: 0.254\n",
            "[925,    10] loss: 0.251\n",
            "[926,    10] loss: 0.251\n",
            "[927,    10] loss: 0.256\n",
            "[928,    10] loss: 0.255\n",
            "[929,    10] loss: 0.256\n",
            "[930,    10] loss: 0.281\n",
            "[931,    10] loss: 0.249\n",
            "[932,    10] loss: 0.257\n",
            "[933,    10] loss: 0.286\n",
            "[934,    10] loss: 0.264\n",
            "[935,    10] loss: 0.290\n",
            "[936,    10] loss: 0.247\n",
            "[937,    10] loss: 0.251\n",
            "[938,    10] loss: 0.251\n",
            "[939,    10] loss: 0.288\n",
            "[940,    10] loss: 0.274\n",
            "[941,    10] loss: 0.271\n",
            "[942,    10] loss: 0.249\n",
            "[943,    10] loss: 0.246\n",
            "[944,    10] loss: 0.240\n",
            "[945,    10] loss: 0.239\n",
            "[946,    10] loss: 0.251\n",
            "[947,    10] loss: 0.236\n",
            "[948,    10] loss: 0.239\n",
            "[949,    10] loss: 0.248\n",
            "[950,    10] loss: 0.264\n",
            "[951,    10] loss: 0.251\n",
            "[952,    10] loss: 0.257\n",
            "[953,    10] loss: 0.237\n",
            "[954,    10] loss: 0.247\n",
            "[955,    10] loss: 0.244\n",
            "[956,    10] loss: 0.286\n",
            "[957,    10] loss: 0.275\n",
            "[958,    10] loss: 0.297\n",
            "[959,    10] loss: 0.292\n",
            "[960,    10] loss: 0.279\n",
            "[961,    10] loss: 0.241\n",
            "[962,    10] loss: 0.255\n",
            "[963,    10] loss: 0.248\n",
            "[964,    10] loss: 0.244\n",
            "[965,    10] loss: 0.261\n",
            "[966,    10] loss: 0.252\n",
            "[967,    10] loss: 0.247\n",
            "[968,    10] loss: 0.254\n",
            "[969,    10] loss: 0.251\n",
            "[970,    10] loss: 0.256\n",
            "[971,    10] loss: 0.235\n",
            "[972,    10] loss: 0.259\n",
            "[973,    10] loss: 0.242\n",
            "[974,    10] loss: 0.240\n",
            "[975,    10] loss: 0.262\n",
            "[976,    10] loss: 0.244\n",
            "[977,    10] loss: 0.254\n",
            "[978,    10] loss: 0.260\n",
            "[979,    10] loss: 0.247\n",
            "[980,    10] loss: 0.245\n",
            "[981,    10] loss: 0.252\n",
            "[982,    10] loss: 0.242\n",
            "[983,    10] loss: 0.242\n",
            "[984,    10] loss: 0.249\n",
            "[985,    10] loss: 0.277\n",
            "[986,    10] loss: 0.284\n",
            "[987,    10] loss: 0.263\n",
            "[988,    10] loss: 0.277\n",
            "[989,    10] loss: 0.258\n",
            "[990,    10] loss: 0.239\n",
            "[991,    10] loss: 0.241\n",
            "[992,    10] loss: 0.252\n",
            "[993,    10] loss: 0.260\n",
            "[994,    10] loss: 0.245\n",
            "[995,    10] loss: 0.265\n",
            "[996,    10] loss: 0.282\n",
            "[997,    10] loss: 0.279\n",
            "[998,    10] loss: 0.292\n",
            "[999,    10] loss: 0.241\n",
            "[1000,    10] loss: 0.245\n",
            "[1001,    10] loss: 0.246\n",
            "[1002,    10] loss: 0.251\n",
            "[1003,    10] loss: 0.259\n",
            "[1004,    10] loss: 0.242\n",
            "[1005,    10] loss: 0.251\n",
            "[1006,    10] loss: 0.241\n",
            "[1007,    10] loss: 0.250\n",
            "[1008,    10] loss: 0.245\n",
            "[1009,    10] loss: 0.251\n",
            "[1010,    10] loss: 0.244\n",
            "[1011,    10] loss: 0.263\n",
            "[1012,    10] loss: 0.246\n",
            "[1013,    10] loss: 0.250\n",
            "[1014,    10] loss: 0.243\n",
            "[1015,    10] loss: 0.252\n",
            "[1016,    10] loss: 0.241\n",
            "[1017,    10] loss: 0.266\n",
            "[1018,    10] loss: 0.258\n",
            "[1019,    10] loss: 0.256\n",
            "[1020,    10] loss: 0.251\n",
            "[1021,    10] loss: 0.270\n",
            "[1022,    10] loss: 0.249\n",
            "[1023,    10] loss: 0.269\n",
            "[1024,    10] loss: 0.267\n",
            "[1025,    10] loss: 0.256\n",
            "[1026,    10] loss: 0.259\n",
            "[1027,    10] loss: 0.242\n",
            "[1028,    10] loss: 0.251\n",
            "[1029,    10] loss: 0.256\n",
            "[1030,    10] loss: 0.247\n",
            "[1031,    10] loss: 0.253\n",
            "[1032,    10] loss: 0.249\n",
            "[1033,    10] loss: 0.248\n",
            "[1034,    10] loss: 0.253\n",
            "[1035,    10] loss: 0.247\n",
            "[1036,    10] loss: 0.256\n",
            "[1037,    10] loss: 0.271\n",
            "[1038,    10] loss: 0.284\n",
            "[1039,    10] loss: 0.302\n",
            "[1040,    10] loss: 0.260\n",
            "[1041,    10] loss: 0.267\n",
            "[1042,    10] loss: 0.257\n",
            "[1043,    10] loss: 0.251\n",
            "[1044,    10] loss: 0.244\n",
            "[1045,    10] loss: 0.279\n",
            "[1046,    10] loss: 0.366\n",
            "[1047,    10] loss: 0.340\n",
            "[1048,    10] loss: 0.474\n",
            "[1049,    10] loss: 0.330\n",
            "[1050,    10] loss: 0.286\n",
            "[1051,    10] loss: 0.264\n",
            "[1052,    10] loss: 0.254\n",
            "[1053,    10] loss: 0.244\n",
            "[1054,    10] loss: 0.251\n",
            "[1055,    10] loss: 0.246\n",
            "[1056,    10] loss: 0.236\n",
            "[1057,    10] loss: 0.248\n",
            "[1058,    10] loss: 0.246\n",
            "[1059,    10] loss: 0.270\n",
            "[1060,    10] loss: 0.253\n",
            "[1061,    10] loss: 0.248\n",
            "[1062,    10] loss: 0.253\n",
            "[1063,    10] loss: 0.251\n",
            "[1064,    10] loss: 0.258\n",
            "[1065,    10] loss: 0.244\n",
            "[1066,    10] loss: 0.256\n",
            "[1067,    10] loss: 0.257\n",
            "[1068,    10] loss: 0.250\n",
            "[1069,    10] loss: 0.236\n",
            "[1070,    10] loss: 0.253\n",
            "[1071,    10] loss: 0.244\n",
            "[1072,    10] loss: 0.240\n",
            "[1073,    10] loss: 0.240\n",
            "[1074,    10] loss: 0.246\n",
            "[1075,    10] loss: 0.255\n",
            "[1076,    10] loss: 0.277\n",
            "[1077,    10] loss: 0.276\n",
            "[1078,    10] loss: 0.248\n",
            "[1079,    10] loss: 0.283\n",
            "[1080,    10] loss: 0.253\n",
            "[1081,    10] loss: 0.247\n",
            "[1082,    10] loss: 0.245\n",
            "[1083,    10] loss: 0.250\n",
            "[1084,    10] loss: 0.264\n",
            "[1085,    10] loss: 0.274\n",
            "[1086,    10] loss: 0.262\n",
            "[1087,    10] loss: 0.280\n",
            "[1088,    10] loss: 0.283\n",
            "[1089,    10] loss: 0.277\n",
            "[1090,    10] loss: 0.247\n",
            "[1091,    10] loss: 0.241\n",
            "[1092,    10] loss: 0.241\n",
            "[1093,    10] loss: 0.249\n",
            "[1094,    10] loss: 0.262\n",
            "[1095,    10] loss: 0.259\n",
            "[1096,    10] loss: 0.241\n",
            "[1097,    10] loss: 0.274\n",
            "[1098,    10] loss: 0.269\n",
            "[1099,    10] loss: 0.255\n",
            "[1100,    10] loss: 0.258\n",
            "[1101,    10] loss: 0.278\n",
            "[1102,    10] loss: 0.256\n",
            "[1103,    10] loss: 0.253\n",
            "[1104,    10] loss: 0.259\n",
            "[1105,    10] loss: 0.239\n",
            "[1106,    10] loss: 0.251\n",
            "[1107,    10] loss: 0.239\n",
            "[1108,    10] loss: 0.239\n",
            "[1109,    10] loss: 0.244\n",
            "[1110,    10] loss: 0.247\n",
            "[1111,    10] loss: 0.257\n",
            "[1112,    10] loss: 0.257\n",
            "[1113,    10] loss: 0.260\n",
            "[1114,    10] loss: 0.238\n",
            "[1115,    10] loss: 0.244\n",
            "[1116,    10] loss: 0.239\n",
            "[1117,    10] loss: 0.253\n",
            "[1118,    10] loss: 0.252\n",
            "[1119,    10] loss: 0.253\n",
            "[1120,    10] loss: 0.252\n",
            "[1121,    10] loss: 0.256\n",
            "[1122,    10] loss: 0.260\n",
            "[1123,    10] loss: 0.257\n",
            "[1124,    10] loss: 0.248\n",
            "[1125,    10] loss: 0.246\n",
            "[1126,    10] loss: 0.244\n",
            "[1127,    10] loss: 0.253\n",
            "[1128,    10] loss: 0.257\n",
            "[1129,    10] loss: 0.258\n",
            "[1130,    10] loss: 0.247\n",
            "[1131,    10] loss: 0.264\n",
            "[1132,    10] loss: 0.263\n",
            "[1133,    10] loss: 0.246\n",
            "[1134,    10] loss: 0.256\n",
            "[1135,    10] loss: 0.252\n",
            "[1136,    10] loss: 0.240\n",
            "[1137,    10] loss: 0.242\n",
            "[1138,    10] loss: 0.265\n",
            "[1139,    10] loss: 0.262\n",
            "[1140,    10] loss: 0.267\n",
            "[1141,    10] loss: 0.250\n",
            "[1142,    10] loss: 0.262\n",
            "[1143,    10] loss: 0.240\n",
            "[1144,    10] loss: 0.241\n",
            "[1145,    10] loss: 0.237\n",
            "[1146,    10] loss: 0.237\n",
            "[1147,    10] loss: 0.268\n",
            "[1148,    10] loss: 0.280\n",
            "[1149,    10] loss: 0.322\n",
            "[1150,    10] loss: 0.254\n",
            "[1151,    10] loss: 0.267\n",
            "[1152,    10] loss: 0.251\n",
            "[1153,    10] loss: 0.258\n",
            "[1154,    10] loss: 0.280\n",
            "[1155,    10] loss: 0.279\n",
            "[1156,    10] loss: 0.249\n",
            "[1157,    10] loss: 0.244\n",
            "[1158,    10] loss: 0.246\n",
            "[1159,    10] loss: 0.241\n",
            "[1160,    10] loss: 0.241\n",
            "[1161,    10] loss: 0.255\n",
            "[1162,    10] loss: 0.244\n",
            "[1163,    10] loss: 0.242\n",
            "[1164,    10] loss: 0.236\n",
            "[1165,    10] loss: 0.243\n",
            "[1166,    10] loss: 0.242\n",
            "[1167,    10] loss: 0.251\n",
            "[1168,    10] loss: 0.254\n",
            "[1169,    10] loss: 0.249\n",
            "[1170,    10] loss: 0.265\n",
            "[1171,    10] loss: 0.251\n",
            "[1172,    10] loss: 0.250\n",
            "[1173,    10] loss: 0.235\n",
            "[1174,    10] loss: 0.243\n",
            "[1175,    10] loss: 0.254\n",
            "[1176,    10] loss: 0.242\n",
            "[1177,    10] loss: 0.245\n",
            "[1178,    10] loss: 0.242\n",
            "[1179,    10] loss: 0.244\n",
            "[1180,    10] loss: 0.250\n",
            "[1181,    10] loss: 0.242\n",
            "[1182,    10] loss: 0.246\n",
            "[1183,    10] loss: 0.240\n",
            "[1184,    10] loss: 0.242\n",
            "[1185,    10] loss: 0.249\n",
            "[1186,    10] loss: 0.254\n",
            "[1187,    10] loss: 0.240\n",
            "[1188,    10] loss: 0.240\n",
            "[1189,    10] loss: 0.244\n",
            "[1190,    10] loss: 0.246\n",
            "[1191,    10] loss: 0.256\n",
            "[1192,    10] loss: 0.244\n",
            "[1193,    10] loss: 0.276\n",
            "[1194,    10] loss: 0.282\n",
            "[1195,    10] loss: 0.248\n",
            "[1196,    10] loss: 0.252\n",
            "[1197,    10] loss: 0.241\n",
            "[1198,    10] loss: 0.244\n",
            "[1199,    10] loss: 0.258\n",
            "[1200,    10] loss: 0.251\n",
            "[1201,    10] loss: 0.241\n",
            "[1202,    10] loss: 0.246\n",
            "[1203,    10] loss: 0.265\n",
            "[1204,    10] loss: 0.252\n",
            "[1205,    10] loss: 0.238\n",
            "[1206,    10] loss: 0.247\n",
            "[1207,    10] loss: 0.254\n",
            "[1208,    10] loss: 0.236\n",
            "[1209,    10] loss: 0.248\n",
            "[1210,    10] loss: 0.244\n",
            "[1211,    10] loss: 0.250\n",
            "[1212,    10] loss: 0.239\n",
            "[1213,    10] loss: 0.257\n",
            "[1214,    10] loss: 0.242\n",
            "[1215,    10] loss: 0.244\n",
            "[1216,    10] loss: 0.243\n",
            "[1217,    10] loss: 0.238\n",
            "[1218,    10] loss: 0.239\n",
            "[1219,    10] loss: 0.237\n",
            "[1220,    10] loss: 0.252\n",
            "[1221,    10] loss: 0.258\n",
            "[1222,    10] loss: 0.245\n",
            "[1223,    10] loss: 0.238\n",
            "[1224,    10] loss: 0.253\n",
            "[1225,    10] loss: 0.278\n",
            "[1226,    10] loss: 0.265\n",
            "[1227,    10] loss: 0.260\n",
            "[1228,    10] loss: 0.292\n",
            "[1229,    10] loss: 0.313\n",
            "[1230,    10] loss: 0.310\n",
            "[1231,    10] loss: 0.299\n",
            "[1232,    10] loss: 0.268\n",
            "[1233,    10] loss: 0.257\n",
            "[1234,    10] loss: 0.241\n",
            "[1235,    10] loss: 0.243\n",
            "[1236,    10] loss: 0.253\n",
            "[1237,    10] loss: 0.270\n",
            "[1238,    10] loss: 0.262\n",
            "[1239,    10] loss: 0.273\n",
            "[1240,    10] loss: 0.261\n",
            "[1241,    10] loss: 0.237\n",
            "[1242,    10] loss: 0.247\n",
            "[1243,    10] loss: 0.239\n",
            "[1244,    10] loss: 0.235\n",
            "[1245,    10] loss: 0.251\n",
            "[1246,    10] loss: 0.248\n",
            "[1247,    10] loss: 0.253\n",
            "[1248,    10] loss: 0.240\n",
            "[1249,    10] loss: 0.254\n",
            "[1250,    10] loss: 0.259\n",
            "[1251,    10] loss: 0.253\n",
            "[1252,    10] loss: 0.252\n",
            "[1253,    10] loss: 0.251\n",
            "[1254,    10] loss: 0.248\n",
            "[1255,    10] loss: 0.257\n",
            "[1256,    10] loss: 0.245\n",
            "[1257,    10] loss: 0.243\n",
            "[1258,    10] loss: 0.250\n",
            "[1259,    10] loss: 0.255\n",
            "[1260,    10] loss: 0.245\n",
            "[1261,    10] loss: 0.249\n",
            "[1262,    10] loss: 0.240\n",
            "[1263,    10] loss: 0.258\n",
            "[1264,    10] loss: 0.245\n",
            "[1265,    10] loss: 0.241\n",
            "[1266,    10] loss: 0.244\n",
            "[1267,    10] loss: 0.245\n",
            "[1268,    10] loss: 0.257\n",
            "[1269,    10] loss: 0.242\n",
            "[1270,    10] loss: 0.243\n",
            "[1271,    10] loss: 0.258\n",
            "[1272,    10] loss: 0.267\n",
            "[1273,    10] loss: 0.247\n",
            "[1274,    10] loss: 0.278\n",
            "[1275,    10] loss: 0.264\n",
            "[1276,    10] loss: 0.275\n",
            "[1277,    10] loss: 0.270\n",
            "[1278,    10] loss: 0.249\n",
            "[1279,    10] loss: 0.243\n",
            "[1280,    10] loss: 0.237\n",
            "[1281,    10] loss: 0.239\n",
            "[1282,    10] loss: 0.247\n",
            "[1283,    10] loss: 0.247\n",
            "[1284,    10] loss: 0.260\n",
            "[1285,    10] loss: 0.255\n",
            "[1286,    10] loss: 0.250\n",
            "[1287,    10] loss: 0.247\n",
            "[1288,    10] loss: 0.243\n",
            "[1289,    10] loss: 0.245\n",
            "[1290,    10] loss: 0.241\n",
            "[1291,    10] loss: 0.238\n",
            "[1292,    10] loss: 0.239\n",
            "[1293,    10] loss: 0.250\n",
            "[1294,    10] loss: 0.255\n",
            "[1295,    10] loss: 0.241\n",
            "[1296,    10] loss: 0.242\n",
            "[1297,    10] loss: 0.244\n",
            "[1298,    10] loss: 0.248\n",
            "[1299,    10] loss: 0.238\n",
            "[1300,    10] loss: 0.250\n",
            "[1301,    10] loss: 0.253\n",
            "[1302,    10] loss: 0.241\n",
            "[1303,    10] loss: 0.237\n",
            "[1304,    10] loss: 0.246\n",
            "[1305,    10] loss: 0.258\n",
            "[1306,    10] loss: 0.241\n",
            "[1307,    10] loss: 0.251\n",
            "[1308,    10] loss: 0.254\n",
            "[1309,    10] loss: 0.249\n",
            "[1310,    10] loss: 0.238\n",
            "[1311,    10] loss: 0.258\n",
            "[1312,    10] loss: 0.239\n",
            "[1313,    10] loss: 0.239\n",
            "[1314,    10] loss: 0.248\n",
            "[1315,    10] loss: 0.239\n",
            "[1316,    10] loss: 0.237\n",
            "[1317,    10] loss: 0.267\n",
            "[1318,    10] loss: 0.247\n",
            "[1319,    10] loss: 0.246\n",
            "[1320,    10] loss: 0.246\n",
            "[1321,    10] loss: 0.246\n",
            "[1322,    10] loss: 0.251\n",
            "[1323,    10] loss: 0.250\n",
            "[1324,    10] loss: 0.261\n",
            "[1325,    10] loss: 0.243\n",
            "[1326,    10] loss: 0.239\n",
            "[1327,    10] loss: 0.247\n",
            "[1328,    10] loss: 0.243\n",
            "[1329,    10] loss: 0.244\n",
            "[1330,    10] loss: 0.273\n",
            "[1331,    10] loss: 0.264\n",
            "[1332,    10] loss: 0.297\n",
            "[1333,    10] loss: 0.259\n",
            "[1334,    10] loss: 0.242\n",
            "[1335,    10] loss: 0.246\n",
            "[1336,    10] loss: 0.236\n",
            "[1337,    10] loss: 0.257\n",
            "[1338,    10] loss: 0.247\n",
            "[1339,    10] loss: 0.244\n",
            "[1340,    10] loss: 0.241\n",
            "[1341,    10] loss: 0.245\n",
            "[1342,    10] loss: 0.246\n",
            "[1343,    10] loss: 0.246\n",
            "[1344,    10] loss: 0.238\n",
            "[1345,    10] loss: 0.246\n",
            "[1346,    10] loss: 0.250\n",
            "[1347,    10] loss: 0.245\n",
            "[1348,    10] loss: 0.235\n",
            "[1349,    10] loss: 0.238\n",
            "[1350,    10] loss: 0.250\n",
            "[1351,    10] loss: 0.248\n",
            "[1352,    10] loss: 0.241\n",
            "[1353,    10] loss: 0.242\n",
            "[1354,    10] loss: 0.260\n",
            "[1355,    10] loss: 0.241\n",
            "[1356,    10] loss: 0.243\n",
            "[1357,    10] loss: 0.241\n",
            "[1358,    10] loss: 0.250\n",
            "[1359,    10] loss: 0.239\n",
            "[1360,    10] loss: 0.238\n",
            "[1361,    10] loss: 0.248\n",
            "[1362,    10] loss: 0.253\n",
            "[1363,    10] loss: 0.253\n",
            "[1364,    10] loss: 0.260\n",
            "[1365,    10] loss: 0.255\n",
            "[1366,    10] loss: 0.247\n",
            "[1367,    10] loss: 0.256\n",
            "[1368,    10] loss: 0.252\n",
            "[1369,    10] loss: 0.247\n",
            "[1370,    10] loss: 0.248\n",
            "[1371,    10] loss: 0.249\n",
            "[1372,    10] loss: 0.248\n",
            "[1373,    10] loss: 0.251\n",
            "[1374,    10] loss: 0.274\n",
            "[1375,    10] loss: 0.271\n",
            "[1376,    10] loss: 0.256\n",
            "[1377,    10] loss: 0.245\n",
            "[1378,    10] loss: 0.239\n",
            "[1379,    10] loss: 0.250\n",
            "[1380,    10] loss: 0.250\n",
            "[1381,    10] loss: 0.239\n",
            "[1382,    10] loss: 0.243\n",
            "[1383,    10] loss: 0.239\n",
            "[1384,    10] loss: 0.245\n",
            "[1385,    10] loss: 0.240\n",
            "[1386,    10] loss: 0.249\n",
            "[1387,    10] loss: 0.246\n",
            "[1388,    10] loss: 0.238\n",
            "[1389,    10] loss: 0.256\n",
            "[1390,    10] loss: 0.269\n",
            "[1391,    10] loss: 0.249\n",
            "[1392,    10] loss: 0.252\n",
            "[1393,    10] loss: 0.247\n",
            "[1394,    10] loss: 0.248\n",
            "[1395,    10] loss: 0.256\n",
            "[1396,    10] loss: 0.242\n",
            "[1397,    10] loss: 0.240\n",
            "[1398,    10] loss: 0.239\n",
            "[1399,    10] loss: 0.236\n",
            "[1400,    10] loss: 0.238\n",
            "[1401,    10] loss: 0.237\n",
            "[1402,    10] loss: 0.240\n",
            "[1403,    10] loss: 0.245\n",
            "[1404,    10] loss: 0.247\n",
            "[1405,    10] loss: 0.245\n",
            "[1406,    10] loss: 0.254\n",
            "[1407,    10] loss: 0.239\n",
            "[1408,    10] loss: 0.250\n",
            "[1409,    10] loss: 0.241\n",
            "[1410,    10] loss: 0.246\n",
            "[1411,    10] loss: 0.253\n",
            "[1412,    10] loss: 0.251\n",
            "[1413,    10] loss: 0.257\n",
            "[1414,    10] loss: 0.260\n",
            "[1415,    10] loss: 0.240\n",
            "[1416,    10] loss: 0.245\n",
            "[1417,    10] loss: 0.243\n",
            "[1418,    10] loss: 0.276\n",
            "[1419,    10] loss: 0.252\n",
            "[1420,    10] loss: 0.262\n",
            "[1421,    10] loss: 0.257\n",
            "[1422,    10] loss: 0.242\n",
            "[1423,    10] loss: 0.241\n",
            "[1424,    10] loss: 0.249\n",
            "[1425,    10] loss: 0.245\n",
            "[1426,    10] loss: 0.265\n",
            "[1427,    10] loss: 0.251\n",
            "[1428,    10] loss: 0.239\n",
            "[1429,    10] loss: 0.242\n",
            "[1430,    10] loss: 0.246\n",
            "[1431,    10] loss: 0.237\n",
            "[1432,    10] loss: 0.254\n",
            "[1433,    10] loss: 0.248\n",
            "[1434,    10] loss: 0.238\n",
            "[1435,    10] loss: 0.260\n",
            "[1436,    10] loss: 0.270\n",
            "[1437,    10] loss: 0.248\n",
            "[1438,    10] loss: 0.250\n",
            "[1439,    10] loss: 0.250\n",
            "[1440,    10] loss: 0.278\n",
            "[1441,    10] loss: 0.243\n",
            "[1442,    10] loss: 0.252\n",
            "[1443,    10] loss: 0.253\n",
            "[1444,    10] loss: 0.252\n",
            "[1445,    10] loss: 0.267\n",
            "[1446,    10] loss: 0.301\n",
            "[1447,    10] loss: 0.299\n",
            "[1448,    10] loss: 0.269\n",
            "[1449,    10] loss: 0.246\n",
            "[1450,    10] loss: 0.245\n",
            "[1451,    10] loss: 0.241\n",
            "[1452,    10] loss: 0.244\n",
            "[1453,    10] loss: 0.248\n",
            "[1454,    10] loss: 0.251\n",
            "[1455,    10] loss: 0.243\n",
            "[1456,    10] loss: 0.239\n",
            "[1457,    10] loss: 0.241\n",
            "[1458,    10] loss: 0.267\n",
            "[1459,    10] loss: 0.257\n",
            "[1460,    10] loss: 0.252\n",
            "[1461,    10] loss: 0.244\n",
            "[1462,    10] loss: 0.241\n",
            "[1463,    10] loss: 0.246\n",
            "[1464,    10] loss: 0.247\n",
            "[1465,    10] loss: 0.246\n",
            "[1466,    10] loss: 0.250\n",
            "[1467,    10] loss: 0.251\n",
            "[1468,    10] loss: 0.252\n",
            "[1469,    10] loss: 0.240\n",
            "[1470,    10] loss: 0.245\n",
            "[1471,    10] loss: 0.250\n",
            "[1472,    10] loss: 0.274\n",
            "[1473,    10] loss: 0.248\n",
            "[1474,    10] loss: 0.244\n",
            "[1475,    10] loss: 0.258\n",
            "[1476,    10] loss: 0.253\n",
            "[1477,    10] loss: 0.240\n",
            "[1478,    10] loss: 0.242\n",
            "[1479,    10] loss: 0.239\n",
            "[1480,    10] loss: 0.247\n",
            "[1481,    10] loss: 0.243\n",
            "[1482,    10] loss: 0.246\n",
            "[1483,    10] loss: 0.250\n",
            "[1484,    10] loss: 0.252\n",
            "[1485,    10] loss: 0.236\n",
            "[1486,    10] loss: 0.244\n",
            "[1487,    10] loss: 0.249\n",
            "[1488,    10] loss: 0.248\n",
            "[1489,    10] loss: 0.263\n",
            "[1490,    10] loss: 0.254\n",
            "[1491,    10] loss: 0.256\n",
            "[1492,    10] loss: 0.242\n",
            "[1493,    10] loss: 0.257\n",
            "[1494,    10] loss: 0.259\n",
            "[1495,    10] loss: 0.256\n",
            "[1496,    10] loss: 0.273\n",
            "[1497,    10] loss: 0.244\n",
            "[1498,    10] loss: 0.244\n",
            "[1499,    10] loss: 0.238\n",
            "[1500,    10] loss: 0.241\n",
            "[1501,    10] loss: 0.238\n",
            "[1502,    10] loss: 0.245\n",
            "[1503,    10] loss: 0.242\n",
            "[1504,    10] loss: 0.246\n",
            "[1505,    10] loss: 0.239\n",
            "[1506,    10] loss: 0.242\n",
            "[1507,    10] loss: 0.244\n",
            "[1508,    10] loss: 0.247\n",
            "[1509,    10] loss: 0.260\n",
            "[1510,    10] loss: 0.243\n",
            "[1511,    10] loss: 0.253\n",
            "[1512,    10] loss: 0.250\n",
            "[1513,    10] loss: 0.236\n",
            "[1514,    10] loss: 0.257\n",
            "[1515,    10] loss: 0.257\n",
            "[1516,    10] loss: 0.248\n",
            "[1517,    10] loss: 0.239\n",
            "[1518,    10] loss: 0.244\n",
            "[1519,    10] loss: 0.254\n",
            "[1520,    10] loss: 0.252\n",
            "[1521,    10] loss: 0.240\n",
            "[1522,    10] loss: 0.244\n",
            "[1523,    10] loss: 0.251\n",
            "[1524,    10] loss: 0.277\n",
            "[1525,    10] loss: 0.269\n",
            "[1526,    10] loss: 0.245\n",
            "[1527,    10] loss: 0.237\n",
            "[1528,    10] loss: 0.245\n",
            "[1529,    10] loss: 0.263\n",
            "[1530,    10] loss: 0.275\n",
            "[1531,    10] loss: 0.247\n",
            "[1532,    10] loss: 0.243\n",
            "[1533,    10] loss: 0.259\n",
            "[1534,    10] loss: 0.241\n",
            "[1535,    10] loss: 0.264\n",
            "[1536,    10] loss: 0.245\n",
            "[1537,    10] loss: 0.278\n",
            "[1538,    10] loss: 0.250\n",
            "[1539,    10] loss: 0.234\n",
            "[1540,    10] loss: 0.263\n",
            "[1541,    10] loss: 0.245\n",
            "[1542,    10] loss: 0.242\n",
            "[1543,    10] loss: 0.240\n",
            "[1544,    10] loss: 0.245\n",
            "[1545,    10] loss: 0.267\n",
            "[1546,    10] loss: 0.254\n",
            "[1547,    10] loss: 0.277\n",
            "[1548,    10] loss: 0.245\n",
            "[1549,    10] loss: 0.251\n",
            "[1550,    10] loss: 0.253\n",
            "[1551,    10] loss: 0.245\n",
            "[1552,    10] loss: 0.240\n",
            "[1553,    10] loss: 0.247\n",
            "[1554,    10] loss: 0.248\n",
            "[1555,    10] loss: 0.240\n",
            "[1556,    10] loss: 0.258\n",
            "[1557,    10] loss: 0.244\n",
            "[1558,    10] loss: 0.246\n",
            "[1559,    10] loss: 0.239\n",
            "[1560,    10] loss: 0.241\n",
            "[1561,    10] loss: 0.249\n",
            "[1562,    10] loss: 0.246\n",
            "[1563,    10] loss: 0.242\n",
            "[1564,    10] loss: 0.250\n",
            "[1565,    10] loss: 0.286\n",
            "[1566,    10] loss: 0.254\n",
            "[1567,    10] loss: 0.237\n",
            "[1568,    10] loss: 0.259\n",
            "[1569,    10] loss: 0.261\n",
            "[1570,    10] loss: 0.250\n",
            "[1571,    10] loss: 0.235\n",
            "[1572,    10] loss: 0.255\n",
            "[1573,    10] loss: 0.262\n",
            "[1574,    10] loss: 0.241\n",
            "[1575,    10] loss: 0.241\n",
            "[1576,    10] loss: 0.249\n",
            "[1577,    10] loss: 0.263\n",
            "[1578,    10] loss: 0.273\n",
            "[1579,    10] loss: 0.251\n",
            "[1580,    10] loss: 0.259\n",
            "[1581,    10] loss: 0.239\n",
            "[1582,    10] loss: 0.239\n",
            "[1583,    10] loss: 0.245\n",
            "[1584,    10] loss: 0.241\n",
            "[1585,    10] loss: 0.242\n",
            "[1586,    10] loss: 0.240\n",
            "[1587,    10] loss: 0.246\n",
            "[1588,    10] loss: 0.243\n",
            "[1589,    10] loss: 0.250\n",
            "[1590,    10] loss: 0.261\n",
            "[1591,    10] loss: 0.244\n",
            "[1592,    10] loss: 0.239\n",
            "[1593,    10] loss: 0.256\n",
            "[1594,    10] loss: 0.255\n",
            "[1595,    10] loss: 0.248\n",
            "[1596,    10] loss: 0.252\n",
            "[1597,    10] loss: 0.247\n",
            "[1598,    10] loss: 0.255\n",
            "[1599,    10] loss: 0.264\n",
            "[1600,    10] loss: 0.256\n",
            "[1601,    10] loss: 0.280\n",
            "[1602,    10] loss: 0.255\n",
            "[1603,    10] loss: 0.259\n",
            "[1604,    10] loss: 0.246\n",
            "[1605,    10] loss: 0.250\n",
            "[1606,    10] loss: 0.248\n",
            "[1607,    10] loss: 0.246\n",
            "[1608,    10] loss: 0.250\n",
            "[1609,    10] loss: 0.243\n",
            "[1610,    10] loss: 0.243\n",
            "[1611,    10] loss: 0.248\n",
            "[1612,    10] loss: 0.244\n",
            "[1613,    10] loss: 0.240\n",
            "[1614,    10] loss: 0.244\n",
            "[1615,    10] loss: 0.251\n",
            "[1616,    10] loss: 0.261\n",
            "[1617,    10] loss: 0.244\n",
            "[1618,    10] loss: 0.268\n",
            "[1619,    10] loss: 0.235\n",
            "[1620,    10] loss: 0.267\n",
            "[1621,    10] loss: 0.246\n",
            "[1622,    10] loss: 0.243\n",
            "[1623,    10] loss: 0.256\n",
            "[1624,    10] loss: 0.254\n",
            "[1625,    10] loss: 0.267\n",
            "[1626,    10] loss: 0.250\n",
            "[1627,    10] loss: 0.239\n",
            "[1628,    10] loss: 0.245\n",
            "[1629,    10] loss: 0.253\n",
            "[1630,    10] loss: 0.248\n",
            "[1631,    10] loss: 0.247\n",
            "[1632,    10] loss: 0.254\n",
            "[1633,    10] loss: 0.237\n",
            "[1634,    10] loss: 0.261\n",
            "[1635,    10] loss: 0.252\n",
            "[1636,    10] loss: 0.240\n",
            "[1637,    10] loss: 0.238\n",
            "[1638,    10] loss: 0.247\n",
            "[1639,    10] loss: 0.250\n",
            "[1640,    10] loss: 0.252\n",
            "[1641,    10] loss: 0.279\n",
            "[1642,    10] loss: 0.257\n",
            "[1643,    10] loss: 0.237\n",
            "[1644,    10] loss: 0.249\n",
            "[1645,    10] loss: 0.247\n",
            "[1646,    10] loss: 0.241\n",
            "[1647,    10] loss: 0.263\n",
            "[1648,    10] loss: 0.255\n",
            "[1649,    10] loss: 0.237\n",
            "[1650,    10] loss: 0.248\n",
            "[1651,    10] loss: 0.242\n",
            "[1652,    10] loss: 0.247\n",
            "[1653,    10] loss: 0.245\n",
            "[1654,    10] loss: 0.240\n",
            "[1655,    10] loss: 0.245\n",
            "[1656,    10] loss: 0.249\n",
            "[1657,    10] loss: 0.245\n",
            "[1658,    10] loss: 0.251\n",
            "[1659,    10] loss: 0.276\n",
            "[1660,    10] loss: 0.271\n",
            "[1661,    10] loss: 0.274\n",
            "[1662,    10] loss: 0.245\n",
            "[1663,    10] loss: 0.246\n",
            "[1664,    10] loss: 0.253\n",
            "[1665,    10] loss: 0.247\n",
            "[1666,    10] loss: 0.239\n",
            "[1667,    10] loss: 0.239\n",
            "[1668,    10] loss: 0.241\n",
            "[1669,    10] loss: 0.248\n",
            "[1670,    10] loss: 0.244\n",
            "[1671,    10] loss: 0.243\n",
            "[1672,    10] loss: 0.251\n",
            "[1673,    10] loss: 0.241\n",
            "[1674,    10] loss: 0.238\n",
            "[1675,    10] loss: 0.243\n",
            "[1676,    10] loss: 0.243\n",
            "[1677,    10] loss: 0.257\n",
            "[1678,    10] loss: 0.239\n",
            "[1679,    10] loss: 0.245\n",
            "[1680,    10] loss: 0.240\n",
            "[1681,    10] loss: 0.258\n",
            "[1682,    10] loss: 0.249\n",
            "[1683,    10] loss: 0.245\n",
            "[1684,    10] loss: 0.253\n",
            "[1685,    10] loss: 0.247\n",
            "[1686,    10] loss: 0.241\n",
            "[1687,    10] loss: 0.239\n",
            "[1688,    10] loss: 0.237\n",
            "[1689,    10] loss: 0.262\n",
            "[1690,    10] loss: 0.243\n",
            "[1691,    10] loss: 0.241\n",
            "[1692,    10] loss: 0.245\n",
            "[1693,    10] loss: 0.267\n",
            "[1694,    10] loss: 0.263\n",
            "[1695,    10] loss: 0.254\n",
            "[1696,    10] loss: 0.237\n",
            "[1697,    10] loss: 0.263\n",
            "[1698,    10] loss: 0.255\n",
            "[1699,    10] loss: 0.267\n",
            "[1700,    10] loss: 0.281\n",
            "[1701,    10] loss: 0.267\n",
            "[1702,    10] loss: 0.248\n",
            "[1703,    10] loss: 0.257\n",
            "[1704,    10] loss: 0.273\n",
            "[1705,    10] loss: 0.271\n",
            "[1706,    10] loss: 0.250\n",
            "[1707,    10] loss: 0.275\n",
            "[1708,    10] loss: 0.256\n",
            "[1709,    10] loss: 0.261\n",
            "[1710,    10] loss: 0.258\n",
            "[1711,    10] loss: 0.252\n",
            "[1712,    10] loss: 0.252\n",
            "[1713,    10] loss: 0.237\n",
            "[1714,    10] loss: 0.239\n",
            "[1715,    10] loss: 0.244\n",
            "[1716,    10] loss: 0.247\n",
            "[1717,    10] loss: 0.236\n",
            "[1718,    10] loss: 0.248\n",
            "[1719,    10] loss: 0.245\n",
            "[1720,    10] loss: 0.238\n",
            "[1721,    10] loss: 0.252\n",
            "[1722,    10] loss: 0.240\n",
            "[1723,    10] loss: 0.239\n",
            "[1724,    10] loss: 0.240\n",
            "[1725,    10] loss: 0.252\n",
            "[1726,    10] loss: 0.247\n",
            "[1727,    10] loss: 0.250\n",
            "[1728,    10] loss: 0.240\n",
            "[1729,    10] loss: 0.241\n",
            "[1730,    10] loss: 0.256\n",
            "[1731,    10] loss: 0.266\n",
            "[1732,    10] loss: 0.245\n",
            "[1733,    10] loss: 0.293\n",
            "[1734,    10] loss: 0.275\n",
            "[1735,    10] loss: 0.261\n",
            "[1736,    10] loss: 0.269\n",
            "[1737,    10] loss: 0.238\n",
            "[1738,    10] loss: 0.258\n",
            "[1739,    10] loss: 0.243\n",
            "[1740,    10] loss: 0.245\n",
            "[1741,    10] loss: 0.240\n",
            "[1742,    10] loss: 0.250\n",
            "[1743,    10] loss: 0.241\n",
            "[1744,    10] loss: 0.243\n",
            "[1745,    10] loss: 0.244\n",
            "[1746,    10] loss: 0.247\n",
            "[1747,    10] loss: 0.261\n",
            "[1748,    10] loss: 0.237\n",
            "[1749,    10] loss: 0.241\n",
            "[1750,    10] loss: 0.244\n",
            "[1751,    10] loss: 0.241\n",
            "[1752,    10] loss: 0.246\n",
            "[1753,    10] loss: 0.248\n",
            "[1754,    10] loss: 0.249\n",
            "[1755,    10] loss: 0.248\n",
            "[1756,    10] loss: 0.250\n",
            "[1757,    10] loss: 0.269\n",
            "[1758,    10] loss: 0.254\n",
            "[1759,    10] loss: 0.261\n",
            "[1760,    10] loss: 0.299\n",
            "[1761,    10] loss: 0.267\n",
            "[1762,    10] loss: 0.251\n",
            "[1763,    10] loss: 0.290\n",
            "[1764,    10] loss: 0.265\n",
            "[1765,    10] loss: 0.274\n",
            "[1766,    10] loss: 0.269\n",
            "[1767,    10] loss: 0.240\n",
            "[1768,    10] loss: 0.251\n",
            "[1769,    10] loss: 0.244\n",
            "[1770,    10] loss: 0.240\n",
            "[1771,    10] loss: 0.241\n",
            "[1772,    10] loss: 0.259\n",
            "[1773,    10] loss: 0.252\n",
            "[1774,    10] loss: 0.244\n",
            "[1775,    10] loss: 0.243\n",
            "[1776,    10] loss: 0.258\n",
            "[1777,    10] loss: 0.279\n",
            "[1778,    10] loss: 0.264\n",
            "[1779,    10] loss: 0.260\n",
            "[1780,    10] loss: 0.249\n",
            "[1781,    10] loss: 0.247\n",
            "[1782,    10] loss: 0.244\n",
            "[1783,    10] loss: 0.236\n",
            "[1784,    10] loss: 0.245\n",
            "[1785,    10] loss: 0.248\n",
            "[1786,    10] loss: 0.241\n",
            "[1787,    10] loss: 0.249\n",
            "[1788,    10] loss: 0.240\n",
            "[1789,    10] loss: 0.249\n",
            "[1790,    10] loss: 0.238\n",
            "[1791,    10] loss: 0.251\n",
            "[1792,    10] loss: 0.240\n",
            "[1793,    10] loss: 0.243\n",
            "[1794,    10] loss: 0.247\n",
            "[1795,    10] loss: 0.244\n",
            "[1796,    10] loss: 0.253\n",
            "[1797,    10] loss: 0.271\n",
            "[1798,    10] loss: 0.260\n",
            "[1799,    10] loss: 0.266\n",
            "[1800,    10] loss: 0.261\n",
            "[1801,    10] loss: 0.246\n",
            "[1802,    10] loss: 0.245\n",
            "[1803,    10] loss: 0.244\n",
            "[1804,    10] loss: 0.236\n",
            "[1805,    10] loss: 0.245\n",
            "[1806,    10] loss: 0.239\n",
            "[1807,    10] loss: 0.239\n",
            "[1808,    10] loss: 0.245\n",
            "[1809,    10] loss: 0.244\n",
            "[1810,    10] loss: 0.265\n",
            "[1811,    10] loss: 0.244\n",
            "[1812,    10] loss: 0.253\n",
            "[1813,    10] loss: 0.246\n",
            "[1814,    10] loss: 0.239\n",
            "[1815,    10] loss: 0.235\n",
            "[1816,    10] loss: 0.246\n",
            "[1817,    10] loss: 0.246\n",
            "[1818,    10] loss: 0.255\n",
            "[1819,    10] loss: 0.246\n",
            "[1820,    10] loss: 0.258\n",
            "[1821,    10] loss: 0.238\n",
            "[1822,    10] loss: 0.239\n",
            "[1823,    10] loss: 0.256\n",
            "[1824,    10] loss: 0.267\n",
            "[1825,    10] loss: 0.262\n",
            "[1826,    10] loss: 0.260\n",
            "[1827,    10] loss: 0.257\n",
            "[1828,    10] loss: 0.251\n",
            "[1829,    10] loss: 0.274\n",
            "[1830,    10] loss: 0.272\n",
            "[1831,    10] loss: 0.279\n",
            "[1832,    10] loss: 0.266\n",
            "[1833,    10] loss: 0.260\n",
            "[1834,    10] loss: 0.255\n",
            "[1835,    10] loss: 0.245\n",
            "[1836,    10] loss: 0.244\n",
            "[1837,    10] loss: 0.303\n",
            "[1838,    10] loss: 0.281\n",
            "[1839,    10] loss: 0.250\n",
            "[1840,    10] loss: 0.247\n",
            "[1841,    10] loss: 0.249\n",
            "[1842,    10] loss: 0.248\n",
            "[1843,    10] loss: 0.249\n",
            "[1844,    10] loss: 0.270\n",
            "[1845,    10] loss: 0.248\n",
            "[1846,    10] loss: 0.244\n",
            "[1847,    10] loss: 0.248\n",
            "[1848,    10] loss: 0.257\n",
            "[1849,    10] loss: 0.256\n",
            "[1850,    10] loss: 0.251\n",
            "[1851,    10] loss: 0.242\n",
            "[1852,    10] loss: 0.251\n",
            "[1853,    10] loss: 0.244\n",
            "[1854,    10] loss: 0.243\n",
            "[1855,    10] loss: 0.239\n",
            "[1856,    10] loss: 0.237\n",
            "[1857,    10] loss: 0.245\n",
            "[1858,    10] loss: 0.244\n",
            "[1859,    10] loss: 0.241\n",
            "[1860,    10] loss: 0.255\n",
            "[1861,    10] loss: 0.270\n",
            "[1862,    10] loss: 0.248\n",
            "[1863,    10] loss: 0.245\n",
            "[1864,    10] loss: 0.258\n",
            "[1865,    10] loss: 0.238\n",
            "[1866,    10] loss: 0.239\n",
            "[1867,    10] loss: 0.242\n",
            "[1868,    10] loss: 0.241\n",
            "[1869,    10] loss: 0.240\n",
            "[1870,    10] loss: 0.244\n",
            "[1871,    10] loss: 0.243\n",
            "[1872,    10] loss: 0.247\n",
            "[1873,    10] loss: 0.236\n",
            "[1874,    10] loss: 0.240\n",
            "[1875,    10] loss: 0.268\n",
            "[1876,    10] loss: 0.247\n",
            "[1877,    10] loss: 0.253\n",
            "[1878,    10] loss: 0.248\n",
            "[1879,    10] loss: 0.280\n",
            "[1880,    10] loss: 0.250\n",
            "[1881,    10] loss: 0.249\n",
            "[1882,    10] loss: 0.257\n",
            "[1883,    10] loss: 0.244\n",
            "[1884,    10] loss: 0.241\n",
            "[1885,    10] loss: 0.246\n",
            "[1886,    10] loss: 0.234\n",
            "[1887,    10] loss: 0.248\n",
            "[1888,    10] loss: 0.239\n",
            "[1889,    10] loss: 0.239\n",
            "[1890,    10] loss: 0.261\n",
            "[1891,    10] loss: 0.257\n",
            "[1892,    10] loss: 0.262\n",
            "[1893,    10] loss: 0.241\n",
            "[1894,    10] loss: 0.244\n",
            "[1895,    10] loss: 0.239\n",
            "[1896,    10] loss: 0.261\n",
            "[1897,    10] loss: 0.255\n",
            "[1898,    10] loss: 0.264\n",
            "[1899,    10] loss: 0.269\n",
            "[1900,    10] loss: 0.251\n",
            "[1901,    10] loss: 0.254\n",
            "[1902,    10] loss: 0.257\n",
            "[1903,    10] loss: 0.289\n",
            "[1904,    10] loss: 0.242\n",
            "[1905,    10] loss: 0.243\n",
            "[1906,    10] loss: 0.242\n",
            "[1907,    10] loss: 0.239\n",
            "[1908,    10] loss: 0.246\n",
            "[1909,    10] loss: 0.246\n",
            "[1910,    10] loss: 0.241\n",
            "[1911,    10] loss: 0.244\n",
            "[1912,    10] loss: 0.259\n",
            "[1913,    10] loss: 0.243\n",
            "[1914,    10] loss: 0.243\n",
            "[1915,    10] loss: 0.254\n",
            "[1916,    10] loss: 0.247\n",
            "[1917,    10] loss: 0.241\n",
            "[1918,    10] loss: 0.245\n",
            "[1919,    10] loss: 0.249\n",
            "[1920,    10] loss: 0.255\n",
            "[1921,    10] loss: 0.255\n",
            "[1922,    10] loss: 0.246\n",
            "[1923,    10] loss: 0.280\n",
            "[1924,    10] loss: 0.255\n",
            "[1925,    10] loss: 0.261\n",
            "[1926,    10] loss: 0.283\n",
            "[1927,    10] loss: 0.267\n",
            "[1928,    10] loss: 0.269\n",
            "[1929,    10] loss: 0.255\n",
            "[1930,    10] loss: 0.238\n",
            "[1931,    10] loss: 0.242\n",
            "[1932,    10] loss: 0.248\n",
            "[1933,    10] loss: 0.256\n",
            "[1934,    10] loss: 0.264\n",
            "[1935,    10] loss: 0.233\n",
            "[1936,    10] loss: 0.246\n",
            "[1937,    10] loss: 0.237\n",
            "[1938,    10] loss: 0.241\n",
            "[1939,    10] loss: 0.238\n",
            "[1940,    10] loss: 0.246\n",
            "[1941,    10] loss: 0.246\n",
            "[1942,    10] loss: 0.253\n",
            "[1943,    10] loss: 0.254\n",
            "[1944,    10] loss: 0.255\n",
            "[1945,    10] loss: 0.249\n",
            "[1946,    10] loss: 0.250\n",
            "[1947,    10] loss: 0.245\n",
            "[1948,    10] loss: 0.254\n",
            "[1949,    10] loss: 0.249\n",
            "[1950,    10] loss: 0.247\n",
            "[1951,    10] loss: 0.241\n",
            "[1952,    10] loss: 0.247\n",
            "[1953,    10] loss: 0.251\n",
            "[1954,    10] loss: 0.284\n",
            "[1955,    10] loss: 0.251\n",
            "[1956,    10] loss: 0.248\n",
            "[1957,    10] loss: 0.260\n",
            "[1958,    10] loss: 0.291\n",
            "[1959,    10] loss: 0.256\n",
            "[1960,    10] loss: 0.246\n",
            "[1961,    10] loss: 0.240\n",
            "[1962,    10] loss: 0.245\n",
            "[1963,    10] loss: 0.248\n",
            "[1964,    10] loss: 0.261\n",
            "[1965,    10] loss: 0.244\n",
            "[1966,    10] loss: 0.242\n",
            "[1967,    10] loss: 0.247\n",
            "[1968,    10] loss: 0.233\n",
            "[1969,    10] loss: 0.247\n",
            "[1970,    10] loss: 0.245\n",
            "[1971,    10] loss: 0.248\n",
            "[1972,    10] loss: 0.245\n",
            "[1973,    10] loss: 0.250\n",
            "[1974,    10] loss: 0.264\n",
            "[1975,    10] loss: 0.259\n",
            "[1976,    10] loss: 0.244\n",
            "[1977,    10] loss: 0.254\n",
            "[1978,    10] loss: 0.264\n",
            "[1979,    10] loss: 0.249\n",
            "[1980,    10] loss: 0.252\n",
            "[1981,    10] loss: 0.251\n",
            "[1982,    10] loss: 0.250\n",
            "[1983,    10] loss: 0.243\n",
            "[1984,    10] loss: 0.242\n",
            "[1985,    10] loss: 0.247\n",
            "[1986,    10] loss: 0.244\n",
            "[1987,    10] loss: 0.242\n",
            "[1988,    10] loss: 0.240\n",
            "[1989,    10] loss: 0.236\n",
            "[1990,    10] loss: 0.252\n",
            "[1991,    10] loss: 0.242\n",
            "[1992,    10] loss: 0.246\n",
            "[1993,    10] loss: 0.238\n",
            "[1994,    10] loss: 0.247\n",
            "[1995,    10] loss: 0.255\n",
            "[1996,    10] loss: 0.252\n",
            "[1997,    10] loss: 0.255\n",
            "[1998,    10] loss: 0.233\n",
            "[1999,    10] loss: 0.235\n",
            "[2000,    10] loss: 0.237\n",
            "Finished Training\n"
          ]
        }
      ],
      "source": [
        "nos_epochs = 2000\n",
        "focus_true_pred_true =0\n",
        "focus_false_pred_true =0\n",
        "focus_true_pred_false =0\n",
        "focus_false_pred_false =0\n",
        "\n",
        "argmax_more_than_half = 0\n",
        "argmax_less_than_half =0\n",
        "\n",
        "\n",
        "for epoch in range(nos_epochs):  # loop over the dataset multiple times\n",
        "\n",
        "  focus_true_pred_true =0\n",
        "  focus_false_pred_true =0\n",
        "  focus_true_pred_false =0\n",
        "  focus_false_pred_false =0\n",
        "  \n",
        "  argmax_more_than_half = 0\n",
        "  argmax_less_than_half =0\n",
        "  \n",
        "  running_loss = 0.0\n",
        "  epoch_loss = []\n",
        "  cnt=0\n",
        "\n",
        "  iteration = desired_num // batch\n",
        "  \n",
        "  #training data set\n",
        "  \n",
        "  for i, data in  enumerate(train_loader):\n",
        "    inputs , labels , fore_idx = data\n",
        "    batch = inputs.size(0)\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "    inputs = inputs.double()\n",
        "    # zero the parameter gradients\n",
        "    \n",
        "    optimizer_focus.zero_grad()\n",
        "    optimizer_classify.zero_grad()\n",
        "    \n",
        "    alphas, avg_images = focus_net(inputs)\n",
        "    outputs = classify(avg_images)\n",
        "\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "#     print(outputs)\n",
        "#     print(outputs.shape,labels.shape , torch.argmax(outputs, dim=1))\n",
        "\n",
        "    loss = my_cross_entropy(outputs, labels,alphas) \n",
        "    \n",
        "    loss.backward()\n",
        "    \n",
        "    \n",
        "    optimizer_focus.step()\n",
        "    optimizer_classify.step()\n",
        "\n",
        "    running_loss += loss.item()\n",
        "    mini = 10\n",
        "    if cnt % mini == mini-1:    # print every 40 mini-batches\n",
        "      print('[%d, %5d] loss: %.3f' %(epoch + 1, cnt + 1, running_loss / mini))\n",
        "      epoch_loss.append(running_loss/mini)\n",
        "      running_loss = 0.0\n",
        "    cnt=cnt+1\n",
        "\n",
        "  if(np.mean(epoch_loss) <= 0.01):\n",
        "      break;\n",
        "  #plot_attended_data(train_loader,focus_net,epoch)\n",
        "\n",
        "    \n",
        "print('Finished Training')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "3xPsiBtU-GDn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07d4b538-e052-4560-8464-07cb25050562"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('fc1.weight', Parameter containing:\n",
            "tensor([[7.6346, 6.9487]], device='cuda:0', dtype=torch.float64,\n",
            "       requires_grad=True))\n"
          ]
        }
      ],
      "source": [
        "for param in focus_net.named_parameters():\n",
        "    print(param)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "jhvhkEAyeRpt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c1a2310-1686-4e18-bed3-6e0b11505c8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the train images: 89.600000 %\n",
            "total correct 448\n",
            "total train set images 500\n"
          ]
        }
      ],
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "focus_net.eval()\n",
        "classify.eval()\n",
        "with torch.no_grad():\n",
        "  for data in train_loader:\n",
        "    inputs, labels , fore_idx = data\n",
        "    inputs = inputs.double()\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "    alphas, avg_images = focus_net(inputs)\n",
        "    outputs = classify(avg_images)\n",
        "    #print(outputs.shape)\n",
        "\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network on the train images: %f %%' % ( 100 * correct / total))\n",
        "print(\"total correct\", correct)\n",
        "print(\"total train set images\", total)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "OKcmpKwGeS8M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8beec733-f19c-4a63-aa7c-eb037d11f308"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the test images: 88.600000 %\n",
            "total correct 886\n",
            "total train set images 1000\n"
          ]
        }
      ],
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "  for data in test_loader:\n",
        "    inputs, labels , fore_idx = data\n",
        "    inputs = inputs.double()\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "    alphas, avg_images = focus_net(inputs)\n",
        "    outputs = classify(avg_images)\n",
        "\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network on the test images: %f %%' % ( 100 * correct / total))\n",
        "print(\"total correct\", correct)\n",
        "print(\"total train set images\", total)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "iSYrp-KG_GuQ"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "hard_attention_5.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
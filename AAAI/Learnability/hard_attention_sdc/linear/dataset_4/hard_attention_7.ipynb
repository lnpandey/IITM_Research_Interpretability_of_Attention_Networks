{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 596,
     "status": "ok",
     "timestamp": 1634805014692,
     "user": {
      "displayName": "Rahul Vashisht cs18d006",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "07093414448675380290"
     },
     "user_tz": -330
    },
    "id": "0vlCAi2JLSzD"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1634805014693,
     "user": {
      "displayName": "Rahul Vashisht cs18d006",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "07093414448675380290"
     },
     "user_tz": -330
    },
    "id": "PiPNZm1iTgHy"
   },
   "outputs": [],
   "source": [
    "# path=\"/content/drive/MyDrive/Research/Hard_Attention/dataset_2/m_5_size_100/run_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1634805014693,
     "user": {
      "displayName": "Rahul Vashisht cs18d006",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "07093414448675380290"
     },
     "user_tz": -330
    },
    "id": "SrZgZMlK-GDe"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25004,
     "status": "ok",
     "timestamp": 1634805039693,
     "user": {
      "displayName": "Rahul Vashisht cs18d006",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "07093414448675380290"
     },
     "user_tz": -330
    },
    "id": "N2_J4Rw2r0SQ",
    "outputId": "2e9af1d0-7605-4b70-90f7-1c488bee5e7b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "%matplotlib inline\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm as tqdm\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 1947,
     "status": "ok",
     "timestamp": 1634805041636,
     "user": {
      "displayName": "Rahul Vashisht cs18d006",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "07093414448675380290"
     },
     "user_tz": -330
    },
    "id": "-Dmy2iPWlgnc"
   },
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter('runs/hard_attention')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F6fjud_Fr0Sa"
   },
   "source": [
    "# Generate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1634805041640,
     "user": {
      "displayName": "Rahul Vashisht cs18d006",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "07093414448675380290"
     },
     "user_tz": -330
    },
    "id": "CqdXHO0Cr0Sd",
    "outputId": "55a65df0-b570-448e-9841-e0bf957827c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 500\n",
      "1 500\n",
      "2 500\n",
      "3 500\n",
      "4 500\n",
      "5 500\n",
      "6 500\n",
      "7 500\n",
      "8 500\n",
      "9 500\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "y = np.concatenate((np.zeros(500),np.ones(500),np.ones(500)*2,np.ones(500)*3,np.ones(500)*4,\n",
    "                    np.ones(500)*5,np.ones(500)*6,np.ones(500)*7,np.ones(500)*8,np.ones(500)*9))\n",
    "#y = np.random.randint(0,3,6000)\n",
    "idx= []\n",
    "for i in range(10):\n",
    "    print(i,sum(y==i))\n",
    "    idx.append(y==i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1634805041640,
     "user": {
      "displayName": "Rahul Vashisht cs18d006",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "07093414448675380290"
     },
     "user_tz": -330
    },
    "id": "ddhXyODwr0Sk"
   },
   "outputs": [],
   "source": [
    "x = np.zeros((5000,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1634805041641,
     "user": {
      "displayName": "Rahul Vashisht cs18d006",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "07093414448675380290"
     },
     "user_tz": -330
    },
    "id": "DyV3N2DIr0Sp"
   },
   "outputs": [],
   "source": [
    "np.random.seed(12)\n",
    "\n",
    "x[idx[0],:] = np.random.multivariate_normal(mean = [4,6.5],cov=[[0.01,0],[0,0.01]],size=sum(idx[0]))\n",
    "x[idx[1],:] = np.random.multivariate_normal(mean = [5.5,6],cov=[[0.01,0],[0,0.01]],size=sum(idx[1]))\n",
    "x[idx[2],:] = np.random.multivariate_normal(mean = [4.5,4.5],cov=[[0.01,0],[0,0.01]],size=sum(idx[2]))\n",
    "x[idx[3],:] = np.random.multivariate_normal(mean = [3,3.5],cov=[[0.01,0],[0,0.01]],size=sum(idx[3]))\n",
    "x[idx[4],:] = np.random.multivariate_normal(mean = [2.5,5.5],cov=[[0.01,0],[0,0.01]],size=sum(idx[4]))\n",
    "x[idx[5],:] = np.random.multivariate_normal(mean = [3.5,8],cov=[[0.01,0],[0,0.01]],size=sum(idx[5]))\n",
    "x[idx[6],:] = np.random.multivariate_normal(mean = [5.5,8],cov=[[0.01,0],[0,0.01]],size=sum(idx[6]))\n",
    "x[idx[7],:] = np.random.multivariate_normal(mean = [7,6.5],cov=[[0.01,0],[0,0.01]],size=sum(idx[7]))\n",
    "x[idx[8],:] = np.random.multivariate_normal(mean = [6.5,4.5],cov=[[0.01,0],[0,0.01]],size=sum(idx[8]))\n",
    "x[idx[9],:] = np.random.multivariate_normal(mean = [5,3],cov=[[0.01,0],[0,0.01]],size=sum(idx[9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1634805041641,
     "user": {
      "displayName": "Rahul Vashisht cs18d006",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "07093414448675380290"
     },
     "user_tz": -330
    },
    "id": "qh1mDScsU07I",
    "outputId": "14b6ce5c-fcd4-4198-810d-38613282dcd2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4.04729858, 6.43185741]), array([4.53008447, 4.5079931 ]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[idx[0]][0], x[idx[2]][5] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1634805041641,
     "user": {
      "displayName": "Rahul Vashisht cs18d006",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "07093414448675380290"
     },
     "user_tz": -330
    },
    "id": "9Vr5ErQ_wSrV",
    "outputId": "54cde011-53f2-4947-9ec6-b4ca2a0d2d42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 2) (5000,)\n"
     ]
    }
   ],
   "source": [
    "print(x.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1634805041642,
     "user": {
      "displayName": "Rahul Vashisht cs18d006",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "07093414448675380290"
     },
     "user_tz": -330
    },
    "id": "NG-3RpffwU_i"
   },
   "outputs": [],
   "source": [
    "idx= []\n",
    "for i in range(10):\n",
    "  idx.append(y==i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "executionInfo": {
     "elapsed": 1108,
     "status": "ok",
     "timestamp": 1634805042742,
     "user": {
      "displayName": "Rahul Vashisht cs18d006",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "07093414448675380290"
     },
     "user_tz": -330
    },
    "id": "hJ8Jm7YUr0St",
    "outputId": "2cda1a1f-c910-460b-a9ae-4c7250d613aa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x28d8dfa30>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    plt.scatter(x[idx[i],0],x[idx[i],1],label=\"class_\"+str(i))\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1634805042743,
     "user": {
      "displayName": "Rahul Vashisht cs18d006",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "07093414448675380290"
     },
     "user_tz": -330
    },
    "id": "fNWgnhUJnWLV"
   },
   "outputs": [],
   "source": [
    "x = ( x -  np.mean(x,axis=0,keepdims=True) ) / np.std(x,axis=0,keepdims=True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1634805042743,
     "user": {
      "displayName": "Rahul Vashisht cs18d006",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "07093414448675380290"
     },
     "user_tz": -330
    },
    "id": "8-VLhUfDDeHt",
    "outputId": "328bf955-e35c-47bd-ca8a-a2290ac75781"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x28ed5ae50>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    plt.scatter(x[idx[i],0],x[idx[i],1],label=\"class_\"+str(i))\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1634805042744,
     "user": {
      "displayName": "Rahul Vashisht cs18d006",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "07093414448675380290"
     },
     "user_tz": -330
    },
    "id": "UfFHcZJOr0Sz"
   },
   "outputs": [],
   "source": [
    "foreground_classes = {'class_0','class_1' }\n",
    "\n",
    "background_classes = {'bg_classes',}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 804,
     "status": "ok",
     "timestamp": 1634805043528,
     "user": {
      "displayName": "Rahul Vashisht cs18d006",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "07093414448675380290"
     },
     "user_tz": -330
    },
    "id": "jqbvfbwVr0TN",
    "outputId": "dc7be6e6-3b9c-4ecd-e5d2-2872d09d0738"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1100/1100 [00:34<00:00, 32.34it/s]\n"
     ]
    }
   ],
   "source": [
    "desired_num = 1100\n",
    "mosaic_list_of_images =[]\n",
    "mosaic_label = []\n",
    "fore_idx=[]\n",
    "m = 2000\n",
    "for j in tqdm(range(desired_num)):\n",
    "    np.random.seed(j)\n",
    "    fg_class  = np.random.randint(0,3)\n",
    "    fg_idx = np.random.randint(0,m)\n",
    "    a = []\n",
    "    for i in range(m):\n",
    "        if i == fg_idx:\n",
    "            b = np.random.choice(np.where(idx[fg_class]==True)[0],size=1)\n",
    "            a.append(x[b])\n",
    "#             print(\"foreground \"+str(fg_class)+\" present at \" + str(fg_idx))\n",
    "        else:\n",
    "            bg_class = np.random.randint(3,10)\n",
    "            b = np.random.choice(np.where(idx[bg_class]==True)[0],size=1)\n",
    "            a.append(x[b])\n",
    "#             print(\"background \"+str(bg_class)+\" present at \" + str(i))\n",
    "    a = np.concatenate(a,axis=0)\n",
    "    mosaic_list_of_images.append(np.reshape(a,(m,2)))\n",
    "    mosaic_label.append(fg_class)\n",
    "    fore_idx.append(fg_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1634805043529,
     "user": {
      "displayName": "Rahul Vashisht cs18d006",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "07093414448675380290"
     },
     "user_tz": -330
    },
    "id": "BOsFmWfMr0TR"
   },
   "outputs": [],
   "source": [
    "# mosaic_list_of_images = np.concatenate(mosaic_list_of_images,axis=1).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1634805043529,
     "user": {
      "displayName": "Rahul Vashisht cs18d006",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "07093414448675380290"
     },
     "user_tz": -330
    },
    "id": "2aIPMgLXNiXW",
    "outputId": "2ceefa71-71ba-4d90-f0de-75e5673f7cd0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1100,\n",
       " array([[ 1.41860713, -0.6338838 ],\n",
       "        [ 0.60177752,  1.40622261],\n",
       "        [ 0.64672986,  1.41882828],\n",
       "        ...,\n",
       "        [ 0.1760838 , -1.64428378],\n",
       "        [ 0.57482959,  1.52453398],\n",
       "        [ 1.71875213,  0.627341  ]]),\n",
       " (2000, 2))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mosaic_list_of_images), mosaic_list_of_images[0],mosaic_list_of_images[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1634805043529,
     "user": {
      "displayName": "Rahul Vashisht cs18d006",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "07093414448675380290"
     },
     "user_tz": -330
    },
    "id": "iPoIwbMHx44n"
   },
   "outputs": [],
   "source": [
    "class MosaicDataset(Dataset):\n",
    "  \"\"\"MosaicDataset dataset.\"\"\"\n",
    "\n",
    "  def __init__(self, mosaic_list_of_images, mosaic_label, fore_idx):\n",
    "    \"\"\"\n",
    "      Args:\n",
    "        csv_file (string): Path to the csv file with annotations.\n",
    "        root_dir (string): Directory with all the images.\n",
    "        transform (callable, optional): Optional transform to be applied\n",
    "            on a sample.\n",
    "    \"\"\"\n",
    "    self.mosaic = mosaic_list_of_images\n",
    "    self.label = mosaic_label\n",
    "    self.fore_idx = fore_idx\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.label)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return self.mosaic[idx] , self.label[idx], self.fore_idx[idx]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1634805043530,
     "user": {
      "displayName": "Rahul Vashisht cs18d006",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "07093414448675380290"
     },
     "user_tz": -330
    },
    "id": "fOPAJQJeW8Ah"
   },
   "outputs": [],
   "source": [
    "batch = 50\n",
    "msd1 = MosaicDataset(mosaic_list_of_images[0:100], mosaic_label[0:100] , fore_idx[0:100])\n",
    "train_loader = DataLoader( msd1 ,batch_size= batch ,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1634805043530,
     "user": {
      "displayName": "Rahul Vashisht cs18d006",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "07093414448675380290"
     },
     "user_tz": -330
    },
    "id": "aWBIcyvGApLt"
   },
   "outputs": [],
   "source": [
    "data,_,_=iter(train_loader).next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1634805043530,
     "user": {
      "displayName": "Rahul Vashisht cs18d006",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "07093414448675380290"
     },
     "user_tz": -330
    },
    "id": "cauJIvKEAxKM",
    "outputId": "c0b5ce01-f4e6-4a17-946d-9d3036719125"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 2000, 2])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1634805043531,
     "user": {
      "displayName": "Rahul Vashisht cs18d006",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "07093414448675380290"
     },
     "user_tz": -330
    },
    "id": "qjNiQgxZW8bA"
   },
   "outputs": [],
   "source": [
    "batch = 250\n",
    "msd2 = MosaicDataset(mosaic_list_of_images[100:], mosaic_label[100:] , fore_idx[100:])\n",
    "test_loader = DataLoader( msd2 ,batch_size= batch ,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1634805043531,
     "user": {
      "displayName": "Rahul Vashisht cs18d006",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "07093414448675380290"
     },
     "user_tz": -330
    },
    "id": "yda1E5ApiKpH"
   },
   "outputs": [],
   "source": [
    "class Focus(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Focus, self).__init__()\n",
    "        self.fc1 = nn.Linear(2,50, bias=False)\n",
    "        self.fc2 = nn.Linear(50,1,bias=False)\n",
    "        torch.nn.init.xavier_normal_(self.fc1.weight)\n",
    "        torch.nn.init.xavier_normal_(self.fc2.weight)\n",
    "        #self.fc2 = nn.Linear(64, 1, bias=False)\n",
    "        #torch.nn.init.xavier_normal_(self.fc2.weight)\n",
    "\n",
    "    def forward(self,z):\n",
    "        #print(\"data\",z)\n",
    "        batch = z.size(0)\n",
    "        patches = z.size(1)\n",
    "        z = z.view(batch,patches,2*1)\n",
    "        alp1,ft1 = self.helper(z)\n",
    "\n",
    "        alpha = F.softmax(alp1,dim=1)\n",
    "        #print(self.training)\n",
    "        \n",
    "        if self.training:\n",
    "            alpha =alpha[:,:,0]\n",
    "            y = ft1 \n",
    "            return alpha,y\n",
    "        else:\n",
    "            #alpha_cumsum = torch.cumsum(alpha, dim = 1)\n",
    "            #print(alpha_cumsum)\n",
    "            #len_batch = alpha_cumsum.size(0)\n",
    "            #patches = alpha_cumsum.size(1)\n",
    "            #rand_prob = torch.rand(len_batch,patches, 1).to(device)\n",
    "            #alpha_relu = F.relu(rand_prob-alpha_cumsum)\n",
    "            #print(alpha_relu)\n",
    "            #alpha_index = torch.count_nonzero(alpha_relu,dim=1)\n",
    "            #alpha_hard = F.one_hot(alpha_index,num_classes=patches)\n",
    "            #print(alpha_hard)\n",
    "            #alpha_hard = torch.transpose(alpha_hard,dim0=1,dim1=2)\n",
    "            #print(ft1,\"alpha_hard\",alpha_hard) \n",
    "            #y = torch.sum(alpha_hard*ft1,dim=1)\n",
    "            #print(alpha,alpha.shape)\n",
    "         \n",
    "        \n",
    "            index = torch.argmax(alpha,dim=1)\n",
    "            hard_alpha = torch.nn.functional.one_hot(index[:,0], patches)\n",
    "            y = torch.sum(hard_alpha[:,:,None]*ft1,dim=1)\n",
    "            alpha = alpha[:,:,0]\n",
    "            return alpha,y\n",
    "    \n",
    "    def helper(self, x):\n",
    "        x1 = x\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x,x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1634805043531,
     "user": {
      "displayName": "Rahul Vashisht cs18d006",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "07093414448675380290"
     },
     "user_tz": -330
    },
    "id": "0dYXnywAD-4l"
   },
   "outputs": [],
   "source": [
    "class Classification(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Classification, self).__init__()\n",
    "    self.fc1 = nn.Linear(2, 3)\n",
    "    torch.nn.init.xavier_normal_(self.fc1.weight)\n",
    "    torch.nn.init.zeros_(self.fc1.bias)\n",
    "\n",
    "  def forward(self, x):\n",
    "    #print(x.shape)\n",
    "    #x = x.view(-1, 1)\n",
    "    #print(x.shape)\n",
    "    x = self.fc1(x)\n",
    "    # print(x.shape)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "lSa6O9f6XNf4"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(12)\n",
    "focus_net = Focus().double()\n",
    "focus_net = focus_net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "36k3H2G-XO9A"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(12)\n",
    "classify = Classification().double()\n",
    "classify = classify.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "bK78aII8-GDl"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "optimizer_classify = optim.Adam(classify.parameters(), lr=0.05 ) #, momentum=0.9)\n",
    "optimizer_focus = optim.Adam(focus_net.parameters(), lr=0.05 ) #, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "h0mjWiFG-GDl"
   },
   "outputs": [],
   "source": [
    "def my_cross_entropy(output,target,alpha):\n",
    "    criterion = nn.CrossEntropyLoss(reduce=False)\n",
    "    \n",
    "    batch = output.size(0)\n",
    "    #print(batch)\n",
    "    patches = output.size(1)\n",
    "    classes = output.size(2)\n",
    "    \n",
    "    \n",
    "    \n",
    "    output = torch.reshape(output,(batch*patches,classes))\n",
    "    \n",
    "    \n",
    "    target = target.repeat_interleave(patches)\n",
    "    \n",
    "    loss = criterion(output,target)\n",
    "    \n",
    "    #print(loss,loss.shape)\n",
    "    loss = torch.reshape(loss,(batch,patches))\n",
    "    #print(loss.size())\n",
    "    final_loss = torch.sum(torch.mul(loss,alpha),dim=1)\n",
    "    #print(final_loss.shape)\n",
    "    final_loss = torch.mean(final_loss,dim=0)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #print(final_loss)\n",
    "    return final_loss\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "pjD2VZuV9Ed4"
   },
   "outputs": [],
   "source": [
    "col1=[]\n",
    "col2=[]\n",
    "col3=[]\n",
    "col4=[]\n",
    "col5=[]\n",
    "col6=[]\n",
    "col7=[]\n",
    "col8=[]\n",
    "col9=[]\n",
    "col10=[]\n",
    "col11=[]\n",
    "col12=[]\n",
    "col13=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "eEVrBg7d-GDl"
   },
   "outputs": [],
   "source": [
    "def plot_attended_data(trainloader,net,epoch):\n",
    "    attd_data =[]\n",
    "    lbls = []\n",
    "    for data in trainloader:\n",
    "        inputs, labels , fore_idx = data\n",
    "        inputs = inputs.double()\n",
    "        inputs, labels , fore_idx = inputs.to(device),labels.to(device), fore_idx.to(device)\n",
    "        alphas, avg_images = focus_net(inputs)\n",
    "        attd_data.append(avg_images.numpy())\n",
    "        lbls.append(labels)\n",
    "    attd_data = np.concatenate(attd_data,axis=0)\n",
    "    lbls = np.concatenate(lbls,axis=0)\n",
    "    plt.figure(figsize=(6,8))\n",
    "    plt.scatter(attd_data[:,0],attd_data[:,1],c=lbls)\n",
    "    plt.title(\"EPOCH_\"+str(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "uALi25pmzQHV"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/pytorch_env/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1957, dtype=torch.float64)\n",
      "tensor(1.1948, dtype=torch.float64)\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "count = 0\n",
    "flag = 1\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "  for data in train_loader:\n",
    "    inputs, labels , fore_idx = data\n",
    "    inputs = inputs.double()\n",
    "    inputs, labels , fore_idx = inputs.to(device),labels.to(device), fore_idx.to(device)\n",
    "    alphas, avg_images = focus_net(inputs)\n",
    "    outputs = classify(avg_images)\n",
    "    loss = my_cross_entropy(outputs,labels,alphas)\n",
    "    print(loss)\n",
    "    # print(outputs.shape)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "\n",
    "print(\"=\"*100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "4vmNprlPzTjP"
   },
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "count = 0\n",
    "flag = 1\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "  for data in test_loader:\n",
    "    inputs, labels , fore_idx = data\n",
    "    inputs = inputs.double()\n",
    "    inputs, labels , fore_idx = inputs.to(device),labels.to(device), fore_idx.to(device)\n",
    "    alphas, avg_images = focus_net(inputs)\n",
    "    outputs = classify(avg_images)\n",
    "\n",
    "    _, predicted = torch.max(outputs.data, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "_nvicAzw-GDm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('fc1.weight', Parameter containing:\n",
      "tensor([[-0.1235,  0.3437],\n",
      "        [ 0.1704, -0.2588],\n",
      "        [ 0.0472,  0.0784],\n",
      "        [ 0.4953, -0.1659],\n",
      "        [ 0.3371,  0.3451],\n",
      "        [ 0.3131, -0.1927],\n",
      "        [-0.1788,  0.0314],\n",
      "        [ 0.1934,  0.0747],\n",
      "        [-0.0699,  0.2935],\n",
      "        [ 0.0524, -0.1017],\n",
      "        [-0.2094, -0.3588],\n",
      "        [ 0.2079, -0.0692],\n",
      "        [-0.0396,  0.2914],\n",
      "        [-0.2280,  0.0319],\n",
      "        [-0.1798,  0.1602],\n",
      "        [ 0.1488, -0.0822],\n",
      "        [ 0.2434,  0.1657],\n",
      "        [-0.1537, -0.0442],\n",
      "        [-0.2106,  0.2442],\n",
      "        [-0.1865, -0.1679],\n",
      "        [-0.3746,  0.1089],\n",
      "        [ 0.0765,  0.0746],\n",
      "        [ 0.0194,  0.5148],\n",
      "        [-0.0150, -0.0721],\n",
      "        [-0.2615,  0.0124],\n",
      "        [ 0.2550,  0.0677],\n",
      "        [ 0.1783, -0.1196],\n",
      "        [ 0.1869,  0.2055],\n",
      "        [-0.0041, -0.1310],\n",
      "        [-0.0923,  0.0208],\n",
      "        [-0.1519, -0.2190],\n",
      "        [-0.0142, -0.0720],\n",
      "        [ 0.1204,  0.0586],\n",
      "        [-0.2943,  0.2803],\n",
      "        [ 0.0041,  0.0944],\n",
      "        [-0.0306, -0.0983],\n",
      "        [ 0.1264, -0.1432],\n",
      "        [-0.2297, -0.2029],\n",
      "        [-0.4360,  0.1309],\n",
      "        [ 0.2729, -0.1434],\n",
      "        [-0.1905, -0.0718],\n",
      "        [ 0.4859,  0.3518],\n",
      "        [-0.1163, -0.1056],\n",
      "        [ 0.0843,  0.2349],\n",
      "        [-0.2124,  0.0694],\n",
      "        [ 0.1559,  0.4671],\n",
      "        [ 0.0944,  0.2184],\n",
      "        [-0.1632, -0.0640],\n",
      "        [ 0.3447,  0.0719],\n",
      "        [-0.0065, -0.2574]], dtype=torch.float64, requires_grad=True))\n",
      "('fc2.weight', Parameter containing:\n",
      "tensor([[ 0.2877,  0.2409, -0.1060,  0.0798,  0.0235,  0.5752,  0.2892,  0.3077,\n",
      "         -0.0946,  0.0924, -0.1857,  0.1151, -0.0581, -0.1602, -0.0622, -0.3845,\n",
      "          0.2236, -0.0697, -0.1184, -0.0754,  0.0007,  0.0488,  0.0265, -0.2130,\n",
      "         -0.1718, -0.1714, -0.2712, -0.1121,  0.2311, -0.3507, -0.0884,  0.1573,\n",
      "          0.0479, -0.2186, -0.1205,  0.1774,  0.4210, -0.0055,  0.0760, -0.3396,\n",
      "          0.0821,  0.1663,  0.0678, -0.4594,  0.1509,  0.1229,  0.0456, -0.3413,\n",
      "          0.1490, -0.1123]], dtype=torch.float64, requires_grad=True))\n"
     ]
    }
   ],
   "source": [
    "for param in focus_net.named_parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "Yl41sE8vFERk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     2] loss: 1.168\n",
      "[2,     2] loss: 1.107\n",
      "[3,     2] loss: 1.095\n",
      "[4,     2] loss: 1.095\n",
      "[5,     2] loss: 1.086\n",
      "[6,     2] loss: 1.088\n",
      "[7,     2] loss: 1.088\n",
      "[8,     2] loss: 1.086\n",
      "[9,     2] loss: 1.087\n",
      "[10,     2] loss: 1.086\n",
      "[11,     2] loss: 1.085\n",
      "[12,     2] loss: 1.087\n",
      "[13,     2] loss: 1.090\n",
      "[14,     2] loss: 1.086\n",
      "[15,     2] loss: 1.086\n",
      "[16,     2] loss: 1.088\n",
      "[17,     2] loss: 1.086\n",
      "[18,     2] loss: 1.087\n",
      "[19,     2] loss: 1.086\n",
      "[20,     2] loss: 1.085\n",
      "[21,     2] loss: 1.089\n",
      "[22,     2] loss: 1.086\n",
      "[23,     2] loss: 1.087\n",
      "[24,     2] loss: 1.085\n",
      "[25,     2] loss: 1.085\n",
      "[26,     2] loss: 1.089\n",
      "[27,     2] loss: 1.088\n",
      "[28,     2] loss: 1.087\n",
      "[29,     2] loss: 1.085\n",
      "[30,     2] loss: 1.085\n",
      "[31,     2] loss: 1.088\n",
      "[32,     2] loss: 1.086\n",
      "[33,     2] loss: 1.086\n",
      "[34,     2] loss: 1.087\n",
      "[35,     2] loss: 1.085\n",
      "[36,     2] loss: 1.087\n",
      "[37,     2] loss: 1.085\n",
      "[38,     2] loss: 1.085\n",
      "[39,     2] loss: 1.089\n",
      "[40,     2] loss: 1.087\n",
      "[41,     2] loss: 1.085\n",
      "[42,     2] loss: 1.085\n",
      "[43,     2] loss: 1.086\n",
      "[44,     2] loss: 1.088\n",
      "[45,     2] loss: 1.086\n",
      "[46,     2] loss: 1.087\n",
      "[47,     2] loss: 1.087\n",
      "[48,     2] loss: 1.086\n",
      "[49,     2] loss: 1.085\n",
      "[50,     2] loss: 1.085\n",
      "[51,     2] loss: 1.088\n",
      "[52,     2] loss: 1.088\n",
      "[53,     2] loss: 1.085\n",
      "[54,     2] loss: 1.085\n",
      "[55,     2] loss: 1.086\n",
      "[56,     2] loss: 1.089\n",
      "[57,     2] loss: 1.085\n",
      "[58,     2] loss: 1.085\n",
      "[59,     2] loss: 1.085\n",
      "[60,     2] loss: 1.087\n",
      "[61,     2] loss: 1.086\n",
      "[62,     2] loss: 1.086\n",
      "[63,     2] loss: 1.086\n",
      "[64,     2] loss: 1.086\n",
      "[65,     2] loss: 1.088\n",
      "[66,     2] loss: 1.086\n",
      "[67,     2] loss: 1.086\n",
      "[68,     2] loss: 1.086\n",
      "[69,     2] loss: 1.085\n",
      "[70,     2] loss: 1.086\n",
      "[71,     2] loss: 1.086\n",
      "[72,     2] loss: 1.086\n",
      "[73,     2] loss: 1.085\n",
      "[74,     2] loss: 1.086\n",
      "[75,     2] loss: 1.085\n",
      "[76,     2] loss: 1.088\n",
      "[77,     2] loss: 1.085\n",
      "[78,     2] loss: 1.085\n",
      "[79,     2] loss: 1.085\n",
      "[80,     2] loss: 1.085\n",
      "[81,     2] loss: 1.085\n",
      "[82,     2] loss: 1.088\n",
      "[83,     2] loss: 1.086\n",
      "[84,     2] loss: 1.085\n",
      "[85,     2] loss: 1.086\n",
      "[86,     2] loss: 1.085\n",
      "[87,     2] loss: 1.085\n",
      "[88,     2] loss: 1.088\n",
      "[89,     2] loss: 1.086\n",
      "[90,     2] loss: 1.087\n",
      "[91,     2] loss: 1.087\n",
      "[92,     2] loss: 1.086\n",
      "[93,     2] loss: 1.088\n",
      "[94,     2] loss: 1.086\n",
      "[95,     2] loss: 1.088\n",
      "[96,     2] loss: 1.086\n",
      "[97,     2] loss: 1.086\n",
      "[98,     2] loss: 1.092\n",
      "[99,     2] loss: 1.087\n",
      "[100,     2] loss: 1.089\n",
      "[101,     2] loss: 1.085\n",
      "[102,     2] loss: 1.085\n",
      "[103,     2] loss: 1.086\n",
      "[104,     2] loss: 1.085\n",
      "[105,     2] loss: 1.086\n",
      "[106,     2] loss: 1.085\n",
      "[107,     2] loss: 1.085\n",
      "[108,     2] loss: 1.086\n",
      "[109,     2] loss: 1.090\n",
      "[110,     2] loss: 1.086\n",
      "[111,     2] loss: 1.085\n",
      "[112,     2] loss: 1.085\n",
      "[113,     2] loss: 1.087\n",
      "[114,     2] loss: 1.085\n",
      "[115,     2] loss: 1.086\n",
      "[116,     2] loss: 1.085\n",
      "[117,     2] loss: 1.086\n",
      "[118,     2] loss: 1.086\n",
      "[119,     2] loss: 1.085\n",
      "[120,     2] loss: 1.086\n",
      "[121,     2] loss: 1.085\n",
      "[122,     2] loss: 1.086\n",
      "[123,     2] loss: 1.086\n",
      "[124,     2] loss: 1.085\n",
      "[125,     2] loss: 1.085\n",
      "[126,     2] loss: 1.085\n",
      "[127,     2] loss: 1.087\n",
      "[128,     2] loss: 1.085\n",
      "[129,     2] loss: 1.087\n",
      "[130,     2] loss: 1.085\n",
      "[131,     2] loss: 1.085\n",
      "[132,     2] loss: 1.086\n",
      "[133,     2] loss: 1.086\n",
      "[134,     2] loss: 1.086\n",
      "[135,     2] loss: 1.086\n",
      "[136,     2] loss: 1.085\n",
      "[137,     2] loss: 1.086\n",
      "[138,     2] loss: 1.088\n",
      "[139,     2] loss: 1.088\n",
      "[140,     2] loss: 1.086\n",
      "[141,     2] loss: 1.087\n",
      "[142,     2] loss: 1.085\n",
      "[143,     2] loss: 1.086\n",
      "[144,     2] loss: 1.086\n",
      "[145,     2] loss: 1.085\n",
      "[146,     2] loss: 1.086\n",
      "[147,     2] loss: 1.087\n",
      "[148,     2] loss: 1.088\n",
      "[149,     2] loss: 1.086\n",
      "[150,     2] loss: 1.086\n",
      "[151,     2] loss: 1.086\n",
      "[152,     2] loss: 1.086\n",
      "[153,     2] loss: 1.085\n",
      "[154,     2] loss: 1.088\n",
      "[155,     2] loss: 1.086\n",
      "[156,     2] loss: 1.085\n",
      "[157,     2] loss: 1.089\n",
      "[158,     2] loss: 1.086\n",
      "[159,     2] loss: 1.087\n",
      "[160,     2] loss: 1.088\n",
      "[161,     2] loss: 1.088\n",
      "[162,     2] loss: 1.085\n",
      "[163,     2] loss: 1.090\n",
      "[164,     2] loss: 1.087\n",
      "[165,     2] loss: 1.086\n",
      "[166,     2] loss: 1.087\n",
      "[167,     2] loss: 1.085\n",
      "[168,     2] loss: 1.088\n",
      "[169,     2] loss: 1.087\n",
      "[170,     2] loss: 1.088\n",
      "[171,     2] loss: 1.085\n",
      "[172,     2] loss: 1.088\n",
      "[173,     2] loss: 1.088\n",
      "[174,     2] loss: 1.086\n",
      "[175,     2] loss: 1.087\n",
      "[176,     2] loss: 1.085\n",
      "[177,     2] loss: 1.086\n",
      "[178,     2] loss: 1.085\n",
      "[179,     2] loss: 1.086\n",
      "[180,     2] loss: 1.085\n",
      "[181,     2] loss: 1.087\n",
      "[182,     2] loss: 1.085\n",
      "[183,     2] loss: 1.085\n",
      "[184,     2] loss: 1.087\n",
      "[185,     2] loss: 1.085\n",
      "[186,     2] loss: 1.085\n",
      "[187,     2] loss: 1.085\n",
      "[188,     2] loss: 1.087\n",
      "[189,     2] loss: 1.085\n",
      "[190,     2] loss: 1.085\n",
      "[191,     2] loss: 1.085\n",
      "[192,     2] loss: 1.085\n",
      "[193,     2] loss: 1.086\n",
      "[194,     2] loss: 1.085\n",
      "[195,     2] loss: 1.086\n",
      "[196,     2] loss: 1.085\n",
      "[197,     2] loss: 1.092\n",
      "[198,     2] loss: 1.086\n",
      "[199,     2] loss: 1.086\n",
      "[200,     2] loss: 1.085\n",
      "[201,     2] loss: 1.085\n",
      "[202,     2] loss: 1.089\n",
      "[203,     2] loss: 1.091\n",
      "[204,     2] loss: 1.086\n",
      "[205,     2] loss: 1.085\n",
      "[206,     2] loss: 1.085\n",
      "[207,     2] loss: 1.086\n",
      "[208,     2] loss: 1.085\n",
      "[209,     2] loss: 1.087\n",
      "[210,     2] loss: 1.088\n",
      "[211,     2] loss: 1.086\n",
      "[212,     2] loss: 1.086\n",
      "[213,     2] loss: 1.087\n",
      "[214,     2] loss: 1.086\n",
      "[215,     2] loss: 1.087\n",
      "[216,     2] loss: 1.088\n",
      "[217,     2] loss: 1.085\n",
      "[218,     2] loss: 1.086\n",
      "[219,     2] loss: 1.086\n",
      "[220,     2] loss: 1.085\n",
      "[221,     2] loss: 1.085\n",
      "[222,     2] loss: 1.085\n",
      "[223,     2] loss: 1.086\n",
      "[224,     2] loss: 1.085\n",
      "[225,     2] loss: 1.086\n",
      "[226,     2] loss: 1.086\n",
      "[227,     2] loss: 1.086\n",
      "[228,     2] loss: 1.086\n",
      "[229,     2] loss: 1.087\n",
      "[230,     2] loss: 1.085\n",
      "[231,     2] loss: 1.087\n",
      "[232,     2] loss: 1.085\n",
      "[233,     2] loss: 1.085\n",
      "[234,     2] loss: 1.087\n",
      "[235,     2] loss: 1.085\n",
      "[236,     2] loss: 1.085\n",
      "[237,     2] loss: 1.086\n",
      "[238,     2] loss: 1.085\n",
      "[239,     2] loss: 1.091\n",
      "[240,     2] loss: 1.085\n",
      "[241,     2] loss: 1.086\n",
      "[242,     2] loss: 1.088\n",
      "[243,     2] loss: 1.087\n",
      "[244,     2] loss: 1.086\n",
      "[245,     2] loss: 1.088\n",
      "[246,     2] loss: 1.086\n",
      "[247,     2] loss: 1.090\n",
      "[248,     2] loss: 1.086\n",
      "[249,     2] loss: 1.085\n",
      "[250,     2] loss: 1.088\n",
      "[251,     2] loss: 1.087\n",
      "[252,     2] loss: 1.086\n",
      "[253,     2] loss: 1.087\n",
      "[254,     2] loss: 1.088\n",
      "[255,     2] loss: 1.087\n",
      "[256,     2] loss: 1.087\n",
      "[257,     2] loss: 1.086\n",
      "[258,     2] loss: 1.087\n",
      "[259,     2] loss: 1.085\n",
      "[260,     2] loss: 1.086\n",
      "[261,     2] loss: 1.085\n",
      "[262,     2] loss: 1.086\n",
      "[263,     2] loss: 1.086\n",
      "[264,     2] loss: 1.085\n",
      "[265,     2] loss: 1.085\n",
      "[266,     2] loss: 1.086\n",
      "[267,     2] loss: 1.087\n",
      "[268,     2] loss: 1.085\n",
      "[269,     2] loss: 1.087\n",
      "[270,     2] loss: 1.088\n",
      "[271,     2] loss: 1.086\n",
      "[272,     2] loss: 1.086\n",
      "[273,     2] loss: 1.090\n",
      "[274,     2] loss: 1.085\n",
      "[275,     2] loss: 1.087\n",
      "[276,     2] loss: 1.086\n",
      "[277,     2] loss: 1.086\n",
      "[278,     2] loss: 1.086\n",
      "[279,     2] loss: 1.085\n",
      "[280,     2] loss: 1.086\n",
      "[281,     2] loss: 1.085\n",
      "[282,     2] loss: 1.085\n",
      "[283,     2] loss: 1.086\n",
      "[284,     2] loss: 1.087\n",
      "[285,     2] loss: 1.086\n",
      "[286,     2] loss: 1.086\n",
      "[287,     2] loss: 1.085\n",
      "[288,     2] loss: 1.085\n",
      "[289,     2] loss: 1.088\n",
      "[290,     2] loss: 1.086\n",
      "[291,     2] loss: 1.085\n",
      "[292,     2] loss: 1.086\n",
      "[293,     2] loss: 1.087\n",
      "[294,     2] loss: 1.086\n",
      "[295,     2] loss: 1.086\n",
      "[296,     2] loss: 1.086\n",
      "[297,     2] loss: 1.085\n",
      "[298,     2] loss: 1.086\n",
      "[299,     2] loss: 1.090\n",
      "[300,     2] loss: 1.088\n",
      "[301,     2] loss: 1.089\n",
      "[302,     2] loss: 1.090\n",
      "[303,     2] loss: 1.085\n",
      "[304,     2] loss: 1.087\n",
      "[305,     2] loss: 1.088\n",
      "[306,     2] loss: 1.088\n",
      "[307,     2] loss: 1.086\n",
      "[308,     2] loss: 1.087\n",
      "[309,     2] loss: 1.085\n",
      "[310,     2] loss: 1.086\n",
      "[311,     2] loss: 1.087\n",
      "[312,     2] loss: 1.086\n",
      "[313,     2] loss: 1.086\n",
      "[314,     2] loss: 1.085\n",
      "[315,     2] loss: 1.087\n",
      "[316,     2] loss: 1.087\n",
      "[317,     2] loss: 1.086\n",
      "[318,     2] loss: 1.086\n",
      "[319,     2] loss: 1.086\n",
      "[320,     2] loss: 1.086\n",
      "[321,     2] loss: 1.086\n",
      "[322,     2] loss: 1.085\n",
      "[323,     2] loss: 1.086\n",
      "[324,     2] loss: 1.086\n",
      "[325,     2] loss: 1.085\n",
      "[326,     2] loss: 1.085\n",
      "[327,     2] loss: 1.085\n",
      "[328,     2] loss: 1.086\n",
      "[329,     2] loss: 1.086\n",
      "[330,     2] loss: 1.086\n",
      "[331,     2] loss: 1.085\n",
      "[332,     2] loss: 1.085\n",
      "[333,     2] loss: 1.085\n",
      "[334,     2] loss: 1.086\n",
      "[335,     2] loss: 1.086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[336,     2] loss: 1.085\n",
      "[337,     2] loss: 1.086\n",
      "[338,     2] loss: 1.085\n",
      "[339,     2] loss: 1.086\n",
      "[340,     2] loss: 1.086\n",
      "[341,     2] loss: 1.085\n",
      "[342,     2] loss: 1.085\n",
      "[343,     2] loss: 1.087\n",
      "[344,     2] loss: 1.085\n",
      "[345,     2] loss: 1.087\n",
      "[346,     2] loss: 1.086\n",
      "[347,     2] loss: 1.087\n",
      "[348,     2] loss: 1.087\n",
      "[349,     2] loss: 1.085\n",
      "[350,     2] loss: 1.085\n",
      "[351,     2] loss: 1.085\n",
      "[352,     2] loss: 1.088\n",
      "[353,     2] loss: 1.087\n",
      "[354,     2] loss: 1.086\n",
      "[355,     2] loss: 1.087\n",
      "[356,     2] loss: 1.086\n",
      "[357,     2] loss: 1.087\n",
      "[358,     2] loss: 1.085\n",
      "[359,     2] loss: 1.086\n",
      "[360,     2] loss: 1.085\n",
      "[361,     2] loss: 1.086\n",
      "[362,     2] loss: 1.085\n",
      "[363,     2] loss: 1.086\n",
      "[364,     2] loss: 1.085\n",
      "[365,     2] loss: 1.085\n",
      "[366,     2] loss: 1.085\n",
      "[367,     2] loss: 1.087\n",
      "[368,     2] loss: 1.085\n",
      "[369,     2] loss: 1.086\n",
      "[370,     2] loss: 1.088\n",
      "[371,     2] loss: 1.087\n",
      "[372,     2] loss: 1.086\n",
      "[373,     2] loss: 1.086\n",
      "[374,     2] loss: 1.087\n",
      "[375,     2] loss: 1.085\n",
      "[376,     2] loss: 1.086\n",
      "[377,     2] loss: 1.085\n",
      "[378,     2] loss: 1.086\n",
      "[379,     2] loss: 1.085\n",
      "[380,     2] loss: 1.085\n",
      "[381,     2] loss: 1.086\n",
      "[382,     2] loss: 1.087\n",
      "[383,     2] loss: 1.089\n",
      "[384,     2] loss: 1.089\n",
      "[385,     2] loss: 1.085\n",
      "[386,     2] loss: 1.086\n",
      "[387,     2] loss: 1.085\n",
      "[388,     2] loss: 1.086\n",
      "[389,     2] loss: 1.085\n",
      "[390,     2] loss: 1.085\n",
      "[391,     2] loss: 1.085\n",
      "[392,     2] loss: 1.088\n",
      "[393,     2] loss: 1.085\n",
      "[394,     2] loss: 1.085\n",
      "[395,     2] loss: 1.085\n",
      "[396,     2] loss: 1.086\n",
      "[397,     2] loss: 1.087\n",
      "[398,     2] loss: 1.085\n",
      "[399,     2] loss: 1.088\n",
      "[400,     2] loss: 1.086\n",
      "[401,     2] loss: 1.085\n",
      "[402,     2] loss: 1.086\n",
      "[403,     2] loss: 1.087\n",
      "[404,     2] loss: 1.086\n",
      "[405,     2] loss: 1.085\n",
      "[406,     2] loss: 1.085\n",
      "[407,     2] loss: 1.086\n",
      "[408,     2] loss: 1.086\n",
      "[409,     2] loss: 1.086\n",
      "[410,     2] loss: 1.087\n",
      "[411,     2] loss: 1.085\n",
      "[412,     2] loss: 1.086\n",
      "[413,     2] loss: 1.088\n",
      "[414,     2] loss: 1.087\n",
      "[415,     2] loss: 1.086\n",
      "[416,     2] loss: 1.085\n",
      "[417,     2] loss: 1.088\n",
      "[418,     2] loss: 1.085\n",
      "[419,     2] loss: 1.085\n",
      "[420,     2] loss: 1.085\n",
      "[421,     2] loss: 1.085\n",
      "[422,     2] loss: 1.087\n",
      "[423,     2] loss: 1.085\n",
      "[424,     2] loss: 1.085\n",
      "[425,     2] loss: 1.086\n",
      "[426,     2] loss: 1.089\n",
      "[427,     2] loss: 1.085\n",
      "[428,     2] loss: 1.086\n",
      "[429,     2] loss: 1.088\n",
      "[430,     2] loss: 1.086\n",
      "[431,     2] loss: 1.087\n",
      "[432,     2] loss: 1.085\n",
      "[433,     2] loss: 1.085\n",
      "[434,     2] loss: 1.085\n",
      "[435,     2] loss: 1.087\n",
      "[436,     2] loss: 1.085\n",
      "[437,     2] loss: 1.085\n",
      "[438,     2] loss: 1.085\n",
      "[439,     2] loss: 1.085\n",
      "[440,     2] loss: 1.086\n",
      "[441,     2] loss: 1.089\n",
      "[442,     2] loss: 1.085\n",
      "[443,     2] loss: 1.085\n",
      "[444,     2] loss: 1.085\n",
      "[445,     2] loss: 1.085\n",
      "[446,     2] loss: 1.086\n",
      "[447,     2] loss: 1.086\n",
      "[448,     2] loss: 1.085\n",
      "[449,     2] loss: 1.085\n",
      "[450,     2] loss: 1.085\n",
      "[451,     2] loss: 1.088\n",
      "[452,     2] loss: 1.086\n",
      "[453,     2] loss: 1.086\n",
      "[454,     2] loss: 1.087\n",
      "[455,     2] loss: 1.086\n",
      "[456,     2] loss: 1.086\n",
      "[457,     2] loss: 1.085\n",
      "[458,     2] loss: 1.085\n",
      "[459,     2] loss: 1.086\n",
      "[460,     2] loss: 1.086\n",
      "[461,     2] loss: 1.086\n",
      "[462,     2] loss: 1.086\n",
      "[463,     2] loss: 1.087\n",
      "[464,     2] loss: 1.085\n",
      "[465,     2] loss: 1.085\n",
      "[466,     2] loss: 1.085\n",
      "[467,     2] loss: 1.085\n",
      "[468,     2] loss: 1.086\n",
      "[469,     2] loss: 1.086\n",
      "[470,     2] loss: 1.085\n",
      "[471,     2] loss: 1.087\n",
      "[472,     2] loss: 1.085\n",
      "[473,     2] loss: 1.085\n",
      "[474,     2] loss: 1.085\n",
      "[475,     2] loss: 1.086\n",
      "[476,     2] loss: 1.086\n",
      "[477,     2] loss: 1.089\n",
      "[478,     2] loss: 1.087\n",
      "[479,     2] loss: 1.088\n",
      "[480,     2] loss: 1.087\n",
      "[481,     2] loss: 1.087\n",
      "[482,     2] loss: 1.085\n",
      "[483,     2] loss: 1.087\n",
      "[484,     2] loss: 1.086\n",
      "[485,     2] loss: 1.090\n",
      "[486,     2] loss: 1.085\n",
      "[487,     2] loss: 1.085\n",
      "[488,     2] loss: 1.087\n",
      "[489,     2] loss: 1.085\n",
      "[490,     2] loss: 1.085\n",
      "[491,     2] loss: 1.086\n",
      "[492,     2] loss: 1.085\n",
      "[493,     2] loss: 1.085\n",
      "[494,     2] loss: 1.085\n",
      "[495,     2] loss: 1.085\n",
      "[496,     2] loss: 1.085\n",
      "[497,     2] loss: 1.088\n",
      "[498,     2] loss: 1.087\n",
      "[499,     2] loss: 1.088\n",
      "[500,     2] loss: 1.088\n",
      "[501,     2] loss: 1.085\n",
      "[502,     2] loss: 1.087\n",
      "[503,     2] loss: 1.086\n",
      "[504,     2] loss: 1.085\n",
      "[505,     2] loss: 1.090\n",
      "[506,     2] loss: 1.087\n",
      "[507,     2] loss: 1.086\n",
      "[508,     2] loss: 1.087\n",
      "[509,     2] loss: 1.087\n",
      "[510,     2] loss: 1.088\n",
      "[511,     2] loss: 1.087\n",
      "[512,     2] loss: 1.085\n",
      "[513,     2] loss: 1.087\n",
      "[514,     2] loss: 1.085\n",
      "[515,     2] loss: 1.085\n",
      "[516,     2] loss: 1.086\n",
      "[517,     2] loss: 1.087\n",
      "[518,     2] loss: 1.088\n",
      "[519,     2] loss: 1.087\n",
      "[520,     2] loss: 1.086\n",
      "[521,     2] loss: 1.085\n",
      "[522,     2] loss: 1.091\n",
      "[523,     2] loss: 1.085\n",
      "[524,     2] loss: 1.089\n",
      "[525,     2] loss: 1.086\n",
      "[526,     2] loss: 1.090\n",
      "[527,     2] loss: 1.085\n",
      "[528,     2] loss: 1.085\n",
      "[529,     2] loss: 1.085\n",
      "[530,     2] loss: 1.086\n",
      "[531,     2] loss: 1.085\n",
      "[532,     2] loss: 1.085\n",
      "[533,     2] loss: 1.085\n",
      "[534,     2] loss: 1.086\n",
      "[535,     2] loss: 1.087\n",
      "[536,     2] loss: 1.085\n",
      "[537,     2] loss: 1.085\n",
      "[538,     2] loss: 1.085\n",
      "[539,     2] loss: 1.086\n",
      "[540,     2] loss: 1.085\n",
      "[541,     2] loss: 1.086\n",
      "[542,     2] loss: 1.085\n",
      "[543,     2] loss: 1.085\n",
      "[544,     2] loss: 1.085\n",
      "[545,     2] loss: 1.085\n",
      "[546,     2] loss: 1.085\n",
      "[547,     2] loss: 1.085\n",
      "[548,     2] loss: 1.087\n",
      "[549,     2] loss: 1.085\n",
      "[550,     2] loss: 1.087\n",
      "[551,     2] loss: 1.089\n",
      "[552,     2] loss: 1.090\n",
      "[553,     2] loss: 1.090\n",
      "[554,     2] loss: 1.092\n",
      "[555,     2] loss: 1.088\n",
      "[556,     2] loss: 1.086\n",
      "[557,     2] loss: 1.084\n",
      "[558,     2] loss: 1.085\n",
      "[559,     2] loss: 1.085\n",
      "[560,     2] loss: 1.086\n",
      "[561,     2] loss: 1.089\n",
      "[562,     2] loss: 1.086\n",
      "[563,     2] loss: 1.086\n",
      "[564,     2] loss: 1.090\n",
      "[565,     2] loss: 1.085\n",
      "[566,     2] loss: 1.087\n",
      "[567,     2] loss: 1.085\n",
      "[568,     2] loss: 1.084\n",
      "[569,     2] loss: 1.086\n",
      "[570,     2] loss: 1.085\n",
      "[571,     2] loss: 1.086\n",
      "[572,     2] loss: 1.085\n",
      "[573,     2] loss: 1.084\n",
      "[574,     2] loss: 1.084\n",
      "[575,     2] loss: 1.086\n",
      "[576,     2] loss: 1.087\n",
      "[577,     2] loss: 1.085\n",
      "[578,     2] loss: 1.085\n",
      "[579,     2] loss: 1.084\n",
      "[580,     2] loss: 1.085\n",
      "[581,     2] loss: 1.086\n",
      "[582,     2] loss: 1.085\n",
      "[583,     2] loss: 1.087\n",
      "[584,     2] loss: 1.089\n",
      "[585,     2] loss: 1.084\n",
      "[586,     2] loss: 1.086\n",
      "[587,     2] loss: 1.085\n",
      "[588,     2] loss: 1.085\n",
      "[589,     2] loss: 1.085\n",
      "[590,     2] loss: 1.084\n",
      "[591,     2] loss: 1.084\n",
      "[592,     2] loss: 1.088\n",
      "[593,     2] loss: 1.087\n",
      "[594,     2] loss: 1.085\n",
      "[595,     2] loss: 1.084\n",
      "[596,     2] loss: 1.087\n",
      "[597,     2] loss: 1.086\n",
      "[598,     2] loss: 1.084\n",
      "[599,     2] loss: 1.085\n",
      "[600,     2] loss: 1.085\n",
      "[601,     2] loss: 1.084\n",
      "[602,     2] loss: 1.086\n",
      "[603,     2] loss: 1.086\n",
      "[604,     2] loss: 1.089\n",
      "[605,     2] loss: 1.085\n",
      "[606,     2] loss: 1.084\n",
      "[607,     2] loss: 1.088\n",
      "[608,     2] loss: 1.086\n",
      "[609,     2] loss: 1.085\n",
      "[610,     2] loss: 1.085\n",
      "[611,     2] loss: 1.085\n",
      "[612,     2] loss: 1.084\n",
      "[613,     2] loss: 1.084\n",
      "[614,     2] loss: 1.088\n",
      "[615,     2] loss: 1.085\n",
      "[616,     2] loss: 1.083\n",
      "[617,     2] loss: 1.086\n",
      "[618,     2] loss: 1.088\n",
      "[619,     2] loss: 1.084\n",
      "[620,     2] loss: 1.086\n",
      "[621,     2] loss: 1.084\n",
      "[622,     2] loss: 1.087\n",
      "[623,     2] loss: 1.086\n",
      "[624,     2] loss: 1.086\n",
      "[625,     2] loss: 1.084\n",
      "[626,     2] loss: 1.086\n",
      "[627,     2] loss: 1.083\n",
      "[628,     2] loss: 1.084\n",
      "[629,     2] loss: 1.083\n",
      "[630,     2] loss: 1.082\n",
      "[631,     2] loss: 1.182\n",
      "[632,     2] loss: 1.089\n",
      "[633,     2] loss: 1.094\n",
      "[634,     2] loss: 1.094\n",
      "[635,     2] loss: 1.093\n",
      "[636,     2] loss: 1.090\n",
      "[637,     2] loss: 1.090\n",
      "[638,     2] loss: 1.087\n",
      "[639,     2] loss: 1.086\n",
      "[640,     2] loss: 1.093\n",
      "[641,     2] loss: 1.088\n",
      "[642,     2] loss: 1.087\n",
      "[643,     2] loss: 1.086\n",
      "[644,     2] loss: 1.086\n",
      "[645,     2] loss: 1.087\n",
      "[646,     2] loss: 1.088\n",
      "[647,     2] loss: 1.087\n",
      "[648,     2] loss: 1.088\n",
      "[649,     2] loss: 1.086\n",
      "[650,     2] loss: 1.087\n",
      "[651,     2] loss: 1.088\n",
      "[652,     2] loss: 1.086\n",
      "[653,     2] loss: 1.086\n",
      "[654,     2] loss: 1.086\n",
      "[655,     2] loss: 1.086\n",
      "[656,     2] loss: 1.087\n",
      "[657,     2] loss: 1.085\n",
      "[658,     2] loss: 1.087\n",
      "[659,     2] loss: 1.086\n",
      "[660,     2] loss: 1.086\n",
      "[661,     2] loss: 1.086\n",
      "[662,     2] loss: 1.085\n",
      "[663,     2] loss: 1.088\n",
      "[664,     2] loss: 1.088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[665,     2] loss: 1.086\n",
      "[666,     2] loss: 1.086\n",
      "[667,     2] loss: 1.087\n",
      "[668,     2] loss: 1.087\n",
      "[669,     2] loss: 1.086\n",
      "[670,     2] loss: 1.087\n",
      "[671,     2] loss: 1.086\n",
      "[672,     2] loss: 1.086\n",
      "[673,     2] loss: 1.086\n",
      "[674,     2] loss: 1.085\n",
      "[675,     2] loss: 1.085\n",
      "[676,     2] loss: 1.086\n",
      "[677,     2] loss: 1.087\n",
      "[678,     2] loss: 1.088\n",
      "[679,     2] loss: 1.085\n",
      "[680,     2] loss: 1.085\n",
      "[681,     2] loss: 1.085\n",
      "[682,     2] loss: 1.089\n",
      "[683,     2] loss: 1.086\n",
      "[684,     2] loss: 1.085\n",
      "[685,     2] loss: 1.085\n",
      "[686,     2] loss: 1.088\n",
      "[687,     2] loss: 1.088\n",
      "[688,     2] loss: 1.086\n",
      "[689,     2] loss: 1.085\n",
      "[690,     2] loss: 1.087\n",
      "[691,     2] loss: 1.089\n",
      "[692,     2] loss: 1.086\n",
      "[693,     2] loss: 1.085\n",
      "[694,     2] loss: 1.086\n",
      "[695,     2] loss: 1.089\n",
      "[696,     2] loss: 1.086\n",
      "[697,     2] loss: 1.086\n",
      "[698,     2] loss: 1.088\n",
      "[699,     2] loss: 1.090\n",
      "[700,     2] loss: 1.087\n",
      "[701,     2] loss: 1.086\n",
      "[702,     2] loss: 1.086\n",
      "[703,     2] loss: 1.086\n",
      "[704,     2] loss: 1.086\n",
      "[705,     2] loss: 1.086\n",
      "[706,     2] loss: 1.088\n",
      "[707,     2] loss: 1.085\n",
      "[708,     2] loss: 1.086\n",
      "[709,     2] loss: 1.086\n",
      "[710,     2] loss: 1.085\n",
      "[711,     2] loss: 1.087\n",
      "[712,     2] loss: 1.085\n",
      "[713,     2] loss: 1.086\n",
      "[714,     2] loss: 1.085\n",
      "[715,     2] loss: 1.087\n",
      "[716,     2] loss: 1.085\n",
      "[717,     2] loss: 1.089\n",
      "[718,     2] loss: 1.086\n",
      "[719,     2] loss: 1.086\n",
      "[720,     2] loss: 1.086\n",
      "[721,     2] loss: 1.085\n",
      "[722,     2] loss: 1.085\n",
      "[723,     2] loss: 1.085\n",
      "[724,     2] loss: 1.086\n",
      "[725,     2] loss: 1.085\n",
      "[726,     2] loss: 1.084\n",
      "[727,     2] loss: 1.085\n",
      "[728,     2] loss: 1.088\n",
      "[729,     2] loss: 1.088\n",
      "[730,     2] loss: 1.086\n",
      "[731,     2] loss: 1.084\n",
      "[732,     2] loss: 1.084\n",
      "[733,     2] loss: 1.086\n",
      "[734,     2] loss: 1.084\n",
      "[735,     2] loss: 1.091\n",
      "[736,     2] loss: 1.084\n",
      "[737,     2] loss: 1.084\n",
      "[738,     2] loss: 1.084\n",
      "[739,     2] loss: 1.084\n",
      "[740,     2] loss: 1.086\n",
      "[741,     2] loss: 1.084\n",
      "[742,     2] loss: 1.087\n",
      "[743,     2] loss: 1.086\n",
      "[744,     2] loss: 1.086\n",
      "[745,     2] loss: 1.084\n",
      "[746,     2] loss: 1.085\n",
      "[747,     2] loss: 1.085\n",
      "[748,     2] loss: 1.084\n",
      "[749,     2] loss: 1.086\n",
      "[750,     2] loss: 1.084\n",
      "[751,     2] loss: 1.085\n",
      "[752,     2] loss: 1.084\n",
      "[753,     2] loss: 1.084\n",
      "[754,     2] loss: 1.084\n",
      "[755,     2] loss: 1.087\n",
      "[756,     2] loss: 1.085\n",
      "[757,     2] loss: 1.084\n",
      "[758,     2] loss: 1.083\n",
      "[759,     2] loss: 1.086\n",
      "[760,     2] loss: 1.085\n",
      "[761,     2] loss: 1.083\n",
      "[762,     2] loss: 1.085\n",
      "[763,     2] loss: 1.084\n",
      "[764,     2] loss: 1.085\n",
      "[765,     2] loss: 1.085\n",
      "[766,     2] loss: 1.083\n",
      "[767,     2] loss: 1.083\n",
      "[768,     2] loss: 1.083\n",
      "[769,     2] loss: 1.084\n",
      "[770,     2] loss: 1.083\n",
      "[771,     2] loss: 1.084\n",
      "[772,     2] loss: 1.085\n",
      "[773,     2] loss: 1.083\n",
      "[774,     2] loss: 1.088\n",
      "[775,     2] loss: 1.083\n",
      "[776,     2] loss: 1.084\n",
      "[777,     2] loss: 1.082\n",
      "[778,     2] loss: 1.082\n",
      "[779,     2] loss: 1.091\n",
      "[780,     2] loss: 1.083\n",
      "[781,     2] loss: 1.083\n",
      "[782,     2] loss: 1.082\n",
      "[783,     2] loss: 1.083\n",
      "[784,     2] loss: 1.083\n",
      "[785,     2] loss: 1.082\n",
      "[786,     2] loss: 1.085\n",
      "[787,     2] loss: 1.085\n",
      "[788,     2] loss: 1.082\n",
      "[789,     2] loss: 1.082\n",
      "[790,     2] loss: 1.081\n",
      "[791,     2] loss: 1.085\n",
      "[792,     2] loss: 1.082\n",
      "[793,     2] loss: 1.081\n",
      "[794,     2] loss: 1.083\n",
      "[795,     2] loss: 1.082\n",
      "[796,     2] loss: 1.082\n",
      "[797,     2] loss: 1.083\n",
      "[798,     2] loss: 1.085\n",
      "[799,     2] loss: 1.082\n",
      "[800,     2] loss: 1.081\n",
      "[801,     2] loss: 1.084\n",
      "[802,     2] loss: 1.086\n",
      "[803,     2] loss: 1.081\n",
      "[804,     2] loss: 1.082\n",
      "[805,     2] loss: 1.081\n",
      "[806,     2] loss: 1.080\n",
      "[807,     2] loss: 1.083\n",
      "[808,     2] loss: 1.081\n",
      "[809,     2] loss: 1.084\n",
      "[810,     2] loss: 1.080\n",
      "[811,     2] loss: 1.082\n",
      "[812,     2] loss: 1.080\n",
      "[813,     2] loss: 1.080\n",
      "[814,     2] loss: 1.080\n",
      "[815,     2] loss: 1.080\n",
      "[816,     2] loss: 1.082\n",
      "[817,     2] loss: 1.081\n",
      "[818,     2] loss: 1.080\n",
      "[819,     2] loss: 1.079\n",
      "[820,     2] loss: 1.083\n",
      "[821,     2] loss: 1.080\n",
      "[822,     2] loss: 1.079\n",
      "[823,     2] loss: 1.080\n",
      "[824,     2] loss: 1.082\n",
      "[825,     2] loss: 1.079\n",
      "[826,     2] loss: 1.081\n",
      "[827,     2] loss: 1.080\n",
      "[828,     2] loss: 1.079\n",
      "[829,     2] loss: 1.080\n",
      "[830,     2] loss: 1.079\n",
      "[831,     2] loss: 1.079\n",
      "[832,     2] loss: 1.078\n",
      "[833,     2] loss: 1.079\n",
      "[834,     2] loss: 1.079\n",
      "[835,     2] loss: 1.078\n",
      "[836,     2] loss: 1.080\n",
      "[837,     2] loss: 1.078\n",
      "[838,     2] loss: 1.081\n",
      "[839,     2] loss: 1.079\n",
      "[840,     2] loss: 1.080\n",
      "[841,     2] loss: 1.078\n",
      "[842,     2] loss: 1.079\n",
      "[843,     2] loss: 1.079\n",
      "[844,     2] loss: 1.080\n",
      "[845,     2] loss: 1.078\n",
      "[846,     2] loss: 1.078\n",
      "[847,     2] loss: 1.078\n",
      "[848,     2] loss: 1.079\n",
      "[849,     2] loss: 1.079\n",
      "[850,     2] loss: 1.077\n",
      "[851,     2] loss: 1.077\n",
      "[852,     2] loss: 1.078\n",
      "[853,     2] loss: 1.079\n",
      "[854,     2] loss: 1.080\n",
      "[855,     2] loss: 1.077\n",
      "[856,     2] loss: 1.077\n",
      "[857,     2] loss: 1.081\n",
      "[858,     2] loss: 1.078\n",
      "[859,     2] loss: 1.076\n",
      "[860,     2] loss: 1.077\n",
      "[861,     2] loss: 1.076\n",
      "[862,     2] loss: 1.076\n",
      "[863,     2] loss: 1.077\n",
      "[864,     2] loss: 1.076\n",
      "[865,     2] loss: 1.078\n",
      "[866,     2] loss: 1.076\n",
      "[867,     2] loss: 1.078\n",
      "[868,     2] loss: 1.076\n",
      "[869,     2] loss: 1.077\n",
      "[870,     2] loss: 1.078\n",
      "[871,     2] loss: 1.079\n",
      "[872,     2] loss: 1.076\n",
      "[873,     2] loss: 1.079\n",
      "[874,     2] loss: 1.076\n",
      "[875,     2] loss: 1.076\n",
      "[876,     2] loss: 1.076\n",
      "[877,     2] loss: 1.079\n",
      "[878,     2] loss: 1.076\n",
      "[879,     2] loss: 1.077\n",
      "[880,     2] loss: 1.075\n",
      "[881,     2] loss: 1.076\n",
      "[882,     2] loss: 1.075\n",
      "[883,     2] loss: 1.075\n",
      "[884,     2] loss: 1.078\n",
      "[885,     2] loss: 1.075\n",
      "[886,     2] loss: 1.075\n",
      "[887,     2] loss: 1.075\n",
      "[888,     2] loss: 1.075\n",
      "[889,     2] loss: 1.074\n",
      "[890,     2] loss: 1.078\n",
      "[891,     2] loss: 1.078\n",
      "[892,     2] loss: 1.074\n",
      "[893,     2] loss: 1.074\n",
      "[894,     2] loss: 1.074\n",
      "[895,     2] loss: 1.075\n",
      "[896,     2] loss: 1.074\n",
      "[897,     2] loss: 1.075\n",
      "[898,     2] loss: 1.077\n",
      "[899,     2] loss: 1.074\n",
      "[900,     2] loss: 1.077\n",
      "[901,     2] loss: 1.075\n",
      "[902,     2] loss: 1.074\n",
      "[903,     2] loss: 1.075\n",
      "[904,     2] loss: 1.075\n",
      "[905,     2] loss: 1.076\n",
      "[906,     2] loss: 1.073\n",
      "[907,     2] loss: 1.073\n",
      "[908,     2] loss: 1.076\n",
      "[909,     2] loss: 1.074\n",
      "[910,     2] loss: 1.073\n",
      "[911,     2] loss: 1.074\n",
      "[912,     2] loss: 1.075\n",
      "[913,     2] loss: 1.074\n",
      "[914,     2] loss: 1.075\n",
      "[915,     2] loss: 1.073\n",
      "[916,     2] loss: 1.073\n",
      "[917,     2] loss: 1.073\n",
      "[918,     2] loss: 1.077\n",
      "[919,     2] loss: 1.077\n",
      "[920,     2] loss: 1.074\n",
      "[921,     2] loss: 1.074\n",
      "[922,     2] loss: 1.073\n",
      "[923,     2] loss: 1.074\n",
      "[924,     2] loss: 1.073\n",
      "[925,     2] loss: 1.073\n",
      "[926,     2] loss: 1.073\n",
      "[927,     2] loss: 1.074\n",
      "[928,     2] loss: 1.075\n",
      "[929,     2] loss: 1.079\n",
      "[930,     2] loss: 1.075\n",
      "[931,     2] loss: 1.073\n",
      "[932,     2] loss: 1.073\n",
      "[933,     2] loss: 1.072\n",
      "[934,     2] loss: 1.072\n",
      "[935,     2] loss: 1.072\n",
      "[936,     2] loss: 1.072\n",
      "[937,     2] loss: 1.072\n",
      "[938,     2] loss: 1.074\n",
      "[939,     2] loss: 1.074\n",
      "[940,     2] loss: 1.072\n",
      "[941,     2] loss: 1.073\n",
      "[942,     2] loss: 1.071\n",
      "[943,     2] loss: 1.075\n",
      "[944,     2] loss: 1.072\n",
      "[945,     2] loss: 1.072\n",
      "[946,     2] loss: 1.073\n",
      "[947,     2] loss: 1.076\n",
      "[948,     2] loss: 1.071\n",
      "[949,     2] loss: 1.074\n",
      "[950,     2] loss: 1.072\n",
      "[951,     2] loss: 1.072\n",
      "[952,     2] loss: 1.071\n",
      "[953,     2] loss: 1.075\n",
      "[954,     2] loss: 1.074\n",
      "[955,     2] loss: 1.072\n",
      "[956,     2] loss: 1.071\n",
      "[957,     2] loss: 1.071\n",
      "[958,     2] loss: 1.073\n",
      "[959,     2] loss: 1.073\n",
      "[960,     2] loss: 1.074\n",
      "[961,     2] loss: 1.072\n",
      "[962,     2] loss: 1.072\n",
      "[963,     2] loss: 1.072\n",
      "[964,     2] loss: 1.071\n",
      "[965,     2] loss: 1.071\n",
      "[966,     2] loss: 1.071\n",
      "[967,     2] loss: 1.073\n",
      "[968,     2] loss: 1.073\n",
      "[969,     2] loss: 1.070\n",
      "[970,     2] loss: 1.070\n",
      "[971,     2] loss: 1.072\n",
      "[972,     2] loss: 1.072\n",
      "[973,     2] loss: 1.071\n",
      "[974,     2] loss: 1.072\n",
      "[975,     2] loss: 1.072\n",
      "[976,     2] loss: 1.072\n",
      "[977,     2] loss: 1.072\n",
      "[978,     2] loss: 1.075\n",
      "[979,     2] loss: 1.071\n",
      "[980,     2] loss: 1.070\n",
      "[981,     2] loss: 1.070\n",
      "[982,     2] loss: 1.070\n",
      "[983,     2] loss: 1.071\n",
      "[984,     2] loss: 1.069\n",
      "[985,     2] loss: 1.072\n",
      "[986,     2] loss: 1.071\n",
      "[987,     2] loss: 1.069\n",
      "[988,     2] loss: 1.069\n",
      "[989,     2] loss: 1.069\n",
      "[990,     2] loss: 1.069\n",
      "[991,     2] loss: 1.070\n",
      "[992,     2] loss: 1.069\n",
      "[993,     2] loss: 1.069\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[994,     2] loss: 1.069\n",
      "[995,     2] loss: 1.072\n",
      "[996,     2] loss: 1.069\n",
      "[997,     2] loss: 1.071\n",
      "[998,     2] loss: 1.069\n",
      "[999,     2] loss: 1.071\n",
      "[1000,     2] loss: 1.069\n",
      "[1001,     2] loss: 1.071\n",
      "[1002,     2] loss: 1.068\n",
      "[1003,     2] loss: 1.068\n",
      "[1004,     2] loss: 1.069\n",
      "[1005,     2] loss: 1.071\n",
      "[1006,     2] loss: 1.068\n",
      "[1007,     2] loss: 1.069\n",
      "[1008,     2] loss: 1.068\n",
      "[1009,     2] loss: 1.068\n",
      "[1010,     2] loss: 1.068\n",
      "[1011,     2] loss: 1.068\n",
      "[1012,     2] loss: 1.068\n",
      "[1013,     2] loss: 1.069\n",
      "[1014,     2] loss: 1.069\n",
      "[1015,     2] loss: 1.071\n",
      "[1016,     2] loss: 1.071\n",
      "[1017,     2] loss: 1.068\n",
      "[1018,     2] loss: 1.068\n",
      "[1019,     2] loss: 1.068\n",
      "[1020,     2] loss: 1.068\n",
      "[1021,     2] loss: 1.068\n",
      "[1022,     2] loss: 1.067\n",
      "[1023,     2] loss: 1.069\n",
      "[1024,     2] loss: 1.067\n",
      "[1025,     2] loss: 1.069\n",
      "[1026,     2] loss: 1.068\n",
      "[1027,     2] loss: 1.068\n",
      "[1028,     2] loss: 1.067\n",
      "[1029,     2] loss: 1.070\n",
      "[1030,     2] loss: 1.069\n",
      "[1031,     2] loss: 1.068\n",
      "[1032,     2] loss: 1.068\n",
      "[1033,     2] loss: 1.069\n",
      "[1034,     2] loss: 1.067\n",
      "[1035,     2] loss: 1.067\n",
      "[1036,     2] loss: 1.069\n",
      "[1037,     2] loss: 1.068\n",
      "[1038,     2] loss: 1.067\n",
      "[1039,     2] loss: 1.068\n",
      "[1040,     2] loss: 1.067\n",
      "[1041,     2] loss: 1.067\n",
      "[1042,     2] loss: 1.070\n",
      "[1043,     2] loss: 1.067\n",
      "[1044,     2] loss: 1.067\n",
      "[1045,     2] loss: 1.068\n",
      "[1046,     2] loss: 1.067\n",
      "[1047,     2] loss: 1.067\n",
      "[1048,     2] loss: 1.069\n",
      "[1049,     2] loss: 1.067\n",
      "[1050,     2] loss: 1.067\n",
      "[1051,     2] loss: 1.067\n",
      "[1052,     2] loss: 1.067\n",
      "[1053,     2] loss: 1.067\n",
      "[1054,     2] loss: 1.067\n",
      "[1055,     2] loss: 1.066\n",
      "[1056,     2] loss: 1.067\n",
      "[1057,     2] loss: 1.068\n",
      "[1058,     2] loss: 1.068\n",
      "[1059,     2] loss: 1.067\n",
      "[1060,     2] loss: 1.066\n",
      "[1061,     2] loss: 1.067\n",
      "[1062,     2] loss: 1.067\n",
      "[1063,     2] loss: 1.066\n",
      "[1064,     2] loss: 1.066\n",
      "[1065,     2] loss: 1.067\n",
      "[1066,     2] loss: 1.068\n",
      "[1067,     2] loss: 1.067\n",
      "[1068,     2] loss: 1.066\n",
      "[1069,     2] loss: 1.066\n",
      "[1070,     2] loss: 1.067\n",
      "[1071,     2] loss: 1.066\n",
      "[1072,     2] loss: 1.069\n",
      "[1073,     2] loss: 1.067\n",
      "[1074,     2] loss: 1.070\n",
      "[1075,     2] loss: 1.066\n",
      "[1076,     2] loss: 1.066\n",
      "[1077,     2] loss: 1.065\n",
      "[1078,     2] loss: 1.069\n",
      "[1079,     2] loss: 1.066\n",
      "[1080,     2] loss: 1.066\n",
      "[1081,     2] loss: 1.066\n",
      "[1082,     2] loss: 1.067\n",
      "[1083,     2] loss: 1.066\n",
      "[1084,     2] loss: 1.066\n",
      "[1085,     2] loss: 1.065\n",
      "[1086,     2] loss: 1.069\n",
      "[1087,     2] loss: 1.066\n",
      "[1088,     2] loss: 1.065\n",
      "[1089,     2] loss: 1.065\n",
      "[1090,     2] loss: 1.066\n",
      "[1091,     2] loss: 1.065\n",
      "[1092,     2] loss: 1.068\n",
      "[1093,     2] loss: 1.068\n",
      "[1094,     2] loss: 1.066\n",
      "[1095,     2] loss: 1.066\n",
      "[1096,     2] loss: 1.066\n",
      "[1097,     2] loss: 1.065\n",
      "[1098,     2] loss: 1.065\n",
      "[1099,     2] loss: 1.065\n",
      "[1100,     2] loss: 1.065\n",
      "[1101,     2] loss: 1.065\n",
      "[1102,     2] loss: 1.066\n",
      "[1103,     2] loss: 1.065\n",
      "[1104,     2] loss: 1.066\n",
      "[1105,     2] loss: 1.065\n",
      "[1106,     2] loss: 1.065\n",
      "[1107,     2] loss: 1.066\n",
      "[1108,     2] loss: 1.066\n",
      "[1109,     2] loss: 1.068\n",
      "[1110,     2] loss: 1.065\n",
      "[1111,     2] loss: 1.066\n",
      "[1112,     2] loss: 1.064\n",
      "[1113,     2] loss: 1.065\n",
      "[1114,     2] loss: 1.064\n",
      "[1115,     2] loss: 1.064\n",
      "[1116,     2] loss: 1.064\n",
      "[1117,     2] loss: 1.065\n",
      "[1118,     2] loss: 1.065\n",
      "[1119,     2] loss: 1.066\n",
      "[1120,     2] loss: 1.069\n",
      "[1121,     2] loss: 1.065\n",
      "[1122,     2] loss: 1.066\n",
      "[1123,     2] loss: 1.067\n",
      "[1124,     2] loss: 1.065\n",
      "[1125,     2] loss: 1.065\n",
      "[1126,     2] loss: 1.064\n",
      "[1127,     2] loss: 1.064\n",
      "[1128,     2] loss: 1.067\n",
      "[1129,     2] loss: 1.067\n",
      "[1130,     2] loss: 1.065\n",
      "[1131,     2] loss: 1.064\n",
      "[1132,     2] loss: 1.065\n",
      "[1133,     2] loss: 1.066\n",
      "[1134,     2] loss: 1.064\n",
      "[1135,     2] loss: 1.065\n",
      "[1136,     2] loss: 1.064\n",
      "[1137,     2] loss: 1.064\n",
      "[1138,     2] loss: 1.063\n",
      "[1139,     2] loss: 1.063\n",
      "[1140,     2] loss: 1.064\n",
      "[1141,     2] loss: 1.063\n",
      "[1142,     2] loss: 1.065\n",
      "[1143,     2] loss: 1.064\n",
      "[1144,     2] loss: 1.064\n",
      "[1145,     2] loss: 1.063\n",
      "[1146,     2] loss: 1.064\n",
      "[1147,     2] loss: 1.067\n",
      "[1148,     2] loss: 1.064\n",
      "[1149,     2] loss: 1.065\n",
      "[1150,     2] loss: 1.064\n",
      "[1151,     2] loss: 1.065\n",
      "[1152,     2] loss: 1.066\n",
      "[1153,     2] loss: 1.063\n",
      "[1154,     2] loss: 1.063\n",
      "[1155,     2] loss: 1.063\n",
      "[1156,     2] loss: 1.063\n",
      "[1157,     2] loss: 1.063\n",
      "[1158,     2] loss: 1.063\n",
      "[1159,     2] loss: 1.063\n",
      "[1160,     2] loss: 1.063\n",
      "[1161,     2] loss: 1.065\n",
      "[1162,     2] loss: 1.069\n",
      "[1163,     2] loss: 1.064\n",
      "[1164,     2] loss: 1.066\n",
      "[1165,     2] loss: 1.065\n",
      "[1166,     2] loss: 1.065\n",
      "[1167,     2] loss: 1.065\n",
      "[1168,     2] loss: 1.064\n",
      "[1169,     2] loss: 1.062\n",
      "[1170,     2] loss: 1.062\n",
      "[1171,     2] loss: 1.063\n",
      "[1172,     2] loss: 1.063\n",
      "[1173,     2] loss: 1.064\n",
      "[1174,     2] loss: 1.065\n",
      "[1175,     2] loss: 1.065\n",
      "[1176,     2] loss: 1.063\n",
      "[1177,     2] loss: 1.063\n",
      "[1178,     2] loss: 1.064\n",
      "[1179,     2] loss: 1.064\n",
      "[1180,     2] loss: 1.064\n",
      "[1181,     2] loss: 1.062\n",
      "[1182,     2] loss: 1.064\n",
      "[1183,     2] loss: 1.063\n",
      "[1184,     2] loss: 1.064\n",
      "[1185,     2] loss: 1.067\n",
      "[1186,     2] loss: 1.062\n",
      "[1187,     2] loss: 1.064\n",
      "[1188,     2] loss: 1.064\n",
      "[1189,     2] loss: 1.063\n",
      "[1190,     2] loss: 1.062\n",
      "[1191,     2] loss: 1.063\n",
      "[1192,     2] loss: 1.063\n",
      "[1193,     2] loss: 1.063\n",
      "[1194,     2] loss: 1.063\n",
      "[1195,     2] loss: 1.062\n",
      "[1196,     2] loss: 1.064\n",
      "[1197,     2] loss: 1.063\n",
      "[1198,     2] loss: 1.062\n",
      "[1199,     2] loss: 1.067\n",
      "[1200,     2] loss: 1.062\n",
      "[1201,     2] loss: 1.063\n",
      "[1202,     2] loss: 1.063\n",
      "[1203,     2] loss: 1.062\n",
      "[1204,     2] loss: 1.065\n",
      "[1205,     2] loss: 1.063\n",
      "[1206,     2] loss: 1.063\n",
      "[1207,     2] loss: 1.062\n",
      "[1208,     2] loss: 1.062\n",
      "[1209,     2] loss: 1.061\n",
      "[1210,     2] loss: 1.063\n",
      "[1211,     2] loss: 1.062\n",
      "[1212,     2] loss: 1.064\n",
      "[1213,     2] loss: 1.063\n",
      "[1214,     2] loss: 1.062\n",
      "[1215,     2] loss: 1.061\n",
      "[1216,     2] loss: 1.062\n",
      "[1217,     2] loss: 1.063\n",
      "[1218,     2] loss: 1.063\n",
      "[1219,     2] loss: 1.063\n",
      "[1220,     2] loss: 1.063\n",
      "[1221,     2] loss: 1.063\n",
      "[1222,     2] loss: 1.064\n",
      "[1223,     2] loss: 1.062\n",
      "[1224,     2] loss: 1.061\n",
      "[1225,     2] loss: 1.061\n",
      "[1226,     2] loss: 1.062\n",
      "[1227,     2] loss: 1.062\n",
      "[1228,     2] loss: 1.061\n",
      "[1229,     2] loss: 1.061\n",
      "[1230,     2] loss: 1.063\n",
      "[1231,     2] loss: 1.061\n",
      "[1232,     2] loss: 1.061\n",
      "[1233,     2] loss: 1.064\n",
      "[1234,     2] loss: 1.062\n",
      "[1235,     2] loss: 1.061\n",
      "[1236,     2] loss: 1.063\n",
      "[1237,     2] loss: 1.061\n",
      "[1238,     2] loss: 1.063\n",
      "[1239,     2] loss: 1.062\n",
      "[1240,     2] loss: 1.061\n",
      "[1241,     2] loss: 1.062\n",
      "[1242,     2] loss: 1.061\n",
      "[1243,     2] loss: 1.063\n",
      "[1244,     2] loss: 1.061\n",
      "[1245,     2] loss: 1.062\n",
      "[1246,     2] loss: 1.062\n",
      "[1247,     2] loss: 1.063\n",
      "[1248,     2] loss: 1.061\n",
      "[1249,     2] loss: 1.061\n",
      "[1250,     2] loss: 1.063\n",
      "[1251,     2] loss: 1.061\n",
      "[1252,     2] loss: 1.063\n",
      "[1253,     2] loss: 1.061\n",
      "[1254,     2] loss: 1.062\n",
      "[1255,     2] loss: 1.063\n",
      "[1256,     2] loss: 1.061\n",
      "[1257,     2] loss: 1.062\n",
      "[1258,     2] loss: 1.062\n",
      "[1259,     2] loss: 1.061\n",
      "[1260,     2] loss: 1.062\n",
      "[1261,     2] loss: 1.061\n",
      "[1262,     2] loss: 1.061\n",
      "[1263,     2] loss: 1.067\n",
      "[1264,     2] loss: 1.062\n",
      "[1265,     2] loss: 1.061\n",
      "[1266,     2] loss: 1.060\n",
      "[1267,     2] loss: 1.061\n",
      "[1268,     2] loss: 1.063\n",
      "[1269,     2] loss: 1.061\n",
      "[1270,     2] loss: 1.060\n",
      "[1271,     2] loss: 1.063\n",
      "[1272,     2] loss: 1.062\n",
      "[1273,     2] loss: 1.061\n",
      "[1274,     2] loss: 1.062\n",
      "[1275,     2] loss: 1.060\n",
      "[1276,     2] loss: 1.062\n",
      "[1277,     2] loss: 1.064\n",
      "[1278,     2] loss: 1.063\n",
      "[1279,     2] loss: 1.062\n",
      "[1280,     2] loss: 1.060\n",
      "[1281,     2] loss: 1.060\n",
      "[1282,     2] loss: 1.062\n",
      "[1283,     2] loss: 1.063\n",
      "[1284,     2] loss: 1.062\n",
      "[1285,     2] loss: 1.060\n",
      "[1286,     2] loss: 1.061\n",
      "[1287,     2] loss: 1.062\n",
      "[1288,     2] loss: 1.061\n",
      "[1289,     2] loss: 1.060\n",
      "[1290,     2] loss: 1.060\n",
      "[1291,     2] loss: 1.060\n",
      "[1292,     2] loss: 1.062\n",
      "[1293,     2] loss: 1.061\n",
      "[1294,     2] loss: 1.060\n",
      "[1295,     2] loss: 1.062\n",
      "[1296,     2] loss: 1.061\n",
      "[1297,     2] loss: 1.062\n",
      "[1298,     2] loss: 1.060\n",
      "[1299,     2] loss: 1.060\n",
      "[1300,     2] loss: 1.061\n",
      "[1301,     2] loss: 1.061\n",
      "[1302,     2] loss: 1.060\n",
      "[1303,     2] loss: 1.060\n",
      "[1304,     2] loss: 1.062\n",
      "[1305,     2] loss: 1.060\n",
      "[1306,     2] loss: 1.061\n",
      "[1307,     2] loss: 1.061\n",
      "[1308,     2] loss: 1.063\n",
      "[1309,     2] loss: 1.060\n",
      "[1310,     2] loss: 1.060\n",
      "[1311,     2] loss: 1.065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1312,     2] loss: 1.061\n",
      "[1313,     2] loss: 1.061\n",
      "[1314,     2] loss: 1.061\n",
      "[1315,     2] loss: 1.059\n",
      "[1316,     2] loss: 1.060\n",
      "[1317,     2] loss: 1.063\n",
      "[1318,     2] loss: 1.059\n",
      "[1319,     2] loss: 1.060\n",
      "[1320,     2] loss: 1.061\n",
      "[1321,     2] loss: 1.060\n",
      "[1322,     2] loss: 1.060\n",
      "[1323,     2] loss: 1.060\n",
      "[1324,     2] loss: 1.060\n",
      "[1325,     2] loss: 1.060\n",
      "[1326,     2] loss: 1.063\n",
      "[1327,     2] loss: 1.060\n",
      "[1328,     2] loss: 1.060\n",
      "[1329,     2] loss: 1.061\n",
      "[1330,     2] loss: 1.062\n",
      "[1331,     2] loss: 1.060\n",
      "[1332,     2] loss: 1.060\n",
      "[1333,     2] loss: 1.061\n",
      "[1334,     2] loss: 1.062\n",
      "[1335,     2] loss: 1.060\n",
      "[1336,     2] loss: 1.061\n",
      "[1337,     2] loss: 1.059\n",
      "[1338,     2] loss: 1.063\n",
      "[1339,     2] loss: 1.062\n",
      "[1340,     2] loss: 1.060\n",
      "[1341,     2] loss: 1.060\n",
      "[1342,     2] loss: 1.059\n",
      "[1343,     2] loss: 1.060\n",
      "[1344,     2] loss: 1.059\n",
      "[1345,     2] loss: 1.059\n",
      "[1346,     2] loss: 1.059\n",
      "[1347,     2] loss: 1.059\n",
      "[1348,     2] loss: 1.060\n",
      "[1349,     2] loss: 1.060\n",
      "[1350,     2] loss: 1.059\n",
      "[1351,     2] loss: 1.065\n",
      "[1352,     2] loss: 1.060\n",
      "[1353,     2] loss: 1.060\n",
      "[1354,     2] loss: 1.061\n",
      "[1355,     2] loss: 1.060\n",
      "[1356,     2] loss: 1.060\n",
      "[1357,     2] loss: 1.059\n",
      "[1358,     2] loss: 1.059\n",
      "[1359,     2] loss: 1.059\n",
      "[1360,     2] loss: 1.060\n",
      "[1361,     2] loss: 1.059\n",
      "[1362,     2] loss: 1.059\n",
      "[1363,     2] loss: 1.059\n",
      "[1364,     2] loss: 1.059\n",
      "[1365,     2] loss: 1.060\n",
      "[1366,     2] loss: 1.061\n",
      "[1367,     2] loss: 1.060\n",
      "[1368,     2] loss: 1.061\n",
      "[1369,     2] loss: 1.060\n",
      "[1370,     2] loss: 1.060\n",
      "[1371,     2] loss: 1.060\n",
      "[1372,     2] loss: 1.060\n",
      "[1373,     2] loss: 1.059\n",
      "[1374,     2] loss: 1.059\n",
      "[1375,     2] loss: 1.062\n",
      "[1376,     2] loss: 1.059\n",
      "[1377,     2] loss: 1.059\n",
      "[1378,     2] loss: 1.059\n",
      "[1379,     2] loss: 1.060\n",
      "[1380,     2] loss: 1.061\n",
      "[1381,     2] loss: 1.061\n",
      "[1382,     2] loss: 1.059\n",
      "[1383,     2] loss: 1.059\n",
      "[1384,     2] loss: 1.059\n",
      "[1385,     2] loss: 1.058\n",
      "[1386,     2] loss: 1.059\n",
      "[1387,     2] loss: 1.061\n",
      "[1388,     2] loss: 1.059\n",
      "[1389,     2] loss: 1.059\n",
      "[1390,     2] loss: 1.059\n",
      "[1391,     2] loss: 1.059\n",
      "[1392,     2] loss: 1.059\n",
      "[1393,     2] loss: 1.059\n",
      "[1394,     2] loss: 1.059\n",
      "[1395,     2] loss: 1.060\n",
      "[1396,     2] loss: 1.060\n",
      "[1397,     2] loss: 1.060\n",
      "[1398,     2] loss: 1.059\n",
      "[1399,     2] loss: 1.062\n",
      "[1400,     2] loss: 1.059\n",
      "[1401,     2] loss: 1.060\n",
      "[1402,     2] loss: 1.059\n",
      "[1403,     2] loss: 1.059\n",
      "[1404,     2] loss: 1.060\n",
      "[1405,     2] loss: 1.061\n",
      "[1406,     2] loss: 1.058\n",
      "[1407,     2] loss: 1.061\n",
      "[1408,     2] loss: 1.060\n",
      "[1409,     2] loss: 1.058\n",
      "[1410,     2] loss: 1.059\n",
      "[1411,     2] loss: 1.059\n",
      "[1412,     2] loss: 1.061\n",
      "[1413,     2] loss: 1.059\n",
      "[1414,     2] loss: 1.059\n",
      "[1415,     2] loss: 1.058\n",
      "[1416,     2] loss: 1.058\n",
      "[1417,     2] loss: 1.059\n",
      "[1418,     2] loss: 1.059\n",
      "[1419,     2] loss: 1.059\n",
      "[1420,     2] loss: 1.058\n",
      "[1421,     2] loss: 1.059\n",
      "[1422,     2] loss: 1.058\n",
      "[1423,     2] loss: 1.060\n",
      "[1424,     2] loss: 1.058\n",
      "[1425,     2] loss: 1.058\n",
      "[1426,     2] loss: 1.059\n",
      "[1427,     2] loss: 1.061\n",
      "[1428,     2] loss: 1.058\n",
      "[1429,     2] loss: 1.058\n",
      "[1430,     2] loss: 1.059\n",
      "[1431,     2] loss: 1.061\n",
      "[1432,     2] loss: 1.061\n",
      "[1433,     2] loss: 1.059\n",
      "[1434,     2] loss: 1.059\n",
      "[1435,     2] loss: 1.059\n",
      "[1436,     2] loss: 1.059\n",
      "[1437,     2] loss: 1.061\n",
      "[1438,     2] loss: 1.058\n",
      "[1439,     2] loss: 1.057\n",
      "[1440,     2] loss: 1.058\n",
      "[1441,     2] loss: 1.059\n",
      "[1442,     2] loss: 1.059\n",
      "[1443,     2] loss: 1.058\n",
      "[1444,     2] loss: 1.058\n",
      "[1445,     2] loss: 1.059\n",
      "[1446,     2] loss: 1.059\n",
      "[1447,     2] loss: 1.059\n",
      "[1448,     2] loss: 1.061\n",
      "[1449,     2] loss: 1.061\n",
      "[1450,     2] loss: 1.060\n",
      "[1451,     2] loss: 1.058\n",
      "[1452,     2] loss: 1.060\n",
      "[1453,     2] loss: 1.059\n",
      "[1454,     2] loss: 1.059\n",
      "[1455,     2] loss: 1.058\n",
      "[1456,     2] loss: 1.058\n",
      "[1457,     2] loss: 1.058\n",
      "[1458,     2] loss: 1.058\n",
      "[1459,     2] loss: 1.058\n",
      "[1460,     2] loss: 1.058\n",
      "[1461,     2] loss: 1.062\n",
      "[1462,     2] loss: 1.058\n",
      "[1463,     2] loss: 1.058\n",
      "[1464,     2] loss: 1.061\n",
      "[1465,     2] loss: 1.061\n",
      "[1466,     2] loss: 1.058\n",
      "[1467,     2] loss: 1.060\n",
      "[1468,     2] loss: 1.060\n",
      "[1469,     2] loss: 1.060\n",
      "[1470,     2] loss: 1.058\n",
      "[1471,     2] loss: 1.060\n",
      "[1472,     2] loss: 1.058\n",
      "[1473,     2] loss: 1.061\n",
      "[1474,     2] loss: 1.058\n",
      "[1475,     2] loss: 1.058\n",
      "[1476,     2] loss: 1.059\n",
      "[1477,     2] loss: 1.058\n",
      "[1478,     2] loss: 1.059\n",
      "[1479,     2] loss: 1.062\n",
      "[1480,     2] loss: 1.060\n",
      "[1481,     2] loss: 1.058\n",
      "[1482,     2] loss: 1.058\n",
      "[1483,     2] loss: 1.058\n",
      "[1484,     2] loss: 1.058\n",
      "[1485,     2] loss: 1.058\n",
      "[1486,     2] loss: 1.058\n",
      "[1487,     2] loss: 1.058\n",
      "[1488,     2] loss: 1.058\n",
      "[1489,     2] loss: 1.058\n",
      "[1490,     2] loss: 1.057\n",
      "[1491,     2] loss: 1.057\n",
      "[1492,     2] loss: 1.058\n",
      "[1493,     2] loss: 1.058\n",
      "[1494,     2] loss: 1.059\n",
      "[1495,     2] loss: 1.059\n",
      "[1496,     2] loss: 1.057\n",
      "[1497,     2] loss: 1.060\n",
      "[1498,     2] loss: 1.059\n",
      "[1499,     2] loss: 1.061\n",
      "[1500,     2] loss: 1.059\n",
      "[1501,     2] loss: 1.058\n",
      "[1502,     2] loss: 1.063\n",
      "[1503,     2] loss: 1.058\n",
      "[1504,     2] loss: 1.060\n",
      "[1505,     2] loss: 1.058\n",
      "[1506,     2] loss: 1.058\n",
      "[1507,     2] loss: 1.059\n",
      "[1508,     2] loss: 1.061\n",
      "[1509,     2] loss: 1.058\n",
      "[1510,     2] loss: 1.058\n",
      "[1511,     2] loss: 1.057\n",
      "[1512,     2] loss: 1.063\n",
      "[1513,     2] loss: 1.061\n",
      "[1514,     2] loss: 1.058\n",
      "[1515,     2] loss: 1.057\n",
      "[1516,     2] loss: 1.058\n",
      "[1517,     2] loss: 1.058\n",
      "[1518,     2] loss: 1.058\n",
      "[1519,     2] loss: 1.059\n",
      "[1520,     2] loss: 1.059\n",
      "[1521,     2] loss: 1.059\n",
      "[1522,     2] loss: 1.058\n",
      "[1523,     2] loss: 1.058\n",
      "[1524,     2] loss: 1.059\n",
      "[1525,     2] loss: 1.057\n",
      "[1526,     2] loss: 1.058\n",
      "[1527,     2] loss: 1.060\n",
      "[1528,     2] loss: 1.057\n",
      "[1529,     2] loss: 1.057\n",
      "[1530,     2] loss: 1.058\n",
      "[1531,     2] loss: 1.057\n",
      "[1532,     2] loss: 1.059\n",
      "[1533,     2] loss: 1.059\n",
      "[1534,     2] loss: 1.059\n",
      "[1535,     2] loss: 1.060\n",
      "[1536,     2] loss: 1.057\n",
      "[1537,     2] loss: 1.058\n",
      "[1538,     2] loss: 1.059\n",
      "[1539,     2] loss: 1.059\n",
      "[1540,     2] loss: 1.057\n",
      "[1541,     2] loss: 1.057\n",
      "[1542,     2] loss: 1.057\n",
      "[1543,     2] loss: 1.057\n",
      "[1544,     2] loss: 1.060\n",
      "[1545,     2] loss: 1.061\n",
      "[1546,     2] loss: 1.058\n",
      "[1547,     2] loss: 1.057\n",
      "[1548,     2] loss: 1.057\n",
      "[1549,     2] loss: 1.058\n",
      "[1550,     2] loss: 1.057\n",
      "[1551,     2] loss: 1.057\n",
      "[1552,     2] loss: 1.058\n",
      "[1553,     2] loss: 1.062\n",
      "[1554,     2] loss: 1.058\n",
      "[1555,     2] loss: 1.057\n",
      "[1556,     2] loss: 1.061\n",
      "[1557,     2] loss: 1.059\n",
      "[1558,     2] loss: 1.058\n",
      "[1559,     2] loss: 1.057\n",
      "[1560,     2] loss: 1.057\n",
      "[1561,     2] loss: 1.058\n",
      "[1562,     2] loss: 1.057\n",
      "[1563,     2] loss: 1.058\n",
      "[1564,     2] loss: 1.057\n",
      "[1565,     2] loss: 1.057\n",
      "[1566,     2] loss: 1.059\n",
      "[1567,     2] loss: 1.057\n",
      "[1568,     2] loss: 1.058\n",
      "[1569,     2] loss: 1.060\n",
      "[1570,     2] loss: 1.057\n",
      "[1571,     2] loss: 1.056\n",
      "[1572,     2] loss: 1.058\n",
      "[1573,     2] loss: 1.057\n",
      "[1574,     2] loss: 1.059\n",
      "[1575,     2] loss: 1.058\n",
      "[1576,     2] loss: 1.057\n",
      "[1577,     2] loss: 1.057\n",
      "[1578,     2] loss: 1.059\n",
      "[1579,     2] loss: 1.059\n",
      "[1580,     2] loss: 1.057\n",
      "[1581,     2] loss: 1.058\n",
      "[1582,     2] loss: 1.057\n",
      "[1583,     2] loss: 1.058\n",
      "[1584,     2] loss: 1.057\n",
      "[1585,     2] loss: 1.057\n",
      "[1586,     2] loss: 1.057\n",
      "[1587,     2] loss: 1.058\n",
      "[1588,     2] loss: 1.057\n",
      "[1589,     2] loss: 1.056\n",
      "[1590,     2] loss: 1.056\n",
      "[1591,     2] loss: 1.057\n",
      "[1592,     2] loss: 1.058\n",
      "[1593,     2] loss: 1.058\n",
      "[1594,     2] loss: 1.057\n",
      "[1595,     2] loss: 1.057\n",
      "[1596,     2] loss: 1.057\n",
      "[1597,     2] loss: 1.057\n",
      "[1598,     2] loss: 1.057\n",
      "[1599,     2] loss: 1.056\n",
      "[1600,     2] loss: 1.057\n",
      "[1601,     2] loss: 1.057\n",
      "[1602,     2] loss: 1.056\n",
      "[1603,     2] loss: 1.058\n",
      "[1604,     2] loss: 1.057\n",
      "[1605,     2] loss: 1.057\n",
      "[1606,     2] loss: 1.057\n",
      "[1607,     2] loss: 1.062\n",
      "[1608,     2] loss: 1.058\n",
      "[1609,     2] loss: 1.056\n",
      "[1610,     2] loss: 1.057\n",
      "[1611,     2] loss: 1.058\n",
      "[1612,     2] loss: 1.057\n",
      "[1613,     2] loss: 1.059\n",
      "[1614,     2] loss: 1.062\n",
      "[1615,     2] loss: 1.058\n",
      "[1616,     2] loss: 1.056\n",
      "[1617,     2] loss: 1.057\n",
      "[1618,     2] loss: 1.057\n",
      "[1619,     2] loss: 1.057\n",
      "[1620,     2] loss: 1.057\n",
      "[1621,     2] loss: 1.057\n",
      "[1622,     2] loss: 1.057\n",
      "[1623,     2] loss: 1.057\n",
      "[1624,     2] loss: 1.057\n",
      "[1625,     2] loss: 1.057\n",
      "[1626,     2] loss: 1.057\n",
      "[1627,     2] loss: 1.056\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1628,     2] loss: 1.057\n",
      "[1629,     2] loss: 1.057\n",
      "[1630,     2] loss: 1.061\n",
      "[1631,     2] loss: 1.057\n",
      "[1632,     2] loss: 1.058\n",
      "[1633,     2] loss: 1.057\n",
      "[1634,     2] loss: 1.058\n",
      "[1635,     2] loss: 1.058\n",
      "[1636,     2] loss: 1.059\n",
      "[1637,     2] loss: 1.057\n",
      "[1638,     2] loss: 1.057\n",
      "[1639,     2] loss: 1.057\n",
      "[1640,     2] loss: 1.057\n",
      "[1641,     2] loss: 1.056\n",
      "[1642,     2] loss: 1.056\n",
      "[1643,     2] loss: 1.060\n",
      "[1644,     2] loss: 1.057\n",
      "[1645,     2] loss: 1.058\n",
      "[1646,     2] loss: 1.057\n",
      "[1647,     2] loss: 1.056\n",
      "[1648,     2] loss: 1.059\n",
      "[1649,     2] loss: 1.057\n",
      "[1650,     2] loss: 1.057\n",
      "[1651,     2] loss: 1.056\n",
      "[1652,     2] loss: 1.057\n",
      "[1653,     2] loss: 1.056\n",
      "[1654,     2] loss: 1.056\n",
      "[1655,     2] loss: 1.060\n",
      "[1656,     2] loss: 1.057\n",
      "[1657,     2] loss: 1.056\n",
      "[1658,     2] loss: 1.057\n",
      "[1659,     2] loss: 1.057\n",
      "[1660,     2] loss: 1.056\n",
      "[1661,     2] loss: 1.057\n",
      "[1662,     2] loss: 1.058\n",
      "[1663,     2] loss: 1.056\n",
      "[1664,     2] loss: 1.059\n",
      "[1665,     2] loss: 1.057\n",
      "[1666,     2] loss: 1.056\n",
      "[1667,     2] loss: 1.057\n",
      "[1668,     2] loss: 1.059\n",
      "[1669,     2] loss: 1.057\n",
      "[1670,     2] loss: 1.057\n",
      "[1671,     2] loss: 1.056\n",
      "[1672,     2] loss: 1.057\n",
      "[1673,     2] loss: 1.057\n",
      "[1674,     2] loss: 1.057\n",
      "[1675,     2] loss: 1.057\n",
      "[1676,     2] loss: 1.056\n",
      "[1677,     2] loss: 1.058\n",
      "[1678,     2] loss: 1.056\n",
      "[1679,     2] loss: 1.057\n",
      "[1680,     2] loss: 1.057\n",
      "[1681,     2] loss: 1.057\n",
      "[1682,     2] loss: 1.059\n",
      "[1683,     2] loss: 1.058\n",
      "[1684,     2] loss: 1.061\n",
      "[1685,     2] loss: 1.056\n",
      "[1686,     2] loss: 1.057\n",
      "[1687,     2] loss: 1.058\n",
      "[1688,     2] loss: 1.057\n",
      "[1689,     2] loss: 1.058\n",
      "[1690,     2] loss: 1.061\n",
      "[1691,     2] loss: 1.058\n",
      "[1692,     2] loss: 1.056\n",
      "[1693,     2] loss: 1.056\n",
      "[1694,     2] loss: 1.058\n",
      "[1695,     2] loss: 1.056\n",
      "[1696,     2] loss: 1.057\n",
      "[1697,     2] loss: 1.057\n",
      "[1698,     2] loss: 1.057\n",
      "[1699,     2] loss: 1.057\n",
      "[1700,     2] loss: 1.056\n",
      "[1701,     2] loss: 1.056\n",
      "[1702,     2] loss: 1.057\n",
      "[1703,     2] loss: 1.056\n",
      "[1704,     2] loss: 1.058\n",
      "[1705,     2] loss: 1.057\n",
      "[1706,     2] loss: 1.057\n",
      "[1707,     2] loss: 1.056\n",
      "[1708,     2] loss: 1.059\n",
      "[1709,     2] loss: 1.057\n",
      "[1710,     2] loss: 1.057\n",
      "[1711,     2] loss: 1.057\n",
      "[1712,     2] loss: 1.057\n",
      "[1713,     2] loss: 1.056\n",
      "[1714,     2] loss: 1.058\n",
      "[1715,     2] loss: 1.056\n",
      "[1716,     2] loss: 1.057\n",
      "[1717,     2] loss: 1.056\n",
      "[1718,     2] loss: 1.058\n",
      "[1719,     2] loss: 1.057\n",
      "[1720,     2] loss: 1.056\n",
      "[1721,     2] loss: 1.056\n",
      "[1722,     2] loss: 1.056\n",
      "[1723,     2] loss: 1.056\n",
      "[1724,     2] loss: 1.058\n",
      "[1725,     2] loss: 1.058\n",
      "[1726,     2] loss: 1.060\n",
      "[1727,     2] loss: 1.057\n",
      "[1728,     2] loss: 1.056\n",
      "[1729,     2] loss: 1.057\n",
      "[1730,     2] loss: 1.058\n",
      "[1731,     2] loss: 1.057\n",
      "[1732,     2] loss: 1.057\n",
      "[1733,     2] loss: 1.056\n",
      "[1734,     2] loss: 1.057\n",
      "[1735,     2] loss: 1.057\n",
      "[1736,     2] loss: 1.056\n",
      "[1737,     2] loss: 1.055\n",
      "[1738,     2] loss: 1.059\n",
      "[1739,     2] loss: 1.057\n",
      "[1740,     2] loss: 1.059\n",
      "[1741,     2] loss: 1.058\n",
      "[1742,     2] loss: 1.056\n",
      "[1743,     2] loss: 1.056\n",
      "[1744,     2] loss: 1.057\n",
      "[1745,     2] loss: 1.056\n",
      "[1746,     2] loss: 1.056\n",
      "[1747,     2] loss: 1.056\n",
      "[1748,     2] loss: 1.058\n",
      "[1749,     2] loss: 1.056\n",
      "[1750,     2] loss: 1.061\n",
      "[1751,     2] loss: 1.060\n",
      "[1752,     2] loss: 1.056\n",
      "[1753,     2] loss: 1.058\n",
      "[1754,     2] loss: 1.055\n",
      "[1755,     2] loss: 1.055\n",
      "[1756,     2] loss: 1.056\n",
      "[1757,     2] loss: 1.056\n",
      "[1758,     2] loss: 1.057\n",
      "[1759,     2] loss: 1.056\n",
      "[1760,     2] loss: 1.059\n",
      "[1761,     2] loss: 1.058\n",
      "[1762,     2] loss: 1.056\n",
      "[1763,     2] loss: 1.056\n",
      "[1764,     2] loss: 1.056\n",
      "[1765,     2] loss: 1.056\n",
      "[1766,     2] loss: 1.055\n",
      "[1767,     2] loss: 1.059\n",
      "[1768,     2] loss: 1.057\n",
      "[1769,     2] loss: 1.056\n",
      "[1770,     2] loss: 1.056\n",
      "[1771,     2] loss: 1.057\n",
      "[1772,     2] loss: 1.056\n",
      "[1773,     2] loss: 1.058\n",
      "[1774,     2] loss: 1.057\n",
      "[1775,     2] loss: 1.057\n",
      "[1776,     2] loss: 1.056\n",
      "[1777,     2] loss: 1.055\n",
      "[1778,     2] loss: 1.056\n",
      "[1779,     2] loss: 1.055\n",
      "[1780,     2] loss: 1.055\n",
      "[1781,     2] loss: 1.056\n",
      "[1782,     2] loss: 1.057\n",
      "[1783,     2] loss: 1.055\n",
      "[1784,     2] loss: 1.056\n",
      "[1785,     2] loss: 1.056\n",
      "[1786,     2] loss: 1.057\n",
      "[1787,     2] loss: 1.056\n",
      "[1788,     2] loss: 1.056\n",
      "[1789,     2] loss: 1.056\n",
      "[1790,     2] loss: 1.055\n",
      "[1791,     2] loss: 1.057\n",
      "[1792,     2] loss: 1.055\n",
      "[1793,     2] loss: 1.055\n",
      "[1794,     2] loss: 1.058\n",
      "[1795,     2] loss: 1.059\n",
      "[1796,     2] loss: 1.057\n",
      "[1797,     2] loss: 1.055\n",
      "[1798,     2] loss: 1.056\n",
      "[1799,     2] loss: 1.057\n",
      "[1800,     2] loss: 1.056\n",
      "[1801,     2] loss: 1.056\n",
      "[1802,     2] loss: 1.056\n",
      "[1803,     2] loss: 1.058\n",
      "[1804,     2] loss: 1.056\n",
      "[1805,     2] loss: 1.060\n",
      "[1806,     2] loss: 1.056\n",
      "[1807,     2] loss: 1.057\n",
      "[1808,     2] loss: 1.058\n",
      "[1809,     2] loss: 1.056\n",
      "[1810,     2] loss: 1.056\n",
      "[1811,     2] loss: 1.056\n",
      "[1812,     2] loss: 1.059\n",
      "[1813,     2] loss: 1.056\n",
      "[1814,     2] loss: 1.056\n",
      "[1815,     2] loss: 1.056\n",
      "[1816,     2] loss: 1.057\n",
      "[1817,     2] loss: 1.058\n",
      "[1818,     2] loss: 1.056\n",
      "[1819,     2] loss: 1.056\n",
      "[1820,     2] loss: 1.056\n",
      "[1821,     2] loss: 1.055\n",
      "[1822,     2] loss: 1.055\n",
      "[1823,     2] loss: 1.059\n",
      "[1824,     2] loss: 1.056\n",
      "[1825,     2] loss: 1.056\n",
      "[1826,     2] loss: 1.056\n",
      "[1827,     2] loss: 1.056\n",
      "[1828,     2] loss: 1.057\n",
      "[1829,     2] loss: 1.055\n",
      "[1830,     2] loss: 1.059\n",
      "[1831,     2] loss: 1.056\n",
      "[1832,     2] loss: 1.058\n",
      "[1833,     2] loss: 1.057\n",
      "[1834,     2] loss: 1.056\n",
      "[1835,     2] loss: 1.057\n",
      "[1836,     2] loss: 1.055\n",
      "[1837,     2] loss: 1.056\n",
      "[1838,     2] loss: 1.056\n",
      "[1839,     2] loss: 1.056\n",
      "[1840,     2] loss: 1.056\n",
      "[1841,     2] loss: 1.058\n",
      "[1842,     2] loss: 1.056\n",
      "[1843,     2] loss: 1.056\n",
      "[1844,     2] loss: 1.056\n",
      "[1845,     2] loss: 1.055\n",
      "[1846,     2] loss: 1.056\n",
      "[1847,     2] loss: 1.055\n",
      "[1848,     2] loss: 1.056\n",
      "[1849,     2] loss: 1.056\n",
      "[1850,     2] loss: 1.058\n",
      "[1851,     2] loss: 1.057\n",
      "[1852,     2] loss: 1.055\n",
      "[1853,     2] loss: 1.055\n",
      "[1854,     2] loss: 1.055\n",
      "[1855,     2] loss: 1.057\n",
      "[1856,     2] loss: 1.057\n",
      "[1857,     2] loss: 1.056\n",
      "[1858,     2] loss: 1.058\n",
      "[1859,     2] loss: 1.056\n",
      "[1860,     2] loss: 1.055\n",
      "[1861,     2] loss: 1.058\n",
      "[1862,     2] loss: 1.055\n",
      "[1863,     2] loss: 1.059\n",
      "[1864,     2] loss: 1.056\n",
      "[1865,     2] loss: 1.057\n",
      "[1866,     2] loss: 1.056\n",
      "[1867,     2] loss: 1.056\n",
      "[1868,     2] loss: 1.056\n",
      "[1869,     2] loss: 1.056\n",
      "[1870,     2] loss: 1.056\n",
      "[1871,     2] loss: 1.056\n",
      "[1872,     2] loss: 1.056\n",
      "[1873,     2] loss: 1.056\n",
      "[1874,     2] loss: 1.056\n",
      "[1875,     2] loss: 1.057\n",
      "[1876,     2] loss: 1.057\n",
      "[1877,     2] loss: 1.056\n",
      "[1878,     2] loss: 1.057\n",
      "[1879,     2] loss: 1.055\n",
      "[1880,     2] loss: 1.056\n",
      "[1881,     2] loss: 1.057\n",
      "[1882,     2] loss: 1.056\n",
      "[1883,     2] loss: 1.057\n",
      "[1884,     2] loss: 1.056\n",
      "[1885,     2] loss: 1.056\n",
      "[1886,     2] loss: 1.058\n",
      "[1887,     2] loss: 1.055\n",
      "[1888,     2] loss: 1.055\n",
      "[1889,     2] loss: 1.055\n",
      "[1890,     2] loss: 1.055\n",
      "[1891,     2] loss: 1.055\n",
      "[1892,     2] loss: 1.055\n",
      "[1893,     2] loss: 1.056\n",
      "[1894,     2] loss: 1.059\n",
      "[1895,     2] loss: 1.057\n",
      "[1896,     2] loss: 1.056\n",
      "[1897,     2] loss: 1.058\n",
      "[1898,     2] loss: 1.056\n",
      "[1899,     2] loss: 1.055\n",
      "[1900,     2] loss: 1.056\n",
      "[1901,     2] loss: 1.055\n",
      "[1902,     2] loss: 1.057\n",
      "[1903,     2] loss: 1.056\n",
      "[1904,     2] loss: 1.056\n",
      "[1905,     2] loss: 1.055\n",
      "[1906,     2] loss: 1.057\n",
      "[1907,     2] loss: 1.058\n",
      "[1908,     2] loss: 1.055\n",
      "[1909,     2] loss: 1.056\n",
      "[1910,     2] loss: 1.060\n",
      "[1911,     2] loss: 1.056\n",
      "[1912,     2] loss: 1.055\n",
      "[1913,     2] loss: 1.056\n",
      "[1914,     2] loss: 1.058\n",
      "[1915,     2] loss: 1.055\n",
      "[1916,     2] loss: 1.055\n",
      "[1917,     2] loss: 1.056\n",
      "[1918,     2] loss: 1.056\n",
      "[1919,     2] loss: 1.058\n",
      "[1920,     2] loss: 1.056\n",
      "[1921,     2] loss: 1.055\n",
      "[1922,     2] loss: 1.055\n",
      "[1923,     2] loss: 1.056\n",
      "[1924,     2] loss: 1.056\n",
      "[1925,     2] loss: 1.057\n",
      "[1926,     2] loss: 1.056\n",
      "[1927,     2] loss: 1.056\n",
      "[1928,     2] loss: 1.059\n",
      "[1929,     2] loss: 1.057\n",
      "[1930,     2] loss: 1.056\n",
      "[1931,     2] loss: 1.057\n",
      "[1932,     2] loss: 1.058\n",
      "[1933,     2] loss: 1.055\n",
      "[1934,     2] loss: 1.055\n",
      "[1935,     2] loss: 1.056\n",
      "[1936,     2] loss: 1.057\n",
      "[1937,     2] loss: 1.056\n",
      "[1938,     2] loss: 1.061\n",
      "[1939,     2] loss: 1.055\n",
      "[1940,     2] loss: 1.055\n",
      "[1941,     2] loss: 1.056\n",
      "[1942,     2] loss: 1.056\n",
      "[1943,     2] loss: 1.055\n",
      "[1944,     2] loss: 1.056\n",
      "[1945,     2] loss: 1.056\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1946,     2] loss: 1.057\n",
      "[1947,     2] loss: 1.055\n",
      "[1948,     2] loss: 1.057\n",
      "[1949,     2] loss: 1.057\n",
      "[1950,     2] loss: 1.055\n",
      "[1951,     2] loss: 1.057\n",
      "[1952,     2] loss: 1.056\n",
      "[1953,     2] loss: 1.060\n",
      "[1954,     2] loss: 1.055\n",
      "[1955,     2] loss: 1.058\n",
      "[1956,     2] loss: 1.058\n",
      "[1957,     2] loss: 1.058\n",
      "[1958,     2] loss: 1.055\n",
      "[1959,     2] loss: 1.056\n",
      "[1960,     2] loss: 1.057\n",
      "[1961,     2] loss: 1.055\n",
      "[1962,     2] loss: 1.057\n",
      "[1963,     2] loss: 1.057\n",
      "[1964,     2] loss: 1.056\n",
      "[1965,     2] loss: 1.056\n",
      "[1966,     2] loss: 1.056\n",
      "[1967,     2] loss: 1.056\n",
      "[1968,     2] loss: 1.055\n",
      "[1969,     2] loss: 1.055\n",
      "[1970,     2] loss: 1.056\n",
      "[1971,     2] loss: 1.060\n",
      "[1972,     2] loss: 1.057\n",
      "[1973,     2] loss: 1.055\n",
      "[1974,     2] loss: 1.056\n",
      "[1975,     2] loss: 1.057\n",
      "[1976,     2] loss: 1.057\n",
      "[1977,     2] loss: 1.058\n",
      "[1978,     2] loss: 1.056\n",
      "[1979,     2] loss: 1.055\n",
      "[1980,     2] loss: 1.056\n",
      "[1981,     2] loss: 1.056\n",
      "[1982,     2] loss: 1.058\n",
      "[1983,     2] loss: 1.055\n",
      "[1984,     2] loss: 1.056\n",
      "[1985,     2] loss: 1.057\n",
      "[1986,     2] loss: 1.056\n",
      "[1987,     2] loss: 1.055\n",
      "[1988,     2] loss: 1.055\n",
      "[1989,     2] loss: 1.057\n",
      "[1990,     2] loss: 1.056\n",
      "[1991,     2] loss: 1.058\n",
      "[1992,     2] loss: 1.055\n",
      "[1993,     2] loss: 1.058\n",
      "[1994,     2] loss: 1.056\n",
      "[1995,     2] loss: 1.056\n",
      "[1996,     2] loss: 1.055\n",
      "[1997,     2] loss: 1.057\n",
      "[1998,     2] loss: 1.056\n",
      "[1999,     2] loss: 1.058\n",
      "[2000,     2] loss: 1.055\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "nos_epochs = 2000\n",
    "focus_true_pred_true =0\n",
    "focus_false_pred_true =0\n",
    "focus_true_pred_false =0\n",
    "focus_false_pred_false =0\n",
    "\n",
    "argmax_more_than_half = 0\n",
    "argmax_less_than_half =0\n",
    "\n",
    "\n",
    "for epoch in range(nos_epochs):  # loop over the dataset multiple times\n",
    "\n",
    "  focus_true_pred_true =0\n",
    "  focus_false_pred_true =0\n",
    "  focus_true_pred_false =0\n",
    "  focus_false_pred_false =0\n",
    "  \n",
    "  argmax_more_than_half = 0\n",
    "  argmax_less_than_half =0\n",
    "  \n",
    "  running_loss = 0.0\n",
    "  epoch_loss = []\n",
    "  cnt=0\n",
    "\n",
    "  iteration = desired_num // batch\n",
    "  \n",
    "  #training data set\n",
    "  \n",
    "  for i, data in  enumerate(train_loader):\n",
    "    inputs , labels , fore_idx = data\n",
    "    batch = inputs.size(0)\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    inputs = inputs.double()\n",
    "    # zero the parameter gradients\n",
    "    \n",
    "    optimizer_focus.zero_grad()\n",
    "    optimizer_classify.zero_grad()\n",
    "    \n",
    "    alphas, avg_images = focus_net(inputs)\n",
    "    outputs = classify(avg_images)\n",
    "\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "#     print(outputs)\n",
    "#     print(outputs.shape,labels.shape , torch.argmax(outputs, dim=1))\n",
    "\n",
    "    loss = my_cross_entropy(outputs, labels,alphas) \n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    \n",
    "    optimizer_focus.step()\n",
    "    optimizer_classify.step()\n",
    "\n",
    "    running_loss += loss.item()\n",
    "    mini = 2\n",
    "    if cnt % mini == mini-1:    # print every 40 mini-batches\n",
    "      print('[%d, %5d] loss: %.3f' %(epoch + 1, cnt + 1, running_loss / mini))\n",
    "      epoch_loss.append(running_loss/mini)\n",
    "      running_loss = 0.0\n",
    "    cnt=cnt+1\n",
    "\n",
    "  if(np.mean(epoch_loss) <= 0.01):\n",
    "      break;\n",
    "  #plot_attended_data(train_loader,focus_net,epoch)\n",
    "\n",
    "    \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "3xPsiBtU-GDn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('fc1.weight', Parameter containing:\n",
      "tensor([[-0.5742, -0.0939],\n",
      "        [ 3.8761,  2.3625],\n",
      "        [-0.9264,  1.3361],\n",
      "        [ 4.1924,  2.5062],\n",
      "        [ 4.7235,  3.5504],\n",
      "        [ 2.3281,  1.3147],\n",
      "        [-0.2786,  0.0543],\n",
      "        [ 3.0185,  1.7554],\n",
      "        [-1.2845,  1.5973],\n",
      "        [ 4.5624,  3.7230],\n",
      "        [ 0.8749, -2.4829],\n",
      "        [ 4.0452,  2.7140],\n",
      "        [-1.2635,  1.6756],\n",
      "        [ 3.4489,  2.2617],\n",
      "        [-1.0524,  1.3988],\n",
      "        [-0.2171, -0.6286],\n",
      "        [ 3.0786,  1.7339],\n",
      "        [ 1.2367, -3.5337],\n",
      "        [-0.2618,  0.9580],\n",
      "        [ 0.9883, -2.8456],\n",
      "        [-0.3838,  0.0869],\n",
      "        [ 5.1037,  3.6930],\n",
      "        [-0.5888,  1.1038],\n",
      "        [ 0.7932, -2.2577],\n",
      "        [ 6.0361,  4.7597],\n",
      "        [-0.3549, -0.5545],\n",
      "        [-0.2567, -0.6760],\n",
      "        [ 2.2260,  1.8498],\n",
      "        [ 4.4250,  2.8116],\n",
      "        [-0.5942,  0.9746],\n",
      "        [ 0.3488, -2.1302],\n",
      "        [ 5.5903,  4.1984],\n",
      "        [ 4.9471,  3.5559],\n",
      "        [-0.9447,  0.6442],\n",
      "        [-1.1424,  1.3947],\n",
      "        [ 4.9006,  3.4152],\n",
      "        [ 2.9356,  1.8088],\n",
      "        [ 0.3929, -2.5030],\n",
      "        [-0.3787, -0.1418],\n",
      "        [-0.2239, -0.7180],\n",
      "        [-0.4864, -0.5067],\n",
      "        [ 3.3371,  1.9277],\n",
      "        [ 9.7446,  5.7586],\n",
      "        [-0.2343,  0.8370],\n",
      "        [-0.2449, -0.1498],\n",
      "        [ 4.1101,  2.5672],\n",
      "        [ 6.1319,  4.4488],\n",
      "        [ 0.2672, -1.6551],\n",
      "        [ 3.6251,  2.1042],\n",
      "        [ 0.9719, -2.7167]], dtype=torch.float64, requires_grad=True))\n",
      "('fc2.weight', Parameter containing:\n",
      "tensor([[-1.3345e-01,  1.2196e+01, -1.8032e+00,  3.4402e+00,  5.1324e+00,\n",
      "          3.9761e+00,  2.1024e-01,  4.6736e+00, -1.4034e+00,  1.2116e+01,\n",
      "         -2.0150e+00,  5.6973e+00, -1.4040e+00,  6.2193e+00, -1.6253e+00,\n",
      "         -4.4723e-01,  3.9846e+00, -2.6663e+00, -4.8314e-02, -2.4276e+00,\n",
      "         -9.5590e-02,  1.3282e+01, -3.4320e-01, -2.4668e+00,  2.8743e+01,\n",
      "         -6.4847e-02, -3.5662e-01,  2.9277e+00,  1.4075e+01, -1.5366e+00,\n",
      "         -8.3184e-01,  2.3342e+01,  9.9263e+00, -9.1872e-01, -1.7462e+00,\n",
      "          1.8041e+01,  7.9119e+00, -7.8251e-01, -1.7178e-03, -1.9083e-01,\n",
      "         -1.0308e+00,  2.9451e+00,  2.8614e+01, -4.2235e-01,  2.6421e-02,\n",
      "          4.1188e+00,  1.1650e+01, -1.0161e+00,  3.6700e+00, -2.1410e+00]],\n",
      "       dtype=torch.float64, requires_grad=True))\n"
     ]
    }
   ],
   "source": [
    "for param in focus_net.named_parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "jhvhkEAyeRpt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the train images: 45.000000 %\n",
      "total correct 45\n",
      "total train set images 100\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "focus_net.eval()\n",
    "classify.eval()\n",
    "with torch.no_grad():\n",
    "  for data in train_loader:\n",
    "    inputs, labels , fore_idx = data\n",
    "    inputs = inputs.double()\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    alphas, avg_images = focus_net(inputs)\n",
    "    outputs = classify(avg_images)\n",
    "    #print(outputs.shape)\n",
    "\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the train images: %f %%' % ( 100 * correct / total))\n",
    "print(\"total correct\", correct)\n",
    "print(\"total train set images\", total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "OKcmpKwGeS8M"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 35.400000 %\n",
      "total correct 354\n",
      "total train set images 1000\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "  for data in test_loader:\n",
    "    inputs, labels , fore_idx = data\n",
    "    inputs = inputs.double()\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    alphas, avg_images = focus_net(inputs)\n",
    "    outputs = classify(avg_images)\n",
    "\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the test images: %f %%' % ( 100 * correct / total))\n",
    "print(\"total correct\", correct)\n",
    "print(\"total train set images\", total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "hard_attention_1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0vlCAi2JLSzD"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "PiPNZm1iTgHy"
      },
      "outputs": [],
      "source": [
        "# path=\"/content/drive/MyDrive/Research/Hard_Attention/dataset_2/m_5_size_100/run_\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "SrZgZMlK-GDe"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2_J4Rw2r0SQ",
        "outputId": "0812e67a-9037-4b76-9c20-a6225cce571b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "from tqdm import tqdm\n",
        "%matplotlib inline\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn import functional as F\n",
        "from tqdm import tqdm as tqdm\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "-Dmy2iPWlgnc"
      },
      "outputs": [],
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "writer = SummaryWriter('runs/hard_attention')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6fjud_Fr0Sa"
      },
      "source": [
        "# Generate dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqdXHO0Cr0Sd",
        "outputId": "ddb5bc60-b88e-4eae-9a4c-ad19f7f56fca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 500\n",
            "1 500\n",
            "2 500\n",
            "3 500\n",
            "4 500\n",
            "5 500\n",
            "6 500\n",
            "7 500\n",
            "8 500\n",
            "9 500\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(1)\n",
        "\n",
        "y = np.concatenate((np.zeros(500),np.ones(500),np.ones(500)*2,np.ones(500)*3,np.ones(500)*4,\n",
        "                    np.ones(500)*5,np.ones(500)*6,np.ones(500)*7,np.ones(500)*8,np.ones(500)*9))\n",
        "#y = np.random.randint(0,3,6000)\n",
        "idx= []\n",
        "for i in range(10):\n",
        "    print(i,sum(y==i))\n",
        "    idx.append(y==i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ddhXyODwr0Sk"
      },
      "outputs": [],
      "source": [
        "x = np.zeros((5000,2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "DyV3N2DIr0Sp"
      },
      "outputs": [],
      "source": [
        "np.random.seed(12)\n",
        "\n",
        "x[idx[0],:] = np.random.multivariate_normal(mean = [4,6.5],cov=[[0.01,0],[0,0.01]],size=sum(idx[0]))\n",
        "x[idx[1],:] = np.random.multivariate_normal(mean = [5.5,6],cov=[[0.01,0],[0,0.01]],size=sum(idx[1]))\n",
        "x[idx[2],:] = np.random.multivariate_normal(mean = [4.5,4.5],cov=[[0.01,0],[0,0.01]],size=sum(idx[2]))\n",
        "x[idx[3],:] = np.random.multivariate_normal(mean = [3,3.5],cov=[[0.01,0],[0,0.01]],size=sum(idx[3]))\n",
        "x[idx[4],:] = np.random.multivariate_normal(mean = [2.5,5.5],cov=[[0.01,0],[0,0.01]],size=sum(idx[4]))\n",
        "x[idx[5],:] = np.random.multivariate_normal(mean = [3.5,8],cov=[[0.01,0],[0,0.01]],size=sum(idx[5]))\n",
        "x[idx[6],:] = np.random.multivariate_normal(mean = [5.5,8],cov=[[0.01,0],[0,0.01]],size=sum(idx[6]))\n",
        "x[idx[7],:] = np.random.multivariate_normal(mean = [7,6.5],cov=[[0.01,0],[0,0.01]],size=sum(idx[7]))\n",
        "x[idx[8],:] = np.random.multivariate_normal(mean = [6.5,4.5],cov=[[0.01,0],[0,0.01]],size=sum(idx[8]))\n",
        "x[idx[9],:] = np.random.multivariate_normal(mean = [5,3],cov=[[0.01,0],[0,0.01]],size=sum(idx[9]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qh1mDScsU07I",
        "outputId": "f210123a-f577-4671-c04e-7e7667fc2ee2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([4.04729858, 6.43185741]), array([4.53008447, 4.5079931 ]))"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "x[idx[0]][0], x[idx[2]][5] "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Vr5ErQ_wSrV",
        "outputId": "740caca4-4051-4ebf-f429-325334c7cc7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5000, 2) (5000,)\n"
          ]
        }
      ],
      "source": [
        "print(x.shape,y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "NG-3RpffwU_i"
      },
      "outputs": [],
      "source": [
        "idx= []\n",
        "for i in range(10):\n",
        "  idx.append(y==i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "id": "hJ8Jm7YUr0St",
        "outputId": "8a516d3b-b23a-49de-93c4-bee6a515c0b8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f791aa91d10>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbcAAAD4CAYAAABv7qjmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXyU1b0/8M+ZmUwSSMhCAglBkqAQwiomKlYpldyKyqIFFby0pb21XKu9KtpWFERErWjhSpF6W2sX+quCCAhGVGQTwQol7JAQwMiShCGJWcg+2/n9MZmQSWaSmWSePE9mPu/XKy/IM8vzDcZ8c875nvMVUkoQEREFEp3aARAREfkbkxsREQUcJjciIgo4TG5ERBRwmNyIiCjgGJR407i4OJmSkqLEWxMRBaSDBw+WSSnj1Y4jUCiS3FJSUpCTk6PEWxMRBSQhxHm1YwgknJYkIqKAw+RGREQBh8mNiIgCDpMbEREFHCY3IiIKOIpUSxJR59UeLsGVredgq2yEPjoUfSaloPfYfmqHRdSjMLkRaUjt4RJUbjwDabEDAGyVjajceAYAmOCIfMBpSSINubL1XHNic5IWO65sPadOQEQ9FJMbkYbYKht9uk5E7nFasofJ27MLe9b+A9XfliG0dwSEABpqahDZNw7jZ/0Y6eNvVztE6gJ9dKjbRKbrxf9ViXzB/2N6kLw9u/DZW6tgNTt++DXWVDc/Vl1Wis/eWgUATHA9VO3hEtgbrW4fs9dZUTh/DyAASMc1Ea5H9LTruBZH5IaQUvr9TTMzMyXPluy6lqO0yL5xqK++Amujd9NTQqfD6Kw78R8PPaJwlOQPrQtJfBEzM40JLgAIIQ5KKTPVjiNQcOSmUdvffhNHt33c/Hl1WalPr5d2e/PrmeDUU77pDOr2mxyjLQH0ujkBsfcOAeBa8t9yROaryg/PMrkRtcLkpjF5e3Zh5+q30FBd3fGTvXB028coOHwA1WWlEDodpN2OyLh4DB57o+N606iQ63X+V77pDOr2ma5ekEDdPhMspXWwFtdC1ttcHussl/chIgBMbprSerTmL85Rn7Tbmz9vPSrkep3/1e03ub1u+fqK3+9Ve7iEozeiFpjcNCJvzy5FEpu3rOZG7Fn7DyY3f/L/crZHFe/lo2rXeUTdnszTTYjA5Ka6vD278NmfV3ldKKKk6m/L1A4hsHRhHa0z7CUNqHgvv/lznm5CwYzJrRu1rn4cPPZGHN3+CaBAxWpnRPaNUzuEgBIyuI8iU5C+cJ5uwuRGwcar5CaEmAfgITh+Dz0O4KdSygYlAws0rfeotV73UpvBGIrxs36sdhgBofZwCaqyv4a9zv2ete7G000oGHV4/JYQIgnAYwAypZQjAegBzFI6sECzZ+0/mhOb1kTGxeOOub/kepsfOPeraSWxAY5TT4iCjbfTkgYA4UIIC4BeAIqVCykwaXY9SwhuA+iC1u1p7I3WTm3EVpKuL5MbBZ8OR25SyiIAywBcAHAJQJWU8jOlAws0ml3PkhIfr1qOtx79KfL27FI7mh7FOUpzTvvZKhs1uefM8vUVFM7fg8Jn9qB80xm1wyHqFt5MS8YAuAdAKoABAHoLIX7o5nlzhRA5Qoic0lLfTtMIBuNn/RgGo+tv0AZjKIRer1JErpx73ZjgvOeuPY2mNW0iZ4KjYOBNy5v/APCNlLJUSmkBsBHAd1o/SUr5lpQyU0qZGR8f7+84e7z08bfjjrm/RGRcPCAEIuPiMWJCFqRNO7/pO/e6kXd6aqGGp83lRIHEmzW3CwDGCSF6AagHkAWApyJ3Qvr4213Wtt569KcqRuOeZtcGNchTe5ru3t/mMy3HRuQn3qy57QewHsAhOLYB6AC8pXBcQUGLiUSza4Ma1GdSCkSI6/9CIkSHXjcntLmuKULtAIiU51W1pJTyeQDPKxxL0InsG+fzaf9K4l433zg3Rrs77qo2Oerqif8q0EeHInRYjOvBzU163ZygQkTUGQcPHuxnMBjeBjAS3i0jBRM7gBNWq/WhjIyMktYP8oQSFY2f9WN8vGq5avcPi4xESGgYOwN0Qe+x/dye/uG8Xjh/T7fHJEJ0LmdKemq5Q9pnMBjeTkhISI+Pj6/Q6XScUG7BbreL0tLS4SaT6W0A01o/zuSmovTxt2Pb23+ApUGdw14mzpnLZKYwj+tyShFA9PQhzYkt9t4hTGY920gmNvd0Op2Mj4+vMplMI90+3t0BkSuLSgcmh0ZEMrF1gz6TUrrtXiJEh5gH2JU7wOiY2Dxr+rdxm8eY3FSmSgGHEMj6ydzuv28Q6j22H0Ku7aP4ffTRoS4jNqJgx+SmMjUKOHQa2TgeLPr/fAx6jUtQpkpRB8TMTEPi/JuY2IhaYHJTWfr42zHm+3e3uW4whuLuXz6Fp977CHf/8qk2J5kIvR4hYWGduqfdauVm7W4We+8QDHxlPAYudXzEzEy7eqBxJ5OeCNcj5n5OQ9JV/9x3Pvaml7ePSp2/JeOml7eP+ue+87FK3OfJJ58csGjRov5KvLfT+vXr+6SkpIwcNGjQyGeffdbnEl8WlGjAfzz0CJLS0l16vbWsXHT+2fpxAC5tdABH0gvt1QsNNTXtbjXQ4h67YNKyytLXispe41jxSG39c9/52Bc/yk1utNp1AFBS3Wh88aPcZAD44bjkcnWj843VasW8efMGbd269fTgwYMtY8aMSZ8xY0ZlRkaG19V3TG4a0fr0El8e95QUAccpKO4SHDdra4fXFZUCLBghj1buOJPkTGxOjVa7buWOM0ldTW6rVq3qu3Llyv5CCKSnp9cPHjy4+Rt2+fLlcX/729/iLRaLSElJaVy/fv03kZGR9r/+9a8xr7zyygCdTicjIyNtOTk5+Tk5OWE//elPUy0Wi7Db7diwYcPXo0aNavPN//nnn/dOTk5uHD58uBkApk+fXr5+/frojIwMr8+OY3Lr4TpKiuNn/bjN6I6btbWlz6QUVG480+4hzCJEx4IRaldpdaPRl+veysnJCVu2bFniV199dSoxMdF6+fJl/auvvto8JTl79uyKp556qgwAHnvssQErV66MW7BgQcnSpUsTP/vss9OpqamWsrIyPQC88cYb8Y888sjlX/ziF+UNDQ3CanXf9/DixYvGpKQks/PzgQMHmvfv3x/hS9xccwtw7g5sZmNSbek9th+ipw9pXoPTR4ei17gEl8+Z2Kgj8ZGhZl+ue2vr1q19pk6dWpGYmGgFgP79+7uc9n7w4MHwjIyMtKFDhw7fsGFD35MnT4YBQGZmZs3s2bNTli9fHudMYrfcckvt8uXLExcsWJBw5swZY0REhGLbHDhyCwIdje5IfZ5OOiHy1mNZQ4parrkBQKhBZ38sa0iRkvedO3du6vr168/ecsst9StXruy7e/fuSAB49913L+zcubP3hx9+GJWRkTH84MGDuQ8//HD5+PHjaz/44IOoKVOmDHnjjTfOT5s2rbr1e15zzTXmoqKi5hFnYWGhy0jOGxy5EREFgB+OSy5/bsrw8/0iQ80CQL/IUPNzU4af7+p626RJk65kZ2fHmEwmPQBcvnzZpXS7rq5ON2jQIEtjY6NYu3Ztc3XmyZMnQydOnFi7YsWK4piYGGtBQYExNzfXmJ6e3rhw4cKSSZMmVR45ciTc3T0nTJhQe+7cubBTp04ZGxoaxMaNG2NnzJhR6UvcHLkREQWIH45LLvd3ZWRmZmbDU089dWn8+PHDdDqdHDlyZF1ycnLzKGr+/PnFN910U3psbKz1hhtuqKmpqdEDwLx58waeO3cuVEopbrvttivjxo2rX7hwYcK6dev6GgwGGR8fb3nxxRcvubtnSEgIli9ffuHOO+8carPZ8J//+Z9lmZmZPp1TKKT0/5RnZmamzMlhyzciIm8JIQ5KKTNbXjt69Oi5MWPGcN9OO44ePRo3ZsyYlNbXOS1JREQBh9OSRESkCpPJpP/e976X1vr6559/np+QkGBz9xpvMbkREZEqEhISbKdOncpV4r05LUlERAGHyY2IiAIOkxsREQUcJjciIgo4TG5ERIHiwF9isWzoKCyOzsCyoaNw4C89tp/b/fffnxIbGztmyJAhIzrzeiY3IqJAcOAvsdj6TDJqLhsBCdRcNmLrM8lKJTil/dd//VfZhx9+eKazr2dyIyIKBLtfTYK10fVnurVRh92vJnX1rVetWtV36NChw9PS0obfe++9qS0fW758edzIkSPT09LShk+aNOna6upqHQD89a9/jRkyZMiItLS04ZmZmWmAo33OqFGj0ocNGzZ86NChw48fPx7q6Z533XVXTXx8vPueOF5gciMiCgQ1Je77tnm67iVnP7fdu3efzs/Pz/3Tn/50oeXjs2fPrjhx4kRefn5+blpaWv3KlSvjAMDZzy0/Pz/3008/PQtc7ed26tSp3GPHjuWlpqZ2qR1Pe5jciIgCQUQ/94nC03Uv9dR+bh0mNyFEmhDiSIuPK0KIJ5QKiIiIOmHC00UwhLq2czeE2jHhacX7ua1aterC6dOnc59++unixkbH1Oi777574aWXXiq+ePGiMSMjY7jJZNI//PDD5Zs3bz4bHh5unzJlypAPP/wwUqm4OkxuUsp8KeX1UsrrAWQAqAPwgVIBERFRJ9z4s3JMeuU8IvqbAQFE9Ddj0ivncePPelw/N3/w9WzJLABfSynPKxEMERF1wY0/K+9qMmtNjX5uADB16tTUffv2RVZUVBj69+8/ev78+cXz5s3zuv2PT/3chBB/BXBISrnKzWNzAcwFgEGDBmWcP8/8R0TkLfZz65wu93MTQhgBTAPwvrvHpZRvSSkzpZSZ8fHxnQ6UiIioq3yZlrwLjlHbZaWCISKi4KGVfm4PAljTlZsRERE5KdnPzavkJoToDeD7AP5biSBI+zYdLsLvtuajuLIeA6LD8etJabh3bJcPPiAiUoRXyU1KWQugr8KxkMY4E1pRZb3L9aLKevx6/VEAYIIjIk3ydSsABYlNh4vwzMbjqLe4n/a22CQWfHCcozlS3bFjx7Bjxw5UVVUhKioKWVlZGD16tMfHhwwZgjNnzrR5fkfvQz0Lkxu52HS4CIs/PInKekuHz60121Brdozqiirr8czG4wA4mqPuc+zYMWRnZ8NicXy/VlVVITs7GwCaE9bmzZths9maH8/JyWl+vfP5Fy5cwNGjRz2+D/U8TG7UbNPhIvz6/aOw2Dt33Fu9xYbfbc0HAI7oqFvs2LGjOSE5WSwW7NixA6NHj8ZHH33UnNg8sVgsLgmv5fUPPvgAn3zyCerr63vEaO69/Pdi/3j0j0nf1n9r7Bve1/zwmIeLZqbN9OumbsDRzy0iIsK2ZMkSRarnz549GzJ79uzUsrKyECEE5syZU/rcc8+V+PIeTG5ByFNxyO+25nc6sTkVVdbjifeOuHzO9TlSSlVVlcfrr776Kszmrh06L6VEfX1983tqeTT3Xv57sa8deC3ZbDPrAKCsvsz42oHXkgFAiQSnpJCQECxfvrzwtttuq6uoqNCNHTt2+N13330lIyOjwdv3YFeAIONcSyuqrIdEU/J5/yhGLPq0TeGIv1hsEk+uO4LU+Vtw69Kd2HS4qDmWW5fubHOdyFvh4Z6PJnQmJX9yjgq16I9H/5jkTGxOZptZ98ejf+xx/dySk5Mtt912Wx0AxMTE2K+99tr6Cxcu+NS6hyO3ILLpcBGeWncUtlZHrlnsEhZzl/ZLdsg5IHSO5HLOl2PDwaLmghWu2ZGvjh071uWRWWdUVVXh5Zdfbp4ODQ8Px1133aX6aO7b+m/d/vD3dN1bzn5uX3311anExETr5cuX9a+++mp/5+OzZ8+ueOqpp8oA4LHHHhuwcuXKuAULFpQ4+7mlpqZaysrK9MDVfm6/+MUvyhsaGoSzFU578vPzjbm5ub0mTJhQ40vcHLkFCeeIrXViU4PFJvHu/gttKjFbrtkRdeSTTz7pcD1NKS3X+err67F582YcO3ZMlVic+ob3dZvpPV33lpr93KqqqnTTp0+/dunSpRdjY2Pt7T23NSa3ILDpcBHmrTvisaxfDZ6W9ooVmhqlwHLs2DFFph07y2azqT5d+fCYh4uMeqNLAjDqjfaHxzzcI/u5NTY2ismTJ197//33l8+ZM6fS17iY3AKcswJSAwM2rwyIVqy9EwWQTz75RO0Q2vBU3NJdZqbNLP/Njb85HxceZxYQiAuPM//mxt+c72oxiRr93Ox2O2bNmpU8dOjQhsWLF3eqIpNrbgHOHxWQ3SU8RI9fT2pzhiqRC62N2pzaK27pLjPTZpb7uzJSjX5u27Zti9i0aVPfIUOG1A8bNmw4ALzwwgtFM2fO9Po3CJ/6uXkrMzNTuts3Qt0vdf4WaDm1iaY/uR+OvPX666+rPkpqT2f3w7GfW+d0uZ8b9Uxan+aTYGIj32g5sQFX98OpXWAS7JjcAtyvJ6UhRCc6fqKKnNsAuM+NvBEVFaV2CB3S8n44LTGZTPphw4YNb/3hXN/rCq65BTjnaKjleZG9QnSos/hUVas45zYAjt6oI1lZWS7nSWqV1keYWqB6Pzfq2e4dm9QmaaTM36JSNJ5xGwAFkp4wwgxknJYMUtHhIWqH0IbW1we71bF1wOsjgcXRjj+PrVM7Ik1o3QVAq0JCQpCVlaV2GEGNI7cgJVRehhOASxVn0G8DOLYO2LEEqCoEwmMAcw1ga6q2rroIZD/m+PvoB9SLUQPcdQHQmp7QPSAYMLkFqco69X5AhOgEZt50DXadKmVbnGPrgE+eBupbbE2qd7NNyVLvSH5Bnty0to4lhICUkglNg5jcgtSA6HDFugC0RwD43f1jgjORtXZsHbD50asjtI5UFbZNhuGxwF2vBk3Si4qK0lSCk1Ji8eLFaofRrHzN2thv33wzyVpWZjTExZn7PvJIUeyDs3pcP7e6ujpx8803DzObzcJms4mpU6dWvP7668W+vAfX3ILUryelITyky9W2PpNwnJoStGX/zWtpUcDGn3uf2AAA0vGa1qO8TY8EzZpcVlYWQkK0s16spaKR8jVrY0uWLk22lpYaISWspaXGkqVLk8vXXD0Sq6cICwuTe/fuzc/Pz889efJk7o4dO/rs2LGjty/vweQWpO4dm4RXpo9CUlMRh9JLcC3fP2j3tR1b51g7q7ro3/e1WxxJLwgKT0aPHo2pU6c2J5WoqChMnz4dmZmZHbzS/7RWNPLtm28myaZDi51kY6Pu2zff7HH93HQ6HaKiouwAYDabhdVqFcLHQgFOSwaxllsENh0uctkL52+tjwALmn1tLQtFhA6QCnZmCJLCk9GjR7dZ23J+fvDgQUgpIYRARkYGBg0apEh1ZUhICKZOnaqpNTZrWZnbvm2erntLrX5uVqsVI0eOHH7hwoXQOXPmlEycOLHWl7iZ3AjA1US3cNNxvLPvQrecRxmw+9qaE9pFuNSFKpnYnCz1wAcPO/4ewAnOnSlTpmDKlCltro8ePRqrV6/GN99847d7LViwwG/v5S+GuDiztbS0TSIzxMUp3s9t0aJFSdXV1fra2lr9hAkTqoCr/dxmzJhRMXv27ArA0c9t2bJliYWFhcZZs2ZVjBo1qtHj12Mw4NSpU7llZWX6yZMnX3vgwIGwG2+8scHbuDktSS5euncUZo8b1OE0ZetvnBCdQG9j2zW88BA9Ynq5XyMJyH1tbaYeVTi2WtocMQT4FKUv5syZg+nTpzdPZ4aHhzef4u+c2vR2/UxL62wt9X3kkSIRGupy9JAIDbX3feSRHtnPzSkuLs42fvz46uzsbJ/+4TlyozZ2nSpt90dyUlPp/u+25rcp5d90uKjNdQB4ZuNxl2apAbuvbccSx+hJbZZ64IP/BjbOBaIGAlmLgm4k15q76cyWNm7c2OF7aG2drSVnVaS/qyUnTZp05b777rtuwYIFpoSEBFtH/dwSExMtwNV+bhMnTqzdvn17VEFBgbG8vNyWnp7eOGLEiJILFy4Yjxw5Ej5t2rTq1vcsLi42GI1GGRcXZ6upqRG7du3q86tf/crkS9xMbtRGe9OFzqTk7kgvwP1RX07ukmHAqSpUO4KrZNMv8VUXHVsOgKBPcO3xtM2gJ+1li31wVrm/S//V6Od28eLFkJ/85CepNpsNUkpxzz33lD/44IM+7QHxqp+bECIawNsARsIxz/JfUsqv2vnHYD+3HuzWpTvd7oHTC4HlD3CPWrteH+n/akh/MfYGnvVpq1BQcXe0V3cWjrCfW+d0tZ/b7wF8KqUcBmAMgDw/xkYa424PXHiInonNG1mL1I7AM3Mt1+Ha4W6bgdYqIsl7HU5LCiGiAHwXwE8AQEppBtCl6hvSNmcCC4ppxGDzydOcmmxHR+ty5F8mk0n/ve99r83i++eff56fkJDQpfJib9bcUgGUAvibEGIMgIMAHpdSuuw5EELMBTAXAAYNGtSVmEgD2ls7o3bsWKJ2BO2rL3eM3pjgSAOU7OfmzbSkAcANAP5PSjkWQC2A+a2fJKV8S0qZKaXMjI+P93OYRD2ElgpKPNF6AibyA2+SWyGAQinl/qbP18OR7IiotaiBakfQsZ6QgIm6qMNpSSmlSQhxUQiRJqXMB5AFQJFhpFJO7zfhq81fo6a8ERGxobjlnmsx9OYEtcOiQJS1yLGBWgt73TzpCQmYqIu83ef2PwDeEUIYARQA+KlyIXnPm6R1er8Ju945BavZseenprwRu945BQBMcOR/zrWs5uO3NCYkXNsVndQjKN3yxslqtWLUqFHDExISzLt27Trry2u9Sm5SyiMAuv/Y7XZ4k7RO7zdh++rc5r2sTlazHV9t/prJjZQx+gHHh2b2vDWdbxl1DU8qCXDHdxfG5nx8LqmuymzsFWU0Z96dUjRqwkC/93PrLi+99FL/6667rt65MdwXPeZsydP7TVj97Jf4w8M7sfrZL/HFuvzmxOZkNduxZ93p5ufveudUm8TmVFPu8bxOIv9Qc4SkMwIQjoQ2/S1gcRUw7wQTWwA7vrsw9sv3zybXVZmNAFBXZTZ++f7Z5OO7C7vcz627W94AwNdffx2ydevWqJ///Oed2sTeI47fcjdK86Sh1oq/PPUFJGSb5NeS0DneF0Dz1KbQOU4s4roc+cXoB1y7ZnenyP6OZEZBI+fjc0k2q91lwGKz2nU5H59L6sroTa2WN48++ug1r732WmFVVVWnuiprLrm5W0f7avPX7Saq1hpqPf+DOUk7sO3vuS4ttpyjPK7Lkd/c9ao6BSasiAw6zhGbt9e9pUbLmzVr1kTFxcVZx48fX/fRRx912DnAHU1NSzpHaM6RmTPJKDaFKD232HKuyxF1yegHgKkrHdODzmnC1AnK35cVkUGnV5TR7clRnq77ixItb/bu3Ruxbdu26KSkpFE/+clPBu/bty/ynnvuSXX3XE80ldzcjdCsZjuESlFyXY78YvQDjinCxZWOP+d8CIR3eRkEEHog82eOCsiWWBEZlDLvTinSG3QuP0D1Bp098+6ULvVzmzRp0pXs7OwYk8mkB4COWt44rztb3qxYsaI4JibGWlBQYMzNzTWmp6c3Lly4sGTSpEmVR44ccdvU8Q9/+EPR5cuXjxUVFR3/+9//XjBu3LjqzZs3+9RtVlPTkp6SiaeiEKWF9u7UVC9Rx+oruvb6kHDHiHD0A8CgcU1bDwrZuy2IOdfV/F0tqUbLG3/wquWNrzrT8sZT2b6awnob8LPl31U7DApEHW0TCI8FRvwAOPOZI2mFxziu11cwgQUotrzpHE8tbzQxcuuobF8t3hSmEHWKu5NMWo7GiKhLNJHcfK2G7C4RsR63YBB1jctJJpxOpOCkdssbxWmxcEPoBG6551q1w6BA5jzJhChIqd3yRnFaHCFJu8SlryvVDoOIiDpBE8ntlnuuhcGoiVBcnNxbrHYIRETUCZqYlnSeAuI8mUQrtFbgQkRE3tHecElD1No8TkREXaOJkVvrg5G1ImlItNohEBFpTnf0c0tKShrVu3dvm06ng8FgkCdOnMjz5fWaSG5a3QpQWarhbspERK0c2fZx7L71a5JqKyuMvaNjzOPue7Do+u/f3WP7ue3evfu088BmX2li4k1L62wtaTUuIqLWjmz7OPbz1X9Orq2sMAJAbWWF8fPVf04+su3jHtnPras0kdy0uBUA0G5cRESt7Vu/Jslmsbj2c7NYdPvWr0nqyvs6+7nt3r37dH5+fu6f/vSnCy0fnz17dsWJEyfy8vPzc9PS0upXrlwZBwDOfm75+fm5n3766Vngaj+3U6dO5R47diwvNTW13Y4FWVlZQ0aMGJG+bNmyOF/j1kRy0+pWAG7iJqKewjli8/a6t7zp55aRkZE2dOjQ4Rs2bOh78uTJMOBqP7fly5fHOZuS3nLLLbXLly9PXLBgQcKZM2eMERERHg833rt376nc3Ny8zz777Myf//znfp988kmEL3FrIqMMvTkBt88e1jxS0sKIKbS3no1KiajH6B0d43YU5Om6vyjRzw0AUlNTLQCQlJRknTx5cuVXX33V25e4NJHcAEeCm/PbW/HoHydizm9v9dhupjva0BiMOnz3gTbHnRERada4+x4s0oeEuPZzCwmxj7vvwR7Xz+3KlSu6iooKnfPvu3bt6jN69GifKvw0US3pzncfSMP2f+S6dMoWesf1bX/z/1FkEbGhqClvRERsKG6551qO2oioR3FWRfq7WlKNfm6FhYWGH/zgB9cBgM1mEzNmzPj2vvvuu+JL3Jrp5+bO6f2m5lNLWiadt5/ajcbaLh0Y7SIiNhRzfnur396PiMhX7OfWOZru5+bJ0JsT3I6g3I3qWtLpBey2tklb6AQgpMvrDEYdC0eIiAKMppObJ63PonQ3lXh6vwlfrMtvHuGF9TZg/ANDO3wdERF1j4Dv59YZnkZ13jzOZEZEpD4l+7l5ldyEEOcAVAOwAbC2nhcmIiLSEl9GbrdLKbmwSUREmqeZfW5ERET+4m1ykwA+E0IcFELMdfcEIcRcIUSOECKntLTUfxESERH5yNvkdpuU8gYAdwF4VAjx3dZPkFK+JaXMlFJmxsfH+zVIIiLSjieffHLAokWL+it5j7KyMv2dd945ODU1dcTgwYNHbN++3afjt7xac5NSFjX9WSKE+ADATQC+8BcWaPUAABW3SURBVD1cIiJSSs2+4tgrOy4m2avNRl2k0dwn65qiiHEDemQ/t7lz515zxx13XPn0008LGhoaRE1NjU/LaB0+WQjRWwgR6fw7gDsAnOhcuEREpISafcWxlR99k2yvNhsBwF5tNlZ+9E1yzb7iHtfP7dtvv9Xv378/8oknnigDgLCwMBkXF+fTvjdvMmF/AHuFEEcB/BvAFinlp77chIiIlHVlx8UkWO2uP9Otdt2VHRd7XD+3/Px8Y2xsrPX+++9PSU9PHz5z5szkK1eu+HfkJqUskFKOafoYIaV82ZcbEBGR8pwjNm+ve0uNfm5Wq1Xk5eX1evTRR0vz8vJye/XqZX/uued8On2DWwFIM7YUbMEd6+/A6NWjccf6O7ClYIsiryHtuWTajC+/HI8dO6/Fjp1Dmv50fOz+IhOXTJvbec11+PLL8W6fE0x0kUa3oyBP1/1FiX5uKSkp5v79+5snTpxYCwAzZ86sOHr0aC9f4uqxx29RYNlSsAWL/7UYDbYGAMCl2kuYv2c+5u+Zj16GXgjRheCK+QoSeifg8Rsex+TBk92+ZvG/FgMAJg+erNaXQh7knVqE4uK1cBx0dJUQvQA0QjafaO7SkgxWawXy8p4GACQm3APAkdhOnVoAu93R4quhsRinTi1weU6w6ZN1TVHlR98ku0xNGnT2PlnXdLmf23333XfdggULTAkJCbaO+rklJiZagKv93CZOnFi7ffv2qIKCAmN5ebktPT29ccSIESUXLlwwHjlyJHzatGnVre85aNAga0JCgvno0aOhY8aMafzss8/6pKWlNfgSN5MbacLvD/2+OUm1Vmeta/57y6TnToOtAb8/9Pvm9zTVmlwSIinrkmkzCr5ehobGSwgLTcTga3+FxIR7cPDQj1BZ+S+3r5Gyzu111+dYcPr0izidvwRWW6Xb59jt9Sj4elnQJjdnVaS/qyXV6OcGAG+88caF2bNnDzabzWLQoEGNa9asOedL3Jru50aBa0vBFpfkc6nW4/d4p4Tpw1ySpUEYEGGMQFVjFZOdQlqPpgBApwtHnz5jPSY2JWRN/Lrb7uVP7OfWOZ76uXHNjbqdczrxUu0lSEi/JzYAbUaBVmlFZWNl8/0W/2sx1+f8rODrZS6JDXCMprozsQEi6NfeyIHTkqS4LQVbsPTfS1HZ6H46SQ3O6UuO3rru6lRksdqhAJBBPTXZ07CfG/VYWwq24Lkvn4PFblE7lDZMtSa1Q+jx3E1Fqq2h0f8zAaQMJfu5cVqSFPX7Q7/XZGIDgITebFrbVe6mItVm0EepHQJpAJMbKUoroyO9cKleRpg+DI/f8LhK0fR8zj1m2piKbEUItSMgDWByI0VpZXQkIBAdGg0BgcTeiVj8ncVcb+sk51SkJhMbAKtVO2u7pB6uuZGiHr/hcU2suVmlFeGGcOyZtUfVOAKBFqciWzIYotUOgTSAIzdSXC+DT6fmKEYrU6Q9ndYLNqzWSm4HUJjS/dyOHj0aOmzYsOHOj4iIiLFLlizp58t7cORGiml9PJbaJCTuWH8HN3B3UVhoomanJB0kcnNdj+sKFgcOHIjdvXt3Uk1NjTEiIsI8YcKEohtvvLHH9XMbM2ZMo7OK0mq1IiEhYcysWbN8mm/myI0U096RWmrhBu6uG3ztr6DThasdRgcsOJ2/RO0gutWBAwdit27dmlxTU2MEgJqaGuPWrVuTDxw40OP6ubX04Ycf9hk0aFDj0KFDfToAmsmNFKPVacCW50+S7xIT7sGwYS8jLHQAAAFA39FLVOHpDMpAtXv37iSr1eryM91qtep2797d4/q5tbRmzZrY++6771tf42ZyI8VopVLSHa0m3p4iMeEe3HrrHmRNPIvWp/iTOpwjNm+ve0uNfm5ODQ0NYvv27VE/+tGPKnyNm8mNFPP4DY8jTB+mdhhuaTnx9jRhoYlqh+CWwRCjdgjdKiIiwu0oyNN1f1Gin5vT+vXro4YPH153zTXXWH2Ni8mNFDN58GQs/s5iJPZOhICATmjn240buP1Hi2twQoRg6NDn1A6jW02YMKHIYDC4DKMNBoN9woQJXe7nlp2dHWMymfQA0FE/N+d1Zz+3FStWFMfExFgLCgqMubm5xvT09MaFCxeWTJo0qfLIkSPtfuOsXbs29oEHHuhUQQyrJUlRkwdPbq5M1Er15My0mayW9CNnRWJu7pOqxiFEL0hZ79JHLpg4qyL9XS2pVj+3K1eu6Pbu3dtn9erV5zsTN/u5Ubdq2cctKjQKVY1VkPD/96AnM9NmYuG4hd12v2Di6LT9jir3HjBgNtKH9ezqSPZz6xxP/dw4cqNu1XIkBziS3TN7numWBMfEpixncikuXgugS91KvKbThWPYsJeDbpRGHWNyI1U5E113TFd+UfiFou9PjgTnTHKOkdy7gJ9+cTHoo9Gv/2SUf7sLDY2Xgnb6MZCwnxsFNGeCc05XJvROaC74mL9nvt/uw/L/7tUy0QEtm5o6ElNs39thMm3s8JxKgyEGE77LZY5ApGQ/NyY30oTW05VOh0sO47389zp8vYDocGqT5f/qSky4p80oKzo6oznhGfRRsNlrIeXVQ7Z1uvCgq3ok/2ByI01bOG4hxvYbi6X/XorKRseJE85Eltg70eWcyJf2vYT3T78Pu2y7qZj927SpdcJrPbrjtCN1FqslKSC1rMp0TnOy/J+0jNWSndPlakkhhB5ADoAiKeUUP8ZG5HeepjmJKDj4cmTE4wDylAqEiIh6BqX7uQHACy+80O+6664bMWTIkBFTp05NraurE7683qvkJoQYCGAygLc7EyQRESmvsPCd2D17bxm1Y+d1GXv23jKqsPCdLre7UcM333wT8tZbb/U/cuRI7pkzZ07abDbx9ttv+/S1eDtyWwHgN2jn+G8hxFwhRI4QIqe0tNSXGIiIqIsKC9+JPXP25WSzucQISJjNJcYzZ19O9keCU6Ofm81mE7W1tTqLxYL6+nrdwIEDLZ6e606HyU0IMQVAiZTyYHvPk1K+JaXMlFJmxsfH+xIDERF10TfnViXZ7Y0uP9Pt9kbdN+dW9bh+bqmpqZZHH33UlJqaOrpfv35jIiMjbdOnT7/iS9zejNxuBTBNCHEOwFoAE4UQ//TlJkREpCyzudRt3zZP172lRj+30tJS/ZYtW6LPnj173GQyHaurq9O9+eab/p2WlFI+I6UcKKVMATALwE4p5Q99uQkRESnLaIx3OwrydN1flOjnlp2d3WfQoEGNAwYMsIaGhsp777238l//+leEL3Fpp8EWERF1WmrKL4t0ulCXugidLtSemvLLHtfPLSUlxXzo0KGI6upqnd1ux86dOyPT09N9OnzWpxNKpJSfA/jcl9f0RFXZ2Sh5fQWsly7BkJiIfvOeQNTUqR6fe+nl30JWOk7P0EdHo/+CZz0+n4hICQMHzi4HHGtvZnOp0WiMN6em/LLIeb2z1OjnNnHixNqpU6dWjB49Ot1gMGDEiBF1Tz75pE+VijyhpJWq7Gxcem4RZMPVXxJEWBgSX1zSJmFVZWej+JlnAWvbDujRD85C4vPPKx4vEQUGnlDSOZ5OKOG0ZCslr69wSWwAIBsaUPL6CpdrVdnZKJ7/jNvEBgCVa9Yib1g6zkzMwqUXXsCZiVnISx+OMxOzUJWdrVj8RETEg5PbsF5y3/XcWlyMvBEjAZsNCAkBLN5tubAWF6NyzVqXz4t/8zTqDh3iyI6Ighr7uXUD59oZ2pumtTX9W3uZ2DySEpVr1qLXDTdwbY6IgpaS/dw4LYmra2fOopDuUvzr3yBvxEhceuGFbr0vEVGgY3KDY53N09qZ4mw2VK5ZywRHRORHTG7wvM7WnSrXva92CEREAYPJDYA+KkrtEK6u5xERUZcxuaGdVgfdjFsEiKgn6I5+bi+++GK/IUOGjLjuuutGLFmypJ+vr2dyAyCrqtQOAQBw6dkFTHBE1Gmri8pix3x5YlTiriMZY748MWp1UVmP7Od24MCBsH/84x/xhw4dysvLyzv56aefRp84ccJjexx3mNwAGBIT1Q4BACAtFlx6fjGqsrO56ZuIfLK6qCx20dmi5Mtmq1ECuGy2GhedLUr2R4Lr7n5ux48fDx87dmxNZGSkPSQkBLfeemv12rVro32JmckNQL95T6gdQjNZV4dLzy6AtbgYkBLW4mJcem4RExwRtet/z5mSGu3S5Wd6o13q/vecqcf1c7v++uvr//3vf0eaTCZ9dXW1btu2bVEXL170qXUPkxuAqKlTEf3gLLXDaCZbbRJ3d/wXEVFLJWar2x/+nq57S41+bjfccEPD448/bsrKyhp6++23DxkxYkSdXq9391SPmNyaJD7/PBDutvuCJmhhuwIRaVc/o8HtKMjTdX9Rop8bAMybN6/s5MmTeTk5OfkxMTG2oUOH+tTyhsmtSVV2NlBfr3YYHmllXZCItOnJlISiUJ1wKf4O1Qn7kykJPa6fGwAUFRUZAODMmTPGLVu2RD/00EM+te7h2ZJNNDPtZzBACOEyNSnCwjS1LkhE2jMnKa4ccKy9lZitxn5Gg/nJlIQi5/XOUqOfGwBMmzbt2srKSoPBYJArVqy4EBcX59NmYPZza5KXPrz9Q5O7Q3g4BixxHMPlbbNUUtcGUzleKbiEokYLkkJD8MzgRMxI6JHV16Qy9nPrHE/93Dhya2JITHRUKKqkdXNTJjNltJeMPD3W3vUn8i7AOcYubLTgiTxHIRkTHJG6mNya9Jv3RJsO3N1lwO9eYzLrBhtM5fhV/kXU2x0j9MJGC36Vf7H5cXeP/buqButMFW5fs/B0IVo3P7I0XWdyI+oY+7l1A2dyKXl9RbeO4ER0NBNbN3ml4FJzknKqt0u8UnAJ5WYL6lvNStfbJVYXt12uqLdLPJp3oc11pwqbVg50owBgt9vtQqfTqbxmooyu9nOz2+0CHk5QZLVkC1FTp2LIzh1IP5WHAb97DSLapw3xPhNhYUhc8Kyi96CrihrdN5ktbLSgzs8/OhJ2HUH6F8ewwdSltXyiE6WlpVFNP8SpBbvdLkpLS6MAnHD3OEduHkRNndo8oqrKzsbll38Lmx+bmRoGDGChSDdLCg1BoYcEp4QKm51rcNQlVqv1IZPJ9LbJZBoJDkZaswM4YbVaH3L3IKslfXDphRdQuWZt599Ar8eApa8woank6fwLbqcZlTYwNAQ53xnR7felnsVdtSR1Hn8T8EHi889jwO9eg2HAgHafp4+OhggLc7kmwsIwYOkrAMBDkVXy4WX/jbx94Wk6lIiUw2lJH7WermxdYSnCwtC/aR2t9V41AC7Pdx6K7HxfUs7T+RdUK/RICg1R5b5EwYzJrQtcKizdbLhunbDOTMxqs9XAeSgyk5ty7j98Bnsqa1W7/zODeXQaUXfrMLkJIcIAfAEgtOn566WUz7f/quDRciTXEU+HH/NQZOVsMJWrmtgAFpMQqcGbNbdGABOllGMAXA/gTiHEOGXDCkyeDj/mocjKeaWAvzgQBaMOk5t0qGn6NKTpIyA3FCqt37wn3Baa8FBk5WihmOPpfM8bvolIGV5VSwoh9EKIIwBKAGyTUu5385y5QogcIUROaWmpv+MMCFFTpyLxxSWOakshYBgwAIkvLuF6m4K0UMzx/1TYfkAU7Hza5yaEiAbwAYD/kVK63RUOBO4+N+p51Nrb1prp9uvVDoE0jvvc/MunfW5SykoAuwDcqUw4RP6zwVSOdaYKtcMgIhV0mNyEEPFNIzYIIcIBfB/AKaUDI+oqdwclq4GHAhJ1P2/2uSUCWC2E0MORDNdJKT9SNiyirtNCMQngqL7K/NdJNjQl6kYdJjcp5TEAY7shFiK/6u6Dkj0RQHMcLfvBMcERKYdnS1LAemZwIsJ16k4KCrTdN+PsIUdEymFyo4A1IyEWy9KuUe3+A0NDPG4I1cqUKVGgYnKjgDYjIRb6briPc3w4MDQEf0gfBNPt1yPnOyMw0MM+Oy3svyMKZExuFPBs7Tz2h/RBfpm6XNUiobVcS3M3NRquEzxMmUhhTG4U8DyNngaGhvhl6lIPz8UhzvcfGBoC0XTPZWnXsJiESGFseUMB75nBifhV/kWXPW8tR08zEmLxSsGlTldW/nBA+4lqRkIskxlRN+PIjQKeN6Mnd9OHIQB666/+L9J68lIPYM6AWLyaNkix2Imoczhyo6DQ0ejJ+dgrBZe42ZooADC5ETXh9CFR4OC0JBERBRwmNyIiCjhMbkREFHCY3IiIKOAwuRERUcARUvq/maMQohTAeQBxAMr8fgNtCqavFQiurzeYvlaAX69akqWU8WoHESgUSW7Nby5EjpQyU7EbaEgwfa1AcH29wfS1Avx6KTBwWpKIiAIOkxsREQUcpZPbWwq/v5YE09cKBNfXG0xfK8CvlwKAomtuREREauC0JBERBRwmNyIiCjh+T25CiGuEELuEELlCiJNCiMf9fQ8tEUKECSH+LYQ42vT1vqB2TEoTQuiFEIeFEB+pHYvShBDnhBDHhRBHhBA5asejNCFEtBBivRDilBAiTwhxi9oxKUEIkdb039T5cUUI8YTacZH/+H3NTQiRCCBRSnlICBEJ4CCAe6WUuX69kUYIIQSA3lLKGiFECIC9AB6XUu5TOTTFCCGeBJAJoI+Ucora8ShJCHEOQKaUUgubfBUnhFgNYI+U8m0hhBFALyllpdpxKUkIoQdQBOBmKeV5teMh//D7yE1KeUlKeajp79UA8gAk+fs+WiEdapo+DWn6CNgqHSHEQACTAbytdizkX0KIKADfBfAXAJBSmgM9sTXJAvA1E1tgUXTNTQiRAmAsgP1K3kdtTdN0RwCUANgmpQzkr3cFgN8AsKsdSDeRAD4TQhwUQsxVOxiFpQIoBfC3pmnnt4UQvdUOqhvMArBG7SDIvxRLbkKICAAbADwhpbyi1H20QEppk1JeD2AggJuEECPVjkkJQogpAEqklAfVjqUb3SalvAHAXQAeFUJ8V+2AFGQAcAOA/5NSjgVQC2C+uiEpq2nqdRqA99WOhfxLkeTWtPa0AcA7UsqNStxDi5qmcHYBuFPtWBRyK4BpTetQawFMFEL8U92QlCWlLGr6swTABwBuUjciRRUCKGwx87AejmQXyO4CcEhKeVntQMi/lKiWFHDM2edJKf/X3++vNUKIeCFEdNPfwwF8H8ApdaNShpTyGSnlQCllChxTOTullD9UOSzFCCF6NxVFoWl67g4AJ9SNSjlSShOAi0KItKZLWQACshCshQfBKcmAZFDgPW8F8CMAx5vWoQDgWSnlxwrcSwsSAaxuqrjSAVgnpQz4Evkg0R/AB47f12AA8K6U8lN1Q1Lc/wB4p2m6rgDAT1WORzFNv7B8H8B/qx0L+R+P3yIiooDDE0qIiCjgMLkREVHAYXIjIqKAw+RGREQBh8mNiIgCDpMbEREFHCY3IiIKOP8fucQXByjkn1wAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "for i in range(10):\n",
        "    plt.scatter(x[idx[i],0],x[idx[i],1],label=\"class_\"+str(i))\n",
        "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "fNWgnhUJnWLV"
      },
      "outputs": [],
      "source": [
        "x = ( x -  np.mean(x,axis=0,keepdims=True) ) / np.std(x,axis=0,keepdims=True) \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "id": "8-VLhUfDDeHt",
        "outputId": "97f576e5-61bb-44fa-b0cc-f94206937491"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f791a4baed0>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAckAAAD4CAYAAACHbh3NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de3yT9dk/8M+VpGkLLT1QoKVAW7At5ThsVZgylO4RBQEHKDi24R4dD1NfKuomWmSIOtHBI0P055jzGT6PAxkoUAGRk4hOGEUoh5ZSqAg9hLb0QM85fX9/JClpmzRJc7jvJNf79eqryX3fyX0RCle/x4uEEGCMMcZYVwqpA2CMMcbkipMkY4wxZgcnScYYY8wOTpKMMcaYHZwkGWOMMTtUUgfQnbi4OJGcnCx1GIwx5jeOHz9eLYToJ3UcgULWSTI5ORl5eXlSh8EYY36DiH6QOoZAwt2tjDHGmB2cJBljjDE7OEkyxhhjdnCSZIwxxuzgJMkYY4zZIevZrYyxnms6UYnrey7BUNcGZXQo+kxJRu9x/aUOizG/wkmSsQDUdKISdZ8UQ+iMAABDXRvqPikGAE6UjLmAu1sZC0DX91xqT5AWQmfE9T2XpAmIMT/FSZKxAGSoa3PpOGPMNu5uDVKFhw/i8KYP0XCtGqG9I0AEtDY2IrJvHCbO+xUyJt4ldYjMDcroUJsJUdGL/8kz5gr+FxOECg8fxBfr10GvNf0n2tbY0H6uoboKX6xfBwCcKP1U04lKGNv0Ns8Zm/UoXXIYIADCdIzClYiecROPVTJmAwkhpI7BrqysLMF7t7rPutUY2TcOLQ3XoW9zrtuNFAqMyb4HP330MS9HyTyh84QdV8TMTedEGQCI6LgQIkvqOAIFtyQD3L7330X+3l3tzxuqq1x6vTAa21/PiVI6NduK0XxUY2r9EdDrtnjE3p8KoONSD+sWoqvqdlzgJMlYJ5wkA1Th4YM4sGE9WhsaHF/shPy9u1By4hgaqqtACgWE0YjIuH4YOu4W03FzK5XHMz2vZlsxmo9obhwQQPMRDXRVzdCXN0G0GDqc66kO78MYA8BJMiB1bj16iqUVKozG9uedW6k8nul5zUc1No/rLl73+L2aTlRya5IxK5wkA0zh4YNeSZDO0mvbcHjTh5wkPcmH0wZqPy5C/cEfEHVXEu/Wwxg8lCSJ6AMA9wGoFEKMsnH+TgDbAXxvPvSJEGKFJ+7NTAoPH8QXf13n9IQcb2q4Vi11CIHFjXHGnjBWtqL246L257xbDwtmnmpJ/h3AOgAfdnPNYSHEfR66X1DrPFt16LhbkL9vNyCTmcqRfeOkDiGghAzt45WuVVdYduvhJMmCjUeSpBDiKyJK9sR7se51XuPYeVxQaip1KCbO+5XUYQSEphOVqM+9CGOz7TWPvsa79bBg5Mtt6SYQUT4R7SaikfYuIqKFRJRHRHlVVa4tVwgGhzd92J4g5SYyrh/uXvgEj0d6gGW9o1wSJGDaxYexYOOriTvfAUgSQjQS0VQA2wCk2rpQCLEewHrAtJmAj+LzG7Id7yPi5R9u6FzWytim79GGAN6k6MtJkgUfn7QkhRDXhRCN5se7AIQQEQ9c9YBsx/uEwK51q7H+8V+j8PBBqaPxK5ZWo6U701DXJss1i7qL11G65DBKXziMmm3FUofDmE/4JEkSUTwRkfnxreb7XvPFvQPNxHm/gkrd8Td6lToUpFRKFFFHlrWSnCidZ6uslayZNzPgRMmCgUeSJBFtBPAtgHQiKiWiR4hoEREtMl8yB8AZIsoHsBbAPCHnTWNlLGPiXbh74ROIjOsHECEyrh9GTsqGMMin5WFZK8mc468TYuxtcsBYIPHU7NaHHJxfB9MSEeYBGRPv6jD2t/7xX0sYjW2yHTuVIXtlrXy9PtJlco6NMQ/hossBQI4JSbZjpzLUZ0oyKKTjP0UKUaDXbfFdjssKSR0AY97H29IFgMi+cS5X9/AmXivpGssCfVvbwDUlRd2o8CEBZXQoQofHdNxg3azXbfESRMR64vjx4/1VKtX7AEaBG0edGQGc0ev1j2ZmZlZ2PslJMgBMnPcr7Fq3WrL7h0VGIiQ0jCuBuKH3uP42d7OxHC9dctjnMVGIosOerfZKdTH5U6lU78fHx2f069evVqFQcEe5FaPRSFVVVSM0Gs37AGZ0Ps9JMgBkTLwLe99/B7rWVknuP3nBQk6KXmZ33NJbCIieldqeIGPvT+Wk6N9GcYK0TaFQiH79+tVrNJou+44D3OwOGDqJNjYPjYjkBOkDfaYk++xeFKJAzIPpvE9rYFFwgrTP/NnYzIecJAOEJBNliJD98ELf3zcI9R7XHyHD+nj9Psro0A4tSMaCHSfJACHFRBmFTDYwCBYDfjMWvcbHe2dWqQKImZuOhCW3coJkzAonyQCRMfEujP2PqV2Oq9ShmPrEs3j2488w9Ylnu+zMQ0olQsLCenRPo17Pmwb4WOz9qRj0+kQMWmn6ipmbfmPj8R4mTwpXIuYB7l5lN/zfkR9ib31t3+iUJTszb31t3+j/O/JDrDfu88wzzwxctmzZAG+8t8WWLVv6JCcnjxoyZMioF1980eUp2TxxJ4D89NHHkJie0aHWpPVMU8v3zucBdCi/BZiSZ2ivXmhtbOx2iYkc12gGE+tZsa7OgO01nmeosq7+78gPsa98VpDUpjcqAKCyoU39ymcFSQDwi/FJNdJG5xq9Xo/FixcP2bNnz/mhQ4fqxo4dmzF79uy6zMxMp2c5cpIMMJ1343HlvL3kCph29bGVKHnTAPlwegYsgSfmMLvW7i9OtCRIiza9UbF2f3Giu0ly3bp1fdeuXTuAiJCRkdEydOjQ9h/Y1atXx/3P//xPP51OR8nJyW1btmz5PjIy0vjBBx/EvP766wMVCoWIjIw05OXlFeXl5YX9+te/TtHpdGQ0GrF169aLo0eP7vLD/+WXX/ZOSkpqGzFihBYAZs2aVbNly5bozMxMp/dU5CTJADhOrhPn/apLa5M3DZCXPlOSUfdJcbebpVOIgifmsG5VNbSpXTnurLy8vLBVq1YlfPvtt+cSEhL0V69eVb7xxhvtXa3z58+vffbZZ6sB4Mknnxy4du3auJycnMqVK1cmfPHFF+dTUlJ01dXVSgB4++23+z322GNXf/vb39a0traSXm+77uqVK1fUiYmJWsvzQYMGaY8ePRrhStw8JsmcYmtjdS6wLC+9x/VH9KzU9jFKZXQoeo2P7/CcEyRzpF9kqNaV487as2dPn+nTp9cmJCToAWDAgAEdqjIcP348PDMzMz0tLW3E1q1b+549ezYMALKyshrnz5+fvHr16jhLMpwwYULT6tWrE3JycuKLi4vVERERXlvewi1J5jRHrU0mPXs79zDmrCezU8usxyQBIFSlMD6ZnVrmzfsuXLgwZcuWLRcmTJjQsnbt2r6HDh2KBIB//OMflw8cONB7x44dUZmZmSOOHz9esGjRopqJEyc2ffrpp1H33Xdf6ttvv/3DjBkzGjq/5+DBg7VlZWXtLeDS0tIOLUtncEuSMcZYu1+MT6p56b4RP/SPDNUSgP6RodqX7hvxg7vjkVOmTLmem5sbo9FolABw9erVDlPtm5ubFUOGDNG1tbXRpk2b2mfTnj17NnTy5MlNa9asKY+JidGXlJSoCwoK1BkZGW1Lly6tnDJlSt3JkyfDbd1z0qRJTZcuXQo7d+6curW1lT755JPY2bNn17kSN7ckGWOMdfCL8Uk1np7JmpWV1frss89WTJw4cbhCoRCjRo1qTkpKam/VLVmypPzWW2/NiI2N1d98882NjY2NSgBYvHjxoEuXLoUKIeiOO+64Pn78+JalS5fGb968ua9KpRL9+vXTvfLKKxW27hkSEoLVq1dfvueee9IMBgN+/vOfV2dlZbm0fyfJufZxVlaWyMvLkzoMxhjzG0R0XAiRZX0sPz//0tixY3m9Vjfy8/Pjxo4dm9z5OHe3MsYYY3ZwdytjjDG/ptFolHfeeWd65+NffvllUXx8vMHWa5zFSZIxxphfi4+PN5w7d67AG+/N3a2MMcaYHR5JkkT0ARFVEtEZO+eJiNYS0QUiOkVEN3vivowxxpg3eaol+XcA93Rz/l4AqeavhQD+n4fuyxhjjHmNR5KkEOIrAN2tqZkJ4ENhcgRANBEleOLejDHGmLf4akwyEcAVq+el5mNdENFCIsojoryqKtvlmRhjjHnRsb/FYlXaaCyPzsSqtNE49je/rSf5wAMPJMfGxo5NTU0d2ZPXy27ijhBivRAiSwiR1a9fP6nDYYyx4HLsb7HY80ISGq+qAQE0XlVjzwtJ3kqU3vaf//mf1Tt27Cju6et9lSTLAAy2ej7IfIwxxpicHHojEfq2jrlB36bAoTds9v65Yt26dX3T0tJGpKenj7j//vtTrM+tXr06btSoURnp6ekjpkyZMqyhoUEBAB988EFMamrqyPT09BFZWVnpgKns1ujRozOGDx8+Ii0tbcTp06dD7d3z3nvvbezXr5/tWlpO8FWS3AHgV+ZZruMB1AshbO61xxhjTEKNlbbrRto77iRLPclDhw6dLyoqKvjLX/5y2fr8/Pnza8+cOVNYVFRUkJ6e3rJ27do4ALDUkywqKir4/PPPLwA36kmeO3eu4NSpU4UpKSlulfHqjkc2EyCijQDuBBBHRKUA/gAgBACEEO8B2AVgKoALAJoB/NoT92WMMeZhEf21pq5WG8fd4Ew9yWXLliU2NDQom5qalJMmTaoHbtSTnD17du38+fNrAVM9yVWrViWUlpaq582bVzt69Oi2rnf0DE/Nbn1ICJEghAgRQgwSQvxNCPGeOUHCPKv1cSHEMCHEaCEE71rOGGNyNOn5MqhCjR2OqUKNmPS81+tJrlu37vL58+cLnn/++fK2NlOX7z/+8Y/Lr776avmVK1fUmZmZIzQajXLRokU127dvvxAeHm687777Unfs2BHprbhkN3GHMcaYhG55pAZTXv8BEQO0AAERA7SY8voPuOURv6sn6Qm8dytjjLGObnmkxt2k2JkU9SQBYPr06SlHjhyJrK2tVQ0YMGDMkiVLyhcvXux02TCuJ8kYYwGE60n2DNeTZIwxxlzE3a2MMcb8GteTZIwxxuzwZj1JTpLMp7adKMOf9hShvK4FA6PD8bsp6bh/nNsbeTDGmFdwkmReZ0mMZXUtHY6X1bXgd1vyAYATJWNMljhJMq/adqIML3xyGi0628MCOoNAzqenuXXJJHfq1Cns378f9fX1iIqKQnZ2NsaMGWP3fGpqKoqLi7tc7+h9mH/hJMm8YtuJMizfcRZ1LTqH1zZpDWjSmlqZZXUteOGT0wC4dcl859SpU8jNzYVOZ/p5ra+vR25uLgC0J77t27fDYDC0n7denma5/vLly8jPz7f7Psz/cJJkHrftRBl+98986Iw9W4PbojPgT3uKAIBbmMwn9u/f357YLHQ6Hfbv348xY8bgs88+a0+Q9uh0Otha163T6fDpp59i9+7daGlp8YvW5cdFH8e+l/9e4rWWa+q+4X21i8YuKpubPtejmwsApnqSERERhhUrVlz19HsDwIULF0Lmz5+fUl1dHUJEWLBgQdVLL71U6cp7cJJkPWZvEs6f9hT1OEFalNW14OmPT3Z4zuOXzFvq6+vtHn/jjTeg1bpXZEIIgZaWlvb3lHPr8uOij2PfPPZmktagVQBAdUu1+s1jbyYBgDcSpTeFhIRg9erVpXfccUdzbW2tYty4cSOmTp16PTMzs9XZ9+DNBFiPWMYay+paIGBOYv/Mx8hln3eZoOMpOoPAM5tPImXJTty+8gC2nShrj+X2lQe6HGfMWeHh9rf+tCQ3T7K0UuXovfz3Ei0J0kJr0Crey3/P7+pJJiUl6e64445mAIiJiTEOGzas5fLlyy6V/OKWJHPZthNleHZzPgydtjTUGQV0WrfW7TpkaaBaWpZ5P9Rg6/Gy9olBPKbJXHXq1Cm3W4o9UV9fj9dee629mzc8PBz33nuv5K3Lay3XbCYRe8edZakn+e23355LSEjQX716VfnGG28MsJyfP39+7bPPPlsNAE8++eTAtWvXxuXk5FRa6kmmpKToqqurlcCNepK//e1va1pbW0mvd1xTuaioSF1QUNBr0qRJja7EzS1J5hJLC7JzgpSCziDwj6OXu8yctR7TZMyR3bt3Oxxv9BbrcdCWlhZs374dp06dkiQWi77hfW3+xmDvuLOcqSeZmZmZnpaWNmLr1q19z549GwbcqCe5evXqOEsynDBhQtPq1asTcnJy4ouLi9URERHd/odUX1+vmDVr1rCVK1deiY2NNXZ3bWecJJnTtp0ow+LNJ+0u55CCvaHPci91+bLAcurUKa90p/aUwWCQvBt20dhFZWqlukMiUSvVxkVjF/llPcm2tjaaNm3asAceeKBmwYIFda7GxUmSOcUyY1UGDUinDIz2Wnk5FkB2794tdQhd2JtE5Ctz0+fW/P6W3/8QFx6nJRDiwuO0v7/l9z+4O2lHinqSRqMR8+bNS0pLS2tdvnx5j2bQ8pgkc4onZqz6SniIEr+b0mWvY8Y6kFsr0qK7SUS+Mjd9bo2nZ7JKUU9y7969Edu2beubmpraMnz48BEA8PLLL5fNnTvX6d9EuJ4kc0rKkp2Q708KQObvvJ6SOeutt96SvNXWnZ6up+R6kj3D9SSZW+TefSnACZK5Rs4JErixnlLqiTzBziNJkojuIaIiIrpAREtsnH+YiKqI6KT561FP3Jf5zu+mpCNEQY4vlJBl+Qevk2TOiIqKkjoEh+S8nlJONBqNcvjw4SM6f1nGP93h9pgkESkBvAPgPwCUAjhGRDuEEJ1re30shHjC3fsxaVhaZ9b7sfYKUaBZ59Jsaq+zLP/g1iRzJDs7u8N+rXIl9xavHMi9nuStAC4IIUoAgIg2AZgJwCsBM+ncPy6xS/JJXrJTomjs4+UfLJD4Q4s3kHmiuzURwBWr56XmY53NJqJTRLSFiAbbezMiWkhEeUSUV1VV5YHwmDdFh4dIHUIXch8/9alTm4G3RgHLo03fT22WOiJZ6Fz1Q65CQkKQnZ0tdRhBzVdLQHIBbBRCtBHRfwHYAGCyrQuFEOsBrAdMs1t9FB/rIZJ4mJKADrNug375x6nNwP4VQH0pEB4DaBsBg3mWff0VIPdJ0+MxD0oXowzYqvohN/5QLSQYeCJJlgGwbhkOMh9rJ4S4ZvX0fQBveuC+TAbqmqX7jyZEQZh762AcPFfF5bRObQZ2Pw+0WC1ta7GxzE3XYkqiQZ4k5TbOR0QQQnBilCFPJMljAFKJKAWm5DgPwM+tLyCiBCGEZbHnDACFHrgvk4GB0eFeq/rRHQLwpwfGBmdC7OzUZmD74zdajI7Ul3ZNquGxwL1vBE3yjIqKklWiFEJg+fLlUofRrmbjpthr776bqK+uVqvi4rR9H3usLPaheX5XT7K5uZluu+224VqtlgwGA02fPr32rbfeKnflPdwekxRC6AE8AWAPTMlvsxDiLBGtIKIZ5sueJKKzRJQP4EkAD7t7XyYPv5uSjvAQt2dZu0zAtAtQ0C73aB9rjAI++Y3zCRIAIEyv6dzq3PZY0IxZZmdnIyREPuPpcpqcU7NxU2zlypVJ+qoqNYSAvqpKXblyZVLNxhtbxfmLsLAw8fXXXxcVFRUVnD17tmD//v199u/f39uV9/DIOkkhxC4hRJoQYpgQ4jXzsWVCiB3mxy8IIUYKIcYKIe4SQpzzxH2Z9O4fl4jXZ41GonmyjLeHKK3fP2jXRZ7abBpbrL/i+FpXGHWm5BkEE3zGjBmD6dOntyenqKgozJo1C1lZWQ5e6Xlym5xz7d13E4V5c3EL0damuPbuu35XT1KhUCAqKsoIAFqtlvR6PZGLEyl471bmNuulIdtOlHVYS+lpnWdyBc26SOsJOaQAhBcrsQTJBJ8xY8Z0GfuzPD9+/DiEECAiZGZmYsiQIV6ZDRsSEoLp06fLagxSX11ts26kvePOkqqepF6vx6hRo0Zcvnw5dMGCBZWTJ09uciVuTpLMoywJc+m20/joyGWf7PcasOsi2xPjFXSYx+vNBGmhawE+XWR6HMCJ0pb77rsP9913X5fjY8aMwYYNG/D999977F45OTkeey9PUcXFafVVVV0Soiouzuv1JJctW5bY0NCgbGpqUk6aNKkeuFFPcvbs2bXz58+vBUz1JFetWpVQWlqqnjdvXu3o0aPb7P55VCqcO3euoLq6Wjlt2rRhx44dC7vllltanY2b925lXvHq/aMxf/wQh92vnX8AQxSE3uquY5zhIUrE9LI9hhSQ6yK7dKlKsBpKGEwxBHjXqysWLFiAWbNmtXfThoeHt1ftsHTZOju+KKdxSGt9H3usjEJDO2ylRaGhxr6PPeaX9SQt4uLiDBMnTmzIzc116YPnliTzmoPnqrr9rz3RvGTjT3uKuizh2HairMtxAHjhk9Mdij4H7LrI/StMrTmp6VqAT/8L+GQhEDUIyF4WdC3Lzmx101r75JNPHL6H3MYhrVlmsXp6duuUKVOuz5kz56acnBxNfHy8wVE9yYSEBB1wo57k5MmTm/bt2xdVUlKirqmpMWRkZLSNHDmy8vLly+qTJ0+Gz5gxo6HzPcvLy1VqtVrExcUZGhsb6eDBg32ee+45jStxc5JkXtNdN6gludna6g6wvQWeha2kGnDqS6WO4AZhblTUXzEtNQGCPlF2x97yEn9aCxn70LwaTy/5kKKe5JUrV0IefvjhFIPBACEEzZw5s+ahhx5yae0P15NkXnP7ygM211AqibD6QV7j2K23Rnl+9qqnqHsDL7q01Cyo2NryzpcTdLieZM9wPUnmc7bWUIaHKDlBOiN7mdQR2Kdt4nHKbthaXiK3GazMedzdyrzGkgiDons02Ox+nrtcu+Fo3JJ5lkajUd55551dJid8+eWXRfHx8W5NB+ckybyqu7FF1o39K6SOoHstNabWJCdKJgPerCfJ3a2MyZGcJu7YI/dEzpgHcJJkTI6iBkkdgWP+kMgZcxN3t7rg/FENvt1+EY01bYiIDcWEmcOQdlu81GGxQJS9zLSQXw5rJe3xh0TOmJs4ScK55Hf+qAYHPzoHvda0Zqyxpg0HPzLt086JknmcZayvfVs6mQkJl/cMXOYXvF0qy0Kv12P06NEj4uPjtQcPHrzgymuDPkk6k/zOH9Vg34aC9jXVFnqtEd9uv8hJknnHmAdNX7JZM2nePzZqMO+8E+BOHyqNzdt1KbG5XqvuFaXWZk1NLhs9aZDH60n6yquvvjrgpptuarFsUOCKoBuTPH9Ugw0vfoN3Fh3Ahhe/wVebi9oTpIVea8Thzefbrz/40bkuCdKiscbuvrqMeYaULTaFGgCZEuOs9cDyemDxGU6QAez0odLYb/55Iam5XqsGgOZ6rfqbf15IOn2o1O16kr4ulQUAFy9eDNmzZ0/Ub37zmx5tphBULUlbrUZ7Wpv0+NuzX0FAdEmi1khhel8A7V22pDDt5MXjlswjxjxoWpfYIsEv8pEDTEmRBY28XZcSDXpjhwaUQW9U5O26lOhOa1KqUlmPP/744DfffLO0vr6+R9XhAzZJ2hpn/Hb7xW4TXmetTfY/eAthBPb+vaBDiT9Lq5PHLZnH3PuGNBN5eAZr0LG0IJ097iwpSmVt3LgxKi4uTj9x4sTmzz77zGGlEFsCsrvV0mK0tBQtycprXaPCfok/y7glY24Z8yAwfa2p29PS/Zkyyfv35RmsQadXlNpm3Uh7xz3FG6Wyvv7664i9e/dGJyYmjn744YeHHjlyJHLmzJkptq61JyCTpK0Wo15rBEn0p+VxS+YRYx40dX0urzN9X7ADCHd7mAggJZD1iGnGqjWewRqUsqYmlylVig7/gSpVCmPW1GS36klOmTLlem5uboxGo1ECgKNSWZbjllJZa9asKY+JidGXlJSoCwoK1BkZGW1Lly6tnDJlSt3JkydtFpV95513yq5evXqqrKzs9N///veS8ePHN2zfvt2lqtke6W4lonsA/BmAEsD7QoiVnc6HAvgQQCaAawDmCiEueeLetthLSvYm33hbaO8edYUz5lhLrXuvDwk3tVDHPAgMGW9eclLKtSODmGXc0dOzW6UoleUJbpfKIiIlgPMA/gNAKYBjAB4SQhRYXfMYgDFCiEVENA/Az4QQcx29d09KZdlbriGlsN4qPLL6J1KHwQKRo+Uh4bHAyJ8BxV+Ykl94jOl4Sy0nwgDFpbJ6xl6pLE+0JG8FcEEIUQIARLQJwEwA1pvNzgSw3Px4C4B1RETCw8UsHS3XkIozE4AY6xFbO/NYtw4ZY27xRJJMBGD9q2wpgNvsXSOE0BNRPYC+ALr8ZkNECwEsBIAhQ4a4FIirs1d9JSLW7hIextzTYWce7iZlwSmoSmUJIdYDWA+Yultdea0cJ8iQgjBh5jCpw2CBzLIzD2NBSu6lssoADLZ6Psh8zOY1RKQCEAXTBB6PkmOLTRgFKi7WSR0GY4yxHvBEkjwGIJWIUohIDWAegB2drtkBYIH58RwABzw9HgkAE2YOg0otv1UtZ78ulzoExhhjPeB2d6t5jPEJAHtgWgLygRDiLBGtAJAnhNgB4G8A/peILgCogSmRepxlVxvLTjtyIbeJRIwxxpzjkTFJIcQuALs6HVtm9bgVwAOeuJc/kmoTA8YYY+6R3cQdd3TewFwuElOjpQ6BMcZkxxf1JBMTE0f37t3boFAooFKpxJkzZwpdeX1AJUm5LgGpq5JxdXnGGOvk5N5dsUe2bExsqqtV946O0Y6f81DZj/5jqt/Wkzx06NB5y8bqrgqojkA5jUNak2tcjDHW2cm9u2K/3PDXpKa6WjUANNXVqr/c8Nekk3t3+WU9SXcFVJKU4xIQQL5xMcZYZ0e2bEw06HQd60nqdIojWzYmuvO+lnqShw4dOl9UVFTwl7/85bL1+fnz59eeOXOmsKioqCA9Pb1l7dq1cQBgqSdZVFRU8Pnnn18AbtSTPHfuXMGpU6cKU1JSuq1Qkp2dnTpy5MiMVatWxbkad0AlSbkuAeHNBBhj/sLSgnT2uLOcqSeZmZmZnpaWNmLr1q19z549GwbcqCe5evXqOEtx5QkTJjStXl0OYYIAABqbSURBVL06IScnJ764uFgdERFhd0nh119/fa6goKDwiy++KP7rX//af/fu3RGuxC2/jOKGtNvicdf84e0tNzm04EJ7K7ngMmPMb/SOjrHZKrN33FO8UU8SAFJSUnQAkJiYqJ82bVrdt99+29uVuAIqSQKmRLngj7fj8fcmY8Efb7dbpsoX5atUagV+8mCX7QQZY0y2xs95qEwZEtKxnmRIiHH8nIf8rp7k9evXFbW1tQrL44MHD/YZM2aMSzMpA2p2qy0/eTAd+z4sgLBq2JPSdHzv/3h+q7+I2FA01rQhIjYUE2YO41YkY8yvWGaxenp2qxT1JEtLS1U/+9nPbgIAg8FAs2fPvjZnzpzrrsTtdj1Jb+pJPUlbzh/VtO/CY5283n/2ENqa3NogvoOI2FAs+OPtHns/xhhzFdeT7Blv1pOUvbTb4m226Gy1Mq0plASjoesvEaQggESH16nUCp6gwxhjASYokqQ9nfd6tdVFev6oBl9tLmpvcYb1VmHig2kOX8cYY8w3gqqepK/Za2U6c56TImOMSU/u9SQZY4yxgMRJkjHGGLODkyRjjDFmBydJxhhjzA5OkowxxiTxzDPPDFy2bNkAb96jurpaec899wxNSUkZOXTo0JH79u1zaVu6oJ/dyhhjrKPGI+Wx1/dfSTQ2aNWKSLW2T/bgsojxA/2ynuTChQsH33333dc///zzktbWVmpsbHSpccgtScYYY+0aj5TH1n32fZKxQasGAGODVl332fdJjUfK/a6e5LVr15RHjx6NfPrpp6sBICwsTMTFxbm0bpKTJGOMsXbX919JhN7YMTfojYrr+6/4XT3JoqIidWxsrP6BBx5IzsjIGDF37tyk69ev+64lSUSxRLSXiIrN32PsXGcgopPmrx3u3JMxxpj3WFqQzh53lhT1JPV6PRUWFvZ6/PHHqwoLCwt69eplfOmll1zaBcbdluQSAPuFEKkA9puf29IihPiR+WuGm/dkrFs7S3bi7i13Y8yGMbh7y93YWbLTK69h8lOh2Y5vvpmI/QeGYf+BVPN309ehr7JQodnezWtuwjffTLR5TTBRRKpttsrsHfcUb9STTE5O1g4YMEA7efLkJgCYO3dubX5+fi9X4nJ34s5MAHeaH28A8CWA5918T8Z6bGfJTiz/13K0GloBABVNFVhyeAmWHF6CXqpeCFGE4Lr2OuJ7x+Opm5/CtKHTbL5m+b+WAwCmDZ0m1R+F2VF4bhnKyzcB6Di0RNQLQBtEe+WBDiURodfXorDQ9N9TQvxMAKYEee5cDoxGU4nB1rZynDuX0+GaYNMne3BZ3WffJ3XoclUpjH2yB7tdT3LOnDk35eTkaOLj4w2O6kkmJCTogBv1JCdPnty0b9++qJKSEnVNTY0hIyOjbeTIkZWXL19Wnzx5MnzGjBkNne85ZMgQfXx8vDY/Pz907NixbV988UWf9PT0VlfidjdJDhBCWOp4aQDYm8obRkR5APQAVgohttl7QyJaCGAhAAwZMsTN8Fiw+fN3f25Pdp0165vbH1snT1taDa3483d/bn9PTZOmQ2Jl3lWh2Y6Si6vQ2laBsNAEDB32HBLiZ+L4d79EXd2/bL5GiGabxzteo8P586/gfNEK6A11Nq8xGltQcnFV0CZJyyxWT89ulaKeJAC8/fbbl+fPnz9Uq9XSkCFD2jZu3HjJlbgd1pMkon0AbPXh5gDYIISItrq2VgjRZVySiBKFEGVENBTAAQDZQoiLjoLzVD1JFrh2luzskMQqmuz+W+mRMGVYh6SrIhUi1BGob6vnpOklnVt3AKBQhKNPn3F2E6Q3ZE92+F+ULHE9yZ7pcT1JIcRP7Z0joqtElCCEqCCiBACVdt6jzPy9hIi+BDAOgH/+BDLZsNVN6mmdW6V6oUddW137/bhb1vNKLq7qkCABU+vOlwkSIFRotgdta5Ld4G536w4ACwCsNH/vMuJtnvHaLIRoI6I4ALcDeNPN+7IgtLNkJ1b+e2V7kpIDS7csJ0n33ehiLZc6FAAiqLtc/Y2c60muBLCZiB4B8AOABwGAiLIALBJCPAogA8BfiMgI02zalUIIr9T9YoFrZ8lOvPTNS9AZdVKH0oWmSSN1CH7PVher1FrbPN8zwbzDm/Uk3UqSQohrALJtHM8D8Kj58b8AjHbnPoz9+bs/yzJBAkB8by6+7S5bXaxSUymjpA6ByQDvuMP8glxaa0rqMGsdYcowPHXzUxJF4/8saxTl0cXaCZHUETAZ4CTJ/IJcWmsEQnRoNAiEhN4JWP7j5Twe2UOWLlZZJkgAer18xr6ZdLgKCPMLT938lCzGJPVCj3BVOA7POyxpHIFAjl2s1lSqaMcXsYDHLUnmN3qpXNpNymvk0vXr7+Q+MUavrwv6Leq8zdv1JPPz80OHDx8+wvIVERExbsWKFf1deQ9uSTLZ67weUmoCAndvuZs3EnBTWGiCbLtaTQQKCjpuYxcsjh07Fnvo0KHExsZGdUREhHbSpEllt9xyi9/Vkxw7dmybZdarXq9HfHz82Hnz5rnUj84tSSZ73W01JxXLRgK8EXrPDR32HBSKcKnDcECH80UrpA7Cp44dOxa7Z8+epMbGRjUANDY2qvfs2ZN07Ngxv6snaW3Hjh19hgwZ0paWlubSRu2cJJnsybV703p/V+a6hPiZGD78NYSFDgRAAJSOXiIJe3u8BqpDhw4l6vX6DrlBr9crDh065Hf1JK1t3Lgxds6cOddcjZuTJJM9ucxstUWuCdxfJMTPxO23H0b25AvoXLWDScPSgnT2uLOkqCdp0draSvv27Yv65S9/Wetq3Jwkmew9dfNTCFOGSR2GTXJO4P4mLDRB6hBsUqls1pIPWBERETZbZfaOe4o36klabNmyJWrEiBHNgwcP1rsaFydJJnvThk7D8h8vR0LvBBAICpLPjy1vJOA5chyjJApBWtpLUofhU5MmTSpTqVQdmvUqlco4adIkt+tJ5ubmxmg0GiUAOKonaTluqSe5Zs2a8piYGH1JSYm6oKBAnZGR0bZ06dLKKVOm1J08ebLbH5xNmzbFPvjggz2aeMSzW5lfmDZ0WvtMUrnMdp2bPpdnt3qQZQZpQcEzksZB1AtCtHSoYxlMLLNYPT27Vap6ktevX1d8/fXXfTZs2PBDT+J2WE9SSlxPktljXUcyKjQK9W31EPDdz/Lc9LlYOn6pz+4XTArPLUN5+UeS3HvgwPnIGO7fs1m5nmTP9LieJGNyZN2yBExJ84XDL/gkUXKC9C5Lkiov3wTArSpHTlMowjF8+GtB12pkjnGSZAHBkjB90Q37VelXXn1/ZkqUlmRpaln+A/DQL0AqZTT6D5iGmmsH0dpWEbTdqoFEzvUkGZMNS6K0dMPG945vn1iz5PASj92Hl334lnXCBKyLM5sSXGzfu6DRfOJwH1iVKgaTfsLDN4FItvUkGZObzt2wFicqT+Djoo8dvp5ADrtsedmHtBLiZ3Zp9UVHZ7YnTpUyCgZjE4S4sRm+QhEedLNUmWdwkmRBYen4pRjXfxxW/nsl6tpMO6hYEmJC74QO+7C+euRV/PP8P2EUXRe3c/1IeeqcODu3Nrk7lfUUz25lrBvWs2gt3be87IPJGc9u7Rme3cpYD9jrvmWMBQf5bF3CGGMsqHi7niQAvPzyy/1vuummkampqSOnT5+e0tzcTK683q0kSUQPENFZIjISUVY3191DREVEdIGIPDfNkDHGmMeVln4Ue/jrCaP3H7gp8/DXE0aXln7kdpksKXz//fch69evH3Dy5MmC4uLiswaDgd5//32X/izutiTPAJgFwO7CMSJSAngHwL0ARgB4iIhGuHlfxhhjXlBa+lFs8YXXkrTaSjUgoNVWqosvvJbkiUQpRT1Jg8FATU1NCp1Oh5aWFsWgQYN09q61xa0kKYQoFEIUObjsVgAXhBAlQggtgE0AeJoZY4zJ0PeX1iUajW0dcoPR2Kb4/tI6v6snmZKSonv88cc1KSkpY/r37z82MjLSMGvWrOuuxO2LMclEAFesnpeaj9lERAuJKI+I8qqqqrweHGOMsRu02iqbdSPtHXeWFPUkq6qqlDt37oy+cOHCaY1Gc6q5uVnx7rvvera7lYj2EdEZG19eaQ0KIdYLIbKEEFn9+vXzxi0YY4zZoVb3s9kqs3fcU7xRTzI3N7fPkCFD2gYOHKgPDQ0V999/f92//vWvCFficpgkhRA/FUKMsvG13cl7lAEYbPV8kPkYY4wxmUlJfqJMoQjtsJOGQhFqTEl+wu/qSSYnJ2u/++67iIaGBoXRaMSBAwciMzIyXNrc2RfrJI8BSCWiFJiS4zwAP/fBfSVXn5uLyrfWQF9RAVVCAvovfhpR06fbvbbitT9C1Jl2g1FGR2NAzot2r2eMMW8YNGh+DWAam9Rqq9RqdT9tSvITZZbjPSVFPcnJkyc3TZ8+vXbMmDEZKpUKI0eObH7mmWdcGsdza8cdIvoZgLcB9ANQB+CkEGIKEQ0E8L4QYqr5uqkA1gBQAvhACPGaM+/vzzvu1OfmouKlZRCtN35pobAwJLyyokviq8/NRfkLLwLm/nZr0Q/NQ8If/uD1eBljgYF33OkZr+y4I4T4FMCnNo6XA5hq9XwXgF3u3MvfVL61pkOCBADR2orKt9Z0SJL1ubkoX/ICYLBdzaVu4ybUbdwE1cCBiJj0EzQe+sqpliljjDH38bZ0XqKvsNn6h768HIUjR5mSYkgIoHNuyY6+vBx1Gzd1eF7+++fR/N133NJkjAU1rifpRyxji+iuG9vSanQyQdolBOo2bkKvm2/mFiVjLGh5s54k793qQZaxRcvkG18p/93vUThyFCpeftmn92WMsUDHSdKDKt9aY3PyjU8YDKjbuIkTJWOMeRAnSQ+yNw7pS3Wb/yl1CIwxFjA4SXqQMipK6hDszpJljDHmOk6SHmR0fIlP1OfmSh0CY4w55It6kq+88kr/1NTUkTfddNPIFStW9Hf19ZwkPUjU10sdAgCg4sUcTpSMsR7bUFYdO/abM6MTDp7MHPvNmdEbyqr9sp7ksWPHwj788MN+3333XWFhYeHZzz//PPrMmTN2y2rZwknSg1QJCVKHAAAQOh0q/rAc9bm5KJ6cjcKMESienM2JkzHm0Iay6thlF8qSrmr1agHgqlavXnahLMkTidLX9SRPnz4dPm7cuMbIyEhjSEgIbr/99oZNmzZFuxIzJ0kP6r/4aalDaCeam1HxYg705eWAENCXl6PipWWcKBlj3frvS5rENqPokBvajELx35c0fldP8kc/+lHLv//970iNRqNsaGhQ7N27N+rKlSsulfziJOlBUdOnI/qheVKH0U502qzAsi0eY4zZU6nV20wi9o47S4p6kjfffHPrU089pcnOzk676667UkeOHNmsVCptXWoXJ0kPS/jDH4Bwm1VbZEEOy1QYY/LVX62y2Sqzd9xTvFFPEgAWL15cffbs2cK8vLyimJgYQ1pamkulsjhJelh9bi7Q0iJ1GHbJZdyUMSZPzyTHl4UqqMNk/VAFGZ9Jjve7epIAUFZWpgKA4uJi9c6dO6MfffRRl0p+8d6tHiab7kyVCkTUocuVwsJkNW7KGJOfBYlxNYBpbLJSq1f3V6u0zyTHl1mO95QU9SQBYMaMGcPq6upUKpVKrFmz5nJcXJxLi8ndqifpbf5YT7IwY0T3m5v7Qng4Bq4wbU/nbNFnJq2tmhq8XlKBsjYdEkND8MLQBMyO98tZ90xiXE+yZ7xST5J1pUpIMM0olUjnIs2cFL2ju6Rm71x3x58uvAxLm7+0TYenC00T/zhRMiYtTpIe1n/x06h4aVmXgsu+MPBPb3JS9IGtmho8V3QFLUZTj0Fpmw7PFV1pP2/r3L/rG7FZU2vzNUvPl6Jz0TSd+TgnScYc43qSfsSSpCrfWuPTFiVFR3OC9JHXSyrak51Fi1Hg9ZIK1Gh1aOnU295iFNhQ3nU4p8Uo8Hjh5S7HLWoNctnokAUAo9FoJIVCId/xNTe4W0/SaDQS7OwsyrNbvSBq+nSkHtiPjHOFGPinN0HRLm3w4DIKC0NCzotevQe7oazNdrHs0jYdmj38X1D8wZPI+OoUtmrcmjPB2JmqqqooczJgVoxGI1VVVUUBOGPrvFstSSJ6AMByABkAbhVC2JxlQ0SXADQAMADQdx5UDmRR06e3t/Dqc3Nx9bU/wuDBosyqgQN5Qo6PJYaGoNROovSGWoORxyiZW/R6/aMajeZ9jUYzCtw46swI4Ixer3/U1km3ZrcSUYb5Bn8B8JyDJJklhHBpdpU/zm51RsXLL6Nu46aev4FSiYErX+fEKJHniy7b7D71tkGhIcj78Uif35f5F1uzW1nPufUbhRCiUAhR5KlggkXCH/6AgX96E6qBA7u9ThkdDQoL63CMwsIwcOXrAMCbl0tkx1XP9QS4wl43L2PMe3zV7BYAviCi40S0sLsLiWghEeURUV5VVZWPwvO9LuOWNpLhgJwXkfDKClMyJYJq4EAkvLICAFDx0jLevFwCzxddlmxCTWJoiCT3ZSyYORyTJKJ9AOJtnMoRQmx38j53CCHKiKg/gL1EdE4I8ZWtC4UQ6wGsB0zdrU6+v1/rMCPWxsL/zt2qxZOzuywxsWxezl2w3vPAiWIcrmuS7P4vDOUtBRnzNYdJUgjxU3dvIoQoM3+vJKJPAdwKwGaSDFbWE3wcsbdJOW9e7j1bNTWSJkiAJ+0wJgWvd7cSUW8iirQ8BnA37Ey1Zc6xt0k5b17uPa+X8C8gjAUjt5IkEf2MiEoBTACwk4j2mI8PJKJd5ssGAPiaiPIB/BvATiHE5+7cN9j1X/y0zTFM3rzce+Qwaeb5IvsbDzDGvMOtdZJCiE8BfGrjeDmAqebHJQDGunMf1pGjMUzmeb5eG2nL/5bX4I30IZLGwFiw4W3p/JQrY5jMfdl9IyVZG2mNN6ljzPd45wXGHNiqqcFmTa3UYTDGJMBJkjEHbG1oLgXedJMx3+PuVsYckMOkHcC0I0fWv85yYWbGfIhbkow5IJedbgimSiMCN+pRcnUQxryLkyRjDrwwNAHhCmk7OwmmlqQ1Sw1Lxpj3cJJkzIHZ8bFYlT5YsvsPCg3pkiAt5NIVzFig4iTJmBNmx8dC6YP7WNqrg0JD8E7GEGju+hHyfjwSg+x0+cqlK5ixQMVJkjEnGbo5907GEI90ya6zSozWk3JsdfmGK4g3PWfMyzhJMuYke625QaEhHumSVcL+JuaW9x8UGgIy33NV+mCe3cqYl/ESEMac9MLQBDxXdKXDmknr1tzs+Fi8XlLR4+3rfjGw+4Q3Oz6WkyJjPsYtScac5Exrzla3aAiA3sob/9Q6d8oqASwYGMv7sjImQ9ySZMwFjlpzlnOvl1Twon/GAgAnScY8jLtFGQsc3N3KGGOM2cFJkjHGGLODkyRjjDFmBydJxhhjzA5OkowxxpgdJIT0xWTtIaIqAD9IHUcncQCqpQ7CCRyn5/hDjADH6Wn+GmeSEKKfVMEEGlknSTkiojwhRJbUcTjCcXqOP8QIcJyexnEygLtbGWOMMbs4STLGGGN2cJJ03XqpA3ASx+k5/hAjwHF6GsfJeEySMcYYs4dbkowxxpgdnCQZY4wxOzhJOkBEDxDRWSIyEpHdadZEdImIThPRSSLK82WM5vs7G+c9RFRERBeIaIkvYzTfP5aI9hJRsfl7jJ3rDObP8iQR7fBRbN1+NkQUSkQfm88fJaJkX8RlIw5HcT5MRFVWn9+jEsT4ARFVEtEZO+eJiNaa/wyniOhmX8dojsNRnHcSUb3VZ7lMghgHE9FBIiow/xt/ysY1svg8A5IQgr+6+QKQASAdwJcAsrq57hKAODnHCVN934sAhgJQA8gHMMLHcb4JYIn58RIAb9i5rtHHcTn8bAA8BuA98+N5AD6W4O/ZmTgfBrDO17F1iuEnAG4GcMbO+akAdsNUg3o8gKMyjfNOAJ9J/FkmALjZ/DgSwHkbf+ey+DwD8Ytbkg4IIQqFEEVSx+GIk3HeCuCCEKJECKEFsAnATO9H18FMABvMjzcAuN/H97fHmc/GOvYtALKJiHwYIyCPv0OHhBBfAajp5pKZAD4UJkcARBNRgm+iu8GJOCUnhKgQQnxnftwAoBBAYqfLZPF5BiJOkp4jAHxBRMeJaKHUwdiRCOCK1fNSdP3H5m0DhBAV5scaAAPsXBdGRHlEdISIfJFInfls2q8RQugB1APo64PYbMZgZu/vcLa5220LEQ32TWgukcPPorMmEFE+Ee0mopFSBmLu4h8H4GinU/70efoVldQByAER7QMQb+NUjhBiu5Nvc4cQooyI+gPYS0TnzL+leoyH4vS67uK0fiKEEERkbw1SkvnzHArgABGdFkJc9HSsASoXwEYhRBsR/RdMrd/JEsfkr76D6WexkYimAtgGIFWKQIgoAsBWAE8LIa5LEUMw4iQJQAjxUw+8R5n5eyURfQpTt5hHk6QH4iwDYN2qGGQ+5lHdxUlEV4koQQhRYe4OqrTzHpbPs4SIvoTpt2dvJklnPhvLNaVEpAIQBeCaF2OyxWGcQgjrmN6HaRxYbnzys+gu62QkhNhFRO8SUZwQwqcbnxNRCEwJ8iMhxCc2LvGLz9MfcXerBxBRbyKKtDwGcDcAm7PlJHYMQCoRpRCRGqbJJz6ZOWplB4AF5scLAHRpARNRDBGFmh/HAbgdQIGX43Lms7GOfQ6AA0IIX+/G4TDOTmNRM2Aaw5KbHQB+ZZ6VOR5AvVU3vGwQUbxl3JmIboXp/0yf/mJkvv/fABQKIf7bzmV+8Xn6JalnDsn9C8DPYOrfbwNwFcAe8/GBAHaZHw+FaZZhPoCzMHV/yi5O8/OpMM2OuyhRnH0B7AdQDGAfgFjz8SwA75sf/xjAafPneRrAIz6KrctnA2AFgBnmx2EA/gngAoB/Axgq0c+kozhfN/8c5gM4CGC4BDFuBFABQGf+uXwEwCIAi8znCcA75j/DaXQzc1ziOJ+w+iyPAPixBDHeAdOch1MATpq/psrx8wzEL96WjjHGGLODu1sZY4wxOzhJMsYYY3ZwkmSMMcbs4CTJGGOM2cFJkjHGGLODkyRjjDFmBydJxhhjzI7/D6KY90N+JPfwAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "for i in range(10):\n",
        "    plt.scatter(x[idx[i],0],x[idx[i],1],label=\"class_\"+str(i))\n",
        "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "UfFHcZJOr0Sz"
      },
      "outputs": [],
      "source": [
        "foreground_classes = {'class_0','class_1' }\n",
        "\n",
        "background_classes = {'bg_classes',}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jqbvfbwVr0TN",
        "outputId": "a537c0b1-454a-4e0b-bab4-ddfd2f02dc1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1100/1100 [02:06<00:00,  8.69it/s]\n"
          ]
        }
      ],
      "source": [
        "desired_num = 1100\n",
        "mosaic_list_of_images =[]\n",
        "mosaic_label = []\n",
        "fore_idx=[]\n",
        "m = 2000\n",
        "for j in tqdm(range(desired_num)):\n",
        "    np.random.seed(j)\n",
        "    fg_class  = np.random.randint(0,3)\n",
        "    fg_idx = np.random.randint(0,m)\n",
        "    a = []\n",
        "    for i in range(m):\n",
        "        if i == fg_idx:\n",
        "            b = np.random.choice(np.where(idx[fg_class]==True)[0],size=1)\n",
        "            a.append(x[b])\n",
        "#             print(\"foreground \"+str(fg_class)+\" present at \" + str(fg_idx))\n",
        "        else:\n",
        "            bg_class = np.random.randint(3,10)\n",
        "            b = np.random.choice(np.where(idx[bg_class]==True)[0],size=1)\n",
        "            a.append(x[b])\n",
        "#             print(\"background \"+str(bg_class)+\" present at \" + str(i))\n",
        "    a = np.concatenate(a,axis=0)\n",
        "    mosaic_list_of_images.append(np.reshape(a,(m,2)))\n",
        "    mosaic_label.append(fg_class)\n",
        "    fore_idx.append(fg_idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "BOsFmWfMr0TR"
      },
      "outputs": [],
      "source": [
        "# mosaic_list_of_images = np.concatenate(mosaic_list_of_images,axis=1).T\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2aIPMgLXNiXW",
        "outputId": "05902d34-5c25-4bc3-eb34-14151761e01c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1100, array([[ 1.41860713, -0.6338838 ],\n",
              "        [ 0.60177752,  1.40622261],\n",
              "        [ 0.64672986,  1.41882828],\n",
              "        ...,\n",
              "        [ 0.1760838 , -1.64428378],\n",
              "        [ 0.57482959,  1.52453398],\n",
              "        [ 1.71875213,  0.627341  ]]), (2000, 2))"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "len(mosaic_list_of_images), mosaic_list_of_images[0],mosaic_list_of_images[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "iPoIwbMHx44n"
      },
      "outputs": [],
      "source": [
        "class MosaicDataset(Dataset):\n",
        "  \"\"\"MosaicDataset dataset.\"\"\"\n",
        "\n",
        "  def __init__(self, mosaic_list_of_images, mosaic_label, fore_idx):\n",
        "    \"\"\"\n",
        "      Args:\n",
        "        csv_file (string): Path to the csv file with annotations.\n",
        "        root_dir (string): Directory with all the images.\n",
        "        transform (callable, optional): Optional transform to be applied\n",
        "            on a sample.\n",
        "    \"\"\"\n",
        "    self.mosaic = mosaic_list_of_images\n",
        "    self.label = mosaic_label\n",
        "    self.fore_idx = fore_idx\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.label)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.mosaic[idx] , self.label[idx], self.fore_idx[idx]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "fOPAJQJeW8Ah"
      },
      "outputs": [],
      "source": [
        "batch = 50\n",
        "msd1 = MosaicDataset(mosaic_list_of_images[0:100], mosaic_label[0:100] , fore_idx[0:100])\n",
        "train_loader = DataLoader( msd1 ,batch_size= batch ,shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "aWBIcyvGApLt"
      },
      "outputs": [],
      "source": [
        "data,_,_=iter(train_loader).next()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cauJIvKEAxKM",
        "outputId": "0e142134-41c9-4889-9c35-da407e3f2d12"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([50, 2000, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "qjNiQgxZW8bA"
      },
      "outputs": [],
      "source": [
        "batch = 250\n",
        "msd2 = MosaicDataset(mosaic_list_of_images[100:], mosaic_label[100:] , fore_idx[100:])\n",
        "test_loader = DataLoader( msd2 ,batch_size= batch ,shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "yda1E5ApiKpH"
      },
      "outputs": [],
      "source": [
        "class Focus(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Focus, self).__init__()\n",
        "        self.fc1 = nn.Linear(2,50, bias=False)\n",
        "        self.fc2 = nn.Linear(50,1,bias=False)\n",
        "        torch.nn.init.xavier_normal_(self.fc1.weight)\n",
        "        torch.nn.init.xavier_normal_(self.fc2.weight)\n",
        "        #self.fc2 = nn.Linear(64, 1, bias=False)\n",
        "        #torch.nn.init.xavier_normal_(self.fc2.weight)\n",
        "\n",
        "    def forward(self,z):\n",
        "        #print(\"data\",z)\n",
        "        batch = z.size(0)\n",
        "        patches = z.size(1)\n",
        "        z = z.view(batch,patches,2*1)\n",
        "        alp1,ft1 = self.helper(z)\n",
        "\n",
        "        alpha = F.softmax(alp1,dim=1)\n",
        "        #print(self.training)\n",
        "        \n",
        "        if self.training:\n",
        "            alpha =alpha[:,:,0]\n",
        "            y = ft1 \n",
        "            return alpha,y\n",
        "        else:\n",
        "            #alpha_cumsum = torch.cumsum(alpha, dim = 1)\n",
        "            #print(alpha_cumsum)\n",
        "            #len_batch = alpha_cumsum.size(0)\n",
        "            #patches = alpha_cumsum.size(1)\n",
        "            #rand_prob = torch.rand(len_batch,patches, 1).to(device)\n",
        "            #alpha_relu = F.relu(rand_prob-alpha_cumsum)\n",
        "            #print(alpha_relu)\n",
        "            #alpha_index = torch.count_nonzero(alpha_relu,dim=1)\n",
        "            #alpha_hard = F.one_hot(alpha_index,num_classes=patches)\n",
        "            #print(alpha_hard)\n",
        "            #alpha_hard = torch.transpose(alpha_hard,dim0=1,dim1=2)\n",
        "            #print(ft1,\"alpha_hard\",alpha_hard) \n",
        "            #y = torch.sum(alpha_hard*ft1,dim=1)\n",
        "            #print(alpha,alpha.shape)\n",
        "         \n",
        "        \n",
        "            index = torch.argmax(alpha,dim=1)\n",
        "            hard_alpha = torch.nn.functional.one_hot(index[:,0], patches)\n",
        "            y = torch.sum(hard_alpha[:,:,None]*ft1,dim=1)\n",
        "            alpha = alpha[:,:,0]\n",
        "            return alpha,y\n",
        "    \n",
        "    def helper(self, x):\n",
        "        x1 = x\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x,x1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "0dYXnywAD-4l"
      },
      "outputs": [],
      "source": [
        "class Classification(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Classification, self).__init__()\n",
        "    self.fc1 = nn.Linear(2, 3)\n",
        "    torch.nn.init.xavier_normal_(self.fc1.weight)\n",
        "    torch.nn.init.zeros_(self.fc1.bias)\n",
        "\n",
        "  def forward(self, x):\n",
        "    #print(x.shape)\n",
        "    #x = x.view(-1, 1)\n",
        "    #print(x.shape)\n",
        "    x = self.fc1(x)\n",
        "    # print(x.shape)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "lSa6O9f6XNf4"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(13)\n",
        "focus_net = Focus().double()\n",
        "focus_net = focus_net.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "36k3H2G-XO9A"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(13)\n",
        "classify = Classification().double()\n",
        "classify = classify.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "bK78aII8-GDl"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "optimizer_classify = optim.Adam(classify.parameters(), lr=0.1 ) #, momentum=0.9)\n",
        "optimizer_focus = optim.Adam(focus_net.parameters(), lr=0.1 ) #, momentum=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "h0mjWiFG-GDl"
      },
      "outputs": [],
      "source": [
        "def my_cross_entropy(output,target,alpha):\n",
        "    criterion = nn.CrossEntropyLoss(reduce=False)\n",
        "    \n",
        "    batch = output.size(0)\n",
        "    #print(batch)\n",
        "    patches = output.size(1)\n",
        "    classes = output.size(2)\n",
        "    \n",
        "    \n",
        "    \n",
        "    output = torch.reshape(output,(batch*patches,classes))\n",
        "    \n",
        "    \n",
        "    target = target.repeat_interleave(patches)\n",
        "    \n",
        "    loss = criterion(output,target)\n",
        "    \n",
        "    #print(loss,loss.shape)\n",
        "    loss = torch.reshape(loss,(batch,patches))\n",
        "    #print(loss.size())\n",
        "    final_loss = torch.sum(torch.mul(loss,alpha),dim=1)\n",
        "    #print(final_loss.shape)\n",
        "    final_loss = torch.mean(final_loss,dim=0)\n",
        "    \n",
        "    \n",
        "    \n",
        "    #print(final_loss)\n",
        "    return final_loss\n",
        "    \n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "pjD2VZuV9Ed4"
      },
      "outputs": [],
      "source": [
        "col1=[]\n",
        "col2=[]\n",
        "col3=[]\n",
        "col4=[]\n",
        "col5=[]\n",
        "col6=[]\n",
        "col7=[]\n",
        "col8=[]\n",
        "col9=[]\n",
        "col10=[]\n",
        "col11=[]\n",
        "col12=[]\n",
        "col13=[]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "eEVrBg7d-GDl"
      },
      "outputs": [],
      "source": [
        "def plot_attended_data(trainloader,net,epoch):\n",
        "    attd_data =[]\n",
        "    lbls = []\n",
        "    for data in trainloader:\n",
        "        inputs, labels , fore_idx = data\n",
        "        inputs = inputs.double()\n",
        "        inputs, labels , fore_idx = inputs.to(device),labels.to(device), fore_idx.to(device)\n",
        "        alphas, avg_images = focus_net(inputs)\n",
        "        attd_data.append(avg_images.numpy())\n",
        "        lbls.append(labels)\n",
        "    attd_data = np.concatenate(attd_data,axis=0)\n",
        "    lbls = np.concatenate(lbls,axis=0)\n",
        "    plt.figure(figsize=(6,8))\n",
        "    plt.scatter(attd_data[:,0],attd_data[:,1],c=lbls)\n",
        "    plt.title(\"EPOCH_\"+str(epoch))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "uALi25pmzQHV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e670e2a3-21ea-450d-9f54-87e8d64808db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1.1721, device='cuda:0', dtype=torch.float64)\n",
            "tensor(1.1741, device='cuda:0', dtype=torch.float64)\n",
            "====================================================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        }
      ],
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "count = 0\n",
        "flag = 1\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "  for data in train_loader:\n",
        "    inputs, labels , fore_idx = data\n",
        "    inputs = inputs.double()\n",
        "    inputs, labels , fore_idx = inputs.to(device),labels.to(device), fore_idx.to(device)\n",
        "    alphas, avg_images = focus_net(inputs)\n",
        "    outputs = classify(avg_images)\n",
        "    loss = my_cross_entropy(outputs,labels,alphas)\n",
        "    print(loss)\n",
        "    # print(outputs.shape)\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "\n",
        "print(\"=\"*100)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "4vmNprlPzTjP"
      },
      "outputs": [],
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "count = 0\n",
        "flag = 1\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "  for data in test_loader:\n",
        "    inputs, labels , fore_idx = data\n",
        "    inputs = inputs.double()\n",
        "    inputs, labels , fore_idx = inputs.to(device),labels.to(device), fore_idx.to(device)\n",
        "    alphas, avg_images = focus_net(inputs)\n",
        "    outputs = classify(avg_images)\n",
        "\n",
        "    _, predicted = torch.max(outputs.data, 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "_nvicAzw-GDm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54b3f92f-ec9b-46ab-9b39-f702c25645d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('fc1.weight', Parameter containing:\n",
            "tensor([[-0.1181, -0.0030],\n",
            "        [-0.4268,  0.2945],\n",
            "        [ 0.2314, -0.0850],\n",
            "        [ 0.2047,  0.3473],\n",
            "        [ 0.1353, -0.0880],\n",
            "        [ 0.0341, -0.1290],\n",
            "        [ 0.0430, -0.1165],\n",
            "        [ 0.0171, -0.0899],\n",
            "        [ 0.2401, -0.0346],\n",
            "        [-0.2021,  0.1027],\n",
            "        [-0.4337, -0.2103],\n",
            "        [ 0.1129, -0.1162],\n",
            "        [ 0.1906, -0.4065],\n",
            "        [-0.1823, -0.6261],\n",
            "        [-0.3808, -0.0406],\n",
            "        [ 0.4075, -0.2476],\n",
            "        [-0.0116, -0.2889],\n",
            "        [ 0.5438, -0.4551],\n",
            "        [ 0.2557,  0.1164],\n",
            "        [-0.1854,  0.4136],\n",
            "        [-0.1875,  0.0379],\n",
            "        [-0.1165, -0.1601],\n",
            "        [-0.0614,  0.1823],\n",
            "        [ 0.0824, -0.0428],\n",
            "        [ 0.1910,  0.2516],\n",
            "        [-0.1856,  0.1568],\n",
            "        [-0.1999, -0.2636],\n",
            "        [-0.0564,  0.2618],\n",
            "        [-0.4972, -0.1039],\n",
            "        [ 0.0093,  0.0324],\n",
            "        [ 0.1483, -0.0172],\n",
            "        [ 0.0681, -0.1813],\n",
            "        [ 0.1324,  0.2163],\n",
            "        [ 0.0371, -0.1833],\n",
            "        [-0.1590, -0.0161],\n",
            "        [-0.3815,  0.1033],\n",
            "        [ 0.0966,  0.0421],\n",
            "        [-0.0925, -0.1502],\n",
            "        [-0.0524, -0.1245],\n",
            "        [-0.1022, -0.2882],\n",
            "        [ 0.1828, -0.2956],\n",
            "        [-0.0681,  0.0178],\n",
            "        [-0.1532, -0.3065],\n",
            "        [-0.0498, -0.0991],\n",
            "        [-0.1932,  0.2572],\n",
            "        [ 0.3471, -0.0632],\n",
            "        [-0.0676, -0.0115],\n",
            "        [ 0.0253,  0.2514],\n",
            "        [-0.0551,  0.2164],\n",
            "        [ 0.3290,  0.2001]], device='cuda:0', dtype=torch.float64,\n",
            "       requires_grad=True))\n",
            "('fc2.weight', Parameter containing:\n",
            "tensor([[-0.1399, -0.1249, -0.0564, -0.4391,  0.1649, -0.2374,  0.0706,  0.2608,\n",
            "         -0.0010, -0.3136,  0.1942,  0.2273,  0.3008,  0.1308,  0.1953,  0.1974,\n",
            "         -0.0421, -0.2338, -0.1033, -0.0973,  0.3072, -0.2419,  0.1255,  0.0583,\n",
            "          0.1224, -0.0315, -0.2101, -0.1591, -0.0293, -0.0339, -0.0007,  0.2735,\n",
            "          0.4036,  0.0824, -0.1448, -0.0010,  0.1737,  0.0394,  0.1208,  0.0653,\n",
            "         -0.1393, -0.1117,  0.0945, -0.1343,  0.1293, -0.0568, -0.0054,  0.0801,\n",
            "          0.1450, -0.2331]], device='cuda:0', dtype=torch.float64,\n",
            "       requires_grad=True))\n"
          ]
        }
      ],
      "source": [
        "for param in focus_net.named_parameters():\n",
        "    print(param)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "Yl41sE8vFERk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba8963be-a87f-45bc-b75f-29320b2505f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,     2] loss: 1.146\n",
            "[2,     2] loss: 1.100\n",
            "[3,     2] loss: 1.101\n",
            "[4,     2] loss: 1.087\n",
            "[5,     2] loss: 1.087\n",
            "[6,     2] loss: 1.086\n",
            "[7,     2] loss: 1.090\n",
            "[8,     2] loss: 1.086\n",
            "[9,     2] loss: 1.086\n",
            "[10,     2] loss: 1.092\n",
            "[11,     2] loss: 1.086\n",
            "[12,     2] loss: 1.086\n",
            "[13,     2] loss: 1.090\n",
            "[14,     2] loss: 1.088\n",
            "[15,     2] loss: 1.087\n",
            "[16,     2] loss: 1.092\n",
            "[17,     2] loss: 1.093\n",
            "[18,     2] loss: 1.094\n",
            "[19,     2] loss: 1.088\n",
            "[20,     2] loss: 1.086\n",
            "[21,     2] loss: 1.089\n",
            "[22,     2] loss: 1.104\n",
            "[23,     2] loss: 1.090\n",
            "[24,     2] loss: 1.087\n",
            "[25,     2] loss: 1.091\n",
            "[26,     2] loss: 1.090\n",
            "[27,     2] loss: 1.085\n",
            "[28,     2] loss: 1.086\n",
            "[29,     2] loss: 1.089\n",
            "[30,     2] loss: 1.094\n",
            "[31,     2] loss: 1.086\n",
            "[32,     2] loss: 1.086\n",
            "[33,     2] loss: 1.085\n",
            "[34,     2] loss: 1.093\n",
            "[35,     2] loss: 1.086\n",
            "[36,     2] loss: 1.093\n",
            "[37,     2] loss: 1.091\n",
            "[38,     2] loss: 1.085\n",
            "[39,     2] loss: 1.087\n",
            "[40,     2] loss: 1.086\n",
            "[41,     2] loss: 1.086\n",
            "[42,     2] loss: 1.087\n",
            "[43,     2] loss: 1.085\n",
            "[44,     2] loss: 1.088\n",
            "[45,     2] loss: 1.088\n",
            "[46,     2] loss: 1.086\n",
            "[47,     2] loss: 1.086\n",
            "[48,     2] loss: 1.089\n",
            "[49,     2] loss: 1.103\n",
            "[50,     2] loss: 1.095\n",
            "[51,     2] loss: 1.097\n",
            "[52,     2] loss: 1.091\n",
            "[53,     2] loss: 1.089\n",
            "[54,     2] loss: 1.089\n",
            "[55,     2] loss: 1.089\n",
            "[56,     2] loss: 1.087\n",
            "[57,     2] loss: 1.084\n",
            "[58,     2] loss: 1.087\n",
            "[59,     2] loss: 1.087\n",
            "[60,     2] loss: 1.091\n",
            "[61,     2] loss: 1.086\n",
            "[62,     2] loss: 1.087\n",
            "[63,     2] loss: 1.087\n",
            "[64,     2] loss: 1.092\n",
            "[65,     2] loss: 1.087\n",
            "[66,     2] loss: 1.087\n",
            "[67,     2] loss: 1.086\n",
            "[68,     2] loss: 1.086\n",
            "[69,     2] loss: 1.086\n",
            "[70,     2] loss: 1.089\n",
            "[71,     2] loss: 1.088\n",
            "[72,     2] loss: 1.086\n",
            "[73,     2] loss: 1.086\n",
            "[74,     2] loss: 1.086\n",
            "[75,     2] loss: 1.087\n",
            "[76,     2] loss: 1.087\n",
            "[77,     2] loss: 1.088\n",
            "[78,     2] loss: 1.087\n",
            "[79,     2] loss: 1.088\n",
            "[80,     2] loss: 1.092\n",
            "[81,     2] loss: 1.086\n",
            "[82,     2] loss: 1.091\n",
            "[83,     2] loss: 1.088\n",
            "[84,     2] loss: 1.088\n",
            "[85,     2] loss: 1.089\n",
            "[86,     2] loss: 1.090\n",
            "[87,     2] loss: 1.085\n",
            "[88,     2] loss: 1.088\n",
            "[89,     2] loss: 1.085\n",
            "[90,     2] loss: 1.085\n",
            "[91,     2] loss: 1.088\n",
            "[92,     2] loss: 1.095\n",
            "[93,     2] loss: 1.087\n",
            "[94,     2] loss: 1.085\n",
            "[95,     2] loss: 1.085\n",
            "[96,     2] loss: 1.086\n",
            "[97,     2] loss: 1.087\n",
            "[98,     2] loss: 1.087\n",
            "[99,     2] loss: 1.086\n",
            "[100,     2] loss: 1.086\n",
            "[101,     2] loss: 1.086\n",
            "[102,     2] loss: 1.087\n",
            "[103,     2] loss: 1.090\n",
            "[104,     2] loss: 1.086\n",
            "[105,     2] loss: 1.093\n",
            "[106,     2] loss: 1.089\n",
            "[107,     2] loss: 1.087\n",
            "[108,     2] loss: 1.087\n",
            "[109,     2] loss: 1.087\n",
            "[110,     2] loss: 1.086\n",
            "[111,     2] loss: 1.086\n",
            "[112,     2] loss: 1.090\n",
            "[113,     2] loss: 1.090\n",
            "[114,     2] loss: 1.085\n",
            "[115,     2] loss: 1.085\n",
            "[116,     2] loss: 1.091\n",
            "[117,     2] loss: 1.087\n",
            "[118,     2] loss: 1.086\n",
            "[119,     2] loss: 1.091\n",
            "[120,     2] loss: 1.087\n",
            "[121,     2] loss: 1.090\n",
            "[122,     2] loss: 1.090\n",
            "[123,     2] loss: 1.087\n",
            "[124,     2] loss: 1.087\n",
            "[125,     2] loss: 1.091\n",
            "[126,     2] loss: 1.089\n",
            "[127,     2] loss: 1.091\n",
            "[128,     2] loss: 1.087\n",
            "[129,     2] loss: 1.089\n",
            "[130,     2] loss: 1.091\n",
            "[131,     2] loss: 1.089\n",
            "[132,     2] loss: 1.085\n",
            "[133,     2] loss: 1.089\n",
            "[134,     2] loss: 1.087\n",
            "[135,     2] loss: 1.086\n",
            "[136,     2] loss: 1.088\n",
            "[137,     2] loss: 1.086\n",
            "[138,     2] loss: 1.088\n",
            "[139,     2] loss: 1.090\n",
            "[140,     2] loss: 1.086\n",
            "[141,     2] loss: 1.091\n",
            "[142,     2] loss: 1.090\n",
            "[143,     2] loss: 1.087\n",
            "[144,     2] loss: 1.089\n",
            "[145,     2] loss: 1.088\n",
            "[146,     2] loss: 1.088\n",
            "[147,     2] loss: 1.086\n",
            "[148,     2] loss: 1.087\n",
            "[149,     2] loss: 1.087\n",
            "[150,     2] loss: 1.087\n",
            "[151,     2] loss: 1.085\n",
            "[152,     2] loss: 1.092\n",
            "[153,     2] loss: 1.086\n",
            "[154,     2] loss: 1.085\n",
            "[155,     2] loss: 1.085\n",
            "[156,     2] loss: 1.086\n",
            "[157,     2] loss: 1.087\n",
            "[158,     2] loss: 1.087\n",
            "[159,     2] loss: 1.085\n",
            "[160,     2] loss: 1.092\n",
            "[161,     2] loss: 1.087\n",
            "[162,     2] loss: 1.085\n",
            "[163,     2] loss: 1.085\n",
            "[164,     2] loss: 1.090\n",
            "[165,     2] loss: 1.087\n",
            "[166,     2] loss: 1.085\n",
            "[167,     2] loss: 1.084\n",
            "[168,     2] loss: 1.086\n",
            "[169,     2] loss: 1.088\n",
            "[170,     2] loss: 1.086\n",
            "[171,     2] loss: 1.085\n",
            "[172,     2] loss: 1.086\n",
            "[173,     2] loss: 1.087\n",
            "[174,     2] loss: 1.086\n",
            "[175,     2] loss: 1.089\n",
            "[176,     2] loss: 1.088\n",
            "[177,     2] loss: 1.085\n",
            "[178,     2] loss: 1.085\n",
            "[179,     2] loss: 1.084\n",
            "[180,     2] loss: 1.089\n",
            "[181,     2] loss: 1.084\n",
            "[182,     2] loss: 1.087\n",
            "[183,     2] loss: 1.086\n",
            "[184,     2] loss: 1.084\n",
            "[185,     2] loss: 1.093\n",
            "[186,     2] loss: 1.085\n",
            "[187,     2] loss: 1.097\n",
            "[188,     2] loss: 1.089\n",
            "[189,     2] loss: 1.091\n",
            "[190,     2] loss: 1.087\n",
            "[191,     2] loss: 1.087\n",
            "[192,     2] loss: 1.086\n",
            "[193,     2] loss: 1.087\n",
            "[194,     2] loss: 1.089\n",
            "[195,     2] loss: 1.088\n",
            "[196,     2] loss: 1.086\n",
            "[197,     2] loss: 1.087\n",
            "[198,     2] loss: 1.088\n",
            "[199,     2] loss: 1.092\n",
            "[200,     2] loss: 1.087\n",
            "[201,     2] loss: 1.094\n",
            "[202,     2] loss: 1.092\n",
            "[203,     2] loss: 1.091\n",
            "[204,     2] loss: 1.088\n",
            "[205,     2] loss: 1.098\n",
            "[206,     2] loss: 1.089\n",
            "[207,     2] loss: 1.087\n",
            "[208,     2] loss: 1.087\n",
            "[209,     2] loss: 1.087\n",
            "[210,     2] loss: 1.088\n",
            "[211,     2] loss: 1.085\n",
            "[212,     2] loss: 1.087\n",
            "[213,     2] loss: 1.086\n",
            "[214,     2] loss: 1.085\n",
            "[215,     2] loss: 1.094\n",
            "[216,     2] loss: 1.087\n",
            "[217,     2] loss: 1.085\n",
            "[218,     2] loss: 1.086\n",
            "[219,     2] loss: 1.098\n",
            "[220,     2] loss: 1.088\n",
            "[221,     2] loss: 1.084\n",
            "[222,     2] loss: 1.092\n",
            "[223,     2] loss: 1.099\n",
            "[224,     2] loss: 1.086\n",
            "[225,     2] loss: 1.092\n",
            "[226,     2] loss: 1.090\n",
            "[227,     2] loss: 1.091\n",
            "[228,     2] loss: 1.092\n",
            "[229,     2] loss: 1.091\n",
            "[230,     2] loss: 1.089\n",
            "[231,     2] loss: 1.092\n",
            "[232,     2] loss: 1.090\n",
            "[233,     2] loss: 1.087\n",
            "[234,     2] loss: 1.086\n",
            "[235,     2] loss: 1.086\n",
            "[236,     2] loss: 1.089\n",
            "[237,     2] loss: 1.089\n",
            "[238,     2] loss: 1.091\n",
            "[239,     2] loss: 1.086\n",
            "[240,     2] loss: 1.086\n",
            "[241,     2] loss: 1.089\n",
            "[242,     2] loss: 1.086\n",
            "[243,     2] loss: 1.087\n",
            "[244,     2] loss: 1.086\n",
            "[245,     2] loss: 1.091\n",
            "[246,     2] loss: 1.087\n",
            "[247,     2] loss: 1.086\n",
            "[248,     2] loss: 1.089\n",
            "[249,     2] loss: 1.095\n",
            "[250,     2] loss: 1.086\n",
            "[251,     2] loss: 1.087\n",
            "[252,     2] loss: 1.086\n",
            "[253,     2] loss: 1.097\n",
            "[254,     2] loss: 1.092\n",
            "[255,     2] loss: 1.086\n",
            "[256,     2] loss: 1.086\n",
            "[257,     2] loss: 1.087\n",
            "[258,     2] loss: 1.087\n",
            "[259,     2] loss: 1.086\n",
            "[260,     2] loss: 1.087\n",
            "[261,     2] loss: 1.086\n",
            "[262,     2] loss: 1.088\n",
            "[263,     2] loss: 1.091\n",
            "[264,     2] loss: 1.085\n",
            "[265,     2] loss: 1.095\n",
            "[266,     2] loss: 1.089\n",
            "[267,     2] loss: 1.087\n",
            "[268,     2] loss: 1.086\n",
            "[269,     2] loss: 1.087\n",
            "[270,     2] loss: 1.086\n",
            "[271,     2] loss: 1.088\n",
            "[272,     2] loss: 1.088\n",
            "[273,     2] loss: 1.092\n",
            "[274,     2] loss: 1.088\n",
            "[275,     2] loss: 1.093\n",
            "[276,     2] loss: 1.098\n",
            "[277,     2] loss: 1.089\n",
            "[278,     2] loss: 1.085\n",
            "[279,     2] loss: 1.086\n",
            "[280,     2] loss: 1.087\n",
            "[281,     2] loss: 1.087\n",
            "[282,     2] loss: 1.088\n",
            "[283,     2] loss: 1.088\n",
            "[284,     2] loss: 1.087\n",
            "[285,     2] loss: 1.086\n",
            "[286,     2] loss: 1.085\n",
            "[287,     2] loss: 1.089\n",
            "[288,     2] loss: 1.092\n",
            "[289,     2] loss: 1.090\n",
            "[290,     2] loss: 1.088\n",
            "[291,     2] loss: 1.096\n",
            "[292,     2] loss: 1.087\n",
            "[293,     2] loss: 1.084\n",
            "[294,     2] loss: 1.088\n",
            "[295,     2] loss: 1.088\n",
            "[296,     2] loss: 1.085\n",
            "[297,     2] loss: 1.085\n",
            "[298,     2] loss: 1.087\n",
            "[299,     2] loss: 1.086\n",
            "[300,     2] loss: 1.091\n",
            "[301,     2] loss: 1.085\n",
            "[302,     2] loss: 1.086\n",
            "[303,     2] loss: 1.088\n",
            "[304,     2] loss: 1.087\n",
            "[305,     2] loss: 1.087\n",
            "[306,     2] loss: 1.086\n",
            "[307,     2] loss: 1.088\n",
            "[308,     2] loss: 1.088\n",
            "[309,     2] loss: 1.086\n",
            "[310,     2] loss: 1.092\n",
            "[311,     2] loss: 1.089\n",
            "[312,     2] loss: 1.085\n",
            "[313,     2] loss: 1.085\n",
            "[314,     2] loss: 1.089\n",
            "[315,     2] loss: 1.088\n",
            "[316,     2] loss: 1.087\n",
            "[317,     2] loss: 1.087\n",
            "[318,     2] loss: 1.087\n",
            "[319,     2] loss: 1.087\n",
            "[320,     2] loss: 1.087\n",
            "[321,     2] loss: 1.096\n",
            "[322,     2] loss: 1.085\n",
            "[323,     2] loss: 1.086\n",
            "[324,     2] loss: 1.088\n",
            "[325,     2] loss: 1.088\n",
            "[326,     2] loss: 1.085\n",
            "[327,     2] loss: 1.089\n",
            "[328,     2] loss: 1.086\n",
            "[329,     2] loss: 1.090\n",
            "[330,     2] loss: 1.089\n",
            "[331,     2] loss: 1.085\n",
            "[332,     2] loss: 1.085\n",
            "[333,     2] loss: 1.088\n",
            "[334,     2] loss: 1.086\n",
            "[335,     2] loss: 1.085\n",
            "[336,     2] loss: 1.090\n",
            "[337,     2] loss: 1.085\n",
            "[338,     2] loss: 1.086\n",
            "[339,     2] loss: 1.092\n",
            "[340,     2] loss: 1.088\n",
            "[341,     2] loss: 1.089\n",
            "[342,     2] loss: 1.086\n",
            "[343,     2] loss: 1.088\n",
            "[344,     2] loss: 1.086\n",
            "[345,     2] loss: 1.088\n",
            "[346,     2] loss: 1.086\n",
            "[347,     2] loss: 1.094\n",
            "[348,     2] loss: 1.091\n",
            "[349,     2] loss: 1.092\n",
            "[350,     2] loss: 1.088\n",
            "[351,     2] loss: 1.086\n",
            "[352,     2] loss: 1.090\n",
            "[353,     2] loss: 1.086\n",
            "[354,     2] loss: 1.090\n",
            "[355,     2] loss: 1.088\n",
            "[356,     2] loss: 1.100\n",
            "[357,     2] loss: 1.090\n",
            "[358,     2] loss: 1.086\n",
            "[359,     2] loss: 1.085\n",
            "[360,     2] loss: 1.090\n",
            "[361,     2] loss: 1.092\n",
            "[362,     2] loss: 1.085\n",
            "[363,     2] loss: 1.092\n",
            "[364,     2] loss: 1.087\n",
            "[365,     2] loss: 1.090\n",
            "[366,     2] loss: 1.087\n",
            "[367,     2] loss: 1.092\n",
            "[368,     2] loss: 1.085\n",
            "[369,     2] loss: 1.085\n",
            "[370,     2] loss: 1.086\n",
            "[371,     2] loss: 1.090\n",
            "[372,     2] loss: 1.086\n",
            "[373,     2] loss: 1.085\n",
            "[374,     2] loss: 1.085\n",
            "[375,     2] loss: 1.090\n",
            "[376,     2] loss: 1.087\n",
            "[377,     2] loss: 1.086\n",
            "[378,     2] loss: 1.086\n",
            "[379,     2] loss: 1.086\n",
            "[380,     2] loss: 1.088\n",
            "[381,     2] loss: 1.086\n",
            "[382,     2] loss: 1.090\n",
            "[383,     2] loss: 1.090\n",
            "[384,     2] loss: 1.088\n",
            "[385,     2] loss: 1.085\n",
            "[386,     2] loss: 1.089\n",
            "[387,     2] loss: 1.086\n",
            "[388,     2] loss: 1.085\n",
            "[389,     2] loss: 1.087\n",
            "[390,     2] loss: 1.087\n",
            "[391,     2] loss: 1.088\n",
            "[392,     2] loss: 1.089\n",
            "[393,     2] loss: 1.088\n",
            "[394,     2] loss: 1.085\n",
            "[395,     2] loss: 1.087\n",
            "[396,     2] loss: 1.085\n",
            "[397,     2] loss: 1.086\n",
            "[398,     2] loss: 1.085\n",
            "[399,     2] loss: 1.088\n",
            "[400,     2] loss: 1.090\n",
            "[401,     2] loss: 1.085\n",
            "[402,     2] loss: 1.085\n",
            "[403,     2] loss: 1.087\n",
            "[404,     2] loss: 1.088\n",
            "[405,     2] loss: 1.091\n",
            "[406,     2] loss: 1.086\n",
            "[407,     2] loss: 1.086\n",
            "[408,     2] loss: 1.085\n",
            "[409,     2] loss: 1.086\n",
            "[410,     2] loss: 1.087\n",
            "[411,     2] loss: 1.087\n",
            "[412,     2] loss: 1.092\n",
            "[413,     2] loss: 1.089\n",
            "[414,     2] loss: 1.092\n",
            "[415,     2] loss: 1.087\n",
            "[416,     2] loss: 1.084\n",
            "[417,     2] loss: 1.094\n",
            "[418,     2] loss: 1.089\n",
            "[419,     2] loss: 1.084\n",
            "[420,     2] loss: 1.088\n",
            "[421,     2] loss: 1.089\n",
            "[422,     2] loss: 1.089\n",
            "[423,     2] loss: 1.097\n",
            "[424,     2] loss: 1.092\n",
            "[425,     2] loss: 1.090\n",
            "[426,     2] loss: 1.088\n",
            "[427,     2] loss: 1.091\n",
            "[428,     2] loss: 1.089\n",
            "[429,     2] loss: 1.086\n",
            "[430,     2] loss: 1.093\n",
            "[431,     2] loss: 1.089\n",
            "[432,     2] loss: 1.085\n",
            "[433,     2] loss: 1.088\n",
            "[434,     2] loss: 1.088\n",
            "[435,     2] loss: 1.088\n",
            "[436,     2] loss: 1.095\n",
            "[437,     2] loss: 1.093\n",
            "[438,     2] loss: 1.088\n",
            "[439,     2] loss: 1.087\n",
            "[440,     2] loss: 1.086\n",
            "[441,     2] loss: 1.085\n",
            "[442,     2] loss: 1.088\n",
            "[443,     2] loss: 1.087\n",
            "[444,     2] loss: 1.085\n",
            "[445,     2] loss: 1.086\n",
            "[446,     2] loss: 1.086\n",
            "[447,     2] loss: 1.087\n",
            "[448,     2] loss: 1.087\n",
            "[449,     2] loss: 1.085\n",
            "[450,     2] loss: 1.085\n",
            "[451,     2] loss: 1.087\n",
            "[452,     2] loss: 1.085\n",
            "[453,     2] loss: 1.086\n",
            "[454,     2] loss: 1.088\n",
            "[455,     2] loss: 1.085\n",
            "[456,     2] loss: 1.084\n",
            "[457,     2] loss: 1.088\n",
            "[458,     2] loss: 1.090\n",
            "[459,     2] loss: 1.091\n",
            "[460,     2] loss: 1.087\n",
            "[461,     2] loss: 1.095\n",
            "[462,     2] loss: 1.088\n",
            "[463,     2] loss: 1.084\n",
            "[464,     2] loss: 1.088\n",
            "[465,     2] loss: 1.088\n",
            "[466,     2] loss: 1.085\n",
            "[467,     2] loss: 1.087\n",
            "[468,     2] loss: 1.088\n",
            "[469,     2] loss: 1.084\n",
            "[470,     2] loss: 1.084\n",
            "[471,     2] loss: 1.084\n",
            "[472,     2] loss: 1.088\n",
            "[473,     2] loss: 1.087\n",
            "[474,     2] loss: 1.086\n",
            "[475,     2] loss: 1.084\n",
            "[476,     2] loss: 1.084\n",
            "[477,     2] loss: 1.087\n",
            "[478,     2] loss: 1.084\n",
            "[479,     2] loss: 1.084\n",
            "[480,     2] loss: 1.085\n",
            "[481,     2] loss: 1.086\n",
            "[482,     2] loss: 1.089\n",
            "[483,     2] loss: 1.089\n",
            "[484,     2] loss: 1.084\n",
            "[485,     2] loss: 1.088\n",
            "[486,     2] loss: 1.088\n",
            "[487,     2] loss: 1.084\n",
            "[488,     2] loss: 1.083\n",
            "[489,     2] loss: 1.086\n",
            "[490,     2] loss: 1.090\n",
            "[491,     2] loss: 1.090\n",
            "[492,     2] loss: 1.088\n",
            "[493,     2] loss: 1.086\n",
            "[494,     2] loss: 1.085\n",
            "[495,     2] loss: 1.084\n",
            "[496,     2] loss: 1.089\n",
            "[497,     2] loss: 1.092\n",
            "[498,     2] loss: 1.094\n",
            "[499,     2] loss: 1.086\n",
            "[500,     2] loss: 1.089\n",
            "[501,     2] loss: 1.084\n",
            "[502,     2] loss: 1.086\n",
            "[503,     2] loss: 1.086\n",
            "[504,     2] loss: 1.086\n",
            "[505,     2] loss: 1.089\n",
            "[506,     2] loss: 1.084\n",
            "[507,     2] loss: 1.084\n",
            "[508,     2] loss: 1.084\n",
            "[509,     2] loss: 1.084\n",
            "[510,     2] loss: 1.086\n",
            "[511,     2] loss: 1.092\n",
            "[512,     2] loss: 1.087\n",
            "[513,     2] loss: 1.084\n",
            "[514,     2] loss: 1.085\n",
            "[515,     2] loss: 1.084\n",
            "[516,     2] loss: 1.085\n",
            "[517,     2] loss: 1.088\n",
            "[518,     2] loss: 1.084\n",
            "[519,     2] loss: 1.087\n",
            "[520,     2] loss: 1.084\n",
            "[521,     2] loss: 1.087\n",
            "[522,     2] loss: 1.084\n",
            "[523,     2] loss: 1.083\n",
            "[524,     2] loss: 1.085\n",
            "[525,     2] loss: 1.085\n",
            "[526,     2] loss: 1.083\n",
            "[527,     2] loss: 1.084\n",
            "[528,     2] loss: 1.085\n",
            "[529,     2] loss: 1.085\n",
            "[530,     2] loss: 1.087\n",
            "[531,     2] loss: 1.088\n",
            "[532,     2] loss: 1.093\n",
            "[533,     2] loss: 1.089\n",
            "[534,     2] loss: 1.085\n",
            "[535,     2] loss: 1.083\n",
            "[536,     2] loss: 1.084\n",
            "[537,     2] loss: 1.085\n",
            "[538,     2] loss: 1.084\n",
            "[539,     2] loss: 1.092\n",
            "[540,     2] loss: 1.085\n",
            "[541,     2] loss: 1.088\n",
            "[542,     2] loss: 1.083\n",
            "[543,     2] loss: 1.086\n",
            "[544,     2] loss: 1.086\n",
            "[545,     2] loss: 1.082\n",
            "[546,     2] loss: 1.092\n",
            "[547,     2] loss: 1.083\n",
            "[548,     2] loss: 1.086\n",
            "[549,     2] loss: 1.086\n",
            "[550,     2] loss: 1.086\n",
            "[551,     2] loss: 1.083\n",
            "[552,     2] loss: 1.083\n",
            "[553,     2] loss: 1.084\n",
            "[554,     2] loss: 1.086\n",
            "[555,     2] loss: 1.085\n",
            "[556,     2] loss: 1.085\n",
            "[557,     2] loss: 1.086\n",
            "[558,     2] loss: 1.084\n",
            "[559,     2] loss: 1.083\n",
            "[560,     2] loss: 1.083\n",
            "[561,     2] loss: 1.083\n",
            "[562,     2] loss: 1.084\n",
            "[563,     2] loss: 1.082\n",
            "[564,     2] loss: 1.086\n",
            "[565,     2] loss: 1.086\n",
            "[566,     2] loss: 1.088\n",
            "[567,     2] loss: 1.089\n",
            "[568,     2] loss: 1.084\n",
            "[569,     2] loss: 1.083\n",
            "[570,     2] loss: 1.083\n",
            "[571,     2] loss: 1.086\n",
            "[572,     2] loss: 1.081\n",
            "[573,     2] loss: 1.084\n",
            "[574,     2] loss: 1.087\n",
            "[575,     2] loss: 1.082\n",
            "[576,     2] loss: 1.083\n",
            "[577,     2] loss: 1.085\n",
            "[578,     2] loss: 1.084\n",
            "[579,     2] loss: 1.081\n",
            "[580,     2] loss: 1.082\n",
            "[581,     2] loss: 1.090\n",
            "[582,     2] loss: 1.085\n",
            "[583,     2] loss: 1.086\n",
            "[584,     2] loss: 1.084\n",
            "[585,     2] loss: 1.089\n",
            "[586,     2] loss: 1.082\n",
            "[587,     2] loss: 1.086\n",
            "[588,     2] loss: 1.086\n",
            "[589,     2] loss: 1.090\n",
            "[590,     2] loss: 1.084\n",
            "[591,     2] loss: 1.082\n",
            "[592,     2] loss: 1.082\n",
            "[593,     2] loss: 1.081\n",
            "[594,     2] loss: 1.081\n",
            "[595,     2] loss: 1.084\n",
            "[596,     2] loss: 1.082\n",
            "[597,     2] loss: 1.081\n",
            "[598,     2] loss: 1.083\n",
            "[599,     2] loss: 1.082\n",
            "[600,     2] loss: 1.086\n",
            "[601,     2] loss: 1.082\n",
            "[602,     2] loss: 1.083\n",
            "[603,     2] loss: 1.096\n",
            "[604,     2] loss: 1.083\n",
            "[605,     2] loss: 1.085\n",
            "[606,     2] loss: 1.082\n",
            "[607,     2] loss: 1.082\n",
            "[608,     2] loss: 1.081\n",
            "[609,     2] loss: 1.081\n",
            "[610,     2] loss: 1.092\n",
            "[611,     2] loss: 1.085\n",
            "[612,     2] loss: 1.082\n",
            "[613,     2] loss: 1.085\n",
            "[614,     2] loss: 1.082\n",
            "[615,     2] loss: 1.082\n",
            "[616,     2] loss: 1.095\n",
            "[617,     2] loss: 1.087\n",
            "[618,     2] loss: 1.081\n",
            "[619,     2] loss: 1.082\n",
            "[620,     2] loss: 1.089\n",
            "[621,     2] loss: 1.089\n",
            "[622,     2] loss: 1.085\n",
            "[623,     2] loss: 1.083\n",
            "[624,     2] loss: 1.081\n",
            "[625,     2] loss: 1.081\n",
            "[626,     2] loss: 1.083\n",
            "[627,     2] loss: 1.081\n",
            "[628,     2] loss: 1.082\n",
            "[629,     2] loss: 1.083\n",
            "[630,     2] loss: 1.084\n",
            "[631,     2] loss: 1.083\n",
            "[632,     2] loss: 1.080\n",
            "[633,     2] loss: 1.083\n",
            "[634,     2] loss: 1.091\n",
            "[635,     2] loss: 1.085\n",
            "[636,     2] loss: 1.083\n",
            "[637,     2] loss: 1.080\n",
            "[638,     2] loss: 1.082\n",
            "[639,     2] loss: 1.083\n",
            "[640,     2] loss: 1.082\n",
            "[641,     2] loss: 1.081\n",
            "[642,     2] loss: 1.082\n",
            "[643,     2] loss: 1.091\n",
            "[644,     2] loss: 1.083\n",
            "[645,     2] loss: 1.085\n",
            "[646,     2] loss: 1.088\n",
            "[647,     2] loss: 1.083\n",
            "[648,     2] loss: 1.084\n",
            "[649,     2] loss: 1.087\n",
            "[650,     2] loss: 1.084\n",
            "[651,     2] loss: 1.082\n",
            "[652,     2] loss: 1.085\n",
            "[653,     2] loss: 1.087\n",
            "[654,     2] loss: 1.079\n",
            "[655,     2] loss: 1.081\n",
            "[656,     2] loss: 1.086\n",
            "[657,     2] loss: 1.083\n",
            "[658,     2] loss: 1.084\n",
            "[659,     2] loss: 1.083\n",
            "[660,     2] loss: 1.081\n",
            "[661,     2] loss: 1.082\n",
            "[662,     2] loss: 1.081\n",
            "[663,     2] loss: 1.081\n",
            "[664,     2] loss: 1.082\n",
            "[665,     2] loss: 1.080\n",
            "[666,     2] loss: 1.083\n",
            "[667,     2] loss: 1.081\n",
            "[668,     2] loss: 1.081\n",
            "[669,     2] loss: 1.082\n",
            "[670,     2] loss: 1.080\n",
            "[671,     2] loss: 1.080\n",
            "[672,     2] loss: 1.082\n",
            "[673,     2] loss: 1.086\n",
            "[674,     2] loss: 1.084\n",
            "[675,     2] loss: 1.080\n",
            "[676,     2] loss: 1.087\n",
            "[677,     2] loss: 1.084\n",
            "[678,     2] loss: 1.079\n",
            "[679,     2] loss: 1.084\n",
            "[680,     2] loss: 1.083\n",
            "[681,     2] loss: 1.080\n",
            "[682,     2] loss: 1.081\n",
            "[683,     2] loss: 1.085\n",
            "[684,     2] loss: 1.081\n",
            "[685,     2] loss: 1.089\n",
            "[686,     2] loss: 1.084\n",
            "[687,     2] loss: 1.081\n",
            "[688,     2] loss: 1.079\n",
            "[689,     2] loss: 1.082\n",
            "[690,     2] loss: 1.084\n",
            "[691,     2] loss: 1.080\n",
            "[692,     2] loss: 1.088\n",
            "[693,     2] loss: 1.083\n",
            "[694,     2] loss: 1.080\n",
            "[695,     2] loss: 1.080\n",
            "[696,     2] loss: 1.087\n",
            "[697,     2] loss: 1.082\n",
            "[698,     2] loss: 1.081\n",
            "[699,     2] loss: 1.081\n",
            "[700,     2] loss: 1.083\n",
            "[701,     2] loss: 1.082\n",
            "[702,     2] loss: 1.081\n",
            "[703,     2] loss: 1.093\n",
            "[704,     2] loss: 1.083\n",
            "[705,     2] loss: 1.079\n",
            "[706,     2] loss: 1.084\n",
            "[707,     2] loss: 1.084\n",
            "[708,     2] loss: 1.080\n",
            "[709,     2] loss: 1.078\n",
            "[710,     2] loss: 1.083\n",
            "[711,     2] loss: 1.079\n",
            "[712,     2] loss: 1.080\n",
            "[713,     2] loss: 1.081\n",
            "[714,     2] loss: 1.081\n",
            "[715,     2] loss: 1.081\n",
            "[716,     2] loss: 1.083\n",
            "[717,     2] loss: 1.081\n",
            "[718,     2] loss: 1.082\n",
            "[719,     2] loss: 1.085\n",
            "[720,     2] loss: 1.082\n",
            "[721,     2] loss: 1.080\n",
            "[722,     2] loss: 1.079\n",
            "[723,     2] loss: 1.080\n",
            "[724,     2] loss: 1.081\n",
            "[725,     2] loss: 1.091\n",
            "[726,     2] loss: 1.082\n",
            "[727,     2] loss: 1.081\n",
            "[728,     2] loss: 1.080\n",
            "[729,     2] loss: 1.082\n",
            "[730,     2] loss: 1.079\n",
            "[731,     2] loss: 1.080\n",
            "[732,     2] loss: 1.081\n",
            "[733,     2] loss: 1.085\n",
            "[734,     2] loss: 1.080\n",
            "[735,     2] loss: 1.079\n",
            "[736,     2] loss: 1.079\n",
            "[737,     2] loss: 1.079\n",
            "[738,     2] loss: 1.080\n",
            "[739,     2] loss: 1.084\n",
            "[740,     2] loss: 1.079\n",
            "[741,     2] loss: 1.082\n",
            "[742,     2] loss: 1.078\n",
            "[743,     2] loss: 1.080\n",
            "[744,     2] loss: 1.081\n",
            "[745,     2] loss: 1.084\n",
            "[746,     2] loss: 1.078\n",
            "[747,     2] loss: 1.080\n",
            "[748,     2] loss: 1.087\n",
            "[749,     2] loss: 1.081\n",
            "[750,     2] loss: 1.085\n",
            "[751,     2] loss: 1.080\n",
            "[752,     2] loss: 1.079\n",
            "[753,     2] loss: 1.081\n",
            "[754,     2] loss: 1.080\n",
            "[755,     2] loss: 1.082\n",
            "[756,     2] loss: 1.083\n",
            "[757,     2] loss: 1.085\n",
            "[758,     2] loss: 1.079\n",
            "[759,     2] loss: 1.078\n",
            "[760,     2] loss: 1.082\n",
            "[761,     2] loss: 1.079\n",
            "[762,     2] loss: 1.080\n",
            "[763,     2] loss: 1.080\n",
            "[764,     2] loss: 1.085\n",
            "[765,     2] loss: 1.080\n",
            "[766,     2] loss: 1.081\n",
            "[767,     2] loss: 1.079\n",
            "[768,     2] loss: 1.078\n",
            "[769,     2] loss: 1.081\n",
            "[770,     2] loss: 1.078\n",
            "[771,     2] loss: 1.077\n",
            "[772,     2] loss: 1.083\n",
            "[773,     2] loss: 1.081\n",
            "[774,     2] loss: 1.078\n",
            "[775,     2] loss: 1.081\n",
            "[776,     2] loss: 1.080\n",
            "[777,     2] loss: 1.085\n",
            "[778,     2] loss: 1.078\n",
            "[779,     2] loss: 1.082\n",
            "[780,     2] loss: 1.080\n",
            "[781,     2] loss: 1.079\n",
            "[782,     2] loss: 1.079\n",
            "[783,     2] loss: 1.083\n",
            "[784,     2] loss: 1.078\n",
            "[785,     2] loss: 1.083\n",
            "[786,     2] loss: 1.081\n",
            "[787,     2] loss: 1.081\n",
            "[788,     2] loss: 1.079\n",
            "[789,     2] loss: 1.078\n",
            "[790,     2] loss: 1.081\n",
            "[791,     2] loss: 1.077\n",
            "[792,     2] loss: 1.080\n",
            "[793,     2] loss: 1.084\n",
            "[794,     2] loss: 1.079\n",
            "[795,     2] loss: 1.080\n",
            "[796,     2] loss: 1.080\n",
            "[797,     2] loss: 1.078\n",
            "[798,     2] loss: 1.078\n",
            "[799,     2] loss: 1.080\n",
            "[800,     2] loss: 1.086\n",
            "[801,     2] loss: 1.081\n",
            "[802,     2] loss: 1.080\n",
            "[803,     2] loss: 1.082\n",
            "[804,     2] loss: 1.078\n",
            "[805,     2] loss: 1.077\n",
            "[806,     2] loss: 1.080\n",
            "[807,     2] loss: 1.080\n",
            "[808,     2] loss: 1.090\n",
            "[809,     2] loss: 1.079\n",
            "[810,     2] loss: 1.079\n",
            "[811,     2] loss: 1.084\n",
            "[812,     2] loss: 1.078\n",
            "[813,     2] loss: 1.080\n",
            "[814,     2] loss: 1.079\n",
            "[815,     2] loss: 1.084\n",
            "[816,     2] loss: 1.080\n",
            "[817,     2] loss: 1.079\n",
            "[818,     2] loss: 1.080\n",
            "[819,     2] loss: 1.081\n",
            "[820,     2] loss: 1.078\n",
            "[821,     2] loss: 1.078\n",
            "[822,     2] loss: 1.080\n",
            "[823,     2] loss: 1.081\n",
            "[824,     2] loss: 1.079\n",
            "[825,     2] loss: 1.078\n",
            "[826,     2] loss: 1.078\n",
            "[827,     2] loss: 1.078\n",
            "[828,     2] loss: 1.078\n",
            "[829,     2] loss: 1.077\n",
            "[830,     2] loss: 1.077\n",
            "[831,     2] loss: 1.079\n",
            "[832,     2] loss: 1.077\n",
            "[833,     2] loss: 1.085\n",
            "[834,     2] loss: 1.079\n",
            "[835,     2] loss: 1.081\n",
            "[836,     2] loss: 1.084\n",
            "[837,     2] loss: 1.079\n",
            "[838,     2] loss: 1.083\n",
            "[839,     2] loss: 1.077\n",
            "[840,     2] loss: 1.076\n",
            "[841,     2] loss: 1.078\n",
            "[842,     2] loss: 1.079\n",
            "[843,     2] loss: 1.080\n",
            "[844,     2] loss: 1.078\n",
            "[845,     2] loss: 1.083\n",
            "[846,     2] loss: 1.078\n",
            "[847,     2] loss: 1.077\n",
            "[848,     2] loss: 1.078\n",
            "[849,     2] loss: 1.079\n",
            "[850,     2] loss: 1.077\n",
            "[851,     2] loss: 1.078\n",
            "[852,     2] loss: 1.077\n",
            "[853,     2] loss: 1.079\n",
            "[854,     2] loss: 1.077\n",
            "[855,     2] loss: 1.077\n",
            "[856,     2] loss: 1.077\n",
            "[857,     2] loss: 1.078\n",
            "[858,     2] loss: 1.078\n",
            "[859,     2] loss: 1.077\n",
            "[860,     2] loss: 1.077\n",
            "[861,     2] loss: 1.081\n",
            "[862,     2] loss: 1.083\n",
            "[863,     2] loss: 1.080\n",
            "[864,     2] loss: 1.076\n",
            "[865,     2] loss: 1.077\n",
            "[866,     2] loss: 1.082\n",
            "[867,     2] loss: 1.078\n",
            "[868,     2] loss: 1.078\n",
            "[869,     2] loss: 1.079\n",
            "[870,     2] loss: 1.079\n",
            "[871,     2] loss: 1.077\n",
            "[872,     2] loss: 1.076\n",
            "[873,     2] loss: 1.081\n",
            "[874,     2] loss: 1.080\n",
            "[875,     2] loss: 1.076\n",
            "[876,     2] loss: 1.078\n",
            "[877,     2] loss: 1.080\n",
            "[878,     2] loss: 1.080\n",
            "[879,     2] loss: 1.076\n",
            "[880,     2] loss: 1.078\n",
            "[881,     2] loss: 1.081\n",
            "[882,     2] loss: 1.087\n",
            "[883,     2] loss: 1.076\n",
            "[884,     2] loss: 1.077\n",
            "[885,     2] loss: 1.080\n",
            "[886,     2] loss: 1.080\n",
            "[887,     2] loss: 1.077\n",
            "[888,     2] loss: 1.077\n",
            "[889,     2] loss: 1.079\n",
            "[890,     2] loss: 1.082\n",
            "[891,     2] loss: 1.077\n",
            "[892,     2] loss: 1.077\n",
            "[893,     2] loss: 1.082\n",
            "[894,     2] loss: 1.077\n",
            "[895,     2] loss: 1.076\n",
            "[896,     2] loss: 1.076\n",
            "[897,     2] loss: 1.083\n",
            "[898,     2] loss: 1.077\n",
            "[899,     2] loss: 1.076\n",
            "[900,     2] loss: 1.081\n",
            "[901,     2] loss: 1.082\n",
            "[902,     2] loss: 1.076\n",
            "[903,     2] loss: 1.078\n",
            "[904,     2] loss: 1.076\n",
            "[905,     2] loss: 1.079\n",
            "[906,     2] loss: 1.078\n",
            "[907,     2] loss: 1.077\n",
            "[908,     2] loss: 1.076\n",
            "[909,     2] loss: 1.078\n",
            "[910,     2] loss: 1.081\n",
            "[911,     2] loss: 1.079\n",
            "[912,     2] loss: 1.077\n",
            "[913,     2] loss: 1.076\n",
            "[914,     2] loss: 1.077\n",
            "[915,     2] loss: 1.077\n",
            "[916,     2] loss: 1.076\n",
            "[917,     2] loss: 1.078\n",
            "[918,     2] loss: 1.082\n",
            "[919,     2] loss: 1.081\n",
            "[920,     2] loss: 1.082\n",
            "[921,     2] loss: 1.082\n",
            "[922,     2] loss: 1.076\n",
            "[923,     2] loss: 1.088\n",
            "[924,     2] loss: 1.078\n",
            "[925,     2] loss: 1.076\n",
            "[926,     2] loss: 1.075\n",
            "[927,     2] loss: 1.079\n",
            "[928,     2] loss: 1.078\n",
            "[929,     2] loss: 1.082\n",
            "[930,     2] loss: 1.078\n",
            "[931,     2] loss: 1.077\n",
            "[932,     2] loss: 1.077\n",
            "[933,     2] loss: 1.079\n",
            "[934,     2] loss: 1.078\n",
            "[935,     2] loss: 1.081\n",
            "[936,     2] loss: 1.078\n",
            "[937,     2] loss: 1.079\n",
            "[938,     2] loss: 1.077\n",
            "[939,     2] loss: 1.079\n",
            "[940,     2] loss: 1.084\n",
            "[941,     2] loss: 1.078\n",
            "[942,     2] loss: 1.075\n",
            "[943,     2] loss: 1.076\n",
            "[944,     2] loss: 1.077\n",
            "[945,     2] loss: 1.078\n",
            "[946,     2] loss: 1.078\n",
            "[947,     2] loss: 1.076\n",
            "[948,     2] loss: 1.077\n",
            "[949,     2] loss: 1.076\n",
            "[950,     2] loss: 1.080\n",
            "[951,     2] loss: 1.078\n",
            "[952,     2] loss: 1.077\n",
            "[953,     2] loss: 1.076\n",
            "[954,     2] loss: 1.075\n",
            "[955,     2] loss: 1.075\n",
            "[956,     2] loss: 1.076\n",
            "[957,     2] loss: 1.080\n",
            "[958,     2] loss: 1.076\n",
            "[959,     2] loss: 1.079\n",
            "[960,     2] loss: 1.078\n",
            "[961,     2] loss: 1.080\n",
            "[962,     2] loss: 1.081\n",
            "[963,     2] loss: 1.076\n",
            "[964,     2] loss: 1.075\n",
            "[965,     2] loss: 1.077\n",
            "[966,     2] loss: 1.078\n",
            "[967,     2] loss: 1.076\n",
            "[968,     2] loss: 1.078\n",
            "[969,     2] loss: 1.078\n",
            "[970,     2] loss: 1.078\n",
            "[971,     2] loss: 1.075\n",
            "[972,     2] loss: 1.085\n",
            "[973,     2] loss: 1.079\n",
            "[974,     2] loss: 1.088\n",
            "[975,     2] loss: 1.084\n",
            "[976,     2] loss: 1.076\n",
            "[977,     2] loss: 1.083\n",
            "[978,     2] loss: 1.081\n",
            "[979,     2] loss: 1.079\n",
            "[980,     2] loss: 1.077\n",
            "[981,     2] loss: 1.077\n",
            "[982,     2] loss: 1.082\n",
            "[983,     2] loss: 1.075\n",
            "[984,     2] loss: 1.075\n",
            "[985,     2] loss: 1.082\n",
            "[986,     2] loss: 1.077\n",
            "[987,     2] loss: 1.074\n",
            "[988,     2] loss: 1.078\n",
            "[989,     2] loss: 1.080\n",
            "[990,     2] loss: 1.074\n",
            "[991,     2] loss: 1.079\n",
            "[992,     2] loss: 1.082\n",
            "[993,     2] loss: 1.076\n",
            "[994,     2] loss: 1.079\n",
            "[995,     2] loss: 1.081\n",
            "[996,     2] loss: 1.076\n",
            "[997,     2] loss: 1.078\n",
            "[998,     2] loss: 1.079\n",
            "[999,     2] loss: 1.080\n",
            "[1000,     2] loss: 1.079\n",
            "[1001,     2] loss: 1.076\n",
            "[1002,     2] loss: 1.077\n",
            "[1003,     2] loss: 1.077\n",
            "[1004,     2] loss: 1.079\n",
            "[1005,     2] loss: 1.082\n",
            "[1006,     2] loss: 1.080\n",
            "[1007,     2] loss: 1.077\n",
            "[1008,     2] loss: 1.079\n",
            "[1009,     2] loss: 1.076\n",
            "[1010,     2] loss: 1.078\n",
            "[1011,     2] loss: 1.084\n",
            "[1012,     2] loss: 1.087\n",
            "[1013,     2] loss: 1.085\n",
            "[1014,     2] loss: 1.076\n",
            "[1015,     2] loss: 1.081\n",
            "[1016,     2] loss: 1.081\n",
            "[1017,     2] loss: 1.074\n",
            "[1018,     2] loss: 1.074\n",
            "[1019,     2] loss: 1.078\n",
            "[1020,     2] loss: 1.079\n",
            "[1021,     2] loss: 1.080\n",
            "[1022,     2] loss: 1.079\n",
            "[1023,     2] loss: 1.083\n",
            "[1024,     2] loss: 1.076\n",
            "[1025,     2] loss: 1.078\n",
            "[1026,     2] loss: 1.076\n",
            "[1027,     2] loss: 1.078\n",
            "[1028,     2] loss: 1.074\n",
            "[1029,     2] loss: 1.074\n",
            "[1030,     2] loss: 1.075\n",
            "[1031,     2] loss: 1.080\n",
            "[1032,     2] loss: 1.075\n",
            "[1033,     2] loss: 1.078\n",
            "[1034,     2] loss: 1.076\n",
            "[1035,     2] loss: 1.080\n",
            "[1036,     2] loss: 1.078\n",
            "[1037,     2] loss: 1.079\n",
            "[1038,     2] loss: 1.078\n",
            "[1039,     2] loss: 1.076\n",
            "[1040,     2] loss: 1.076\n",
            "[1041,     2] loss: 1.078\n",
            "[1042,     2] loss: 1.077\n",
            "[1043,     2] loss: 1.074\n",
            "[1044,     2] loss: 1.077\n",
            "[1045,     2] loss: 1.080\n",
            "[1046,     2] loss: 1.074\n",
            "[1047,     2] loss: 1.074\n",
            "[1048,     2] loss: 1.074\n",
            "[1049,     2] loss: 1.078\n",
            "[1050,     2] loss: 1.075\n",
            "[1051,     2] loss: 1.075\n",
            "[1052,     2] loss: 1.074\n",
            "[1053,     2] loss: 1.075\n",
            "[1054,     2] loss: 1.074\n",
            "[1055,     2] loss: 1.075\n",
            "[1056,     2] loss: 1.075\n",
            "[1057,     2] loss: 1.075\n",
            "[1058,     2] loss: 1.074\n",
            "[1059,     2] loss: 1.076\n",
            "[1060,     2] loss: 1.081\n",
            "[1061,     2] loss: 1.076\n",
            "[1062,     2] loss: 1.076\n",
            "[1063,     2] loss: 1.076\n",
            "[1064,     2] loss: 1.075\n",
            "[1065,     2] loss: 1.075\n",
            "[1066,     2] loss: 1.075\n",
            "[1067,     2] loss: 1.077\n",
            "[1068,     2] loss: 1.081\n",
            "[1069,     2] loss: 1.077\n",
            "[1070,     2] loss: 1.076\n",
            "[1071,     2] loss: 1.076\n",
            "[1072,     2] loss: 1.077\n",
            "[1073,     2] loss: 1.074\n",
            "[1074,     2] loss: 1.077\n",
            "[1075,     2] loss: 1.074\n",
            "[1076,     2] loss: 1.078\n",
            "[1077,     2] loss: 1.074\n",
            "[1078,     2] loss: 1.076\n",
            "[1079,     2] loss: 1.075\n",
            "[1080,     2] loss: 1.076\n",
            "[1081,     2] loss: 1.074\n",
            "[1082,     2] loss: 1.075\n",
            "[1083,     2] loss: 1.075\n",
            "[1084,     2] loss: 1.076\n",
            "[1085,     2] loss: 1.079\n",
            "[1086,     2] loss: 1.079\n",
            "[1087,     2] loss: 1.076\n",
            "[1088,     2] loss: 1.083\n",
            "[1089,     2] loss: 1.084\n",
            "[1090,     2] loss: 1.080\n",
            "[1091,     2] loss: 1.075\n",
            "[1092,     2] loss: 1.081\n",
            "[1093,     2] loss: 1.075\n",
            "[1094,     2] loss: 1.074\n",
            "[1095,     2] loss: 1.074\n",
            "[1096,     2] loss: 1.074\n",
            "[1097,     2] loss: 1.076\n",
            "[1098,     2] loss: 1.078\n",
            "[1099,     2] loss: 1.074\n",
            "[1100,     2] loss: 1.075\n",
            "[1101,     2] loss: 1.077\n",
            "[1102,     2] loss: 1.076\n",
            "[1103,     2] loss: 1.074\n",
            "[1104,     2] loss: 1.077\n",
            "[1105,     2] loss: 1.075\n",
            "[1106,     2] loss: 1.075\n",
            "[1107,     2] loss: 1.074\n",
            "[1108,     2] loss: 1.075\n",
            "[1109,     2] loss: 1.076\n",
            "[1110,     2] loss: 1.075\n",
            "[1111,     2] loss: 1.078\n",
            "[1112,     2] loss: 1.075\n",
            "[1113,     2] loss: 1.074\n",
            "[1114,     2] loss: 1.074\n",
            "[1115,     2] loss: 1.075\n",
            "[1116,     2] loss: 1.076\n",
            "[1117,     2] loss: 1.079\n",
            "[1118,     2] loss: 1.074\n",
            "[1119,     2] loss: 1.075\n",
            "[1120,     2] loss: 1.074\n",
            "[1121,     2] loss: 1.074\n",
            "[1122,     2] loss: 1.078\n",
            "[1123,     2] loss: 1.074\n",
            "[1124,     2] loss: 1.078\n",
            "[1125,     2] loss: 1.077\n",
            "[1126,     2] loss: 1.074\n",
            "[1127,     2] loss: 1.076\n",
            "[1128,     2] loss: 1.076\n",
            "[1129,     2] loss: 1.076\n",
            "[1130,     2] loss: 1.074\n",
            "[1131,     2] loss: 1.074\n",
            "[1132,     2] loss: 1.076\n",
            "[1133,     2] loss: 1.077\n",
            "[1134,     2] loss: 1.075\n",
            "[1135,     2] loss: 1.079\n",
            "[1136,     2] loss: 1.081\n",
            "[1137,     2] loss: 1.078\n",
            "[1138,     2] loss: 1.076\n",
            "[1139,     2] loss: 1.077\n",
            "[1140,     2] loss: 1.077\n",
            "[1141,     2] loss: 1.075\n",
            "[1142,     2] loss: 1.078\n",
            "[1143,     2] loss: 1.074\n",
            "[1144,     2] loss: 1.076\n",
            "[1145,     2] loss: 1.077\n",
            "[1146,     2] loss: 1.078\n",
            "[1147,     2] loss: 1.079\n",
            "[1148,     2] loss: 1.076\n",
            "[1149,     2] loss: 1.074\n",
            "[1150,     2] loss: 1.073\n",
            "[1151,     2] loss: 1.076\n",
            "[1152,     2] loss: 1.077\n",
            "[1153,     2] loss: 1.083\n",
            "[1154,     2] loss: 1.078\n",
            "[1155,     2] loss: 1.075\n",
            "[1156,     2] loss: 1.074\n",
            "[1157,     2] loss: 1.076\n",
            "[1158,     2] loss: 1.080\n",
            "[1159,     2] loss: 1.075\n",
            "[1160,     2] loss: 1.074\n",
            "[1161,     2] loss: 1.080\n",
            "[1162,     2] loss: 1.081\n",
            "[1163,     2] loss: 1.074\n",
            "[1164,     2] loss: 1.078\n",
            "[1165,     2] loss: 1.077\n",
            "[1166,     2] loss: 1.075\n",
            "[1167,     2] loss: 1.079\n",
            "[1168,     2] loss: 1.079\n",
            "[1169,     2] loss: 1.078\n",
            "[1170,     2] loss: 1.073\n",
            "[1171,     2] loss: 1.074\n",
            "[1172,     2] loss: 1.075\n",
            "[1173,     2] loss: 1.074\n",
            "[1174,     2] loss: 1.075\n",
            "[1175,     2] loss: 1.092\n",
            "[1176,     2] loss: 1.076\n",
            "[1177,     2] loss: 1.075\n",
            "[1178,     2] loss: 1.078\n",
            "[1179,     2] loss: 1.076\n",
            "[1180,     2] loss: 1.075\n",
            "[1181,     2] loss: 1.075\n",
            "[1182,     2] loss: 1.076\n",
            "[1183,     2] loss: 1.078\n",
            "[1184,     2] loss: 1.080\n",
            "[1185,     2] loss: 1.081\n",
            "[1186,     2] loss: 1.083\n",
            "[1187,     2] loss: 1.080\n",
            "[1188,     2] loss: 1.079\n",
            "[1189,     2] loss: 1.078\n",
            "[1190,     2] loss: 1.075\n",
            "[1191,     2] loss: 1.079\n",
            "[1192,     2] loss: 1.076\n",
            "[1193,     2] loss: 1.077\n",
            "[1194,     2] loss: 1.074\n",
            "[1195,     2] loss: 1.079\n",
            "[1196,     2] loss: 1.074\n",
            "[1197,     2] loss: 1.072\n",
            "[1198,     2] loss: 1.074\n",
            "[1199,     2] loss: 1.082\n",
            "[1200,     2] loss: 1.079\n",
            "[1201,     2] loss: 1.074\n",
            "[1202,     2] loss: 1.073\n",
            "[1203,     2] loss: 1.073\n",
            "[1204,     2] loss: 1.080\n",
            "[1205,     2] loss: 1.074\n",
            "[1206,     2] loss: 1.073\n",
            "[1207,     2] loss: 1.076\n",
            "[1208,     2] loss: 1.075\n",
            "[1209,     2] loss: 1.074\n",
            "[1210,     2] loss: 1.081\n",
            "[1211,     2] loss: 1.075\n",
            "[1212,     2] loss: 1.073\n",
            "[1213,     2] loss: 1.074\n",
            "[1214,     2] loss: 1.078\n",
            "[1215,     2] loss: 1.076\n",
            "[1216,     2] loss: 1.076\n",
            "[1217,     2] loss: 1.077\n",
            "[1218,     2] loss: 1.076\n",
            "[1219,     2] loss: 1.072\n",
            "[1220,     2] loss: 1.075\n",
            "[1221,     2] loss: 1.074\n",
            "[1222,     2] loss: 1.079\n",
            "[1223,     2] loss: 1.076\n",
            "[1224,     2] loss: 1.073\n",
            "[1225,     2] loss: 1.078\n",
            "[1226,     2] loss: 1.073\n",
            "[1227,     2] loss: 1.077\n",
            "[1228,     2] loss: 1.076\n",
            "[1229,     2] loss: 1.075\n",
            "[1230,     2] loss: 1.079\n",
            "[1231,     2] loss: 1.075\n",
            "[1232,     2] loss: 1.078\n",
            "[1233,     2] loss: 1.077\n",
            "[1234,     2] loss: 1.077\n",
            "[1235,     2] loss: 1.079\n",
            "[1236,     2] loss: 1.073\n",
            "[1237,     2] loss: 1.074\n",
            "[1238,     2] loss: 1.076\n",
            "[1239,     2] loss: 1.074\n",
            "[1240,     2] loss: 1.077\n",
            "[1241,     2] loss: 1.074\n",
            "[1242,     2] loss: 1.073\n",
            "[1243,     2] loss: 1.075\n",
            "[1244,     2] loss: 1.074\n",
            "[1245,     2] loss: 1.073\n",
            "[1246,     2] loss: 1.073\n",
            "[1247,     2] loss: 1.075\n",
            "[1248,     2] loss: 1.073\n",
            "[1249,     2] loss: 1.075\n",
            "[1250,     2] loss: 1.074\n",
            "[1251,     2] loss: 1.072\n",
            "[1252,     2] loss: 1.073\n",
            "[1253,     2] loss: 1.084\n",
            "[1254,     2] loss: 1.074\n",
            "[1255,     2] loss: 1.076\n",
            "[1256,     2] loss: 1.074\n",
            "[1257,     2] loss: 1.074\n",
            "[1258,     2] loss: 1.074\n",
            "[1259,     2] loss: 1.074\n",
            "[1260,     2] loss: 1.075\n",
            "[1261,     2] loss: 1.080\n",
            "[1262,     2] loss: 1.075\n",
            "[1263,     2] loss: 1.073\n",
            "[1264,     2] loss: 1.074\n",
            "[1265,     2] loss: 1.074\n",
            "[1266,     2] loss: 1.077\n",
            "[1267,     2] loss: 1.079\n",
            "[1268,     2] loss: 1.074\n",
            "[1269,     2] loss: 1.072\n",
            "[1270,     2] loss: 1.074\n",
            "[1271,     2] loss: 1.074\n",
            "[1272,     2] loss: 1.078\n",
            "[1273,     2] loss: 1.073\n",
            "[1274,     2] loss: 1.073\n",
            "[1275,     2] loss: 1.077\n",
            "[1276,     2] loss: 1.081\n",
            "[1277,     2] loss: 1.074\n",
            "[1278,     2] loss: 1.073\n",
            "[1279,     2] loss: 1.075\n",
            "[1280,     2] loss: 1.076\n",
            "[1281,     2] loss: 1.076\n",
            "[1282,     2] loss: 1.076\n",
            "[1283,     2] loss: 1.075\n",
            "[1284,     2] loss: 1.075\n",
            "[1285,     2] loss: 1.073\n",
            "[1286,     2] loss: 1.074\n",
            "[1287,     2] loss: 1.073\n",
            "[1288,     2] loss: 1.072\n",
            "[1289,     2] loss: 1.072\n",
            "[1290,     2] loss: 1.072\n",
            "[1291,     2] loss: 1.075\n",
            "[1292,     2] loss: 1.073\n",
            "[1293,     2] loss: 1.072\n",
            "[1294,     2] loss: 1.074\n",
            "[1295,     2] loss: 1.075\n",
            "[1296,     2] loss: 1.076\n",
            "[1297,     2] loss: 1.073\n",
            "[1298,     2] loss: 1.075\n",
            "[1299,     2] loss: 1.074\n",
            "[1300,     2] loss: 1.081\n",
            "[1301,     2] loss: 1.076\n",
            "[1302,     2] loss: 1.075\n",
            "[1303,     2] loss: 1.072\n",
            "[1304,     2] loss: 1.075\n",
            "[1305,     2] loss: 1.074\n",
            "[1306,     2] loss: 1.082\n",
            "[1307,     2] loss: 1.072\n",
            "[1308,     2] loss: 1.075\n",
            "[1309,     2] loss: 1.077\n",
            "[1310,     2] loss: 1.073\n",
            "[1311,     2] loss: 1.073\n",
            "[1312,     2] loss: 1.077\n",
            "[1313,     2] loss: 1.075\n",
            "[1314,     2] loss: 1.080\n",
            "[1315,     2] loss: 1.078\n",
            "[1316,     2] loss: 1.078\n",
            "[1317,     2] loss: 1.076\n",
            "[1318,     2] loss: 1.077\n",
            "[1319,     2] loss: 1.075\n",
            "[1320,     2] loss: 1.080\n",
            "[1321,     2] loss: 1.073\n",
            "[1322,     2] loss: 1.075\n",
            "[1323,     2] loss: 1.072\n",
            "[1324,     2] loss: 1.074\n",
            "[1325,     2] loss: 1.073\n",
            "[1326,     2] loss: 1.073\n",
            "[1327,     2] loss: 1.077\n",
            "[1328,     2] loss: 1.098\n",
            "[1329,     2] loss: 1.075\n",
            "[1330,     2] loss: 1.073\n",
            "[1331,     2] loss: 1.076\n",
            "[1332,     2] loss: 1.075\n",
            "[1333,     2] loss: 1.073\n",
            "[1334,     2] loss: 1.076\n",
            "[1335,     2] loss: 1.075\n",
            "[1336,     2] loss: 1.074\n",
            "[1337,     2] loss: 1.083\n",
            "[1338,     2] loss: 1.079\n",
            "[1339,     2] loss: 1.076\n",
            "[1340,     2] loss: 1.074\n",
            "[1341,     2] loss: 1.075\n",
            "[1342,     2] loss: 1.077\n",
            "[1343,     2] loss: 1.080\n",
            "[1344,     2] loss: 1.073\n",
            "[1345,     2] loss: 1.074\n",
            "[1346,     2] loss: 1.075\n",
            "[1347,     2] loss: 1.073\n",
            "[1348,     2] loss: 1.073\n",
            "[1349,     2] loss: 1.076\n",
            "[1350,     2] loss: 1.075\n",
            "[1351,     2] loss: 1.072\n",
            "[1352,     2] loss: 1.074\n",
            "[1353,     2] loss: 1.075\n",
            "[1354,     2] loss: 1.073\n",
            "[1355,     2] loss: 1.078\n",
            "[1356,     2] loss: 1.075\n",
            "[1357,     2] loss: 1.075\n",
            "[1358,     2] loss: 1.073\n",
            "[1359,     2] loss: 1.075\n",
            "[1360,     2] loss: 1.074\n",
            "[1361,     2] loss: 1.075\n",
            "[1362,     2] loss: 1.074\n",
            "[1363,     2] loss: 1.074\n",
            "[1364,     2] loss: 1.075\n",
            "[1365,     2] loss: 1.076\n",
            "[1366,     2] loss: 1.075\n",
            "[1367,     2] loss: 1.075\n",
            "[1368,     2] loss: 1.076\n",
            "[1369,     2] loss: 1.079\n",
            "[1370,     2] loss: 1.076\n",
            "[1371,     2] loss: 1.075\n",
            "[1372,     2] loss: 1.078\n",
            "[1373,     2] loss: 1.077\n",
            "[1374,     2] loss: 1.072\n",
            "[1375,     2] loss: 1.072\n",
            "[1376,     2] loss: 1.073\n",
            "[1377,     2] loss: 1.076\n",
            "[1378,     2] loss: 1.075\n",
            "[1379,     2] loss: 1.074\n",
            "[1380,     2] loss: 1.082\n",
            "[1381,     2] loss: 1.080\n",
            "[1382,     2] loss: 1.072\n",
            "[1383,     2] loss: 1.074\n",
            "[1384,     2] loss: 1.073\n",
            "[1385,     2] loss: 1.076\n",
            "[1386,     2] loss: 1.077\n",
            "[1387,     2] loss: 1.078\n",
            "[1388,     2] loss: 1.072\n",
            "[1389,     2] loss: 1.072\n",
            "[1390,     2] loss: 1.075\n",
            "[1391,     2] loss: 1.072\n",
            "[1392,     2] loss: 1.072\n",
            "[1393,     2] loss: 1.075\n",
            "[1394,     2] loss: 1.072\n",
            "[1395,     2] loss: 1.072\n",
            "[1396,     2] loss: 1.072\n",
            "[1397,     2] loss: 1.073\n",
            "[1398,     2] loss: 1.076\n",
            "[1399,     2] loss: 1.075\n",
            "[1400,     2] loss: 1.073\n",
            "[1401,     2] loss: 1.076\n",
            "[1402,     2] loss: 1.080\n",
            "[1403,     2] loss: 1.077\n",
            "[1404,     2] loss: 1.075\n",
            "[1405,     2] loss: 1.073\n",
            "[1406,     2] loss: 1.076\n",
            "[1407,     2] loss: 1.076\n",
            "[1408,     2] loss: 1.074\n",
            "[1409,     2] loss: 1.072\n",
            "[1410,     2] loss: 1.076\n",
            "[1411,     2] loss: 1.074\n",
            "[1412,     2] loss: 1.075\n",
            "[1413,     2] loss: 1.074\n",
            "[1414,     2] loss: 1.072\n",
            "[1415,     2] loss: 1.078\n",
            "[1416,     2] loss: 1.076\n",
            "[1417,     2] loss: 1.074\n",
            "[1418,     2] loss: 1.072\n",
            "[1419,     2] loss: 1.074\n",
            "[1420,     2] loss: 1.077\n",
            "[1421,     2] loss: 1.073\n",
            "[1422,     2] loss: 1.075\n",
            "[1423,     2] loss: 1.073\n",
            "[1424,     2] loss: 1.073\n",
            "[1425,     2] loss: 1.075\n",
            "[1426,     2] loss: 1.074\n",
            "[1427,     2] loss: 1.074\n",
            "[1428,     2] loss: 1.073\n",
            "[1429,     2] loss: 1.072\n",
            "[1430,     2] loss: 1.072\n",
            "[1431,     2] loss: 1.072\n",
            "[1432,     2] loss: 1.077\n",
            "[1433,     2] loss: 1.074\n",
            "[1434,     2] loss: 1.073\n",
            "[1435,     2] loss: 1.072\n",
            "[1436,     2] loss: 1.074\n",
            "[1437,     2] loss: 1.074\n",
            "[1438,     2] loss: 1.072\n",
            "[1439,     2] loss: 1.071\n",
            "[1440,     2] loss: 1.073\n",
            "[1441,     2] loss: 1.072\n",
            "[1442,     2] loss: 1.074\n",
            "[1443,     2] loss: 1.074\n",
            "[1444,     2] loss: 1.072\n",
            "[1445,     2] loss: 1.076\n",
            "[1446,     2] loss: 1.072\n",
            "[1447,     2] loss: 1.072\n",
            "[1448,     2] loss: 1.073\n",
            "[1449,     2] loss: 1.073\n",
            "[1450,     2] loss: 1.072\n",
            "[1451,     2] loss: 1.073\n",
            "[1452,     2] loss: 1.077\n",
            "[1453,     2] loss: 1.072\n",
            "[1454,     2] loss: 1.074\n",
            "[1455,     2] loss: 1.072\n",
            "[1456,     2] loss: 1.072\n",
            "[1457,     2] loss: 1.072\n",
            "[1458,     2] loss: 1.075\n",
            "[1459,     2] loss: 1.075\n",
            "[1460,     2] loss: 1.075\n",
            "[1461,     2] loss: 1.074\n",
            "[1462,     2] loss: 1.073\n",
            "[1463,     2] loss: 1.071\n",
            "[1464,     2] loss: 1.073\n",
            "[1465,     2] loss: 1.079\n",
            "[1466,     2] loss: 1.072\n",
            "[1467,     2] loss: 1.072\n",
            "[1468,     2] loss: 1.078\n",
            "[1469,     2] loss: 1.073\n",
            "[1470,     2] loss: 1.072\n",
            "[1471,     2] loss: 1.072\n",
            "[1472,     2] loss: 1.080\n",
            "[1473,     2] loss: 1.072\n",
            "[1474,     2] loss: 1.073\n",
            "[1475,     2] loss: 1.071\n",
            "[1476,     2] loss: 1.076\n",
            "[1477,     2] loss: 1.073\n",
            "[1478,     2] loss: 1.073\n",
            "[1479,     2] loss: 1.072\n",
            "[1480,     2] loss: 1.073\n",
            "[1481,     2] loss: 1.075\n",
            "[1482,     2] loss: 1.073\n",
            "[1483,     2] loss: 1.072\n",
            "[1484,     2] loss: 1.073\n",
            "[1485,     2] loss: 1.076\n",
            "[1486,     2] loss: 1.073\n",
            "[1487,     2] loss: 1.071\n",
            "[1488,     2] loss: 1.074\n",
            "[1489,     2] loss: 1.076\n",
            "[1490,     2] loss: 1.072\n",
            "[1491,     2] loss: 1.072\n",
            "[1492,     2] loss: 1.077\n",
            "[1493,     2] loss: 1.073\n",
            "[1494,     2] loss: 1.071\n",
            "[1495,     2] loss: 1.075\n",
            "[1496,     2] loss: 1.074\n",
            "[1497,     2] loss: 1.072\n",
            "[1498,     2] loss: 1.074\n",
            "[1499,     2] loss: 1.073\n",
            "[1500,     2] loss: 1.072\n",
            "[1501,     2] loss: 1.073\n",
            "[1502,     2] loss: 1.073\n",
            "[1503,     2] loss: 1.074\n",
            "[1504,     2] loss: 1.077\n",
            "[1505,     2] loss: 1.079\n",
            "[1506,     2] loss: 1.081\n",
            "[1507,     2] loss: 1.073\n",
            "[1508,     2] loss: 1.071\n",
            "[1509,     2] loss: 1.073\n",
            "[1510,     2] loss: 1.072\n",
            "[1511,     2] loss: 1.074\n",
            "[1512,     2] loss: 1.074\n",
            "[1513,     2] loss: 1.071\n",
            "[1514,     2] loss: 1.077\n",
            "[1515,     2] loss: 1.076\n",
            "[1516,     2] loss: 1.072\n",
            "[1517,     2] loss: 1.075\n",
            "[1518,     2] loss: 1.076\n",
            "[1519,     2] loss: 1.072\n",
            "[1520,     2] loss: 1.072\n",
            "[1521,     2] loss: 1.075\n",
            "[1522,     2] loss: 1.071\n",
            "[1523,     2] loss: 1.073\n",
            "[1524,     2] loss: 1.075\n",
            "[1525,     2] loss: 1.072\n",
            "[1526,     2] loss: 1.072\n",
            "[1527,     2] loss: 1.075\n",
            "[1528,     2] loss: 1.074\n",
            "[1529,     2] loss: 1.071\n",
            "[1530,     2] loss: 1.076\n",
            "[1531,     2] loss: 1.074\n",
            "[1532,     2] loss: 1.074\n",
            "[1533,     2] loss: 1.078\n",
            "[1534,     2] loss: 1.077\n",
            "[1535,     2] loss: 1.079\n",
            "[1536,     2] loss: 1.073\n",
            "[1537,     2] loss: 1.071\n",
            "[1538,     2] loss: 1.076\n",
            "[1539,     2] loss: 1.076\n",
            "[1540,     2] loss: 1.072\n",
            "[1541,     2] loss: 1.074\n",
            "[1542,     2] loss: 1.074\n",
            "[1543,     2] loss: 1.071\n",
            "[1544,     2] loss: 1.076\n",
            "[1545,     2] loss: 1.075\n",
            "[1546,     2] loss: 1.073\n",
            "[1547,     2] loss: 1.074\n",
            "[1548,     2] loss: 1.075\n",
            "[1549,     2] loss: 1.074\n",
            "[1550,     2] loss: 1.074\n",
            "[1551,     2] loss: 1.073\n",
            "[1552,     2] loss: 1.072\n",
            "[1553,     2] loss: 1.074\n",
            "[1554,     2] loss: 1.075\n",
            "[1555,     2] loss: 1.075\n",
            "[1556,     2] loss: 1.074\n",
            "[1557,     2] loss: 1.079\n",
            "[1558,     2] loss: 1.074\n",
            "[1559,     2] loss: 1.072\n",
            "[1560,     2] loss: 1.075\n",
            "[1561,     2] loss: 1.073\n",
            "[1562,     2] loss: 1.073\n",
            "[1563,     2] loss: 1.074\n",
            "[1564,     2] loss: 1.072\n",
            "[1565,     2] loss: 1.075\n",
            "[1566,     2] loss: 1.072\n",
            "[1567,     2] loss: 1.075\n",
            "[1568,     2] loss: 1.073\n",
            "[1569,     2] loss: 1.078\n",
            "[1570,     2] loss: 1.072\n",
            "[1571,     2] loss: 1.074\n",
            "[1572,     2] loss: 1.079\n",
            "[1573,     2] loss: 1.078\n",
            "[1574,     2] loss: 1.075\n",
            "[1575,     2] loss: 1.074\n",
            "[1576,     2] loss: 1.072\n",
            "[1577,     2] loss: 1.075\n",
            "[1578,     2] loss: 1.073\n",
            "[1579,     2] loss: 1.072\n",
            "[1580,     2] loss: 1.072\n",
            "[1581,     2] loss: 1.074\n",
            "[1582,     2] loss: 1.072\n",
            "[1583,     2] loss: 1.071\n",
            "[1584,     2] loss: 1.073\n",
            "[1585,     2] loss: 1.073\n",
            "[1586,     2] loss: 1.071\n",
            "[1587,     2] loss: 1.073\n",
            "[1588,     2] loss: 1.080\n",
            "[1589,     2] loss: 1.074\n",
            "[1590,     2] loss: 1.071\n",
            "[1591,     2] loss: 1.082\n",
            "[1592,     2] loss: 1.074\n",
            "[1593,     2] loss: 1.072\n",
            "[1594,     2] loss: 1.072\n",
            "[1595,     2] loss: 1.075\n",
            "[1596,     2] loss: 1.075\n",
            "[1597,     2] loss: 1.078\n",
            "[1598,     2] loss: 1.071\n",
            "[1599,     2] loss: 1.073\n",
            "[1600,     2] loss: 1.073\n",
            "[1601,     2] loss: 1.073\n",
            "[1602,     2] loss: 1.072\n",
            "[1603,     2] loss: 1.071\n",
            "[1604,     2] loss: 1.074\n",
            "[1605,     2] loss: 1.078\n",
            "[1606,     2] loss: 1.074\n",
            "[1607,     2] loss: 1.071\n",
            "[1608,     2] loss: 1.072\n",
            "[1609,     2] loss: 1.074\n",
            "[1610,     2] loss: 1.073\n",
            "[1611,     2] loss: 1.078\n",
            "[1612,     2] loss: 1.076\n",
            "[1613,     2] loss: 1.070\n",
            "[1614,     2] loss: 1.078\n",
            "[1615,     2] loss: 1.073\n",
            "[1616,     2] loss: 1.071\n",
            "[1617,     2] loss: 1.075\n",
            "[1618,     2] loss: 1.078\n",
            "[1619,     2] loss: 1.075\n",
            "[1620,     2] loss: 1.072\n",
            "[1621,     2] loss: 1.076\n",
            "[1622,     2] loss: 1.075\n",
            "[1623,     2] loss: 1.076\n",
            "[1624,     2] loss: 1.072\n",
            "[1625,     2] loss: 1.076\n",
            "[1626,     2] loss: 1.073\n",
            "[1627,     2] loss: 1.072\n",
            "[1628,     2] loss: 1.072\n",
            "[1629,     2] loss: 1.072\n",
            "[1630,     2] loss: 1.072\n",
            "[1631,     2] loss: 1.075\n",
            "[1632,     2] loss: 1.071\n",
            "[1633,     2] loss: 1.071\n",
            "[1634,     2] loss: 1.073\n",
            "[1635,     2] loss: 1.073\n",
            "[1636,     2] loss: 1.073\n",
            "[1637,     2] loss: 1.072\n",
            "[1638,     2] loss: 1.074\n",
            "[1639,     2] loss: 1.073\n",
            "[1640,     2] loss: 1.071\n",
            "[1641,     2] loss: 1.078\n",
            "[1642,     2] loss: 1.073\n",
            "[1643,     2] loss: 1.071\n",
            "[1644,     2] loss: 1.076\n",
            "[1645,     2] loss: 1.075\n",
            "[1646,     2] loss: 1.073\n",
            "[1647,     2] loss: 1.074\n",
            "[1648,     2] loss: 1.077\n",
            "[1649,     2] loss: 1.072\n",
            "[1650,     2] loss: 1.076\n",
            "[1651,     2] loss: 1.075\n",
            "[1652,     2] loss: 1.073\n",
            "[1653,     2] loss: 1.072\n",
            "[1654,     2] loss: 1.075\n",
            "[1655,     2] loss: 1.072\n",
            "[1656,     2] loss: 1.072\n",
            "[1657,     2] loss: 1.075\n",
            "[1658,     2] loss: 1.074\n",
            "[1659,     2] loss: 1.077\n",
            "[1660,     2] loss: 1.073\n",
            "[1661,     2] loss: 1.070\n",
            "[1662,     2] loss: 1.073\n",
            "[1663,     2] loss: 1.074\n",
            "[1664,     2] loss: 1.077\n",
            "[1665,     2] loss: 1.074\n",
            "[1666,     2] loss: 1.073\n",
            "[1667,     2] loss: 1.071\n",
            "[1668,     2] loss: 1.074\n",
            "[1669,     2] loss: 1.074\n",
            "[1670,     2] loss: 1.073\n",
            "[1671,     2] loss: 1.072\n",
            "[1672,     2] loss: 1.072\n",
            "[1673,     2] loss: 1.076\n",
            "[1674,     2] loss: 1.071\n",
            "[1675,     2] loss: 1.071\n",
            "[1676,     2] loss: 1.073\n",
            "[1677,     2] loss: 1.074\n",
            "[1678,     2] loss: 1.071\n",
            "[1679,     2] loss: 1.073\n",
            "[1680,     2] loss: 1.072\n",
            "[1681,     2] loss: 1.077\n",
            "[1682,     2] loss: 1.072\n",
            "[1683,     2] loss: 1.075\n",
            "[1684,     2] loss: 1.071\n",
            "[1685,     2] loss: 1.078\n",
            "[1686,     2] loss: 1.076\n",
            "[1687,     2] loss: 1.071\n",
            "[1688,     2] loss: 1.073\n",
            "[1689,     2] loss: 1.077\n",
            "[1690,     2] loss: 1.076\n",
            "[1691,     2] loss: 1.071\n",
            "[1692,     2] loss: 1.072\n",
            "[1693,     2] loss: 1.073\n",
            "[1694,     2] loss: 1.072\n",
            "[1695,     2] loss: 1.071\n",
            "[1696,     2] loss: 1.072\n",
            "[1697,     2] loss: 1.078\n",
            "[1698,     2] loss: 1.077\n",
            "[1699,     2] loss: 1.072\n",
            "[1700,     2] loss: 1.074\n",
            "[1701,     2] loss: 1.074\n",
            "[1702,     2] loss: 1.082\n",
            "[1703,     2] loss: 1.075\n",
            "[1704,     2] loss: 1.071\n",
            "[1705,     2] loss: 1.072\n",
            "[1706,     2] loss: 1.074\n",
            "[1707,     2] loss: 1.071\n",
            "[1708,     2] loss: 1.073\n",
            "[1709,     2] loss: 1.071\n",
            "[1710,     2] loss: 1.073\n",
            "[1711,     2] loss: 1.071\n",
            "[1712,     2] loss: 1.071\n",
            "[1713,     2] loss: 1.077\n",
            "[1714,     2] loss: 1.073\n",
            "[1715,     2] loss: 1.080\n",
            "[1716,     2] loss: 1.075\n",
            "[1717,     2] loss: 1.075\n",
            "[1718,     2] loss: 1.071\n",
            "[1719,     2] loss: 1.073\n",
            "[1720,     2] loss: 1.073\n",
            "[1721,     2] loss: 1.075\n",
            "[1722,     2] loss: 1.071\n",
            "[1723,     2] loss: 1.076\n",
            "[1724,     2] loss: 1.072\n",
            "[1725,     2] loss: 1.073\n",
            "[1726,     2] loss: 1.074\n",
            "[1727,     2] loss: 1.073\n",
            "[1728,     2] loss: 1.071\n",
            "[1729,     2] loss: 1.074\n",
            "[1730,     2] loss: 1.073\n",
            "[1731,     2] loss: 1.071\n",
            "[1732,     2] loss: 1.076\n",
            "[1733,     2] loss: 1.078\n",
            "[1734,     2] loss: 1.076\n",
            "[1735,     2] loss: 1.075\n",
            "[1736,     2] loss: 1.071\n",
            "[1737,     2] loss: 1.072\n",
            "[1738,     2] loss: 1.077\n",
            "[1739,     2] loss: 1.072\n",
            "[1740,     2] loss: 1.072\n",
            "[1741,     2] loss: 1.083\n",
            "[1742,     2] loss: 1.073\n",
            "[1743,     2] loss: 1.075\n",
            "[1744,     2] loss: 1.077\n",
            "[1745,     2] loss: 1.074\n",
            "[1746,     2] loss: 1.072\n",
            "[1747,     2] loss: 1.074\n",
            "[1748,     2] loss: 1.079\n",
            "[1749,     2] loss: 1.071\n",
            "[1750,     2] loss: 1.074\n",
            "[1751,     2] loss: 1.073\n",
            "[1752,     2] loss: 1.073\n",
            "[1753,     2] loss: 1.071\n",
            "[1754,     2] loss: 1.072\n",
            "[1755,     2] loss: 1.074\n",
            "[1756,     2] loss: 1.073\n",
            "[1757,     2] loss: 1.076\n",
            "[1758,     2] loss: 1.075\n",
            "[1759,     2] loss: 1.072\n",
            "[1760,     2] loss: 1.077\n",
            "[1761,     2] loss: 1.074\n",
            "[1762,     2] loss: 1.073\n",
            "[1763,     2] loss: 1.071\n",
            "[1764,     2] loss: 1.073\n",
            "[1765,     2] loss: 1.074\n",
            "[1766,     2] loss: 1.072\n",
            "[1767,     2] loss: 1.072\n",
            "[1768,     2] loss: 1.074\n",
            "[1769,     2] loss: 1.073\n",
            "[1770,     2] loss: 1.075\n",
            "[1771,     2] loss: 1.073\n",
            "[1772,     2] loss: 1.072\n",
            "[1773,     2] loss: 1.077\n",
            "[1774,     2] loss: 1.076\n",
            "[1775,     2] loss: 1.073\n",
            "[1776,     2] loss: 1.074\n",
            "[1777,     2] loss: 1.072\n",
            "[1778,     2] loss: 1.076\n",
            "[1779,     2] loss: 1.075\n",
            "[1780,     2] loss: 1.074\n",
            "[1781,     2] loss: 1.082\n",
            "[1782,     2] loss: 1.082\n",
            "[1783,     2] loss: 1.075\n",
            "[1784,     2] loss: 1.071\n",
            "[1785,     2] loss: 1.071\n",
            "[1786,     2] loss: 1.072\n",
            "[1787,     2] loss: 1.075\n",
            "[1788,     2] loss: 1.077\n",
            "[1789,     2] loss: 1.073\n",
            "[1790,     2] loss: 1.071\n",
            "[1791,     2] loss: 1.081\n",
            "[1792,     2] loss: 1.074\n",
            "[1793,     2] loss: 1.072\n",
            "[1794,     2] loss: 1.070\n",
            "[1795,     2] loss: 1.074\n",
            "[1796,     2] loss: 1.074\n",
            "[1797,     2] loss: 1.075\n",
            "[1798,     2] loss: 1.076\n",
            "[1799,     2] loss: 1.075\n",
            "[1800,     2] loss: 1.074\n",
            "[1801,     2] loss: 1.075\n",
            "[1802,     2] loss: 1.078\n",
            "[1803,     2] loss: 1.078\n",
            "[1804,     2] loss: 1.074\n",
            "[1805,     2] loss: 1.073\n",
            "[1806,     2] loss: 1.073\n",
            "[1807,     2] loss: 1.074\n",
            "[1808,     2] loss: 1.073\n",
            "[1809,     2] loss: 1.072\n",
            "[1810,     2] loss: 1.072\n",
            "[1811,     2] loss: 1.075\n",
            "[1812,     2] loss: 1.074\n",
            "[1813,     2] loss: 1.070\n",
            "[1814,     2] loss: 1.073\n",
            "[1815,     2] loss: 1.073\n",
            "[1816,     2] loss: 1.084\n",
            "[1817,     2] loss: 1.079\n",
            "[1818,     2] loss: 1.071\n",
            "[1819,     2] loss: 1.078\n",
            "[1820,     2] loss: 1.073\n",
            "[1821,     2] loss: 1.072\n",
            "[1822,     2] loss: 1.070\n",
            "[1823,     2] loss: 1.073\n",
            "[1824,     2] loss: 1.070\n",
            "[1825,     2] loss: 1.072\n",
            "[1826,     2] loss: 1.073\n",
            "[1827,     2] loss: 1.071\n",
            "[1828,     2] loss: 1.076\n",
            "[1829,     2] loss: 1.071\n",
            "[1830,     2] loss: 1.070\n",
            "[1831,     2] loss: 1.074\n",
            "[1832,     2] loss: 1.072\n",
            "[1833,     2] loss: 1.074\n",
            "[1834,     2] loss: 1.071\n",
            "[1835,     2] loss: 1.071\n",
            "[1836,     2] loss: 1.072\n",
            "[1837,     2] loss: 1.077\n",
            "[1838,     2] loss: 1.074\n",
            "[1839,     2] loss: 1.073\n",
            "[1840,     2] loss: 1.071\n",
            "[1841,     2] loss: 1.070\n",
            "[1842,     2] loss: 1.075\n",
            "[1843,     2] loss: 1.070\n",
            "[1844,     2] loss: 1.071\n",
            "[1845,     2] loss: 1.071\n",
            "[1846,     2] loss: 1.074\n",
            "[1847,     2] loss: 1.072\n",
            "[1848,     2] loss: 1.072\n",
            "[1849,     2] loss: 1.074\n",
            "[1850,     2] loss: 1.079\n",
            "[1851,     2] loss: 1.070\n",
            "[1852,     2] loss: 1.074\n",
            "[1853,     2] loss: 1.076\n",
            "[1854,     2] loss: 1.071\n",
            "[1855,     2] loss: 1.070\n",
            "[1856,     2] loss: 1.075\n",
            "[1857,     2] loss: 1.074\n",
            "[1858,     2] loss: 1.071\n",
            "[1859,     2] loss: 1.073\n",
            "[1860,     2] loss: 1.076\n",
            "[1861,     2] loss: 1.073\n",
            "[1862,     2] loss: 1.070\n",
            "[1863,     2] loss: 1.073\n",
            "[1864,     2] loss: 1.071\n",
            "[1865,     2] loss: 1.071\n",
            "[1866,     2] loss: 1.073\n",
            "[1867,     2] loss: 1.071\n",
            "[1868,     2] loss: 1.076\n",
            "[1869,     2] loss: 1.072\n",
            "[1870,     2] loss: 1.071\n",
            "[1871,     2] loss: 1.071\n",
            "[1872,     2] loss: 1.079\n",
            "[1873,     2] loss: 1.071\n",
            "[1874,     2] loss: 1.073\n",
            "[1875,     2] loss: 1.074\n",
            "[1876,     2] loss: 1.073\n",
            "[1877,     2] loss: 1.071\n",
            "[1878,     2] loss: 1.073\n",
            "[1879,     2] loss: 1.073\n",
            "[1880,     2] loss: 1.071\n",
            "[1881,     2] loss: 1.073\n",
            "[1882,     2] loss: 1.070\n",
            "[1883,     2] loss: 1.070\n",
            "[1884,     2] loss: 1.071\n",
            "[1885,     2] loss: 1.071\n",
            "[1886,     2] loss: 1.073\n",
            "[1887,     2] loss: 1.072\n",
            "[1888,     2] loss: 1.071\n",
            "[1889,     2] loss: 1.081\n",
            "[1890,     2] loss: 1.071\n",
            "[1891,     2] loss: 1.072\n",
            "[1892,     2] loss: 1.078\n",
            "[1893,     2] loss: 1.077\n",
            "[1894,     2] loss: 1.081\n",
            "[1895,     2] loss: 1.083\n",
            "[1896,     2] loss: 1.070\n",
            "[1897,     2] loss: 1.077\n",
            "[1898,     2] loss: 1.071\n",
            "[1899,     2] loss: 1.070\n",
            "[1900,     2] loss: 1.073\n",
            "[1901,     2] loss: 1.072\n",
            "[1902,     2] loss: 1.071\n",
            "[1903,     2] loss: 1.072\n",
            "[1904,     2] loss: 1.075\n",
            "[1905,     2] loss: 1.071\n",
            "[1906,     2] loss: 1.075\n",
            "[1907,     2] loss: 1.074\n",
            "[1908,     2] loss: 1.070\n",
            "[1909,     2] loss: 1.070\n",
            "[1910,     2] loss: 1.080\n",
            "[1911,     2] loss: 1.070\n",
            "[1912,     2] loss: 1.071\n",
            "[1913,     2] loss: 1.074\n",
            "[1914,     2] loss: 1.075\n",
            "[1915,     2] loss: 1.072\n",
            "[1916,     2] loss: 1.072\n",
            "[1917,     2] loss: 1.071\n",
            "[1918,     2] loss: 1.073\n",
            "[1919,     2] loss: 1.073\n",
            "[1920,     2] loss: 1.071\n",
            "[1921,     2] loss: 1.072\n",
            "[1922,     2] loss: 1.071\n",
            "[1923,     2] loss: 1.075\n",
            "[1924,     2] loss: 1.074\n",
            "[1925,     2] loss: 1.072\n",
            "[1926,     2] loss: 1.072\n",
            "[1927,     2] loss: 1.071\n",
            "[1928,     2] loss: 1.070\n",
            "[1929,     2] loss: 1.071\n",
            "[1930,     2] loss: 1.070\n",
            "[1931,     2] loss: 1.070\n",
            "[1932,     2] loss: 1.070\n",
            "[1933,     2] loss: 1.073\n",
            "[1934,     2] loss: 1.072\n",
            "[1935,     2] loss: 1.073\n",
            "[1936,     2] loss: 1.072\n",
            "[1937,     2] loss: 1.072\n",
            "[1938,     2] loss: 1.070\n",
            "[1939,     2] loss: 1.076\n",
            "[1940,     2] loss: 1.074\n",
            "[1941,     2] loss: 1.073\n",
            "[1942,     2] loss: 1.070\n",
            "[1943,     2] loss: 1.083\n",
            "[1944,     2] loss: 1.076\n",
            "[1945,     2] loss: 1.074\n",
            "[1946,     2] loss: 1.079\n",
            "[1947,     2] loss: 1.079\n",
            "[1948,     2] loss: 1.072\n",
            "[1949,     2] loss: 1.076\n",
            "[1950,     2] loss: 1.074\n",
            "[1951,     2] loss: 1.074\n",
            "[1952,     2] loss: 1.069\n",
            "[1953,     2] loss: 1.071\n",
            "[1954,     2] loss: 1.073\n",
            "[1955,     2] loss: 1.076\n",
            "[1956,     2] loss: 1.073\n",
            "[1957,     2] loss: 1.074\n",
            "[1958,     2] loss: 1.071\n",
            "[1959,     2] loss: 1.072\n",
            "[1960,     2] loss: 1.072\n",
            "[1961,     2] loss: 1.073\n",
            "[1962,     2] loss: 1.072\n",
            "[1963,     2] loss: 1.072\n",
            "[1964,     2] loss: 1.071\n",
            "[1965,     2] loss: 1.070\n",
            "[1966,     2] loss: 1.073\n",
            "[1967,     2] loss: 1.071\n",
            "[1968,     2] loss: 1.074\n",
            "[1969,     2] loss: 1.075\n",
            "[1970,     2] loss: 1.073\n",
            "[1971,     2] loss: 1.070\n",
            "[1972,     2] loss: 1.070\n",
            "[1973,     2] loss: 1.072\n",
            "[1974,     2] loss: 1.073\n",
            "[1975,     2] loss: 1.074\n",
            "[1976,     2] loss: 1.076\n",
            "[1977,     2] loss: 1.075\n",
            "[1978,     2] loss: 1.070\n",
            "[1979,     2] loss: 1.076\n",
            "[1980,     2] loss: 1.075\n",
            "[1981,     2] loss: 1.071\n",
            "[1982,     2] loss: 1.077\n",
            "[1983,     2] loss: 1.072\n",
            "[1984,     2] loss: 1.071\n",
            "[1985,     2] loss: 1.072\n",
            "[1986,     2] loss: 1.072\n",
            "[1987,     2] loss: 1.073\n",
            "[1988,     2] loss: 1.072\n",
            "[1989,     2] loss: 1.074\n",
            "[1990,     2] loss: 1.071\n",
            "[1991,     2] loss: 1.072\n",
            "[1992,     2] loss: 1.071\n",
            "[1993,     2] loss: 1.072\n",
            "[1994,     2] loss: 1.070\n",
            "[1995,     2] loss: 1.070\n",
            "[1996,     2] loss: 1.071\n",
            "[1997,     2] loss: 1.071\n",
            "[1998,     2] loss: 1.075\n",
            "[1999,     2] loss: 1.073\n",
            "[2000,     2] loss: 1.071\n",
            "Finished Training\n"
          ]
        }
      ],
      "source": [
        "nos_epochs = 2000\n",
        "focus_true_pred_true =0\n",
        "focus_false_pred_true =0\n",
        "focus_true_pred_false =0\n",
        "focus_false_pred_false =0\n",
        "\n",
        "argmax_more_than_half = 0\n",
        "argmax_less_than_half =0\n",
        "\n",
        "\n",
        "for epoch in range(nos_epochs):  # loop over the dataset multiple times\n",
        "\n",
        "  focus_true_pred_true =0\n",
        "  focus_false_pred_true =0\n",
        "  focus_true_pred_false =0\n",
        "  focus_false_pred_false =0\n",
        "  \n",
        "  argmax_more_than_half = 0\n",
        "  argmax_less_than_half =0\n",
        "  \n",
        "  running_loss = 0.0\n",
        "  epoch_loss = []\n",
        "  cnt=0\n",
        "\n",
        "  iteration = desired_num // batch\n",
        "  \n",
        "  #training data set\n",
        "  \n",
        "  for i, data in  enumerate(train_loader):\n",
        "    inputs , labels , fore_idx = data\n",
        "    batch = inputs.size(0)\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "    inputs = inputs.double()\n",
        "    # zero the parameter gradients\n",
        "    \n",
        "    optimizer_focus.zero_grad()\n",
        "    optimizer_classify.zero_grad()\n",
        "    \n",
        "    alphas, avg_images = focus_net(inputs)\n",
        "    outputs = classify(avg_images)\n",
        "\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "#     print(outputs)\n",
        "#     print(outputs.shape,labels.shape , torch.argmax(outputs, dim=1))\n",
        "\n",
        "    loss = my_cross_entropy(outputs, labels,alphas) \n",
        "    \n",
        "    loss.backward()\n",
        "    \n",
        "    \n",
        "    optimizer_focus.step()\n",
        "    optimizer_classify.step()\n",
        "\n",
        "    running_loss += loss.item()\n",
        "    mini = 2\n",
        "    if cnt % mini == mini-1:    # print every 40 mini-batches\n",
        "      print('[%d, %5d] loss: %.3f' %(epoch + 1, cnt + 1, running_loss / mini))\n",
        "      epoch_loss.append(running_loss/mini)\n",
        "      running_loss = 0.0\n",
        "    cnt=cnt+1\n",
        "\n",
        "  if(np.mean(epoch_loss) <= 0.01):\n",
        "      break;\n",
        "  #plot_attended_data(train_loader,focus_net,epoch)\n",
        "\n",
        "    \n",
        "print('Finished Training')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "3xPsiBtU-GDn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34f8ba98-d025-4703-cf69-5a77047cafca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('fc1.weight', Parameter containing:\n",
            "tensor([[-8.8892e-01, -3.8771e-01],\n",
            "        [-1.1981e+00, -2.3822e-01],\n",
            "        [ 1.0971e+01,  1.1323e+00],\n",
            "        [-4.5799e-01, -2.0478e-01],\n",
            "        [ 8.6294e+00,  2.1909e-01],\n",
            "        [-6.3992e-01, -5.4036e-01],\n",
            "        [ 1.3626e+01,  1.5266e+00],\n",
            "        [ 1.0843e+01,  1.0723e+00],\n",
            "        [ 1.2877e+01,  9.5447e-01],\n",
            "        [-9.1943e-01, -4.3421e-01],\n",
            "        [ 1.5231e-02, -6.3514e-01],\n",
            "        [ 7.1811e+00,  7.6625e-01],\n",
            "        [ 4.1888e+00,  8.7621e-01],\n",
            "        [ 5.7911e-01, -2.6002e+00],\n",
            "        [-3.1988e-01,  5.0750e-01],\n",
            "        [ 5.9173e+00, -1.8800e-01],\n",
            "        [-8.0049e-01, -7.0405e-01],\n",
            "        [ 2.4641e+00,  4.3749e-01],\n",
            "        [-5.1758e-01, -3.4092e-01],\n",
            "        [-7.8968e-01, -2.7476e-01],\n",
            "        [ 8.2157e-01,  5.7399e-01],\n",
            "        [-7.9973e-01, -8.3953e-01],\n",
            "        [ 9.0895e+00,  1.0708e+00],\n",
            "        [ 1.3052e+01,  1.0188e+00],\n",
            "        [ 8.4236e+00,  9.4509e-01],\n",
            "        [-1.1066e+00, -1.0376e-01],\n",
            "        [-8.8645e-01, -9.4865e-01],\n",
            "        [-7.0029e-01, -3.7613e-01],\n",
            "        [-1.4253e+00,  2.4869e-01],\n",
            "        [-2.1499e-01, -4.1297e-01],\n",
            "        [ 1.6378e+01,  1.0835e+00],\n",
            "        [ 5.6581e+00,  1.1464e+00],\n",
            "        [ 5.5673e+00,  7.6993e-01],\n",
            "        [ 9.8697e+00,  1.7751e+00],\n",
            "        [-9.2797e-01, -3.6388e-01],\n",
            "        [-2.4086e+00,  4.5953e+00],\n",
            "        [ 1.0445e+01,  9.7122e-01],\n",
            "        [-2.0197e+00,  8.1048e-01],\n",
            "        [ 3.4308e+00,  4.3001e-01],\n",
            "        [-1.7891e-01, -9.0977e-01],\n",
            "        [-5.1365e-01, -4.1896e-01],\n",
            "        [-8.5644e-01, -3.3718e-01],\n",
            "        [ 4.7605e-01, -3.0855e+00],\n",
            "        [-7.4135e-01, -8.8555e-01],\n",
            "        [ 6.3596e+00,  9.1610e-01],\n",
            "        [ 9.2296e+00,  8.5075e-01],\n",
            "        [-9.9218e-01, -2.9312e-01],\n",
            "        [ 1.0550e+01,  1.1071e+00],\n",
            "        [ 7.2231e+00,  1.0209e+00],\n",
            "        [-3.5545e-01, -4.5019e-01]], device='cuda:0', dtype=torch.float64,\n",
            "       requires_grad=True))\n",
            "('fc2.weight', Parameter containing:\n",
            "tensor([[-0.8756, -0.9573,  8.9493,  0.1663,  8.8594,  0.0594, 29.7152, 29.6003,\n",
            "         11.7351, -1.1088, -0.4542, 11.1785,  5.5756, -0.3989, -0.4432,  4.1425,\n",
            "         -0.8513,  1.3141,  0.5049,  0.0872,  0.8881, -0.9493, 12.2902, 16.2612,\n",
            "          7.0706, -0.9391, -0.8972,  0.2674, -0.7240, -0.1759, 18.7770, 10.0895,\n",
            "          6.7399, 16.2529, -0.8635, -2.1242, 16.2042, -0.6956,  4.4952, -0.3958,\n",
            "          0.4951, -0.9108, -0.8414, -0.8961,  6.4756,  6.2153, -0.7587,  9.2622,\n",
            "          9.1421,  0.4367]], device='cuda:0', dtype=torch.float64,\n",
            "       requires_grad=True))\n"
          ]
        }
      ],
      "source": [
        "for param in focus_net.named_parameters():\n",
        "    print(param)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "jhvhkEAyeRpt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3089b98c-e10f-4a4c-e275-53edbc3a23a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the train images: 40.000000 %\n",
            "total correct 40\n",
            "total train set images 100\n"
          ]
        }
      ],
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "focus_net.eval()\n",
        "classify.eval()\n",
        "with torch.no_grad():\n",
        "  for data in train_loader:\n",
        "    inputs, labels , fore_idx = data\n",
        "    inputs = inputs.double()\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "    alphas, avg_images = focus_net(inputs)\n",
        "    outputs = classify(avg_images)\n",
        "    #print(outputs.shape)\n",
        "\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network on the train images: %f %%' % ( 100 * correct / total))\n",
        "print(\"total correct\", correct)\n",
        "print(\"total train set images\", total)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "OKcmpKwGeS8M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b8bffc0-1e3b-47b2-b67d-5eb79283c70f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the test images: 31.200000 %\n",
            "total correct 312\n",
            "total train set images 1000\n"
          ]
        }
      ],
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "  for data in test_loader:\n",
        "    inputs, labels , fore_idx = data\n",
        "    inputs = inputs.double()\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "    alphas, avg_images = focus_net(inputs)\n",
        "    outputs = classify(avg_images)\n",
        "\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network on the test images: %f %%' % ( 100 * correct / total))\n",
        "print(\"total correct\", correct)\n",
        "print(\"total train set images\", total)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "y6sKB8hKpXQA"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "hard_attention_7.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1DJIuCzEH3NU",
    "outputId": "94ec8faf-fd97-412c-9949-04ae7eb64503"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "%matplotlib inline\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm as tqdm\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3FZslyHQIcjh",
    "outputId": "bae5dcaa-26fa-49ae-fd38-d5e2a47e138f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 500\n",
      "1 500\n",
      "2 500\n",
      "3 500\n",
      "4 500\n",
      "5 500\n",
      "6 500\n",
      "7 500\n",
      "8 500\n",
      "9 500\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "n_points = 500\n",
    "y = np.concatenate((np.zeros(n_points),np.ones(n_points),np.ones(n_points)*2,np.ones(n_points)*3,np.ones(n_points)*4,\n",
    "                    np.ones(n_points)*5,np.ones(n_points)*6,np.ones(n_points)*7,np.ones(n_points)*8,np.ones(n_points)*9))\n",
    "#y = np.random.randint(0,3,6000)\n",
    "idx= []\n",
    "for i in range(10):\n",
    "    print(i,sum(y==i))\n",
    "    idx.append(y==i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "6I4WHXbpIhLD"
   },
   "outputs": [],
   "source": [
    "x = np.zeros((n_points*10,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "-pLHSdR6IlTU"
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "cov_mat = [[0.002,0,0,0,0],[0,0.002,0,0,0],[0,0,0.002,0,0],\n",
    "                                                 [0,0,0,0.002,0],[0,0,0,0,0.002]]\n",
    "\n",
    "x[idx[0],:] = np.random.multivariate_normal(mean = [1,0,0,0,0],\n",
    "                                            cov=cov_mat,size=sum(idx[0]))\n",
    "\n",
    "\n",
    "x[idx[1],:] = np.random.multivariate_normal(mean = [0,1,0,0,0],\n",
    "                                            cov=cov_mat,size=sum(idx[1]))\n",
    "\n",
    "\n",
    "x[idx[2],:] = np.random.multivariate_normal(mean = [0,0,1,0,0],\n",
    "                                            cov=cov_mat,size=sum(idx[2]))\n",
    "\n",
    "\n",
    "x[idx[3],:] = np.random.multivariate_normal(mean = [0,0,0,-0.75,0.5],\n",
    "                                            cov=cov_mat,size=sum(idx[3]))\n",
    "\n",
    "\n",
    "\n",
    "x[idx[4],:] = np.random.multivariate_normal(mean = [0,0,0,0.65,-0.65],\n",
    "                                            cov=cov_mat,size=sum(idx[4]))\n",
    "\n",
    "\n",
    "x[idx[5],:] = np.random.multivariate_normal(mean = [0,0,0,-0.9,-0.75],\n",
    "                                            cov=cov_mat,size=sum(idx[5]))\n",
    "\n",
    "\n",
    "x[idx[6],:] = np.random.multivariate_normal(mean = [0,0,0,0.8,-0.8],\n",
    "                                            cov=cov_mat,size=sum(idx[6]))\n",
    "\n",
    "x[idx[7],:] = np.random.multivariate_normal(mean = [0,0,0,-0.5,0.8],\n",
    "                                            cov=cov_mat,size=sum(idx[7]))\n",
    "\n",
    "\n",
    "x[idx[8],:] = np.random.multivariate_normal(mean = [0,0,0,0.9,0.75],\n",
    "                                            cov=cov_mat,size=sum(idx[8]))\n",
    "\n",
    "\n",
    "x[idx[9],:] = np.random.multivariate_normal(mean = [0,0,0,-1,0.65],\n",
    "                                            cov=cov_mat,size=sum(idx[9]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "uAJnVrGTIndm",
    "outputId": "05ce1caa-222e-46bf-fa69-df46cef26374"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x16b830fa0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(x[idx[0],0],x[idx[0],1],label=\"class_\"+str(0))\n",
    "\n",
    "plt.scatter(x[idx[1],0],x[idx[1],1],label=\"class_\"+str(1))\n",
    "\n",
    "\n",
    "\n",
    "plt.scatter(x[idx[2],1],x[idx[2],2],label=\"class_\"+str(2))\n",
    "\n",
    "# plt.scatter(x[idx[3],0],x[idx[3],1],label=\"class_\"+str(3))\n",
    "\n",
    "\n",
    "\n",
    "# plt.scatter(x[idx[4],0],x[idx[4],1],label=\"class_\"+str(4))\n",
    "\n",
    "\n",
    "# plt.scatter(x[idx[5],0],x[idx[5],1],label=\"class_\"+str(5))\n",
    "\n",
    "\n",
    "# plt.scatter(x[idx[6],0],x[idx[6],1],label=\"class_\"+str(6))\n",
    "\n",
    "\n",
    "# plt.scatter(x[idx[7],0],x[idx[7],1],label=\"class_\"+str(7))\n",
    "\n",
    "\n",
    "# plt.scatter(x[idx[8],0],x[idx[8],1],label=\"class_\"+str(8))\n",
    "\n",
    "\n",
    "# plt.scatter(x[idx[9],0],x[idx[9],1],label=\"class_\"+str(9))\n",
    "\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "OZkuAUoLIp2i"
   },
   "outputs": [],
   "source": [
    "mean_x = np.mean(x,axis=0,keepdims=True)\n",
    "std_x = np.std(x,axis=0,keepdims=True)\n",
    "x = ( x -  mean_x ) / std_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "id": "I2OnOH9rItCh",
    "outputId": "24783e9f-1ea9-4b18-8314-60e789c6d153"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x16b958df0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(x[idx[0],0],x[idx[0],1],label=\"class_\"+str(0))\n",
    "\n",
    "plt.scatter(x[idx[1],0],x[idx[1],1],label=\"class_\"+str(1))\n",
    "\n",
    "\n",
    "\n",
    "plt.scatter(x[idx[2],1],x[idx[2],2],label=\"class_\"+str(2))\n",
    "\n",
    "\n",
    "# plt.scatter(x[idx[3],0],x[idx[3],1],label=\"class_\"+str(3))\n",
    "\n",
    "\n",
    "\n",
    "# plt.scatter(x[idx[4],0],x[idx[4],1],label=\"class_\"+str(4))\n",
    "\n",
    "\n",
    "# plt.scatter(x[idx[5],0],x[idx[5],1],label=\"class_\"+str(5))\n",
    "\n",
    "\n",
    "# plt.scatter(x[idx[6],0],x[idx[6],1],label=\"class_\"+str(6))\n",
    "\n",
    "\n",
    "# plt.scatter(x[idx[7],0],x[idx[7],1],label=\"class_\"+str(7))\n",
    "\n",
    "\n",
    "# plt.scatter(x[idx[8],0],x[idx[8],1],label=\"class_\"+str(8))\n",
    "\n",
    "\n",
    "# plt.scatter(x[idx[9],0],x[idx[9],1],label=\"class_\"+str(9))\n",
    "\n",
    "\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.423306331271305e-17\n",
      "1.0000000000000004\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(np.mean(x)),print(np.std(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net1, self).__init__()\n",
    "        self.fc1 = nn.Linear(5,1, bias=False)\n",
    "        torch.nn.init.xavier_normal_(self.fc1.weight)\n",
    "\n",
    "    def forward(self,z):\n",
    "        out = self.helper(z)\n",
    "        return out[:,0]\n",
    "  \n",
    "    \n",
    "    def helper(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net2, self).__init__()\n",
    "        self.fc1 = nn.Linear(5,3, bias=True)\n",
    "        torch.nn.init.xavier_normal_(self.fc1.weight)\n",
    "        torch.nn.init.zeros_(self.fc1.bias)\n",
    "\n",
    "    def forward(self,z):\n",
    "        out = self.helper(z)\n",
    "        return out\n",
    "    def helper(self, x):\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fg vs bg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_labels = np.zeros(y.shape)\n",
    "y_labels[idx[0]] = 1\n",
    "y_labels[idx[1]] = 1\n",
    "y_labels[idx[2]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SyntheticDataset(Dataset):\n",
    "  \"\"\"SyntheticDataset dataset.\"\"\"\n",
    "\n",
    "  def __init__(self, data,labels):\n",
    "    self.data = data\n",
    "    self.labels = labels\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.labels)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return self.data[idx] , self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch  = 250\n",
    "ds = SyntheticDataset(x,y_labels)\n",
    "train_loader =  DataLoader( ds,batch_size= batch ,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(12)\n",
    "net1 = Net1().double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(net1.parameters(), lr=0.001,weight_decay=1e-5 ) #, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    20] loss: 0.550\n",
      "Accuracy:  2606 5000 0.5212\n",
      "[2,    20] loss: 0.535\n",
      "Accuracy:  2810 5000 0.562\n",
      "[3,    20] loss: 0.520\n",
      "Accuracy:  3000 5000 0.6\n",
      "[4,    20] loss: 0.506\n",
      "Accuracy:  3203 5000 0.6406\n",
      "[5,    20] loss: 0.493\n",
      "Accuracy:  3430 5000 0.686\n",
      "[6,    20] loss: 0.480\n",
      "Accuracy:  3637 5000 0.7274\n",
      "[7,    20] loss: 0.468\n",
      "Accuracy:  3810 5000 0.762\n",
      "[8,    20] loss: 0.456\n",
      "Accuracy:  3993 5000 0.7986\n",
      "[9,    20] loss: 0.445\n",
      "Accuracy:  4130 5000 0.826\n",
      "[10,    20] loss: 0.434\n",
      "Accuracy:  4268 5000 0.8536\n",
      "[11,    20] loss: 0.424\n",
      "Accuracy:  4380 5000 0.876\n",
      "[12,    20] loss: 0.414\n",
      "Accuracy:  4478 5000 0.8956\n",
      "[13,    20] loss: 0.404\n",
      "Accuracy:  4562 5000 0.9124\n",
      "[14,    20] loss: 0.395\n",
      "Accuracy:  4632 5000 0.9264\n",
      "[15,    20] loss: 0.386\n",
      "Accuracy:  4674 5000 0.9348\n",
      "[16,    20] loss: 0.378\n",
      "Accuracy:  4720 5000 0.944\n",
      "[17,    20] loss: 0.369\n",
      "Accuracy:  4760 5000 0.952\n",
      "[18,    20] loss: 0.362\n",
      "Accuracy:  4807 5000 0.9614\n",
      "[19,    20] loss: 0.354\n",
      "Accuracy:  4843 5000 0.9686\n",
      "[20,    20] loss: 0.347\n",
      "Accuracy:  4861 5000 0.9722\n",
      "[21,    20] loss: 0.340\n",
      "Accuracy:  4880 5000 0.976\n",
      "[22,    20] loss: 0.333\n",
      "Accuracy:  4898 5000 0.9796\n",
      "[23,    20] loss: 0.326\n",
      "Accuracy:  4908 5000 0.9816\n",
      "[24,    20] loss: 0.320\n",
      "Accuracy:  4918 5000 0.9836\n",
      "[25,    20] loss: 0.314\n",
      "Accuracy:  4930 5000 0.986\n",
      "[26,    20] loss: 0.308\n",
      "Accuracy:  4943 5000 0.9886\n",
      "[27,    20] loss: 0.302\n",
      "Accuracy:  4947 5000 0.9894\n",
      "[28,    20] loss: 0.297\n",
      "Accuracy:  4958 5000 0.9916\n",
      "[29,    20] loss: 0.291\n",
      "Accuracy:  4961 5000 0.9922\n",
      "[30,    20] loss: 0.286\n",
      "Accuracy:  4966 5000 0.9932\n",
      "[31,    20] loss: 0.281\n",
      "Accuracy:  4974 5000 0.9948\n",
      "[32,    20] loss: 0.276\n",
      "Accuracy:  4980 5000 0.996\n",
      "[33,    20] loss: 0.272\n",
      "Accuracy:  4982 5000 0.9964\n",
      "[34,    20] loss: 0.267\n",
      "Accuracy:  4983 5000 0.9966\n",
      "[35,    20] loss: 0.263\n",
      "Accuracy:  4987 5000 0.9974\n",
      "[36,    20] loss: 0.258\n",
      "Accuracy:  4989 5000 0.9978\n",
      "[37,    20] loss: 0.254\n",
      "Accuracy:  4989 5000 0.9978\n",
      "[38,    20] loss: 0.250\n",
      "Accuracy:  4992 5000 0.9984\n",
      "[39,    20] loss: 0.246\n",
      "Accuracy:  4993 5000 0.9986\n",
      "[40,    20] loss: 0.242\n",
      "Accuracy:  4995 5000 0.999\n",
      "[41,    20] loss: 0.238\n",
      "Accuracy:  4995 5000 0.999\n",
      "[42,    20] loss: 0.235\n",
      "Accuracy:  4995 5000 0.999\n",
      "[43,    20] loss: 0.231\n",
      "Accuracy:  4995 5000 0.999\n",
      "[44,    20] loss: 0.228\n",
      "Accuracy:  4997 5000 0.9994\n",
      "[45,    20] loss: 0.224\n",
      "Accuracy:  4997 5000 0.9994\n",
      "[46,    20] loss: 0.221\n",
      "Accuracy:  4997 5000 0.9994\n",
      "[47,    20] loss: 0.218\n",
      "Accuracy:  4997 5000 0.9994\n",
      "[48,    20] loss: 0.215\n",
      "Accuracy:  4999 5000 0.9998\n",
      "[49,    20] loss: 0.211\n",
      "Accuracy:  4999 5000 0.9998\n",
      "[50,    20] loss: 0.208\n",
      "Accuracy:  4999 5000 0.9998\n",
      "[51,    20] loss: 0.206\n",
      "Accuracy:  4999 5000 0.9998\n",
      "[52,    20] loss: 0.203\n",
      "Accuracy:  4999 5000 0.9998\n",
      "[53,    20] loss: 0.200\n",
      "Accuracy:  4999 5000 0.9998\n",
      "[54,    20] loss: 0.197\n",
      "Accuracy:  4999 5000 0.9998\n",
      "[55,    20] loss: 0.195\n",
      "Accuracy:  4999 5000 0.9998\n",
      "[56,    20] loss: 0.192\n",
      "Accuracy:  4999 5000 0.9998\n",
      "[57,    20] loss: 0.190\n",
      "Accuracy:  4999 5000 0.9998\n",
      "[58,    20] loss: 0.187\n",
      "Accuracy:  5000 5000 1.0\n",
      "[59,    20] loss: 0.185\n",
      "Accuracy:  5000 5000 1.0\n",
      "[60,    20] loss: 0.182\n",
      "Accuracy:  5000 5000 1.0\n",
      "[61,    20] loss: 0.180\n",
      "Accuracy:  5000 5000 1.0\n",
      "[62,    20] loss: 0.178\n",
      "Accuracy:  5000 5000 1.0\n",
      "[63,    20] loss: 0.175\n",
      "Accuracy:  5000 5000 1.0\n",
      "[64,    20] loss: 0.173\n",
      "Accuracy:  5000 5000 1.0\n",
      "[65,    20] loss: 0.171\n",
      "Accuracy:  5000 5000 1.0\n",
      "[66,    20] loss: 0.169\n",
      "Accuracy:  5000 5000 1.0\n",
      "[67,    20] loss: 0.167\n",
      "Accuracy:  5000 5000 1.0\n",
      "[68,    20] loss: 0.165\n",
      "Accuracy:  5000 5000 1.0\n",
      "[69,    20] loss: 0.163\n",
      "Accuracy:  5000 5000 1.0\n",
      "[70,    20] loss: 0.161\n",
      "Accuracy:  5000 5000 1.0\n",
      "[71,    20] loss: 0.159\n",
      "Accuracy:  5000 5000 1.0\n",
      "[72,    20] loss: 0.157\n",
      "Accuracy:  5000 5000 1.0\n",
      "[73,    20] loss: 0.156\n",
      "Accuracy:  5000 5000 1.0\n",
      "[74,    20] loss: 0.154\n",
      "Accuracy:  5000 5000 1.0\n",
      "[75,    20] loss: 0.152\n",
      "Accuracy:  5000 5000 1.0\n",
      "[76,    20] loss: 0.150\n",
      "Accuracy:  5000 5000 1.0\n",
      "[77,    20] loss: 0.149\n",
      "Accuracy:  5000 5000 1.0\n",
      "[78,    20] loss: 0.147\n",
      "Accuracy:  5000 5000 1.0\n",
      "[79,    20] loss: 0.145\n",
      "Accuracy:  5000 5000 1.0\n",
      "[80,    20] loss: 0.144\n",
      "Accuracy:  5000 5000 1.0\n",
      "[81,    20] loss: 0.142\n",
      "Accuracy:  5000 5000 1.0\n",
      "[82,    20] loss: 0.141\n",
      "Accuracy:  5000 5000 1.0\n",
      "[83,    20] loss: 0.139\n",
      "Accuracy:  5000 5000 1.0\n",
      "[84,    20] loss: 0.138\n",
      "Accuracy:  5000 5000 1.0\n",
      "[85,    20] loss: 0.136\n",
      "Accuracy:  5000 5000 1.0\n",
      "[86,    20] loss: 0.135\n",
      "Accuracy:  5000 5000 1.0\n",
      "[87,    20] loss: 0.133\n",
      "Accuracy:  5000 5000 1.0\n",
      "[88,    20] loss: 0.132\n",
      "Accuracy:  5000 5000 1.0\n",
      "[89,    20] loss: 0.130\n",
      "Accuracy:  5000 5000 1.0\n",
      "[90,    20] loss: 0.129\n",
      "Accuracy:  5000 5000 1.0\n",
      "[91,    20] loss: 0.128\n",
      "Accuracy:  5000 5000 1.0\n",
      "[92,    20] loss: 0.126\n",
      "Accuracy:  5000 5000 1.0\n",
      "[93,    20] loss: 0.125\n",
      "Accuracy:  5000 5000 1.0\n",
      "[94,    20] loss: 0.124\n",
      "Accuracy:  5000 5000 1.0\n",
      "[95,    20] loss: 0.122\n",
      "Accuracy:  5000 5000 1.0\n",
      "[96,    20] loss: 0.121\n",
      "Accuracy:  5000 5000 1.0\n",
      "[97,    20] loss: 0.120\n",
      "Accuracy:  5000 5000 1.0\n",
      "[98,    20] loss: 0.119\n",
      "Accuracy:  5000 5000 1.0\n",
      "[99,    20] loss: 0.118\n",
      "Accuracy:  5000 5000 1.0\n",
      "[100,    20] loss: 0.116\n",
      "Accuracy:  5000 5000 1.0\n",
      "[101,    20] loss: 0.115\n",
      "Accuracy:  5000 5000 1.0\n",
      "[102,    20] loss: 0.114\n",
      "Accuracy:  5000 5000 1.0\n",
      "[103,    20] loss: 0.113\n",
      "Accuracy:  5000 5000 1.0\n",
      "[104,    20] loss: 0.112\n",
      "Accuracy:  5000 5000 1.0\n",
      "[105,    20] loss: 0.111\n",
      "Accuracy:  5000 5000 1.0\n",
      "[106,    20] loss: 0.110\n",
      "Accuracy:  5000 5000 1.0\n",
      "[107,    20] loss: 0.109\n",
      "Accuracy:  5000 5000 1.0\n",
      "[108,    20] loss: 0.108\n",
      "Accuracy:  5000 5000 1.0\n",
      "[109,    20] loss: 0.107\n",
      "Accuracy:  5000 5000 1.0\n",
      "[110,    20] loss: 0.106\n",
      "Accuracy:  5000 5000 1.0\n",
      "[111,    20] loss: 0.105\n",
      "Accuracy:  5000 5000 1.0\n",
      "[112,    20] loss: 0.104\n",
      "Accuracy:  5000 5000 1.0\n",
      "[113,    20] loss: 0.103\n",
      "Accuracy:  5000 5000 1.0\n",
      "[114,    20] loss: 0.102\n",
      "Accuracy:  5000 5000 1.0\n",
      "[115,    20] loss: 0.101\n",
      "Accuracy:  5000 5000 1.0\n",
      "[116,    20] loss: 0.100\n",
      "Accuracy:  5000 5000 1.0\n",
      "[117,    20] loss: 0.099\n",
      "Accuracy:  5000 5000 1.0\n",
      "[118,    20] loss: 0.098\n",
      "Accuracy:  5000 5000 1.0\n",
      "[119,    20] loss: 0.097\n",
      "Accuracy:  5000 5000 1.0\n",
      "[120,    20] loss: 0.096\n",
      "Accuracy:  5000 5000 1.0\n",
      "[121,    20] loss: 0.095\n",
      "Accuracy:  5000 5000 1.0\n",
      "[122,    20] loss: 0.094\n",
      "Accuracy:  5000 5000 1.0\n",
      "[123,    20] loss: 0.094\n",
      "Accuracy:  5000 5000 1.0\n",
      "[124,    20] loss: 0.093\n",
      "Accuracy:  5000 5000 1.0\n",
      "[125,    20] loss: 0.092\n",
      "Accuracy:  5000 5000 1.0\n",
      "[126,    20] loss: 0.091\n",
      "Accuracy:  5000 5000 1.0\n",
      "[127,    20] loss: 0.090\n",
      "Accuracy:  5000 5000 1.0\n",
      "[128,    20] loss: 0.089\n",
      "Accuracy:  5000 5000 1.0\n",
      "[129,    20] loss: 0.089\n",
      "Accuracy:  5000 5000 1.0\n",
      "[130,    20] loss: 0.088\n",
      "Accuracy:  5000 5000 1.0\n",
      "[131,    20] loss: 0.087\n",
      "Accuracy:  5000 5000 1.0\n",
      "[132,    20] loss: 0.086\n",
      "Accuracy:  5000 5000 1.0\n",
      "[133,    20] loss: 0.086\n",
      "Accuracy:  5000 5000 1.0\n",
      "[134,    20] loss: 0.085\n",
      "Accuracy:  5000 5000 1.0\n",
      "[135,    20] loss: 0.084\n",
      "Accuracy:  5000 5000 1.0\n",
      "[136,    20] loss: 0.083\n",
      "Accuracy:  5000 5000 1.0\n",
      "[137,    20] loss: 0.083\n",
      "Accuracy:  5000 5000 1.0\n",
      "[138,    20] loss: 0.082\n",
      "Accuracy:  5000 5000 1.0\n",
      "[139,    20] loss: 0.081\n",
      "Accuracy:  5000 5000 1.0\n",
      "[140,    20] loss: 0.080\n",
      "Accuracy:  5000 5000 1.0\n",
      "[141,    20] loss: 0.080\n",
      "Accuracy:  5000 5000 1.0\n",
      "[142,    20] loss: 0.079\n",
      "Accuracy:  5000 5000 1.0\n",
      "[143,    20] loss: 0.078\n",
      "Accuracy:  5000 5000 1.0\n",
      "[144,    20] loss: 0.078\n",
      "Accuracy:  5000 5000 1.0\n",
      "[145,    20] loss: 0.077\n",
      "Accuracy:  5000 5000 1.0\n",
      "[146,    20] loss: 0.076\n",
      "Accuracy:  5000 5000 1.0\n",
      "[147,    20] loss: 0.076\n",
      "Accuracy:  5000 5000 1.0\n",
      "[148,    20] loss: 0.075\n",
      "Accuracy:  5000 5000 1.0\n",
      "[149,    20] loss: 0.074\n",
      "Accuracy:  5000 5000 1.0\n",
      "[150,    20] loss: 0.074\n",
      "Accuracy:  5000 5000 1.0\n",
      "[151,    20] loss: 0.073\n",
      "Accuracy:  5000 5000 1.0\n",
      "[152,    20] loss: 0.073\n",
      "Accuracy:  5000 5000 1.0\n",
      "[153,    20] loss: 0.072\n",
      "Accuracy:  5000 5000 1.0\n",
      "[154,    20] loss: 0.071\n",
      "Accuracy:  5000 5000 1.0\n",
      "[155,    20] loss: 0.071\n",
      "Accuracy:  5000 5000 1.0\n",
      "[156,    20] loss: 0.070\n",
      "Accuracy:  5000 5000 1.0\n",
      "[157,    20] loss: 0.070\n",
      "Accuracy:  5000 5000 1.0\n",
      "[158,    20] loss: 0.069\n",
      "Accuracy:  5000 5000 1.0\n",
      "[159,    20] loss: 0.069\n",
      "Accuracy:  5000 5000 1.0\n",
      "[160,    20] loss: 0.068\n",
      "Accuracy:  5000 5000 1.0\n",
      "[161,    20] loss: 0.067\n",
      "Accuracy:  5000 5000 1.0\n",
      "[162,    20] loss: 0.067\n",
      "Accuracy:  5000 5000 1.0\n",
      "[163,    20] loss: 0.066\n",
      "Accuracy:  5000 5000 1.0\n",
      "[164,    20] loss: 0.066\n",
      "Accuracy:  5000 5000 1.0\n",
      "[165,    20] loss: 0.065\n",
      "Accuracy:  5000 5000 1.0\n",
      "[166,    20] loss: 0.065\n",
      "Accuracy:  5000 5000 1.0\n",
      "[167,    20] loss: 0.064\n",
      "Accuracy:  5000 5000 1.0\n",
      "[168,    20] loss: 0.064\n",
      "Accuracy:  5000 5000 1.0\n",
      "[169,    20] loss: 0.063\n",
      "Accuracy:  5000 5000 1.0\n",
      "[170,    20] loss: 0.063\n",
      "Accuracy:  5000 5000 1.0\n",
      "[171,    20] loss: 0.062\n",
      "Accuracy:  5000 5000 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[172,    20] loss: 0.062\n",
      "Accuracy:  5000 5000 1.0\n",
      "[173,    20] loss: 0.061\n",
      "Accuracy:  5000 5000 1.0\n",
      "[174,    20] loss: 0.061\n",
      "Accuracy:  5000 5000 1.0\n",
      "[175,    20] loss: 0.060\n",
      "Accuracy:  5000 5000 1.0\n",
      "[176,    20] loss: 0.060\n",
      "Accuracy:  5000 5000 1.0\n",
      "[177,    20] loss: 0.059\n",
      "Accuracy:  5000 5000 1.0\n",
      "[178,    20] loss: 0.059\n",
      "Accuracy:  5000 5000 1.0\n",
      "[179,    20] loss: 0.058\n",
      "Accuracy:  5000 5000 1.0\n",
      "[180,    20] loss: 0.058\n",
      "Accuracy:  5000 5000 1.0\n",
      "[181,    20] loss: 0.057\n",
      "Accuracy:  5000 5000 1.0\n",
      "[182,    20] loss: 0.057\n",
      "Accuracy:  5000 5000 1.0\n",
      "[183,    20] loss: 0.057\n",
      "Accuracy:  5000 5000 1.0\n",
      "[184,    20] loss: 0.056\n",
      "Accuracy:  5000 5000 1.0\n",
      "[185,    20] loss: 0.056\n",
      "Accuracy:  5000 5000 1.0\n",
      "[186,    20] loss: 0.055\n",
      "Accuracy:  5000 5000 1.0\n",
      "[187,    20] loss: 0.055\n",
      "Accuracy:  5000 5000 1.0\n",
      "[188,    20] loss: 0.054\n",
      "Accuracy:  5000 5000 1.0\n",
      "[189,    20] loss: 0.054\n",
      "Accuracy:  5000 5000 1.0\n",
      "[190,    20] loss: 0.054\n",
      "Accuracy:  5000 5000 1.0\n",
      "[191,    20] loss: 0.053\n",
      "Accuracy:  5000 5000 1.0\n",
      "[192,    20] loss: 0.053\n",
      "Accuracy:  5000 5000 1.0\n",
      "[193,    20] loss: 0.052\n",
      "Accuracy:  5000 5000 1.0\n",
      "[194,    20] loss: 0.052\n",
      "Accuracy:  5000 5000 1.0\n",
      "[195,    20] loss: 0.052\n",
      "Accuracy:  5000 5000 1.0\n",
      "[196,    20] loss: 0.051\n",
      "Accuracy:  5000 5000 1.0\n",
      "[197,    20] loss: 0.051\n",
      "Accuracy:  5000 5000 1.0\n",
      "[198,    20] loss: 0.050\n",
      "Accuracy:  5000 5000 1.0\n",
      "[199,    20] loss: 0.050\n",
      "Accuracy:  5000 5000 1.0\n",
      "[200,    20] loss: 0.050\n",
      "Accuracy:  5000 5000 1.0\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "nos_epochs = 200\n",
    "epoch_loss = []\n",
    "for epoch in range(nos_epochs):  # loop over the dataset multiple times\n",
    "    cnt = 0 \n",
    "    running_loss = 0.0\n",
    "\n",
    "    correct = 0\n",
    "    total = 0 \n",
    "    #training data set\n",
    "    for i, data in  enumerate(train_loader):\n",
    "        inputs , labels = data\n",
    "        batch = inputs.size(0)\n",
    "        #inputs, labels = inputs.to(\"cuda\"), labels.to(\"cuda\")\n",
    "        inputs = inputs.double()\n",
    "        # zero the parameter gradients\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        outputs = net1(inputs)\n",
    "        \n",
    "        \n",
    "        prediction = outputs>0.5\n",
    "        \n",
    "        \n",
    "        correct += torch.sum(prediction == labels)\n",
    "        total += len(labels)\n",
    "        \n",
    "        \n",
    "        loss = criterion(outputs, labels) \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        mini = 20\n",
    "        if cnt % mini == mini-1:    # print every 40 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %(epoch + 1, cnt + 1, running_loss / mini))\n",
    "            epoch_loss.append(running_loss/mini)\n",
    "            running_loss = 0.0\n",
    "        cnt=cnt+1\n",
    "        \n",
    "    print(\"Accuracy: \",correct.item(),total,correct.item()/total)\n",
    "    if(np.mean(epoch_loss) <= 0.001):\n",
    "        break;\n",
    "    \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x16ba06310>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fg1 vs fg2 vs fg3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = x[:1500]\n",
    "y_data = y[:1500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch  = 250\n",
    "ds1 = SyntheticDataset(x_data,y_data)\n",
    "train_loader1 =  DataLoader( ds1,batch_size= batch ,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(12)\n",
    "net2 = Net2().double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net2.parameters(), lr=0.007,weight_decay=1e-5 ) #, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     6] loss: 3.027\n",
      "Accuracy:  187 1500 0.12466666666666666\n",
      "[2,     6] loss: 2.785\n",
      "Accuracy:  462 1500 0.308\n",
      "[3,     6] loss: 2.554\n",
      "Accuracy:  500 1500 0.3333333333333333\n",
      "[4,     6] loss: 2.334\n",
      "Accuracy:  500 1500 0.3333333333333333\n",
      "[5,     6] loss: 2.125\n",
      "Accuracy:  500 1500 0.3333333333333333\n",
      "[6,     6] loss: 1.923\n",
      "Accuracy:  500 1500 0.3333333333333333\n",
      "[7,     6] loss: 1.732\n",
      "Accuracy:  500 1500 0.3333333333333333\n",
      "[8,     6] loss: 1.550\n",
      "Accuracy:  500 1500 0.3333333333333333\n",
      "[9,     6] loss: 1.377\n",
      "Accuracy:  500 1500 0.3333333333333333\n",
      "[10,     6] loss: 1.217\n",
      "Accuracy:  500 1500 0.3333333333333333\n",
      "[11,     6] loss: 1.066\n",
      "Accuracy:  500 1500 0.3333333333333333\n",
      "[12,     6] loss: 0.932\n",
      "Accuracy:  500 1500 0.3333333333333333\n",
      "[13,     6] loss: 0.810\n",
      "Accuracy:  500 1500 0.3333333333333333\n",
      "[14,     6] loss: 0.704\n",
      "Accuracy:  534 1500 0.356\n",
      "[15,     6] loss: 0.609\n",
      "Accuracy:  882 1500 0.588\n",
      "[16,     6] loss: 0.531\n",
      "Accuracy:  1379 1500 0.9193333333333333\n",
      "[17,     6] loss: 0.463\n",
      "Accuracy:  1497 1500 0.998\n",
      "[18,     6] loss: 0.405\n",
      "Accuracy:  1500 1500 1.0\n",
      "[19,     6] loss: 0.357\n",
      "Accuracy:  1500 1500 1.0\n",
      "[20,     6] loss: 0.316\n",
      "Accuracy:  1500 1500 1.0\n",
      "[21,     6] loss: 0.282\n",
      "Accuracy:  1500 1500 1.0\n",
      "[22,     6] loss: 0.253\n",
      "Accuracy:  1500 1500 1.0\n",
      "[23,     6] loss: 0.229\n",
      "Accuracy:  1500 1500 1.0\n",
      "[24,     6] loss: 0.208\n",
      "Accuracy:  1500 1500 1.0\n",
      "[25,     6] loss: 0.190\n",
      "Accuracy:  1500 1500 1.0\n",
      "[26,     6] loss: 0.175\n",
      "Accuracy:  1500 1500 1.0\n",
      "[27,     6] loss: 0.161\n",
      "Accuracy:  1500 1500 1.0\n",
      "[28,     6] loss: 0.149\n",
      "Accuracy:  1500 1500 1.0\n",
      "[29,     6] loss: 0.139\n",
      "Accuracy:  1500 1500 1.0\n",
      "[30,     6] loss: 0.129\n",
      "Accuracy:  1500 1500 1.0\n",
      "[31,     6] loss: 0.121\n",
      "Accuracy:  1500 1500 1.0\n",
      "[32,     6] loss: 0.114\n",
      "Accuracy:  1500 1500 1.0\n",
      "[33,     6] loss: 0.107\n",
      "Accuracy:  1500 1500 1.0\n",
      "[34,     6] loss: 0.101\n",
      "Accuracy:  1500 1500 1.0\n",
      "[35,     6] loss: 0.095\n",
      "Accuracy:  1500 1500 1.0\n",
      "[36,     6] loss: 0.090\n",
      "Accuracy:  1500 1500 1.0\n",
      "[37,     6] loss: 0.086\n",
      "Accuracy:  1500 1500 1.0\n",
      "[38,     6] loss: 0.082\n",
      "Accuracy:  1500 1500 1.0\n",
      "[39,     6] loss: 0.078\n",
      "Accuracy:  1500 1500 1.0\n",
      "[40,     6] loss: 0.074\n",
      "Accuracy:  1500 1500 1.0\n",
      "[41,     6] loss: 0.071\n",
      "Accuracy:  1500 1500 1.0\n",
      "[42,     6] loss: 0.068\n",
      "Accuracy:  1500 1500 1.0\n",
      "[43,     6] loss: 0.065\n",
      "Accuracy:  1500 1500 1.0\n",
      "[44,     6] loss: 0.062\n",
      "Accuracy:  1500 1500 1.0\n",
      "[45,     6] loss: 0.060\n",
      "Accuracy:  1500 1500 1.0\n",
      "[46,     6] loss: 0.058\n",
      "Accuracy:  1500 1500 1.0\n",
      "[47,     6] loss: 0.055\n",
      "Accuracy:  1500 1500 1.0\n",
      "[48,     6] loss: 0.053\n",
      "Accuracy:  1500 1500 1.0\n",
      "[49,     6] loss: 0.051\n",
      "Accuracy:  1500 1500 1.0\n",
      "[50,     6] loss: 0.050\n",
      "Accuracy:  1500 1500 1.0\n",
      "[51,     6] loss: 0.048\n",
      "Accuracy:  1500 1500 1.0\n",
      "[52,     6] loss: 0.046\n",
      "Accuracy:  1500 1500 1.0\n",
      "[53,     6] loss: 0.045\n",
      "Accuracy:  1500 1500 1.0\n",
      "[54,     6] loss: 0.043\n",
      "Accuracy:  1500 1500 1.0\n",
      "[55,     6] loss: 0.042\n",
      "Accuracy:  1500 1500 1.0\n",
      "[56,     6] loss: 0.041\n",
      "Accuracy:  1500 1500 1.0\n",
      "[57,     6] loss: 0.040\n",
      "Accuracy:  1500 1500 1.0\n",
      "[58,     6] loss: 0.038\n",
      "Accuracy:  1500 1500 1.0\n",
      "[59,     6] loss: 0.037\n",
      "Accuracy:  1500 1500 1.0\n",
      "[60,     6] loss: 0.036\n",
      "Accuracy:  1500 1500 1.0\n",
      "[61,     6] loss: 0.035\n",
      "Accuracy:  1500 1500 1.0\n",
      "[62,     6] loss: 0.034\n",
      "Accuracy:  1500 1500 1.0\n",
      "[63,     6] loss: 0.033\n",
      "Accuracy:  1500 1500 1.0\n",
      "[64,     6] loss: 0.032\n",
      "Accuracy:  1500 1500 1.0\n",
      "[65,     6] loss: 0.032\n",
      "Accuracy:  1500 1500 1.0\n",
      "[66,     6] loss: 0.031\n",
      "Accuracy:  1500 1500 1.0\n",
      "[67,     6] loss: 0.030\n",
      "Accuracy:  1500 1500 1.0\n",
      "[68,     6] loss: 0.029\n",
      "Accuracy:  1500 1500 1.0\n",
      "[69,     6] loss: 0.029\n",
      "Accuracy:  1500 1500 1.0\n",
      "[70,     6] loss: 0.028\n",
      "Accuracy:  1500 1500 1.0\n",
      "[71,     6] loss: 0.027\n",
      "Accuracy:  1500 1500 1.0\n",
      "[72,     6] loss: 0.027\n",
      "Accuracy:  1500 1500 1.0\n",
      "[73,     6] loss: 0.026\n",
      "Accuracy:  1500 1500 1.0\n",
      "[74,     6] loss: 0.025\n",
      "Accuracy:  1500 1500 1.0\n",
      "[75,     6] loss: 0.025\n",
      "Accuracy:  1500 1500 1.0\n",
      "[76,     6] loss: 0.024\n",
      "Accuracy:  1500 1500 1.0\n",
      "[77,     6] loss: 0.024\n",
      "Accuracy:  1500 1500 1.0\n",
      "[78,     6] loss: 0.023\n",
      "Accuracy:  1500 1500 1.0\n",
      "[79,     6] loss: 0.023\n",
      "Accuracy:  1500 1500 1.0\n",
      "[80,     6] loss: 0.022\n",
      "Accuracy:  1500 1500 1.0\n",
      "[81,     6] loss: 0.022\n",
      "Accuracy:  1500 1500 1.0\n",
      "[82,     6] loss: 0.021\n",
      "Accuracy:  1500 1500 1.0\n",
      "[83,     6] loss: 0.021\n",
      "Accuracy:  1500 1500 1.0\n",
      "[84,     6] loss: 0.021\n",
      "Accuracy:  1500 1500 1.0\n",
      "[85,     6] loss: 0.020\n",
      "Accuracy:  1500 1500 1.0\n",
      "[86,     6] loss: 0.020\n",
      "Accuracy:  1500 1500 1.0\n",
      "[87,     6] loss: 0.019\n",
      "Accuracy:  1500 1500 1.0\n",
      "[88,     6] loss: 0.019\n",
      "Accuracy:  1500 1500 1.0\n",
      "[89,     6] loss: 0.019\n",
      "Accuracy:  1500 1500 1.0\n",
      "[90,     6] loss: 0.018\n",
      "Accuracy:  1500 1500 1.0\n",
      "[91,     6] loss: 0.018\n",
      "Accuracy:  1500 1500 1.0\n",
      "[92,     6] loss: 0.018\n",
      "Accuracy:  1500 1500 1.0\n",
      "[93,     6] loss: 0.017\n",
      "Accuracy:  1500 1500 1.0\n",
      "[94,     6] loss: 0.017\n",
      "Accuracy:  1500 1500 1.0\n",
      "[95,     6] loss: 0.017\n",
      "Accuracy:  1500 1500 1.0\n",
      "[96,     6] loss: 0.016\n",
      "Accuracy:  1500 1500 1.0\n",
      "[97,     6] loss: 0.016\n",
      "Accuracy:  1500 1500 1.0\n",
      "[98,     6] loss: 0.016\n",
      "Accuracy:  1500 1500 1.0\n",
      "[99,     6] loss: 0.016\n",
      "Accuracy:  1500 1500 1.0\n",
      "[100,     6] loss: 0.015\n",
      "Accuracy:  1500 1500 1.0\n",
      "[101,     6] loss: 0.015\n",
      "Accuracy:  1500 1500 1.0\n",
      "[102,     6] loss: 0.015\n",
      "Accuracy:  1500 1500 1.0\n",
      "[103,     6] loss: 0.015\n",
      "Accuracy:  1500 1500 1.0\n",
      "[104,     6] loss: 0.014\n",
      "Accuracy:  1500 1500 1.0\n",
      "[105,     6] loss: 0.014\n",
      "Accuracy:  1500 1500 1.0\n",
      "[106,     6] loss: 0.014\n",
      "Accuracy:  1500 1500 1.0\n",
      "[107,     6] loss: 0.014\n",
      "Accuracy:  1500 1500 1.0\n",
      "[108,     6] loss: 0.014\n",
      "Accuracy:  1500 1500 1.0\n",
      "[109,     6] loss: 0.013\n",
      "Accuracy:  1500 1500 1.0\n",
      "[110,     6] loss: 0.013\n",
      "Accuracy:  1500 1500 1.0\n",
      "[111,     6] loss: 0.013\n",
      "Accuracy:  1500 1500 1.0\n",
      "[112,     6] loss: 0.013\n",
      "Accuracy:  1500 1500 1.0\n",
      "[113,     6] loss: 0.013\n",
      "Accuracy:  1500 1500 1.0\n",
      "[114,     6] loss: 0.012\n",
      "Accuracy:  1500 1500 1.0\n",
      "[115,     6] loss: 0.012\n",
      "Accuracy:  1500 1500 1.0\n",
      "[116,     6] loss: 0.012\n",
      "Accuracy:  1500 1500 1.0\n",
      "[117,     6] loss: 0.012\n",
      "Accuracy:  1500 1500 1.0\n",
      "[118,     6] loss: 0.012\n",
      "Accuracy:  1500 1500 1.0\n",
      "[119,     6] loss: 0.011\n",
      "Accuracy:  1500 1500 1.0\n",
      "[120,     6] loss: 0.011\n",
      "Accuracy:  1500 1500 1.0\n",
      "[121,     6] loss: 0.011\n",
      "Accuracy:  1500 1500 1.0\n",
      "[122,     6] loss: 0.011\n",
      "Accuracy:  1500 1500 1.0\n",
      "[123,     6] loss: 0.011\n",
      "Accuracy:  1500 1500 1.0\n",
      "[124,     6] loss: 0.011\n",
      "Accuracy:  1500 1500 1.0\n",
      "[125,     6] loss: 0.011\n",
      "Accuracy:  1500 1500 1.0\n",
      "[126,     6] loss: 0.010\n",
      "Accuracy:  1500 1500 1.0\n",
      "[127,     6] loss: 0.010\n",
      "Accuracy:  1500 1500 1.0\n",
      "[128,     6] loss: 0.010\n",
      "Accuracy:  1500 1500 1.0\n",
      "[129,     6] loss: 0.010\n",
      "Accuracy:  1500 1500 1.0\n",
      "[130,     6] loss: 0.010\n",
      "Accuracy:  1500 1500 1.0\n",
      "[131,     6] loss: 0.010\n",
      "Accuracy:  1500 1500 1.0\n",
      "[132,     6] loss: 0.010\n",
      "Accuracy:  1500 1500 1.0\n",
      "[133,     6] loss: 0.010\n",
      "Accuracy:  1500 1500 1.0\n",
      "[134,     6] loss: 0.009\n",
      "Accuracy:  1500 1500 1.0\n",
      "[135,     6] loss: 0.009\n",
      "Accuracy:  1500 1500 1.0\n",
      "[136,     6] loss: 0.009\n",
      "Accuracy:  1500 1500 1.0\n",
      "[137,     6] loss: 0.009\n",
      "Accuracy:  1500 1500 1.0\n",
      "[138,     6] loss: 0.009\n",
      "Accuracy:  1500 1500 1.0\n",
      "[139,     6] loss: 0.009\n",
      "Accuracy:  1500 1500 1.0\n",
      "[140,     6] loss: 0.009\n",
      "Accuracy:  1500 1500 1.0\n",
      "[141,     6] loss: 0.009\n",
      "Accuracy:  1500 1500 1.0\n",
      "[142,     6] loss: 0.009\n",
      "Accuracy:  1500 1500 1.0\n",
      "[143,     6] loss: 0.008\n",
      "Accuracy:  1500 1500 1.0\n",
      "[144,     6] loss: 0.008\n",
      "Accuracy:  1500 1500 1.0\n",
      "[145,     6] loss: 0.008\n",
      "Accuracy:  1500 1500 1.0\n",
      "[146,     6] loss: 0.008\n",
      "Accuracy:  1500 1500 1.0\n",
      "[147,     6] loss: 0.008\n",
      "Accuracy:  1500 1500 1.0\n",
      "[148,     6] loss: 0.008\n",
      "Accuracy:  1500 1500 1.0\n",
      "[149,     6] loss: 0.008\n",
      "Accuracy:  1500 1500 1.0\n",
      "[150,     6] loss: 0.008\n",
      "Accuracy:  1500 1500 1.0\n",
      "[151,     6] loss: 0.008\n",
      "Accuracy:  1500 1500 1.0\n",
      "[152,     6] loss: 0.008\n",
      "Accuracy:  1500 1500 1.0\n",
      "[153,     6] loss: 0.008\n",
      "Accuracy:  1500 1500 1.0\n",
      "[154,     6] loss: 0.007\n",
      "Accuracy:  1500 1500 1.0\n",
      "[155,     6] loss: 0.007\n",
      "Accuracy:  1500 1500 1.0\n",
      "[156,     6] loss: 0.007\n",
      "Accuracy:  1500 1500 1.0\n",
      "[157,     6] loss: 0.007\n",
      "Accuracy:  1500 1500 1.0\n",
      "[158,     6] loss: 0.007\n",
      "Accuracy:  1500 1500 1.0\n",
      "[159,     6] loss: 0.007\n",
      "Accuracy:  1500 1500 1.0\n",
      "[160,     6] loss: 0.007\n",
      "Accuracy:  1500 1500 1.0\n",
      "[161,     6] loss: 0.007\n",
      "Accuracy:  1500 1500 1.0\n",
      "[162,     6] loss: 0.007\n",
      "Accuracy:  1500 1500 1.0\n",
      "[163,     6] loss: 0.007\n",
      "Accuracy:  1500 1500 1.0\n",
      "[164,     6] loss: 0.007\n",
      "Accuracy:  1500 1500 1.0\n",
      "[165,     6] loss: 0.007\n",
      "Accuracy:  1500 1500 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[166,     6] loss: 0.007\n",
      "Accuracy:  1500 1500 1.0\n",
      "[167,     6] loss: 0.007\n",
      "Accuracy:  1500 1500 1.0\n",
      "[168,     6] loss: 0.006\n",
      "Accuracy:  1500 1500 1.0\n",
      "[169,     6] loss: 0.006\n",
      "Accuracy:  1500 1500 1.0\n",
      "[170,     6] loss: 0.006\n",
      "Accuracy:  1500 1500 1.0\n",
      "[171,     6] loss: 0.006\n",
      "Accuracy:  1500 1500 1.0\n",
      "[172,     6] loss: 0.006\n",
      "Accuracy:  1500 1500 1.0\n",
      "[173,     6] loss: 0.006\n",
      "Accuracy:  1500 1500 1.0\n",
      "[174,     6] loss: 0.006\n",
      "Accuracy:  1500 1500 1.0\n",
      "[175,     6] loss: 0.006\n",
      "Accuracy:  1500 1500 1.0\n",
      "[176,     6] loss: 0.006\n",
      "Accuracy:  1500 1500 1.0\n",
      "[177,     6] loss: 0.006\n",
      "Accuracy:  1500 1500 1.0\n",
      "[178,     6] loss: 0.006\n",
      "Accuracy:  1500 1500 1.0\n",
      "[179,     6] loss: 0.006\n",
      "Accuracy:  1500 1500 1.0\n",
      "[180,     6] loss: 0.006\n",
      "Accuracy:  1500 1500 1.0\n",
      "[181,     6] loss: 0.006\n",
      "Accuracy:  1500 1500 1.0\n",
      "[182,     6] loss: 0.006\n",
      "Accuracy:  1500 1500 1.0\n",
      "[183,     6] loss: 0.006\n",
      "Accuracy:  1500 1500 1.0\n",
      "[184,     6] loss: 0.006\n",
      "Accuracy:  1500 1500 1.0\n",
      "[185,     6] loss: 0.005\n",
      "Accuracy:  1500 1500 1.0\n",
      "[186,     6] loss: 0.005\n",
      "Accuracy:  1500 1500 1.0\n",
      "[187,     6] loss: 0.005\n",
      "Accuracy:  1500 1500 1.0\n",
      "[188,     6] loss: 0.005\n",
      "Accuracy:  1500 1500 1.0\n",
      "[189,     6] loss: 0.005\n",
      "Accuracy:  1500 1500 1.0\n",
      "[190,     6] loss: 0.005\n",
      "Accuracy:  1500 1500 1.0\n",
      "[191,     6] loss: 0.005\n",
      "Accuracy:  1500 1500 1.0\n",
      "[192,     6] loss: 0.005\n",
      "Accuracy:  1500 1500 1.0\n",
      "[193,     6] loss: 0.005\n",
      "Accuracy:  1500 1500 1.0\n",
      "[194,     6] loss: 0.005\n",
      "Accuracy:  1500 1500 1.0\n",
      "[195,     6] loss: 0.005\n",
      "Accuracy:  1500 1500 1.0\n",
      "[196,     6] loss: 0.005\n",
      "Accuracy:  1500 1500 1.0\n",
      "[197,     6] loss: 0.005\n",
      "Accuracy:  1500 1500 1.0\n",
      "[198,     6] loss: 0.005\n",
      "Accuracy:  1500 1500 1.0\n",
      "[199,     6] loss: 0.005\n",
      "Accuracy:  1500 1500 1.0\n",
      "[200,     6] loss: 0.005\n",
      "Accuracy:  1500 1500 1.0\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "nos_epochs = 200\n",
    "epoch_loss = []\n",
    "for epoch in range(nos_epochs):  # loop over the dataset multiple times\n",
    "    cnt = 0 \n",
    "    running_loss = 0.0\n",
    "\n",
    "    correct = 0\n",
    "    total = 0 \n",
    "    #training data set\n",
    "    for i, data in  enumerate(train_loader1):\n",
    "        inputs , labels = data\n",
    "        batch = inputs.size(0)\n",
    "        #inputs, labels = inputs.to(\"cuda\"), labels.to(\"cuda\")\n",
    "        inputs = inputs.double()\n",
    "        labels = labels.long()\n",
    "        # zero the parameter gradients\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        outputs = net2(inputs)\n",
    "        \n",
    "        \n",
    "        #prediction = outputs>0.5\n",
    "        _, prediction = torch.max(outputs.data, 1)\n",
    "        \n",
    "        \n",
    "        correct += torch.sum(prediction == labels)\n",
    "        total += len(labels)\n",
    "        \n",
    "        \n",
    "        \n",
    "        loss = criterion(outputs, labels) \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        mini = 6\n",
    "        if cnt % mini == mini-1:    # print every 40 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %(epoch + 1, cnt + 1, running_loss / mini))\n",
    "            epoch_loss.append(running_loss/mini)\n",
    "            running_loss = 0.0\n",
    "        cnt=cnt+1\n",
    "        \n",
    "    print(\"Accuracy: \",correct.item(),total,correct.item()/total)\n",
    "    if(np.mean(epoch_loss) <= 0.001):\n",
    "        break;\n",
    "    \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x16ba6a4c0>]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hard attention mosaic data training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "TQX649ZCIvee"
   },
   "outputs": [],
   "source": [
    "foreground_classes = {'class_0','class_1' }\n",
    "\n",
    "background_classes = {'bg_classes',}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OyyXD2N0Ix-k",
    "outputId": "296a6cc1-d651-46c5-90d1-3243541da1d4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 1100/1100 [00:02<00:00, 389.23it/s]\n"
     ]
    }
   ],
   "source": [
    "desired_num = 1100  # 100 + 1000\n",
    "\n",
    "\n",
    "m = 100\n",
    "\n",
    "\n",
    "\n",
    "mosaic_list_of_images =[]\n",
    "mosaic_label = []\n",
    "fore_idx=[]\n",
    "for j in tqdm(range(desired_num)):\n",
    "    np.random.seed(j)\n",
    "    fg_class  = np.random.randint(0,3)\n",
    "    fg_idx = np.random.randint(0,m)\n",
    "    a = []\n",
    "    for i in range(m):\n",
    "        if i == fg_idx:\n",
    "            b = np.random.choice(np.where(idx[fg_class]==True)[0],size=1)\n",
    "            a.append(x[b])\n",
    "#             print(\"foreground \"+str(fg_class)+\" present at \" + str(fg_idx))\n",
    "        else:\n",
    "            bg_class = np.random.randint(3,10)\n",
    "            b = np.random.choice(np.where(idx[bg_class]==True)[0],size=1)\n",
    "            a.append(x[b])\n",
    "#             print(\"background \"+str(bg_class)+\" present at \" + str(i))\n",
    "    a = np.concatenate(a,axis=0)\n",
    "    mosaic_list_of_images.append(np.reshape(a,(m,5)))\n",
    "    mosaic_label.append(fg_class)\n",
    "    fore_idx.append(fg_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qxEc1Rp6Iz2k",
    "outputId": "87d7662b-a38a-49e8-c66b-cd5aececa1a7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1100, (100, 5))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mosaic_list_of_images),mosaic_list_of_images[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_analysis(trainloader,testloader,focus,classification):\n",
    "    focus.eval()\n",
    "    classification.eval()\n",
    "    fg_indexes = []\n",
    "    alpha_argmax = []\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "    with torch.no_grad():\n",
    "        for data in trainloader:\n",
    "            inputs, labels,foregrnd_index = data\n",
    "            inputs = inputs.double()\n",
    "            inputs = inputs.to(device)\n",
    "            alphas,attnd_inputs= focus(inputs)\n",
    "            outputs = classification(attnd_inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            true_labels.append(labels)\n",
    "            predicted_labels.append(predicted.cpu().numpy())\n",
    "            index = torch.argmax(alphas,dim=1)\n",
    "            fg_indexes.append(foregrnd_index.numpy())\n",
    "            alpha_argmax.append(index.cpu().numpy())\n",
    "    fg_indexes = np.concatenate(fg_indexes,axis=0)\n",
    "    alpha_argmax = np.concatenate(alpha_argmax,axis=0)\n",
    "    true_labels = np.concatenate(true_labels,axis=0)\n",
    "    predicted_labels = np.concatenate(predicted_labels,axis=0)\n",
    "\n",
    "    \n",
    "    ftpt = np.sum(np.logical_and(alpha_argmax == fg_indexes, predicted_labels == true_labels))\n",
    "    ffpt = np.sum(np.logical_and(alpha_argmax != fg_indexes, predicted_labels == true_labels))\n",
    "    ftpf = np.sum(np.logical_and(alpha_argmax == fg_indexes, predicted_labels != true_labels))\n",
    "    ffpf = np.sum(np.logical_and(alpha_argmax != fg_indexes, predicted_labels != true_labels))\n",
    "    trn_analysis = [ftpt,ffpt,ftpf,ffpf]\n",
    "    \n",
    "    fg_indexes = []\n",
    "    alpha_argmax = []\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            inputs, labels,foregrnd_index = data\n",
    "            inputs = inputs.double()\n",
    "            inputs = inputs.to(device)\n",
    "            alphas,attnd_inputs= focus(inputs)\n",
    "            outputs = classification(attnd_inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            true_labels.append(labels)\n",
    "            predicted_labels.append(predicted.cpu().numpy())\n",
    "            index = torch.argmax(alphas,dim=1)\n",
    "            fg_indexes.append(foregrnd_index.numpy())\n",
    "            alpha_argmax.append(index.cpu().numpy())\n",
    "    fg_indexes = np.concatenate(fg_indexes,axis=0)\n",
    "    alpha_argmax = np.concatenate(alpha_argmax,axis=0)\n",
    "    true_labels = np.concatenate(true_labels,axis=0)\n",
    "    predicted_labels = np.concatenate(predicted_labels,axis=0)    \n",
    "    ftpt = np.sum(np.logical_and(alpha_argmax == fg_indexes, predicted_labels == true_labels))\n",
    "    ffpt = np.sum(np.logical_and(alpha_argmax != fg_indexes, predicted_labels == true_labels))\n",
    "    ftpf = np.sum(np.logical_and(alpha_argmax == fg_indexes, predicted_labels != true_labels))\n",
    "    ffpf = np.sum(np.logical_and(alpha_argmax != fg_indexes, predicted_labels != true_labels))\n",
    "    tst_analysis = [ftpt,ffpt,ftpf,ffpf]\n",
    "    \n",
    "    \n",
    "    return trn_analysis,tst_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "70EdKKtpI266"
   },
   "outputs": [],
   "source": [
    "class MosaicDataset(Dataset):\n",
    "  \"\"\"MosaicDataset dataset.\"\"\"\n",
    "\n",
    "  def __init__(self, mosaic_list_of_images, mosaic_label, fore_idx):\n",
    "    \"\"\"\n",
    "      Args:\n",
    "        csv_file (string): Path to the csv file with annotations.\n",
    "        root_dir (string): Directory with all the images.\n",
    "        transform (callable, optional): Optional transform to be applied\n",
    "            on a sample.\n",
    "    \"\"\"\n",
    "    self.mosaic = mosaic_list_of_images\n",
    "    self.label = mosaic_label\n",
    "    self.fore_idx = fore_idx\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.label)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return self.mosaic[idx] , self.label[idx], self.fore_idx[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "UWstKuYfI44x"
   },
   "outputs": [],
   "source": [
    "batch = 250\n",
    "msd1 = MosaicDataset(mosaic_list_of_images[0:100], mosaic_label[0:100] , fore_idx[0:100])\n",
    "train_loader = DataLoader( msd1 ,batch_size= batch ,shuffle=True) \n",
    "\n",
    "\n",
    "batch = 250\n",
    "msd2 = MosaicDataset(mosaic_list_of_images[100:], mosaic_label[100:] , fore_idx[100:])\n",
    "test_loader = DataLoader( msd2 ,batch_size= batch ,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "id": "trajegMCI6wh"
   },
   "outputs": [],
   "source": [
    "class Focus(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Focus, self).__init__()\n",
    "        self.fc1 = nn.Linear(5,1, bias=False)\n",
    "        # self.fc2 = nn.Linear(50,1,bias=False)\n",
    "        torch.nn.init.zeros_(self.fc1.weight)\n",
    "        #torch.nn.init.xavier_normal_(self.fc2.weight)\n",
    "        #self.fc2 = nn.Linear(64, 1, bias=False)\n",
    "        #torch.nn.init.xavier_normal_(self.fc2.weight)\n",
    "\n",
    "    def forward(self,z):\n",
    "        #print(\"data\",z)\n",
    "        batch = z.size(0)\n",
    "        patches = z.size(1)\n",
    "        z = z.view(batch,patches,5*1)\n",
    "        alp1,ft1 = self.helper(z)\n",
    "\n",
    "        alpha = F.softmax(alp1,dim=1)\n",
    "        #print(self.training)\n",
    "        \n",
    "        if self.training:\n",
    "            alpha =alpha[:,:,0]\n",
    "            y = ft1 \n",
    "            return alpha,y\n",
    "        else:\n",
    "            index = torch.argmax(alpha,dim=1)\n",
    "            hard_alpha = torch.nn.functional.one_hot(index[:,0], patches)\n",
    "            y = torch.sum(hard_alpha[:,:,None]*ft1,dim=1)\n",
    "            alpha = alpha[:,:,0]\n",
    "            return alpha,y\n",
    "    \n",
    "    def helper(self, x):\n",
    "        x1 = x\n",
    "        x = self.fc1(x)\n",
    "        return x,x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "id": "8yBS5QcPJOa8"
   },
   "outputs": [],
   "source": [
    "class Classification(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Classification, self).__init__()\n",
    "    self.fc1 = nn.Linear(5, 3)\n",
    "    #self.fc2 = nn.Linear(50,3)\n",
    "    torch.nn.init.xavier_normal_(self.fc1.weight)\n",
    "    torch.nn.init.zeros_(self.fc1.bias)\n",
    "    #torch.nn.init.xavier_normal_(self.fc2.weight)\n",
    "    #torch.nn.init.zeros_(self.fc2.bias)\n",
    "\n",
    "  def forward(self, x):\n",
    "    #print(x.shape)\n",
    "    #x = x.view(-1, 1)\n",
    "    #print(x.shape)\n",
    "    #x = F.relu(self.fc1(x))\n",
    "    x = self.fc1(x)\n",
    "    # print(x.shape)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "id": "b5XDsKNlJORU"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(13)\n",
    "focus_net = Focus().double()\n",
    "focus_net = focus_net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "id": "_V51sgRtJd9H"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(13)\n",
    "classify = Classification().double()\n",
    "classify = classify.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "id": "lk14ZiBcJhMT"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "optimizer_classify = optim.Adam(classify.parameters(), lr=0.01)#,weight_decay=1e-5 ) #, momentum=0.9)\n",
    "optimizer_focus = optim.Adam(focus_net.parameters(), lr=0.01)#,weight_decay=1e-5 ) #, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "id": "ij2NuflCJhJ5"
   },
   "outputs": [],
   "source": [
    "def my_cross_entropy(output,target,alpha):\n",
    "    criterion = nn.CrossEntropyLoss(reduce=False)\n",
    "    \n",
    "    batch = output.size(0)\n",
    "    #print(batch)\n",
    "    patches = output.size(1)\n",
    "    classes = output.size(2)\n",
    "    \n",
    "    \n",
    "    \n",
    "    output = torch.reshape(output,(batch*patches,classes))\n",
    "    \n",
    "    \n",
    "    target = target.repeat_interleave(patches)\n",
    "    \n",
    "    loss = criterion(output,target)\n",
    "    \n",
    "    #print(loss,loss.shape)\n",
    "    loss = torch.reshape(loss,(batch,patches))\n",
    "    #print(loss.size())\n",
    "    final_loss = torch.sum(torch.mul(loss,alpha),dim=1)\n",
    "    #print(final_loss.shape)\n",
    "    final_loss = torch.mean(final_loss,dim=0)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #print(final_loss)\n",
    "    return final_loss\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "id": "R4zasvFeJhEr"
   },
   "outputs": [],
   "source": [
    "col1=[]\n",
    "col2=[]\n",
    "col3=[]\n",
    "col4=[]\n",
    "col5=[]\n",
    "col6=[]\n",
    "col7=[]\n",
    "col8=[]\n",
    "col9=[]\n",
    "col10=[]\n",
    "col11=[]\n",
    "col12=[]\n",
    "col13=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QLqhknFsJhCY",
    "outputId": "71f9fdf5-da4f-4117-ea6e-e0a8f99315f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.2442, dtype=torch.float64)\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "count = 0\n",
    "flag = 1\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "  for data in train_loader:\n",
    "    inputs, labels , fore_idx = data\n",
    "    inputs = inputs.double()\n",
    "    inputs, labels , fore_idx = inputs.to(device),labels.to(device), fore_idx.to(device)\n",
    "    alphas, avg_images = focus_net(inputs)\n",
    "    outputs = classify(avg_images)\n",
    "    loss = my_cross_entropy(outputs,labels,alphas)\n",
    "    print(loss)\n",
    "    # print(outputs.shape)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "id": "PzDDBXvYJqup"
   },
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "count = 0\n",
    "flag = 1\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "  for data in test_loader:\n",
    "    inputs, labels , fore_idx = data\n",
    "    inputs = inputs.double()\n",
    "    inputs, labels , fore_idx = inputs.to(device),labels.to(device), fore_idx.to(device)\n",
    "    alphas, avg_images = focus_net(inputs)\n",
    "    outputs = classify(avg_images)\n",
    "\n",
    "    _, predicted = torch.max(outputs.data, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bDKOz58EJqsB",
    "outputId": "d8f2f75e-89a4-44cd-d58e-0a13dc1ebfaa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('fc1.weight', Parameter containing:\n",
      "tensor([[0., 0., 0., 0., 0.]], dtype=torch.float64, requires_grad=True))\n"
     ]
    }
   ],
   "source": [
    "for param in focus_net.named_parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.2922, -0.7858,  0.1510,  0.9348, -0.0463],\n",
      "        [ 0.6800, -0.4712,  0.8751,  0.2293, -0.3314],\n",
      "        [ 0.5566, -0.6110, -0.0110,  0.2452, -0.3715]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0.], dtype=torch.float64, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for param in classify.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 32, 0, 68], [4, 343, 9, 644])"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_analysis(train_loader,test_loader,focus_net,classify) # ftpt ffpt ftpf ffpf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M_Zrxjv6JqpL",
    "outputId": "11272261-de08-4b0d-dd9d-3efed0e36f9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     1] loss: 1.244\n",
      "[2,     1] loss: 1.231\n",
      "[3,     1] loss: 1.218\n",
      "[4,     1] loss: 1.207\n",
      "[5,     1] loss: 1.196\n",
      "[6,     1] loss: 1.186\n",
      "[7,     1] loss: 1.176\n",
      "[8,     1] loss: 1.168\n",
      "[9,     1] loss: 1.160\n",
      "[10,     1] loss: 1.153\n",
      "[11,     1] loss: 1.146\n",
      "[12,     1] loss: 1.140\n",
      "[13,     1] loss: 1.134\n",
      "[14,     1] loss: 1.129\n",
      "[15,     1] loss: 1.125\n",
      "[16,     1] loss: 1.120\n",
      "[17,     1] loss: 1.117\n",
      "[18,     1] loss: 1.113\n",
      "[19,     1] loss: 1.110\n",
      "[20,     1] loss: 1.107\n",
      "[21,     1] loss: 1.105\n",
      "[22,     1] loss: 1.103\n",
      "[23,     1] loss: 1.101\n",
      "[24,     1] loss: 1.099\n",
      "[25,     1] loss: 1.098\n",
      "[26,     1] loss: 1.096\n",
      "[27,     1] loss: 1.095\n",
      "[28,     1] loss: 1.094\n",
      "[29,     1] loss: 1.093\n",
      "[30,     1] loss: 1.092\n",
      "[31,     1] loss: 1.091\n",
      "[32,     1] loss: 1.091\n",
      "[33,     1] loss: 1.090\n",
      "[34,     1] loss: 1.089\n",
      "[35,     1] loss: 1.089\n",
      "[36,     1] loss: 1.088\n",
      "[37,     1] loss: 1.088\n",
      "[38,     1] loss: 1.087\n",
      "[39,     1] loss: 1.087\n",
      "[40,     1] loss: 1.086\n",
      "[41,     1] loss: 1.086\n",
      "[42,     1] loss: 1.086\n",
      "[43,     1] loss: 1.085\n",
      "[44,     1] loss: 1.085\n",
      "[45,     1] loss: 1.084\n",
      "[46,     1] loss: 1.084\n",
      "[47,     1] loss: 1.083\n",
      "[48,     1] loss: 1.083\n",
      "[49,     1] loss: 1.083\n",
      "[50,     1] loss: 1.082\n",
      "[51,     1] loss: 1.081\n",
      "[52,     1] loss: 1.081\n",
      "[53,     1] loss: 1.080\n",
      "[54,     1] loss: 1.080\n",
      "[55,     1] loss: 1.079\n",
      "[56,     1] loss: 1.078\n",
      "[57,     1] loss: 1.078\n",
      "[58,     1] loss: 1.077\n",
      "[59,     1] loss: 1.076\n",
      "[60,     1] loss: 1.075\n",
      "[61,     1] loss: 1.074\n",
      "[62,     1] loss: 1.073\n",
      "[63,     1] loss: 1.072\n",
      "[64,     1] loss: 1.070\n",
      "[65,     1] loss: 1.069\n",
      "[66,     1] loss: 1.067\n",
      "[67,     1] loss: 1.066\n",
      "[68,     1] loss: 1.064\n",
      "[69,     1] loss: 1.062\n",
      "[70,     1] loss: 1.060\n",
      "[71,     1] loss: 1.058\n",
      "[72,     1] loss: 1.056\n",
      "[73,     1] loss: 1.054\n",
      "[74,     1] loss: 1.051\n",
      "[75,     1] loss: 1.049\n",
      "[76,     1] loss: 1.046\n",
      "[77,     1] loss: 1.043\n",
      "[78,     1] loss: 1.039\n",
      "[79,     1] loss: 1.036\n",
      "[80,     1] loss: 1.032\n",
      "[81,     1] loss: 1.029\n",
      "[82,     1] loss: 1.025\n",
      "[83,     1] loss: 1.020\n",
      "[84,     1] loss: 1.016\n",
      "[85,     1] loss: 1.011\n",
      "[86,     1] loss: 1.006\n",
      "[87,     1] loss: 1.001\n",
      "[88,     1] loss: 0.995\n",
      "[89,     1] loss: 0.990\n",
      "[90,     1] loss: 0.984\n",
      "[91,     1] loss: 0.978\n",
      "[92,     1] loss: 0.971\n",
      "[93,     1] loss: 0.964\n",
      "[94,     1] loss: 0.957\n",
      "[95,     1] loss: 0.950\n",
      "[96,     1] loss: 0.942\n",
      "[97,     1] loss: 0.935\n",
      "[98,     1] loss: 0.927\n",
      "[99,     1] loss: 0.919\n",
      "[100,     1] loss: 0.910\n",
      "[101,     1] loss: 0.902\n",
      "[102,     1] loss: 0.893\n",
      "[103,     1] loss: 0.884\n",
      "[104,     1] loss: 0.875\n",
      "[105,     1] loss: 0.866\n",
      "[106,     1] loss: 0.856\n",
      "[107,     1] loss: 0.847\n",
      "[108,     1] loss: 0.838\n",
      "[109,     1] loss: 0.828\n",
      "[110,     1] loss: 0.819\n",
      "[111,     1] loss: 0.809\n",
      "[112,     1] loss: 0.800\n",
      "[113,     1] loss: 0.791\n",
      "[114,     1] loss: 0.782\n",
      "[115,     1] loss: 0.772\n",
      "[116,     1] loss: 0.763\n",
      "[117,     1] loss: 0.755\n",
      "[118,     1] loss: 0.746\n",
      "[119,     1] loss: 0.737\n",
      "[120,     1] loss: 0.729\n",
      "[121,     1] loss: 0.721\n",
      "[122,     1] loss: 0.713\n",
      "[123,     1] loss: 0.705\n",
      "[124,     1] loss: 0.697\n",
      "[125,     1] loss: 0.690\n",
      "[126,     1] loss: 0.682\n",
      "[127,     1] loss: 0.675\n",
      "[128,     1] loss: 0.669\n",
      "[129,     1] loss: 0.662\n",
      "[130,     1] loss: 0.655\n",
      "[131,     1] loss: 0.649\n",
      "[132,     1] loss: 0.643\n",
      "[133,     1] loss: 0.638\n",
      "[134,     1] loss: 0.632\n",
      "[135,     1] loss: 0.627\n",
      "[136,     1] loss: 0.621\n",
      "[137,     1] loss: 0.616\n",
      "[138,     1] loss: 0.611\n",
      "[139,     1] loss: 0.607\n",
      "[140,     1] loss: 0.602\n",
      "[141,     1] loss: 0.598\n",
      "[142,     1] loss: 0.594\n",
      "[143,     1] loss: 0.590\n",
      "[144,     1] loss: 0.586\n",
      "[145,     1] loss: 0.582\n",
      "[146,     1] loss: 0.579\n",
      "[147,     1] loss: 0.575\n",
      "[148,     1] loss: 0.572\n",
      "[149,     1] loss: 0.569\n",
      "[150,     1] loss: 0.566\n",
      "[151,     1] loss: 0.563\n",
      "[152,     1] loss: 0.560\n",
      "[153,     1] loss: 0.557\n",
      "[154,     1] loss: 0.554\n",
      "[155,     1] loss: 0.552\n",
      "[156,     1] loss: 0.549\n",
      "[157,     1] loss: 0.547\n",
      "[158,     1] loss: 0.545\n",
      "[159,     1] loss: 0.543\n",
      "[160,     1] loss: 0.540\n",
      "[161,     1] loss: 0.538\n",
      "[162,     1] loss: 0.536\n",
      "[163,     1] loss: 0.534\n",
      "[164,     1] loss: 0.533\n",
      "[165,     1] loss: 0.531\n",
      "[166,     1] loss: 0.529\n",
      "[167,     1] loss: 0.527\n",
      "[168,     1] loss: 0.526\n",
      "[169,     1] loss: 0.524\n",
      "[170,     1] loss: 0.523\n",
      "[171,     1] loss: 0.521\n",
      "[172,     1] loss: 0.520\n",
      "[173,     1] loss: 0.518\n",
      "[174,     1] loss: 0.517\n",
      "[175,     1] loss: 0.516\n",
      "[176,     1] loss: 0.514\n",
      "[177,     1] loss: 0.513\n",
      "[178,     1] loss: 0.512\n",
      "[179,     1] loss: 0.511\n",
      "[180,     1] loss: 0.510\n",
      "[181,     1] loss: 0.509\n",
      "[182,     1] loss: 0.507\n",
      "[183,     1] loss: 0.506\n",
      "[184,     1] loss: 0.505\n",
      "[185,     1] loss: 0.504\n",
      "[186,     1] loss: 0.503\n",
      "[187,     1] loss: 0.503\n",
      "[188,     1] loss: 0.502\n",
      "[189,     1] loss: 0.501\n",
      "[190,     1] loss: 0.500\n",
      "[191,     1] loss: 0.499\n",
      "[192,     1] loss: 0.498\n",
      "[193,     1] loss: 0.497\n",
      "[194,     1] loss: 0.497\n",
      "[195,     1] loss: 0.496\n",
      "[196,     1] loss: 0.495\n",
      "[197,     1] loss: 0.494\n",
      "[198,     1] loss: 0.494\n",
      "[199,     1] loss: 0.493\n",
      "[200,     1] loss: 0.492\n",
      "[201,     1] loss: 0.492\n",
      "[202,     1] loss: 0.491\n",
      "[203,     1] loss: 0.490\n",
      "[204,     1] loss: 0.490\n",
      "[205,     1] loss: 0.489\n",
      "[206,     1] loss: 0.488\n",
      "[207,     1] loss: 0.488\n",
      "[208,     1] loss: 0.487\n",
      "[209,     1] loss: 0.487\n",
      "[210,     1] loss: 0.486\n",
      "[211,     1] loss: 0.486\n",
      "[212,     1] loss: 0.485\n",
      "[213,     1] loss: 0.485\n",
      "[214,     1] loss: 0.484\n",
      "[215,     1] loss: 0.484\n",
      "[216,     1] loss: 0.483\n",
      "[217,     1] loss: 0.483\n",
      "[218,     1] loss: 0.482\n",
      "[219,     1] loss: 0.482\n",
      "[220,     1] loss: 0.481\n",
      "[221,     1] loss: 0.481\n",
      "[222,     1] loss: 0.480\n",
      "[223,     1] loss: 0.480\n",
      "[224,     1] loss: 0.479\n",
      "[225,     1] loss: 0.479\n",
      "[226,     1] loss: 0.479\n",
      "[227,     1] loss: 0.478\n",
      "[228,     1] loss: 0.478\n",
      "[229,     1] loss: 0.477\n",
      "[230,     1] loss: 0.477\n",
      "[231,     1] loss: 0.477\n",
      "[232,     1] loss: 0.476\n",
      "[233,     1] loss: 0.476\n",
      "[234,     1] loss: 0.476\n",
      "[235,     1] loss: 0.475\n",
      "[236,     1] loss: 0.475\n",
      "[237,     1] loss: 0.474\n",
      "[238,     1] loss: 0.474\n",
      "[239,     1] loss: 0.474\n",
      "[240,     1] loss: 0.473\n",
      "[241,     1] loss: 0.473\n",
      "[242,     1] loss: 0.473\n",
      "[243,     1] loss: 0.473\n",
      "[244,     1] loss: 0.472\n",
      "[245,     1] loss: 0.472\n",
      "[246,     1] loss: 0.472\n",
      "[247,     1] loss: 0.471\n",
      "[248,     1] loss: 0.471\n",
      "[249,     1] loss: 0.471\n",
      "[250,     1] loss: 0.470\n",
      "[251,     1] loss: 0.470\n",
      "[252,     1] loss: 0.470\n",
      "[253,     1] loss: 0.470\n",
      "[254,     1] loss: 0.469\n",
      "[255,     1] loss: 0.469\n",
      "[256,     1] loss: 0.469\n",
      "[257,     1] loss: 0.469\n",
      "[258,     1] loss: 0.468\n",
      "[259,     1] loss: 0.468\n",
      "[260,     1] loss: 0.468\n",
      "[261,     1] loss: 0.468\n",
      "[262,     1] loss: 0.467\n",
      "[263,     1] loss: 0.467\n",
      "[264,     1] loss: 0.467\n",
      "[265,     1] loss: 0.467\n",
      "[266,     1] loss: 0.466\n",
      "[267,     1] loss: 0.466\n",
      "[268,     1] loss: 0.466\n",
      "[269,     1] loss: 0.466\n",
      "[270,     1] loss: 0.466\n",
      "[271,     1] loss: 0.465\n",
      "[272,     1] loss: 0.465\n",
      "[273,     1] loss: 0.465\n",
      "[274,     1] loss: 0.465\n",
      "[275,     1] loss: 0.465\n",
      "[276,     1] loss: 0.464\n",
      "[277,     1] loss: 0.464\n",
      "[278,     1] loss: 0.464\n",
      "[279,     1] loss: 0.464\n",
      "[280,     1] loss: 0.464\n",
      "[281,     1] loss: 0.463\n",
      "[282,     1] loss: 0.463\n",
      "[283,     1] loss: 0.463\n",
      "[284,     1] loss: 0.463\n",
      "[285,     1] loss: 0.463\n",
      "[286,     1] loss: 0.462\n",
      "[287,     1] loss: 0.462\n",
      "[288,     1] loss: 0.462\n",
      "[289,     1] loss: 0.462\n",
      "[290,     1] loss: 0.462\n",
      "[291,     1] loss: 0.462\n",
      "[292,     1] loss: 0.461\n",
      "[293,     1] loss: 0.461\n",
      "[294,     1] loss: 0.461\n",
      "[295,     1] loss: 0.461\n",
      "[296,     1] loss: 0.461\n",
      "[297,     1] loss: 0.460\n",
      "[298,     1] loss: 0.460\n",
      "[299,     1] loss: 0.460\n",
      "[300,     1] loss: 0.460\n",
      "[301,     1] loss: 0.460\n",
      "[302,     1] loss: 0.460\n",
      "[303,     1] loss: 0.459\n",
      "[304,     1] loss: 0.459\n",
      "[305,     1] loss: 0.459\n",
      "[306,     1] loss: 0.459\n",
      "[307,     1] loss: 0.459\n",
      "[308,     1] loss: 0.459\n",
      "[309,     1] loss: 0.459\n",
      "[310,     1] loss: 0.458\n",
      "[311,     1] loss: 0.458\n",
      "[312,     1] loss: 0.458\n",
      "[313,     1] loss: 0.458\n",
      "[314,     1] loss: 0.458\n",
      "[315,     1] loss: 0.458\n",
      "[316,     1] loss: 0.457\n",
      "[317,     1] loss: 0.457\n",
      "[318,     1] loss: 0.457\n",
      "[319,     1] loss: 0.457\n",
      "[320,     1] loss: 0.457\n",
      "[321,     1] loss: 0.456\n",
      "[322,     1] loss: 0.456\n",
      "[323,     1] loss: 0.456\n",
      "[324,     1] loss: 0.456\n",
      "[325,     1] loss: 0.456\n",
      "[326,     1] loss: 0.456\n",
      "[327,     1] loss: 0.455\n",
      "[328,     1] loss: 0.455\n",
      "[329,     1] loss: 0.455\n",
      "[330,     1] loss: 0.455\n",
      "[331,     1] loss: 0.454\n",
      "[332,     1] loss: 0.454\n",
      "[333,     1] loss: 0.454\n",
      "[334,     1] loss: 0.454\n",
      "[335,     1] loss: 0.453\n",
      "[336,     1] loss: 0.453\n",
      "[337,     1] loss: 0.453\n",
      "[338,     1] loss: 0.452\n",
      "[339,     1] loss: 0.452\n",
      "[340,     1] loss: 0.451\n",
      "[341,     1] loss: 0.451\n",
      "[342,     1] loss: 0.450\n",
      "[343,     1] loss: 0.450\n",
      "[344,     1] loss: 0.449\n",
      "[345,     1] loss: 0.448\n",
      "[346,     1] loss: 0.447\n",
      "[347,     1] loss: 0.446\n",
      "[348,     1] loss: 0.445\n",
      "[349,     1] loss: 0.444\n",
      "[350,     1] loss: 0.442\n",
      "[351,     1] loss: 0.441\n",
      "[352,     1] loss: 0.439\n",
      "[353,     1] loss: 0.437\n",
      "[354,     1] loss: 0.434\n",
      "[355,     1] loss: 0.431\n",
      "[356,     1] loss: 0.428\n",
      "[357,     1] loss: 0.425\n",
      "[358,     1] loss: 0.421\n",
      "[359,     1] loss: 0.417\n",
      "[360,     1] loss: 0.412\n",
      "[361,     1] loss: 0.407\n",
      "[362,     1] loss: 0.401\n",
      "[363,     1] loss: 0.395\n",
      "[364,     1] loss: 0.388\n",
      "[365,     1] loss: 0.381\n",
      "[366,     1] loss: 0.373\n",
      "[367,     1] loss: 0.365\n",
      "[368,     1] loss: 0.356\n",
      "[369,     1] loss: 0.347\n",
      "[370,     1] loss: 0.337\n",
      "[371,     1] loss: 0.328\n",
      "[372,     1] loss: 0.317\n",
      "[373,     1] loss: 0.307\n",
      "[374,     1] loss: 0.296\n",
      "[375,     1] loss: 0.285\n",
      "[376,     1] loss: 0.274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[377,     1] loss: 0.263\n",
      "[378,     1] loss: 0.252\n",
      "[379,     1] loss: 0.241\n",
      "[380,     1] loss: 0.230\n",
      "[381,     1] loss: 0.219\n",
      "[382,     1] loss: 0.209\n",
      "[383,     1] loss: 0.199\n",
      "[384,     1] loss: 0.190\n",
      "[385,     1] loss: 0.180\n",
      "[386,     1] loss: 0.171\n",
      "[387,     1] loss: 0.163\n",
      "[388,     1] loss: 0.155\n",
      "[389,     1] loss: 0.147\n",
      "[390,     1] loss: 0.140\n",
      "[391,     1] loss: 0.133\n",
      "[392,     1] loss: 0.127\n",
      "[393,     1] loss: 0.121\n",
      "[394,     1] loss: 0.115\n",
      "[395,     1] loss: 0.110\n",
      "[396,     1] loss: 0.105\n",
      "[397,     1] loss: 0.100\n",
      "[398,     1] loss: 0.096\n",
      "[399,     1] loss: 0.092\n",
      "[400,     1] loss: 0.088\n",
      "[401,     1] loss: 0.085\n",
      "[402,     1] loss: 0.082\n",
      "[403,     1] loss: 0.078\n",
      "[404,     1] loss: 0.076\n",
      "[405,     1] loss: 0.073\n",
      "[406,     1] loss: 0.070\n",
      "[407,     1] loss: 0.068\n",
      "[408,     1] loss: 0.066\n",
      "[409,     1] loss: 0.064\n",
      "[410,     1] loss: 0.062\n",
      "[411,     1] loss: 0.060\n",
      "[412,     1] loss: 0.059\n",
      "[413,     1] loss: 0.057\n",
      "[414,     1] loss: 0.055\n",
      "[415,     1] loss: 0.054\n",
      "[416,     1] loss: 0.053\n",
      "[417,     1] loss: 0.051\n",
      "[418,     1] loss: 0.050\n",
      "[419,     1] loss: 0.049\n",
      "[420,     1] loss: 0.048\n",
      "[421,     1] loss: 0.047\n",
      "[422,     1] loss: 0.046\n",
      "[423,     1] loss: 0.045\n",
      "[424,     1] loss: 0.044\n",
      "[425,     1] loss: 0.044\n",
      "[426,     1] loss: 0.043\n",
      "[427,     1] loss: 0.042\n",
      "[428,     1] loss: 0.041\n",
      "[429,     1] loss: 0.041\n",
      "[430,     1] loss: 0.040\n",
      "[431,     1] loss: 0.039\n",
      "[432,     1] loss: 0.039\n",
      "[433,     1] loss: 0.038\n",
      "[434,     1] loss: 0.037\n",
      "[435,     1] loss: 0.037\n",
      "[436,     1] loss: 0.036\n",
      "[437,     1] loss: 0.036\n",
      "[438,     1] loss: 0.035\n",
      "[439,     1] loss: 0.035\n",
      "[440,     1] loss: 0.035\n",
      "[441,     1] loss: 0.034\n",
      "[442,     1] loss: 0.034\n",
      "[443,     1] loss: 0.033\n",
      "[444,     1] loss: 0.033\n",
      "[445,     1] loss: 0.032\n",
      "[446,     1] loss: 0.032\n",
      "[447,     1] loss: 0.032\n",
      "[448,     1] loss: 0.031\n",
      "[449,     1] loss: 0.031\n",
      "[450,     1] loss: 0.031\n",
      "[451,     1] loss: 0.030\n",
      "[452,     1] loss: 0.030\n",
      "[453,     1] loss: 0.030\n",
      "[454,     1] loss: 0.029\n",
      "[455,     1] loss: 0.029\n",
      "[456,     1] loss: 0.029\n",
      "[457,     1] loss: 0.029\n",
      "[458,     1] loss: 0.028\n",
      "[459,     1] loss: 0.028\n",
      "[460,     1] loss: 0.028\n",
      "[461,     1] loss: 0.027\n",
      "[462,     1] loss: 0.027\n",
      "[463,     1] loss: 0.027\n",
      "[464,     1] loss: 0.027\n",
      "[465,     1] loss: 0.027\n",
      "[466,     1] loss: 0.026\n",
      "[467,     1] loss: 0.026\n",
      "[468,     1] loss: 0.026\n",
      "[469,     1] loss: 0.026\n",
      "[470,     1] loss: 0.025\n",
      "[471,     1] loss: 0.025\n",
      "[472,     1] loss: 0.025\n",
      "[473,     1] loss: 0.025\n",
      "[474,     1] loss: 0.025\n",
      "[475,     1] loss: 0.024\n",
      "[476,     1] loss: 0.024\n",
      "[477,     1] loss: 0.024\n",
      "[478,     1] loss: 0.024\n",
      "[479,     1] loss: 0.024\n",
      "[480,     1] loss: 0.023\n",
      "[481,     1] loss: 0.023\n",
      "[482,     1] loss: 0.023\n",
      "[483,     1] loss: 0.023\n",
      "[484,     1] loss: 0.023\n",
      "[485,     1] loss: 0.023\n",
      "[486,     1] loss: 0.022\n",
      "[487,     1] loss: 0.022\n",
      "[488,     1] loss: 0.022\n",
      "[489,     1] loss: 0.022\n",
      "[490,     1] loss: 0.022\n",
      "[491,     1] loss: 0.022\n",
      "[492,     1] loss: 0.022\n",
      "[493,     1] loss: 0.021\n",
      "[494,     1] loss: 0.021\n",
      "[495,     1] loss: 0.021\n",
      "[496,     1] loss: 0.021\n",
      "[497,     1] loss: 0.021\n",
      "[498,     1] loss: 0.021\n",
      "[499,     1] loss: 0.021\n",
      "[500,     1] loss: 0.020\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "nos_epochs = 500\n",
    "focus_true_pred_true =0\n",
    "focus_false_pred_true =0\n",
    "focus_true_pred_false =0\n",
    "focus_false_pred_false =0\n",
    "\n",
    "argmax_more_than_half = 0\n",
    "argmax_less_than_half =0\n",
    "\n",
    "\n",
    "for epoch in range(nos_epochs):  # loop over the dataset multiple times\n",
    "\n",
    "  focus_true_pred_true =0\n",
    "  focus_false_pred_true =0\n",
    "  focus_true_pred_false =0\n",
    "  focus_false_pred_false =0\n",
    "  \n",
    "  argmax_more_than_half = 0\n",
    "  argmax_less_than_half =0\n",
    "  \n",
    "  running_loss = 0.0\n",
    "  epoch_loss = []\n",
    "  cnt=0\n",
    "\n",
    "  iteration = desired_num // batch\n",
    "  \n",
    "  #training data set\n",
    "  focus_net.train()\n",
    "  classify.train()\n",
    "  for i, data in  enumerate(train_loader):\n",
    "    inputs , labels , fore_idx = data\n",
    "    batch = inputs.size(0)\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    inputs = inputs.double()\n",
    "    # zero the parameter gradients\n",
    "    \n",
    "    optimizer_focus.zero_grad()\n",
    "    optimizer_classify.zero_grad()\n",
    "    \n",
    "    alphas, avg_images = focus_net(inputs)\n",
    "    outputs = classify(avg_images)\n",
    "\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "#     print(outputs)\n",
    "#     print(outputs.shape,labels.shape , torch.argmax(outputs, dim=1))\n",
    "\n",
    "    loss = my_cross_entropy(outputs, labels,alphas) \n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    \n",
    "    optimizer_focus.step()\n",
    "    optimizer_classify.step()\n",
    "\n",
    "    running_loss += loss.item()\n",
    "    mini = 1\n",
    "    if cnt % mini == mini-1:    # print every 40 mini-batches\n",
    "      print('[%d, %5d] loss: %.3f' %(epoch + 1, cnt + 1, running_loss / mini))\n",
    "      epoch_loss.append(running_loss/mini)\n",
    "      running_loss = 0.0\n",
    "    cnt=cnt+1\n",
    "\n",
    "  if(np.mean(epoch_loss) <= 0.01):\n",
    "      break;\n",
    "  #plot_attended_data(train_loader,focus_net,epoch)\n",
    "\n",
    "    \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CmLUPgR6Jqmc",
    "outputId": "8f9683c8-27c9-46c3-848d-7266bbbb607b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the train images: 100.000000 %\n",
      "total correct 100\n",
      "total train set images 100\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "focus_net.eval()\n",
    "classify.eval()\n",
    "with torch.no_grad():\n",
    "  for data in train_loader:\n",
    "    inputs, labels , fore_idx = data\n",
    "    inputs = inputs.double()\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    alphas, avg_images = focus_net(inputs)\n",
    "    outputs = classify(avg_images)\n",
    "    #print(outputs.shape)\n",
    "\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the train images: %f %%' % ( 100 * correct / total))\n",
    "print(\"total correct\", correct)\n",
    "print(\"total train set images\", total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mnoPNVf6Jqju",
    "outputId": "b9728bf5-8acf-4ad1-ea5b-4c8fcfef480a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 100.000000 %\n",
      "total correct 1000\n",
      "total train set images 1000\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "  for data in test_loader:\n",
    "    inputs, labels , fore_idx = data\n",
    "    inputs = inputs.double()\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    alphas, avg_images = focus_net(inputs)\n",
    "    outputs = classify(avg_images)\n",
    "\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the test images: %f %%' % ( 100 * correct / total))\n",
    "print(\"total correct\", correct)\n",
    "print(\"total train set images\", total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "id": "I2Do2fYhJqhG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-1.0778,  3.2494,  3.2327,  0.1357, -0.0293]], dtype=torch.float64,\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for p in focus_net.parameters():\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "id": "mROMjICKJg_d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-1.1454, -2.7303, -1.1417,  0.6162, -0.2062],\n",
      "        [ 1.4107,  1.4932,  0.7492,  0.5338, -0.2176],\n",
      "        [ 0.8754, -1.2693,  2.6831,  0.5725, -0.2319]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 1.5060, -1.2154, -0.3961], dtype=torch.float64, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for p in classify.parameters():\n",
    "    print(p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([74, 26, 0, 0], [677, 323, 0, 0])"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_analysis(train_loader,test_loader,focus_net,classify)  # ftpt ffpt ftpf ffpf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 500\n",
      "1 500\n",
      "2 500\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(2)\n",
    "\n",
    "y_test = np.concatenate((np.zeros(500),np.ones(500),np.ones(500)*2))\n",
    "#y = np.random.randint(0,3,6000)\n",
    "idx_t= []\n",
    "for i in range(3):\n",
    "    print(i,sum(y_test==i))\n",
    "    idx_t.append(y_test==i)\n",
    "x_test = np.zeros((1500,5))\n",
    "\n",
    "\n",
    "np.random.seed(12)\n",
    "x_test[idx_t[0],:] = np.random.multivariate_normal(mean = [1,0,0,0,0],\n",
    "                                            cov=cov_mat,size=sum(idx_t[0]))\n",
    "x_test[idx_t[1],:] =  np.random.multivariate_normal(mean = [0,1,0,0,0],\n",
    "                                            cov=cov_mat,size=sum(idx_t[1]))\n",
    "x_test[idx_t[2],:] = np.random.multivariate_normal(mean = [0,0,1,0,0],\n",
    "                                            cov=cov_mat,size=sum(idx_t[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test= (x_test - mean_x)/std_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on True Data  66.66666666666667 %\n"
     ]
    }
   ],
   "source": [
    "x_test = torch.tensor(x_test).to(device)\n",
    "outputs = classify(x_test)\n",
    "_, predicted = torch.max(outputs.data, 1)\n",
    "print(\"Accuracy on True Data \", np.sum(predicted.cpu().numpy()  == y_test) /15,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "SDC_hard_attention_dataset5.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

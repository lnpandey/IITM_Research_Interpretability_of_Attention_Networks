{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "type_0_demo_2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2_J4Rw2r0SQ",
        "outputId": "8516e9d2-b20a-47cd-9633-f0eae1f15c62"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "from tqdm import tqdm\n",
        "%matplotlib inline\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn import functional as F\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6fjud_Fr0Sa"
      },
      "source": [
        "# Generate dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqdXHO0Cr0Sd",
        "outputId": "fde2e0eb-4914-471f-859f-f63c015c146c"
      },
      "source": [
        "np.random.seed(12)\n",
        "y = np.random.randint(0,3,5000)\n",
        "idx= []\n",
        "for i in range(3):\n",
        "    print(i,sum(y==i))\n",
        "    idx.append(y==i)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 1703\n",
            "1 1677\n",
            "2 1620\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddhXyODwr0Sk"
      },
      "source": [
        "x = np.zeros((5000,))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyV3N2DIr0Sp"
      },
      "source": [
        "np.random.seed(12)\n",
        "x[idx[0]] = np.random.uniform(low =-1,high =0,size= sum(idx[0]))\n",
        "x[idx[1]] = np.random.uniform(low =0,high =1,size= sum(idx[1]))\n",
        "x[idx[2]] = np.random.uniform(low =-3,high =-2,size= sum(idx[2]))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qh1mDScsU07I",
        "outputId": "d0421bb6-7382-48d7-bacb-3584415d013e"
      },
      "source": [
        "x[idx[0]][0], x[idx[2]][5] "
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-0.8458371576203276, -2.463258811404848)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Vr5ErQ_wSrV",
        "outputId": "19ed9fbc-c0a4-46a1-fafc-5cfec55f1195"
      },
      "source": [
        "print(x.shape,y.shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5000,) (5000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NG-3RpffwU_i"
      },
      "source": [
        "idx= []\n",
        "for i in range(3):\n",
        "  idx.append(y==i)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "hJ8Jm7YUr0St",
        "outputId": "0c3a405d-f354-4f35-8f8b-00ee27275a4d"
      },
      "source": [
        "for i in range(3):\n",
        "    y= np.zeros(x[idx[i]].shape[0])\n",
        "    plt.scatter(x[idx[i]],y,label=\"class_\"+str(i))\n",
        "plt.legend()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f6e4195fb50>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lMBZEHNBlF2",
        "outputId": "00329c44-a46c-4091-9216-6e928ea01559"
      },
      "source": [
        "bg_idx = [ np.where(idx[2] == True)[0]]\n",
        "\n",
        "bg_idx = np.concatenate(bg_idx, axis = 0)\n",
        "bg_idx.shape"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1620,)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "blRbGZHeCwXU",
        "outputId": "0006d3c2-5ca2-4fb1-e4ae-24bdb86dc782"
      },
      "source": [
        "np.unique(bg_idx).shape"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1620,)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y43sWeX7C15F"
      },
      "source": [
        "# x = x - np.mean(x[bg_idx], axis = 0, keepdims = True)\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ooII7N6UDWe0"
      },
      "source": [
        "# np.mean(x[bg_idx], axis = 0, keepdims = True), np.mean(x, axis = 0, keepdims = True)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g21bvPRYDL9k"
      },
      "source": [
        "# x = x/np.std(x[bg_idx], axis = 0, keepdims = True)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtFvIeHsDZJk"
      },
      "source": [
        "# np.std(x[bg_idx], axis = 0, keepdims = True), np.std(x, axis = 0, keepdims = True)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "8-VLhUfDDeHt",
        "outputId": "02b5fd63-0339-402a-9a00-196298fe2d32"
      },
      "source": [
        "for i in range(3):\n",
        "    y= np.zeros(x[idx[i]].shape[0])\n",
        "    plt.scatter(x[idx[i]],y,label=\"class_\"+str(i))\n",
        "plt.legend()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f6e418f7e90>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfFHcZJOr0Sz"
      },
      "source": [
        "foreground_classes = {'class_0','class_1' }\n",
        "\n",
        "background_classes = {'class_2'}"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OplNpNQVr0S2"
      },
      "source": [
        "# fg_class  = np.random.randint(0,2)\n",
        "# fg_idx = np.random.randint(0,9)\n",
        "\n",
        "# a = []\n",
        "# for i in range(9):\n",
        "#     if i == fg_idx:\n",
        "#         b = np.random.choice(np.where(idx[fg_class]==True)[0],size=1)\n",
        "#         a.append(x[b])\n",
        "#         print(\"foreground \"+str(fg_class)+\" present at \" + str(fg_idx))\n",
        "#     else:\n",
        "#         bg_class = np.random.randint(2,3)\n",
        "#         b = np.random.choice(np.where(idx[bg_class]==True)[0],size=1)\n",
        "#         a.append(x[b])\n",
        "#         print(\"background \"+str(bg_class)+\" present at \" + str(i))\n",
        "# a = np.concatenate(a,axis=0)\n",
        "# print(a.shape)\n",
        "\n",
        "# print(fg_class , fg_idx)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwZVmmRBr0S8"
      },
      "source": [
        "# a.shape"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OoxzYI-ur0S_"
      },
      "source": [
        "# np.reshape(a,(9,1))"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4ruI0cxr0TE"
      },
      "source": [
        "# a=np.reshape(a,(3,3))"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTUTFhJIr0TI"
      },
      "source": [
        "# plt.imshow(a)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqbvfbwVr0TN"
      },
      "source": [
        "desired_num = 20000\n",
        "mosaic_list_of_images =[]\n",
        "mosaic_label = []\n",
        "fore_idx=[]\n",
        "for j in range(desired_num):\n",
        "    np.random.seed(j)\n",
        "    fg_class  = np.random.randint(0,2)\n",
        "    fg_idx = np.random.randint(0,9)\n",
        "    a = []\n",
        "    for i in range(9):\n",
        "        if i == fg_idx:\n",
        "            b = np.random.choice(np.where(idx[fg_class]==True)[0],size=1)\n",
        "            a.append(x[b])\n",
        "#             print(\"foreground \"+str(fg_class)+\" present at \" + str(fg_idx))\n",
        "        else:\n",
        "            bg_class = np.random.randint(2,3)\n",
        "            b = np.random.choice(np.where(idx[bg_class]==True)[0],size=1)\n",
        "            a.append(x[b])\n",
        "#             print(\"background \"+str(bg_class)+\" present at \" + str(i))\n",
        "    a = np.concatenate(a,axis=0)\n",
        "    mosaic_list_of_images.append(np.reshape(a,(9,1)))\n",
        "    mosaic_label.append(fg_class)\n",
        "    fore_idx.append(fg_idx)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOsFmWfMr0TR"
      },
      "source": [
        "mosaic_list_of_images = np.concatenate(mosaic_list_of_images,axis=1).T\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2aIPMgLXNiXW",
        "outputId": "98c294a5-879f-4b06-ea5f-2798309770e4"
      },
      "source": [
        "mosaic_list_of_images.shape, mosaic_list_of_images[0]"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((20000, 9),\n",
              " array([-2.42616393, -2.25207001, -2.35259335, -2.31899737, -2.74722109,\n",
              "        -0.39131949, -2.82564377, -2.59621961, -2.97531583]))"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A3qcsbbzPfRG",
        "outputId": "0f22b7fb-810c-4fa2-d976-9f12450d720a"
      },
      "source": [
        "for j in range(9):\n",
        "  print(mosaic_list_of_images[1][j])\n",
        "  "
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-2.611312119203442\n",
            "-2.6192831002261845\n",
            "-2.981902624056117\n",
            "-2.2960626832656224\n",
            "-2.802930235515106\n",
            "-2.924929142000389\n",
            "-2.8403515527156786\n",
            "-2.430214000361957\n",
            "0.6042359616682469\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOzkdRYvsQrY",
        "outputId": "5da259ed-18c1-435b-bda2-97f429bad44a"
      },
      "source": [
        "fore_idx[1],mosaic_label[1]"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPoIwbMHx44n"
      },
      "source": [
        "class MosaicDataset(Dataset):\n",
        "  \"\"\"MosaicDataset dataset.\"\"\"\n",
        "\n",
        "  def __init__(self, mosaic_list_of_images, mosaic_label, fore_idx):\n",
        "    \"\"\"\n",
        "      Args:\n",
        "        csv_file (string): Path to the csv file with annotations.\n",
        "        root_dir (string): Directory with all the images.\n",
        "        transform (callable, optional): Optional transform to be applied\n",
        "            on a sample.\n",
        "    \"\"\"\n",
        "    self.mosaic = mosaic_list_of_images\n",
        "    self.label = mosaic_label\n",
        "    self.fore_idx = fore_idx\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.label)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.mosaic[idx] , self.label[idx], self.fore_idx[idx]\n",
        "\n"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOPAJQJeW8Ah"
      },
      "source": [
        "batch = 10000\n",
        "msd1 = MosaicDataset(mosaic_list_of_images[0:10000], mosaic_label[0:10000] , fore_idx[0:10000])\n",
        "train_loader = DataLoader( msd1 ,batch_size= batch ,shuffle=True)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjNiQgxZW8bA"
      },
      "source": [
        "batch = 10000\n",
        "msd2 = MosaicDataset(mosaic_list_of_images[10000:20000], mosaic_label[10000:20000] , fore_idx[10000:20000])\n",
        "test_loader = DataLoader( msd2 ,batch_size= batch ,shuffle=True)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30ZAjix3x8CM"
      },
      "source": [
        "class Focus(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Focus, self).__init__()\n",
        "\n",
        "    self.fc1 = nn.Linear(1, 1, bias= False)\n",
        "    # self.fc2 = nn.Linear(2, 1)\n",
        "\n",
        "  def forward(self,z):  #y is avg image #z batch of list of 9 images\n",
        "    y = torch.zeros([batch], dtype=torch.float64)\n",
        "    x = torch.zeros([batch,9],dtype=torch.float64)\n",
        "    y = y.to(\"cuda\")\n",
        "    x = x.to(\"cuda\")\n",
        "    for i in range(9):\n",
        "      x[:,i] = self.helper(z[:,i])[:,0]\n",
        "\n",
        "    x = F.softmax(x,dim=1)\n",
        "\n",
        "    for i in range(9):            \n",
        "      # x1 = x[:,i]          \n",
        "      y = y + torch.mul(x[:,i],z[:,i])\n",
        "\n",
        "    # print(x.shape, y.shape)\n",
        "    return x, y\n",
        "    \n",
        "  def helper(self, x):\n",
        "    x = x.view(-1, 1)\n",
        "    # x = F.relu(self.fc1(x))\n",
        "    x = (self.fc1(x))\n",
        "\n",
        "    return x"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dYXnywAD-4l"
      },
      "source": [
        "class Classification(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Classification, self).__init__()\n",
        "    self.fc1 = nn.Linear(1, 1)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x.view(-1, 1)\n",
        "    x = self.fc1(x)\n",
        "    # print(x.shape)\n",
        "    return x"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQfSQzYjtat-"
      },
      "source": [
        ""
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yl41sE8vFERk"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "def train(train_loader, test_loader, focus_net, classify):    \n",
        "  criterion = nn.BCEWithLogitsLoss()\n",
        "  #optimizer_classify = optim.SGD(classify.parameters(), lr=0.01 ) #, momentum=0.9)\n",
        "  optimizer_focus = optim.SGD(focus_net.parameters(), lr=0.01 ) #, momentum=0.9)\n",
        "\n",
        "  print('-'*50)\n",
        "  print(focus_net.fc1.weight, classify.fc1.weight, classify.fc1.bias)\n",
        "  nos_epochs = 3000\n",
        "  loss_ret=0.0\n",
        "  every_what_epoch = 1\n",
        "\n",
        "  for epoch in range(nos_epochs):  # loop over the dataset multiple times\n",
        "    \n",
        "    running_loss = 0.0\n",
        "    epoch_loss = []\n",
        "    cnt=0\n",
        "\n",
        "    iteration = desired_num // batch\n",
        "    \n",
        "    #training data set\n",
        "    #if ((epoch) % (every_what_epoch*2) ) <= every_what_epoch-1 :\n",
        "      #print(epoch+1,\"updating focus_net, classify_net is freezed\")\n",
        "      #print(\"--\"*40)\n",
        "    # elif ((epoch) % (every_what_epoch*2)) > every_what_epoch-1 :\n",
        "    #   print(epoch+1,\"updating classify_net, focus_net is freezed\")\n",
        "    #   print(\"--\"*40)    \n",
        "    for i, data in  enumerate(train_loader):\n",
        "      #print(i)\n",
        "      inputs , labels , fore_idx = data\n",
        "      inputs, labels = inputs.to(\"cuda\"), labels.to(\"cuda\")\n",
        "      inputs = inputs.double()\n",
        "      labels = labels.float()\n",
        "      # zero the parameter gradients\n",
        "      \n",
        "      optimizer_focus.zero_grad()\n",
        "      #optimizer_classify.zero_grad()\n",
        "      \n",
        "      alphas, avg_images = focus_net(inputs)\n",
        "      outputs = classify(avg_images)\n",
        "\n",
        "      predicted = np.round(torch.sigmoid(outputs.data).cpu().numpy())\n",
        "\n",
        "      loss = criterion(outputs[:,0], labels) \n",
        "      loss.backward()\n",
        "      if ((epoch) % (every_what_epoch*2) ) <= every_what_epoch-1 :\n",
        "        optimizer_focus.step()\n",
        "      # elif ( (epoch) % (every_what_epoch*2)) > every_what_epoch-1 :\n",
        "      #   optimizer_classify.step()\n",
        "\n",
        "      running_loss += loss.item()\n",
        "      # mini = 3\n",
        "      # if cnt % mini == mini-1 :    # print every 40 mini-batches\n",
        "      epoch_loss.append(running_loss)\n",
        "      running_loss = 0.0\n",
        "      cnt=cnt+1\n",
        "    loss_ret = np.mean(epoch_loss)\n",
        "    if(epoch%200==0):\n",
        "      print('[%d, %5d] loss: %.3f' %(epoch + 1, cnt + 1, np.mean(epoch_loss)))\n",
        "    if(np.mean(epoch_loss) <= 0.01):\n",
        "        break;\n",
        "\n",
        "  with torch.no_grad():\n",
        "    focus_true_pred_true =0\n",
        "    focus_false_pred_true =0\n",
        "    focus_true_pred_false =0\n",
        "    focus_false_pred_false =0\n",
        "\n",
        "    argmax_more_than_half = 0\n",
        "    argmax_less_than_half =0\n",
        "    for data in test_loader:\n",
        "      inputs, labels , fore_idx = data\n",
        "      inputs = inputs.double()\n",
        "      inputs, labels = inputs.to(\"cuda\"), labels.to(\"cuda\")\n",
        "      alphas, avg_images = focus_net(inputs)\n",
        "      outputs = classify(avg_images)\n",
        "      predicted = np.round(torch.sigmoid(outputs.data).cpu().numpy())\n",
        "\n",
        "      for j in range (batch):\n",
        "        focus = torch.argmax(alphas[j])\n",
        "\n",
        "        if(alphas[j][focus] >= 0.5):\n",
        "          argmax_more_than_half +=1\n",
        "        else:\n",
        "          argmax_less_than_half +=1\n",
        "\n",
        "        if(focus == fore_idx[j] and predicted[j] == labels[j].item()):\n",
        "          focus_true_pred_true += 1\n",
        "\n",
        "        elif(focus != fore_idx[j] and predicted[j] == labels[j].item()):\n",
        "          focus_false_pred_true +=1\n",
        "\n",
        "        elif(focus == fore_idx[j] and predicted[j] != labels[j].item()):\n",
        "          focus_true_pred_false +=1\n",
        "\n",
        "        elif(focus != fore_idx[j] and predicted[j] != labels[j].item()):\n",
        "          focus_false_pred_false +=1\n",
        "\n",
        "  print('Finished Training')  \n",
        "  return loss_ret, argmax_more_than_half/100, focus_true_pred_true/100, focus_false_pred_true/100, focus_true_pred_false/100 , focus_false_pred_false/100, focus_net.fc1.weight.item(), classify.fc1.weight.item(), classify.fc1.bias.item()\n",
        "    \n"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fd7ScJj9g6rl"
      },
      "source": [
        "a =  [0.]\n",
        "b = [-10.,-5.,-2.,0.,2.,5.,10.]\n",
        "c = [-10.,-5.,-2.,0.,2.,5.,10.]"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zo5QyZobhHpM",
        "outputId": "e8b2e603-b746-470c-93cf-33ef669481ca"
      },
      "source": [
        "all_loss=[]\n",
        "# all_alphas_more_than_half=[]\n",
        "all_ftpt=[]\n",
        "all_ffpt=[]\n",
        "all_ftpf = []\n",
        "all_ffpf= []\n",
        "init_a = []\n",
        "init_b = []\n",
        "init_c = []\n",
        "final_a=[]\n",
        "final_b = []\n",
        "final_c = []\n",
        "\n",
        "for a1 in a:\n",
        "  for b1 in b:\n",
        "    for c1 in c:\n",
        "      print(\"for a value %.3f, b value %.3f, and c value %.3f \" %(a1,b1,c1))\n",
        "      print(\"*\"*70)\n",
        "      torch.manual_seed(12)\n",
        "      focus_net = Focus().double()\n",
        "      focus_net.fc1.weight = torch.nn.Parameter(torch.tensor(np.array([[a1]])))\n",
        "      torch.manual_seed(12)\n",
        "      classify = Classification().double()\n",
        "      classify.fc1.weight = torch.nn.Parameter(torch.tensor(np.array([[b1]])))\n",
        "      classify.fc1.bias = torch.nn.Parameter(torch.tensor(np.array([c1])))\n",
        "      focus_net = focus_net.to(\"cuda\")\n",
        "      classify = classify.to(\"cuda\")\n",
        "      print(\"--\"*40,\"a,b,c = \",a1,b1,c1)\n",
        "      cost, alpha_per, ftpt, ffpt, ftpf, ffpf, f_a, f_b, f_c = train(train_loader, test_loader, focus_net, classify)\n",
        "      print(cost, alpha_per, ftpt, ffpt, ftpf, ffpf)\n",
        "      init_a.append(a1)\n",
        "      init_b.append(b1)\n",
        "      init_c.append(c1)\n",
        "      final_a.append(np.round(f_a,3))\n",
        "      final_b.append(np.round(f_b,3))\n",
        "      final_c.append(np.round(f_c,3))\n",
        "      all_loss.append(np.round(cost,3))\n",
        "      # all_alphas_more_than_half.append(alpha_per)\n",
        "      all_ftpt.append(ftpt)\n",
        "      all_ffpt.append(ffpt)\n",
        "      all_ftpf.append(ftpf)\n",
        "      all_ffpf.append(ffpf)\n",
        "\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "for a value 0.000, b value -10.000, and c value -10.000 \n",
            "**********************************************************************\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 -10.0 -10.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[-10.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([-10.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     2] loss: 6.369\n",
            "[201,     2] loss: 4.603\n",
            "[401,     2] loss: 4.603\n",
            "[601,     2] loss: 4.603\n",
            "[801,     2] loss: 4.603\n",
            "[1001,     2] loss: 4.603\n",
            "[1201,     2] loss: 4.603\n",
            "[1401,     2] loss: 4.603\n",
            "[1601,     2] loss: 4.603\n",
            "[1801,     2] loss: 4.603\n",
            "[2001,     2] loss: 4.603\n",
            "[2201,     2] loss: 4.603\n",
            "[2401,     2] loss: 4.603\n",
            "[2601,     2] loss: 4.603\n",
            "[2801,     2] loss: 4.603\n",
            "Finished Training\n",
            "4.603089332580566 5.85 35.79 0.0 64.21 0.0\n",
            "for a value 0.000, b value -10.000, and c value -5.000 \n",
            "**********************************************************************\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 -10.0 -5.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[-10.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([-5.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     2] loss: 8.867\n",
            "[201,     2] loss: 6.151\n",
            "[401,     2] loss: 6.151\n",
            "[601,     2] loss: 6.151\n",
            "[801,     2] loss: 6.151\n",
            "[1001,     2] loss: 6.151\n",
            "[1201,     2] loss: 6.151\n",
            "[1401,     2] loss: 6.151\n",
            "[1601,     2] loss: 6.151\n",
            "[1801,     2] loss: 6.151\n",
            "[2001,     2] loss: 6.151\n",
            "[2201,     2] loss: 6.151\n",
            "[2401,     2] loss: 6.151\n",
            "[2601,     2] loss: 6.151\n",
            "[2801,     2] loss: 6.151\n",
            "Finished Training\n",
            "6.1513824462890625 59.32 29.17 0.0 70.83 0.0\n",
            "for a value 0.000, b value -10.000, and c value -2.000 \n",
            "**********************************************************************\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 -10.0 -2.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[-10.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([-2.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     2] loss: 10.365\n",
            "[201,     2] loss: 6.372\n",
            "[401,     2] loss: 5.562\n",
            "[601,     2] loss: 5.339\n",
            "[801,     2] loss: 5.257\n",
            "[1001,     2] loss: 5.216\n",
            "[1201,     2] loss: 5.191\n",
            "[1401,     2] loss: 5.175\n",
            "[1601,     2] loss: 5.163\n",
            "[1801,     2] loss: 5.154\n",
            "[2001,     2] loss: 5.148\n",
            "[2201,     2] loss: 5.142\n",
            "[2401,     2] loss: 5.138\n",
            "[2601,     2] loss: 5.134\n",
            "[2801,     2] loss: 5.131\n",
            "Finished Training\n",
            "5.1287031173706055 100.0 10.27 0.0 89.73 0.0\n",
            "for a value 0.000, b value -10.000, and c value 0.000 \n",
            "**********************************************************************\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 -10.0 0.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[-10.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([0.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     2] loss: 11.364\n",
            "[201,     2] loss: 5.881\n",
            "[401,     2] loss: 5.314\n",
            "[601,     2] loss: 5.179\n",
            "[801,     2] loss: 5.123\n",
            "[1001,     2] loss: 5.092\n",
            "[1201,     2] loss: 5.073\n",
            "[1401,     2] loss: 5.060\n",
            "[1601,     2] loss: 5.050\n",
            "[1801,     2] loss: 5.043\n",
            "[2001,     2] loss: 5.037\n",
            "[2201,     2] loss: 5.033\n",
            "[2401,     2] loss: 5.029\n",
            "[2601,     2] loss: 5.026\n",
            "[2801,     2] loss: 5.023\n",
            "Finished Training\n",
            "5.020594596862793 100.0 0.09 0.0 99.91 0.0\n",
            "for a value 0.000, b value -10.000, and c value 2.000 \n",
            "**********************************************************************\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 -10.0 2.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[-10.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([2.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     2] loss: 12.363\n",
            "[201,     2] loss: 5.817\n",
            "[401,     2] loss: 5.370\n",
            "[601,     2] loss: 5.260\n",
            "[801,     2] loss: 5.212\n",
            "[1001,     2] loss: 5.185\n",
            "[1201,     2] loss: 5.168\n",
            "[1401,     2] loss: 5.156\n",
            "[1601,     2] loss: 5.148\n",
            "[1801,     2] loss: 5.141\n",
            "[2001,     2] loss: 5.136\n",
            "[2201,     2] loss: 5.131\n",
            "[2401,     2] loss: 5.128\n",
            "[2601,     2] loss: 5.125\n",
            "[2801,     2] loss: 5.122\n",
            "Finished Training\n",
            "5.119970798492432 100.0 10.65 0.0 89.35 0.0\n",
            "for a value 0.000, b value -10.000, and c value 5.000 \n",
            "**********************************************************************\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 -10.0 5.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[-10.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([5.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     2] loss: 13.862\n",
            "[201,     2] loss: 6.253\n",
            "[401,     2] loss: 5.885\n",
            "[601,     2] loss: 5.790\n",
            "[801,     2] loss: 5.748\n",
            "[1001,     2] loss: 5.723\n",
            "[1201,     2] loss: 5.708\n",
            "[1401,     2] loss: 5.697\n",
            "[1601,     2] loss: 5.689\n",
            "[1801,     2] loss: 5.682\n",
            "[2001,     2] loss: 5.677\n",
            "[2201,     2] loss: 5.673\n",
            "[2401,     2] loss: 5.670\n",
            "[2601,     2] loss: 5.667\n",
            "[2801,     2] loss: 5.665\n",
            "Finished Training\n",
            "5.662544250488281 100.0 27.66 0.0 72.34 0.0\n",
            "for a value 0.000, b value -10.000, and c value 10.000 \n",
            "**********************************************************************\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 -10.0 10.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[-10.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([10.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     2] loss: 16.359\n",
            "[201,     2] loss: 8.055\n",
            "[401,     2] loss: 7.728\n",
            "[601,     2] loss: 7.643\n",
            "[801,     2] loss: 7.603\n",
            "[1001,     2] loss: 7.580\n",
            "[1201,     2] loss: 7.565\n",
            "[1401,     2] loss: 7.555\n",
            "[1601,     2] loss: 7.547\n",
            "[1801,     2] loss: 7.541\n",
            "[2001,     2] loss: 7.536\n",
            "[2201,     2] loss: 7.532\n",
            "[2401,     2] loss: 7.529\n",
            "[2601,     2] loss: 7.526\n",
            "[2801,     2] loss: 7.524\n",
            "Finished Training\n",
            "7.52186918258667 100.0 50.98 0.0 49.02 0.0\n",
            "for a value 0.000, b value -5.000, and c value -10.000 \n",
            "**********************************************************************\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 -5.0 -10.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[-5.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([-10.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     2] loss: 1.002\n",
            "[201,     2] loss: 0.998\n",
            "[401,     2] loss: 0.998\n",
            "[601,     2] loss: 0.998\n",
            "[801,     2] loss: 0.998\n",
            "[1001,     2] loss: 0.998\n",
            "[1201,     2] loss: 0.998\n",
            "[1401,     2] loss: 0.998\n",
            "[1601,     2] loss: 0.998\n",
            "[1801,     2] loss: 0.998\n",
            "[2001,     2] loss: 0.998\n",
            "[2201,     2] loss: 0.998\n",
            "[2401,     2] loss: 0.998\n",
            "[2601,     2] loss: 0.998\n",
            "[2801,     2] loss: 0.998\n",
            "Finished Training\n",
            "0.9975959658622742 0.0 46.04 0.0 53.96 0.0\n",
            "for a value 0.000, b value -5.000, and c value -5.000 \n",
            "**********************************************************************\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 -5.0 -5.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[-5.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([-5.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     2] loss: 3.187\n",
            "[201,     2] loss: 2.419\n",
            "[401,     2] loss: 2.419\n",
            "[601,     2] loss: 2.419\n",
            "[801,     2] loss: 2.419\n",
            "[1001,     2] loss: 2.419\n",
            "[1201,     2] loss: 2.419\n",
            "[1401,     2] loss: 2.419\n",
            "[1601,     2] loss: 2.419\n",
            "[1801,     2] loss: 2.419\n",
            "[2001,     2] loss: 2.419\n",
            "[2201,     2] loss: 2.419\n",
            "[2401,     2] loss: 2.419\n",
            "[2601,     2] loss: 2.419\n",
            "[2801,     2] loss: 2.419\n",
            "Finished Training\n",
            "2.418630361557007 3.31 37.18 0.0 62.82 0.0\n",
            "for a value 0.000, b value -5.000, and c value -2.000 \n",
            "**********************************************************************\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 -5.0 -2.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[-5.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([-2.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     2] loss: 4.683\n",
            "[201,     2] loss: 3.299\n",
            "[401,     2] loss: 3.296\n",
            "[601,     2] loss: 3.296\n",
            "[801,     2] loss: 3.296\n",
            "[1001,     2] loss: 3.296\n",
            "[1201,     2] loss: 3.296\n",
            "[1401,     2] loss: 3.296\n",
            "[1601,     2] loss: 3.296\n",
            "[1801,     2] loss: 3.296\n",
            "[2001,     2] loss: 3.296\n",
            "[2201,     2] loss: 3.296\n",
            "[2401,     2] loss: 3.296\n",
            "[2601,     2] loss: 3.296\n",
            "[2801,     2] loss: 3.296\n",
            "Finished Training\n",
            "3.296311616897583 71.15 25.16 0.0 74.84 0.0\n",
            "for a value 0.000, b value -5.000, and c value 0.000 \n",
            "**********************************************************************\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 -5.0 0.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[-5.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([0.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     2] loss: 5.682\n",
            "[201,     2] loss: 3.565\n",
            "[401,     2] loss: 3.074\n",
            "[601,     2] loss: 2.869\n",
            "[801,     2] loss: 2.784\n",
            "[1001,     2] loss: 2.742\n",
            "[1201,     2] loss: 2.716\n",
            "[1401,     2] loss: 2.699\n",
            "[1601,     2] loss: 2.687\n",
            "[1801,     2] loss: 2.679\n",
            "[2001,     2] loss: 2.672\n",
            "[2201,     2] loss: 2.667\n",
            "[2401,     2] loss: 2.662\n",
            "[2601,     2] loss: 2.659\n",
            "[2801,     2] loss: 2.656\n",
            "Finished Training\n",
            "2.6529269218444824 100.0 0.09 0.0 99.91 0.0\n",
            "for a value 0.000, b value -5.000, and c value 2.000 \n",
            "**********************************************************************\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 -5.0 2.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[-5.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([2.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     2] loss: 6.681\n",
            "[201,     2] loss: 3.736\n",
            "[401,     2] loss: 3.151\n",
            "[601,     2] loss: 3.010\n",
            "[801,     2] loss: 2.952\n",
            "[1001,     2] loss: 2.921\n",
            "[1201,     2] loss: 2.902\n",
            "[1401,     2] loss: 2.889\n",
            "[1601,     2] loss: 2.880\n",
            "[1801,     2] loss: 2.873\n",
            "[2001,     2] loss: 2.867\n",
            "[2201,     2] loss: 2.863\n",
            "[2401,     2] loss: 2.859\n",
            "[2601,     2] loss: 2.856\n",
            "[2801,     2] loss: 2.853\n",
            "Finished Training\n",
            "2.8509230613708496 100.0 22.42 0.0 77.58 0.0\n",
            "for a value 0.000, b value -5.000, and c value 5.000 \n",
            "**********************************************************************\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 -5.0 5.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[-5.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([5.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     2] loss: 8.180\n",
            "[201,     2] loss: 4.658\n",
            "[401,     2] loss: 4.092\n",
            "[601,     2] loss: 3.974\n",
            "[801,     2] loss: 3.925\n",
            "[1001,     2] loss: 3.898\n",
            "[1201,     2] loss: 3.881\n",
            "[1401,     2] loss: 3.870\n",
            "[1601,     2] loss: 3.861\n",
            "[1801,     2] loss: 3.855\n",
            "[2001,     2] loss: 3.850\n",
            "[2201,     2] loss: 3.846\n",
            "[2401,     2] loss: 3.842\n",
            "[2601,     2] loss: 3.839\n",
            "[2801,     2] loss: 3.837\n",
            "Finished Training\n",
            "3.8349502086639404 100.0 50.98 0.0 49.02 0.0\n",
            "for a value 0.000, b value -5.000, and c value 10.000 \n",
            "**********************************************************************\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 -5.0 10.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[-5.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([10.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     2] loss: 10.677\n",
            "[201,     2] loss: 7.070\n",
            "[401,     2] loss: 6.505\n",
            "[601,     2] loss: 6.391\n",
            "[801,     2] loss: 6.343\n",
            "[1001,     2] loss: 6.316\n",
            "[1201,     2] loss: 6.300\n",
            "[1401,     2] loss: 6.288\n",
            "[1601,     2] loss: 6.280\n",
            "[1801,     2] loss: 6.274\n",
            "[2001,     2] loss: 6.269\n",
            "[2201,     2] loss: 6.265\n",
            "[2401,     2] loss: 6.261\n",
            "[2601,     2] loss: 6.258\n",
            "[2801,     2] loss: 6.256\n",
            "Finished Training\n",
            "6.253927230834961 100.0 50.98 0.0 49.02 0.0\n",
            "for a value 0.000, b value -2.000, and c value -10.000 \n",
            "**********************************************************************\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 -2.0 -10.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[-2.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([-10.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     2] loss: 2.840\n",
            "[201,     2] loss: 2.551\n",
            "[401,     2] loss: 2.495\n",
            "[601,     2] loss: 2.469\n",
            "[801,     2] loss: 2.452\n",
            "[1001,     2] loss: 2.440\n",
            "[1201,     2] loss: 2.430\n",
            "[1401,     2] loss: 2.422\n",
            "[1601,     2] loss: 2.414\n",
            "[1801,     2] loss: 2.408\n",
            "[2001,     2] loss: 2.402\n",
            "[2201,     2] loss: 2.397\n",
            "[2401,     2] loss: 2.392\n",
            "[2601,     2] loss: 2.387\n",
            "[2801,     2] loss: 2.382\n",
            "Finished Training\n",
            "2.3781301975250244 0.0 0.0 49.02 0.0 50.98\n",
            "for a value 0.000, b value -2.000, and c value -5.000 \n",
            "**********************************************************************\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 -2.0 -5.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[-2.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([-5.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     2] loss: 0.792\n",
            "[201,     2] loss: 0.719\n",
            "[401,     2] loss: 0.704\n",
            "[601,     2] loss: 0.699\n",
            "[801,     2] loss: 0.696\n",
            "[1001,     2] loss: 0.694\n",
            "[1201,     2] loss: 0.693\n",
            "[1401,     2] loss: 0.692\n",
            "[1601,     2] loss: 0.692\n",
            "[1801,     2] loss: 0.692\n",
            "[2001,     2] loss: 0.691\n",
            "[2201,     2] loss: 0.691\n",
            "[2401,     2] loss: 0.691\n",
            "[2601,     2] loss: 0.691\n",
            "[2801,     2] loss: 0.691\n",
            "Finished Training\n",
            "0.6907365322113037 0.0 0.0 53.86 0.0 46.14\n",
            "for a value 0.000, b value -2.000, and c value -2.000 \n",
            "**********************************************************************\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 -2.0 -2.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[-2.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([-2.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     2] loss: 1.359\n",
            "[201,     2] loss: 1.243\n",
            "[401,     2] loss: 1.219\n",
            "[601,     2] loss: 1.218\n",
            "[801,     2] loss: 1.218\n",
            "[1001,     2] loss: 1.218\n",
            "[1201,     2] loss: 1.218\n",
            "[1401,     2] loss: 1.218\n",
            "[1601,     2] loss: 1.218\n",
            "[1801,     2] loss: 1.218\n",
            "[2001,     2] loss: 1.218\n",
            "[2201,     2] loss: 1.218\n",
            "[2401,     2] loss: 1.218\n",
            "[2601,     2] loss: 1.218\n",
            "[2801,     2] loss: 1.218\n",
            "Finished Training\n",
            "1.2184581756591797 0.0 50.7 0.0 49.3 0.0\n",
            "for a value 0.000, b value -2.000, and c value 0.000 \n",
            "**********************************************************************\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 -2.0 0.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[-2.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([0.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     2] loss: 2.285\n",
            "[201,     2] loss: 1.939\n",
            "[401,     2] loss: 1.738\n",
            "[601,     2] loss: 1.677\n",
            "[801,     2] loss: 1.619\n",
            "[1001,     2] loss: 1.557\n",
            "[1201,     2] loss: 1.505\n",
            "[1401,     2] loss: 1.467\n",
            "[1601,     2] loss: 1.440\n",
            "[1801,     2] loss: 1.422\n",
            "[2001,     2] loss: 1.408\n",
            "[2201,     2] loss: 1.398\n",
            "[2401,     2] loss: 1.390\n",
            "[2601,     2] loss: 1.383\n",
            "[2801,     2] loss: 1.378\n",
            "Finished Training\n",
            "1.37379789352417 100.0 0.66 0.0 99.34 0.0\n",
            "for a value 0.000, b value -2.000, and c value 2.000 \n",
            "**********************************************************************\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 -2.0 2.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[-2.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([2.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     2] loss: 3.274\n",
            "[201,     2] loss: 2.825\n",
            "[401,     2] loss: 2.210\n",
            "[601,     2] loss: 1.960\n",
            "[801,     2] loss: 1.861\n",
            "[1001,     2] loss: 1.813\n",
            "[1201,     2] loss: 1.785\n",
            "[1401,     2] loss: 1.767\n",
            "[1601,     2] loss: 1.755\n",
            "[1801,     2] loss: 1.746\n",
            "[2001,     2] loss: 1.739\n",
            "[2201,     2] loss: 1.734\n",
            "[2401,     2] loss: 1.730\n",
            "[2601,     2] loss: 1.726\n",
            "[2801,     2] loss: 1.723\n",
            "Finished Training\n",
            "1.720450758934021 100.0 50.98 0.0 49.02 0.0\n",
            "for a value 0.000, b value -2.000, and c value 5.000 \n",
            "**********************************************************************\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 -2.0 5.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[-2.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([5.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     2] loss: 4.770\n",
            "[201,     2] loss: 4.300\n",
            "[401,     2] loss: 3.514\n",
            "[601,     2] loss: 3.248\n",
            "[801,     2] loss: 3.157\n",
            "[1001,     2] loss: 3.114\n",
            "[1201,     2] loss: 3.090\n",
            "[1401,     2] loss: 3.074\n",
            "[1601,     2] loss: 3.063\n",
            "[1801,     2] loss: 3.055\n",
            "[2001,     2] loss: 3.048\n",
            "[2201,     2] loss: 3.043\n",
            "[2401,     2] loss: 3.040\n",
            "[2601,     2] loss: 3.036\n",
            "[2801,     2] loss: 3.033\n",
            "Finished Training\n",
            "3.0311279296875 100.0 50.98 0.0 49.02 0.0\n",
            "for a value 0.000, b value -2.000, and c value 10.000 \n",
            "**********************************************************************\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 -2.0 10.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[-2.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([10.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     2] loss: 7.268\n",
            "[201,     2] loss: 6.797\n",
            "[401,     2] loss: 5.999\n",
            "[601,     2] loss: 5.732\n",
            "[801,     2] loss: 5.642\n",
            "[1001,     2] loss: 5.599\n",
            "[1201,     2] loss: 5.575\n",
            "[1401,     2] loss: 5.559\n",
            "[1601,     2] loss: 5.548\n",
            "[1801,     2] loss: 5.540\n",
            "[2001,     2] loss: 5.534\n",
            "[2201,     2] loss: 5.529\n",
            "[2401,     2] loss: 5.525\n",
            "[2601,     2] loss: 5.522\n",
            "[2801,     2] loss: 5.519\n",
            "Finished Training\n",
            "5.516765594482422 100.0 50.98 0.0 49.02 0.0\n",
            "for a value 0.000, b value 0.000, and c value -10.000 \n",
            "**********************************************************************\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 0.0 -10.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([-10.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     2] loss: 5.005\n",
            "[201,     2] loss: 5.005\n",
            "[401,     2] loss: 5.005\n",
            "[601,     2] loss: 5.005\n",
            "[801,     2] loss: 5.005\n",
            "[1001,     2] loss: 5.005\n",
            "[1201,     2] loss: 5.005\n",
            "[1401,     2] loss: 5.005\n",
            "[1601,     2] loss: 5.005\n",
            "[1801,     2] loss: 5.005\n",
            "[2001,     2] loss: 5.005\n",
            "[2201,     2] loss: 5.005\n",
            "[2401,     2] loss: 5.005\n",
            "[2601,     2] loss: 5.005\n",
            "[2801,     2] loss: 5.005\n",
            "Finished Training\n",
            "5.0050458908081055 0.0 4.94 44.08 5.28 45.7\n",
            "for a value 0.000, b value 0.000, and c value -5.000 \n",
            "**********************************************************************\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 0.0 -5.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([-5.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     2] loss: 2.509\n",
            "[201,     2] loss: 2.509\n",
            "[401,     2] loss: 2.509\n",
            "[601,     2] loss: 2.509\n",
            "[801,     2] loss: 2.509\n",
            "[1001,     2] loss: 2.509\n",
            "[1201,     2] loss: 2.509\n",
            "[1401,     2] loss: 2.509\n",
            "[1601,     2] loss: 2.509\n",
            "[1801,     2] loss: 2.509\n",
            "[2001,     2] loss: 2.509\n",
            "[2201,     2] loss: 2.509\n",
            "[2401,     2] loss: 2.509\n",
            "[2601,     2] loss: 2.509\n",
            "[2801,     2] loss: 2.509\n",
            "Finished Training\n",
            "2.5092153549194336 0.0 4.94 44.08 5.28 45.7\n",
            "for a value 0.000, b value 0.000, and c value -2.000 \n",
            "**********************************************************************\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 0.0 -2.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([-2.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     2] loss: 1.128\n",
            "[201,     2] loss: 1.128\n",
            "[401,     2] loss: 1.128\n",
            "[601,     2] loss: 1.128\n",
            "[801,     2] loss: 1.128\n",
            "[1001,     2] loss: 1.128\n",
            "[1201,     2] loss: 1.128\n",
            "[1401,     2] loss: 1.128\n",
            "[1601,     2] loss: 1.128\n",
            "[1801,     2] loss: 1.128\n",
            "[2001,     2] loss: 1.128\n",
            "[2201,     2] loss: 1.128\n",
            "[2401,     2] loss: 1.128\n",
            "[2601,     2] loss: 1.128\n",
            "[2801,     2] loss: 1.128\n",
            "Finished Training\n",
            "1.1279281377792358 0.0 4.94 44.08 5.28 45.7\n",
            "for a value 0.000, b value 0.000, and c value 0.000 \n",
            "**********************************************************************\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 0.0 0.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([0.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     2] loss: 0.693\n",
            "[201,     2] loss: 0.693\n",
            "[401,     2] loss: 0.693\n",
            "[601,     2] loss: 0.693\n",
            "[801,     2] loss: 0.693\n",
            "[1001,     2] loss: 0.693\n",
            "[1201,     2] loss: 0.693\n",
            "[1401,     2] loss: 0.693\n",
            "[1601,     2] loss: 0.693\n",
            "[1801,     2] loss: 0.693\n",
            "[2001,     2] loss: 0.693\n",
            "[2201,     2] loss: 0.693\n",
            "[2401,     2] loss: 0.693\n",
            "[2601,     2] loss: 0.693\n",
            "[2801,     2] loss: 0.693\n",
            "Finished Training\n",
            "0.6931471228599548 0.0 4.94 44.08 5.28 45.7\n",
            "for a value 0.000, b value 0.000, and c value 2.000 \n",
            "**********************************************************************\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 0.0 2.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([2.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     2] loss: 1.126\n",
            "[201,     2] loss: 1.126\n",
            "[401,     2] loss: 1.126\n",
            "[601,     2] loss: 1.126\n",
            "[801,     2] loss: 1.126\n",
            "[1001,     2] loss: 1.126\n",
            "[1201,     2] loss: 1.126\n",
            "[1401,     2] loss: 1.126\n",
            "[1601,     2] loss: 1.126\n",
            "[1801,     2] loss: 1.126\n",
            "[2001,     2] loss: 1.126\n",
            "[2201,     2] loss: 1.126\n",
            "[2401,     2] loss: 1.126\n",
            "[2601,     2] loss: 1.126\n",
            "[2801,     2] loss: 1.126\n",
            "Finished Training\n",
            "1.1259280443191528 0.0 5.28 45.7 4.94 44.08\n",
            "for a value 0.000, b value 0.000, and c value 5.000 \n",
            "**********************************************************************\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 0.0 5.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([5.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     2] loss: 2.504\n",
            "[201,     2] loss: 2.504\n",
            "[401,     2] loss: 2.504\n",
            "[601,     2] loss: 2.504\n",
            "[801,     2] loss: 2.504\n",
            "[1001,     2] loss: 2.504\n",
            "[1201,     2] loss: 2.504\n",
            "[1401,     2] loss: 2.504\n",
            "[1601,     2] loss: 2.504\n",
            "[1801,     2] loss: 2.504\n",
            "[2001,     2] loss: 2.504\n",
            "[2201,     2] loss: 2.504\n",
            "[2401,     2] loss: 2.504\n",
            "[2601,     2] loss: 2.504\n",
            "[2801,     2] loss: 2.504\n",
            "Finished Training\n",
            "2.5042154788970947 0.0 5.28 45.7 4.94 44.08\n",
            "for a value 0.000, b value 0.000, and c value 10.000 \n",
            "**********************************************************************\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 0.0 10.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([10.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     2] loss: 4.995\n",
            "[201,     2] loss: 4.995\n",
            "[401,     2] loss: 4.995\n",
            "[601,     2] loss: 4.995\n",
            "[801,     2] loss: 4.995\n",
            "[1001,     2] loss: 4.995\n",
            "[1201,     2] loss: 4.995\n",
            "[1401,     2] loss: 4.995\n",
            "[1601,     2] loss: 4.995\n",
            "[1801,     2] loss: 4.995\n",
            "[2001,     2] loss: 4.995\n",
            "[2201,     2] loss: 4.995\n",
            "[2401,     2] loss: 4.995\n",
            "[2601,     2] loss: 4.995\n",
            "[2801,     2] loss: 4.995\n",
            "Finished Training\n",
            "4.995046138763428 0.0 5.28 45.7 4.94 44.08\n",
            "for a value 0.000, b value 2.000, and c value -10.000 \n",
            "**********************************************************************\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 2.0 -10.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[2.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([-10.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     2] loss: 7.174\n",
            "[201,     2] loss: 4.803\n",
            "[401,     2] loss: 4.613\n",
            "[601,     2] loss: 4.574\n",
            "[801,     2] loss: 4.557\n",
            "[1001,     2] loss: 4.548\n",
            "[1201,     2] loss: 4.542\n",
            "[1401,     2] loss: 4.538\n",
            "[1601,     2] loss: 4.535\n",
            "[1801,     2] loss: 4.533\n",
            "[2001,     2] loss: 4.531\n",
            "[2201,     2] loss: 4.529\n",
            "[2401,     2] loss: 4.528\n",
            "[2601,     2] loss: 4.527\n",
            "[2801,     2] loss: 4.526\n",
            "Finished Training\n",
            "4.525592803955078 100.0 49.02 0.0 50.98 0.0\n",
            "for a value 0.000, b value 2.000, and c value -5.000 \n",
            "**********************************************************************\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 2.0 -5.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[2.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([-5.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     2] loss: 4.672\n",
            "[201,     2] loss: 2.310\n",
            "[401,     2] loss: 2.122\n",
            "[601,     2] loss: 2.082\n",
            "[801,     2] loss: 2.066\n",
            "[1001,     2] loss: 2.057\n",
            "[1201,     2] loss: 2.051\n",
            "[1401,     2] loss: 2.047\n",
            "[1601,     2] loss: 2.044\n",
            "[1801,     2] loss: 2.042\n",
            "[2001,     2] loss: 2.040\n",
            "[2201,     2] loss: 2.038\n",
            "[2401,     2] loss: 2.037\n",
            "[2601,     2] loss: 2.036\n",
            "[2801,     2] loss: 2.036\n",
            "Finished Training\n",
            "2.0347800254821777 100.0 49.02 0.0 50.98 0.0\n",
            "for a value 0.000, b value 2.000, and c value -2.000 \n",
            "**********************************************************************\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 2.0 -2.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[2.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([-2.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     2] loss: 3.172\n",
            "[201,     2] loss: 0.954\n",
            "[401,     2] loss: 0.793\n",
            "[601,     2] loss: 0.759\n",
            "[801,     2] loss: 0.744\n",
            "[1001,     2] loss: 0.736\n",
            "[1201,     2] loss: 0.731\n",
            "[1401,     2] loss: 0.728\n",
            "[1601,     2] loss: 0.725\n",
            "[1801,     2] loss: 0.723\n",
            "[2001,     2] loss: 0.722\n",
            "[2201,     2] loss: 0.721\n",
            "[2401,     2] loss: 0.720\n",
            "[2601,     2] loss: 0.719\n",
            "[2801,     2] loss: 0.718\n",
            "Finished Training\n",
            "0.7174201011657715 100.0 49.02 0.0 50.98 0.0\n",
            "for a value 0.000, b value 2.000, and c value 0.000 \n",
            "**********************************************************************\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 2.0 0.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[2.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([0.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     2] loss: 2.181\n",
            "[201,     2] loss: 0.433\n",
            "[401,     2] loss: 0.344\n",
            "[601,     2] loss: 0.330\n",
            "[801,     2] loss: 0.325\n",
            "[1001,     2] loss: 0.322\n",
            "[1201,     2] loss: 0.321\n",
            "[1401,     2] loss: 0.321\n",
            "[1601,     2] loss: 0.320\n",
            "[1801,     2] loss: 0.320\n",
            "[2001,     2] loss: 0.320\n",
            "[2201,     2] loss: 0.320\n",
            "[2401,     2] loss: 0.320\n",
            "[2601,     2] loss: 0.320\n",
            "[2801,     2] loss: 0.320\n",
            "Finished Training\n",
            "0.3194867968559265 100.0 92.91 0.0 7.09 0.0\n",
            "for a value 0.000, b value 2.000, and c value 2.000 \n",
            "**********************************************************************\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 2.0 2.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[2.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([2.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     2] loss: 1.253\n",
            "[201,     2] loss: 0.342\n",
            "[401,     2] loss: 0.319\n",
            "[601,     2] loss: 0.319\n",
            "[801,     2] loss: 0.318\n",
            "[1001,     2] loss: 0.318\n",
            "[1201,     2] loss: 0.318\n",
            "[1401,     2] loss: 0.318\n",
            "[1601,     2] loss: 0.318\n",
            "[1801,     2] loss: 0.318\n",
            "[2001,     2] loss: 0.318\n",
            "[2201,     2] loss: 0.318\n",
            "[2401,     2] loss: 0.318\n",
            "[2601,     2] loss: 0.318\n",
            "[2801,     2] loss: 0.318\n",
            "Finished Training\n",
            "0.31846946477890015 75.65 95.82 0.0 4.18 0.0\n",
            "for a value 0.000, b value 2.000, and c value 5.000 \n",
            "**********************************************************************\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 2.0 5.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[2.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([5.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     2] loss: 0.683\n",
            "[201,     2] loss: 0.682\n",
            "[401,     2] loss: 0.681\n",
            "[601,     2] loss: 0.681\n",
            "[801,     2] loss: 0.681\n",
            "[1001,     2] loss: 0.681\n",
            "[1201,     2] loss: 0.681\n",
            "[1401,     2] loss: 0.681\n",
            "[1601,     2] loss: 0.681\n",
            "[1801,     2] loss: 0.681\n",
            "[2001,     2] loss: 0.681\n",
            "[2201,     2] loss: 0.681\n",
            "[2401,     2] loss: 0.681\n",
            "[2601,     2] loss: 0.681\n",
            "[2801,     2] loss: 0.681\n",
            "Finished Training\n",
            "0.6805183291435242 0.0 51.06 0.0 48.94 0.0\n",
            "for a value 0.000, b value 2.000, and c value 10.000 \n",
            "**********************************************************************\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 2.0 10.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[2.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([10.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     2] loss: 2.726\n",
            "[201,     2] loss: 2.593\n",
            "[401,     2] loss: 2.533\n",
            "[601,     2] loss: 2.499\n",
            "[801,     2] loss: 2.475\n",
            "[1001,     2] loss: 2.458\n",
            "[1201,     2] loss: 2.444\n",
            "[1401,     2] loss: 2.433\n",
            "[1601,     2] loss: 2.423\n",
            "[1801,     2] loss: 2.414\n",
            "[2001,     2] loss: 2.407\n",
            "[2201,     2] loss: 2.400\n",
            "[2401,     2] loss: 2.393\n",
            "[2601,     2] loss: 2.388\n",
            "[2801,     2] loss: 2.382\n",
            "Finished Training\n",
            "2.377007007598877 0.0 0.0 50.98 0.0 49.02\n",
            "for a value 0.000, b value 5.000, and c value -10.000 \n",
            "**********************************************************************\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 5.0 -10.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[5.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([-10.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     2] loss: 10.428\n",
            "[201,     2] loss: 3.963\n",
            "[401,     2] loss: 3.862\n",
            "[601,     2] loss: 3.833\n",
            "[801,     2] loss: 3.820\n",
            "[1001,     2] loss: 3.812\n",
            "[1201,     2] loss: 3.807\n",
            "[1401,     2] loss: 3.803\n",
            "[1601,     2] loss: 3.801\n",
            "[1801,     2] loss: 3.799\n",
            "[2001,     2] loss: 3.797\n",
            "[2201,     2] loss: 3.796\n",
            "[2401,     2] loss: 3.794\n",
            "[2601,     2] loss: 3.793\n",
            "[2801,     2] loss: 3.793\n",
            "Finished Training\n",
            "3.7919507026672363 100.0 49.02 0.0 50.98 0.0\n",
            "for a value 0.000, b value 5.000, and c value -5.000 \n",
            "**********************************************************************\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 5.0 -5.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[5.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([-5.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     2] loss: 7.926\n",
            "[201,     2] loss: 1.535\n",
            "[401,     2] loss: 1.437\n",
            "[601,     2] loss: 1.409\n",
            "[801,     2] loss: 1.396\n",
            "[1001,     2] loss: 1.388\n",
            "[1201,     2] loss: 1.383\n",
            "[1401,     2] loss: 1.379\n",
            "[1601,     2] loss: 1.377\n",
            "[1801,     2] loss: 1.374\n",
            "[2001,     2] loss: 1.373\n",
            "[2201,     2] loss: 1.371\n",
            "[2401,     2] loss: 1.370\n",
            "[2601,     2] loss: 1.369\n",
            "[2801,     2] loss: 1.369\n",
            "Finished Training\n",
            "1.367859959602356 100.0 49.02 0.0 50.98 0.0\n",
            "for a value 0.000, b value 5.000, and c value -2.000 \n",
            "**********************************************************************\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 5.0 -2.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[5.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([-2.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     2] loss: 6.424\n",
            "[201,     2] loss: 0.519\n",
            "[401,     2] loss: 0.439\n",
            "[601,     2] loss: 0.415\n",
            "[801,     2] loss: 0.404\n",
            "[1001,     2] loss: 0.397\n",
            "[1201,     2] loss: 0.392\n",
            "[1401,     2] loss: 0.389\n",
            "[1601,     2] loss: 0.387\n",
            "[1801,     2] loss: 0.385\n",
            "[2001,     2] loss: 0.383\n",
            "[2201,     2] loss: 0.382\n",
            "[2401,     2] loss: 0.381\n",
            "[2601,     2] loss: 0.380\n",
            "[2801,     2] loss: 0.379\n",
            "Finished Training\n",
            "0.3787728548049927 100.0 77.01 0.0 22.99 0.0\n",
            "for a value 0.000, b value 5.000, and c value 0.000 \n",
            "**********************************************************************\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 5.0 0.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[5.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([0.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     2] loss: 5.423\n",
            "[201,     2] loss: 0.219\n",
            "[401,     2] loss: 0.177\n",
            "[601,     2] loss: 0.166\n",
            "[801,     2] loss: 0.162\n",
            "[1001,     2] loss: 0.160\n",
            "[1201,     2] loss: 0.159\n",
            "[1401,     2] loss: 0.158\n",
            "[1601,     2] loss: 0.157\n",
            "[1801,     2] loss: 0.157\n",
            "[2001,     2] loss: 0.156\n",
            "[2201,     2] loss: 0.156\n",
            "[2401,     2] loss: 0.156\n",
            "[2601,     2] loss: 0.156\n",
            "[2801,     2] loss: 0.156\n",
            "Finished Training\n",
            "0.15579597651958466 100.0 96.91 0.0 3.09 0.0\n",
            "for a value 0.000, b value 5.000, and c value 2.000 \n",
            "**********************************************************************\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 5.0 2.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[5.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([2.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     2] loss: 4.422\n",
            "[201,     2] loss: 0.135\n",
            "[401,     2] loss: 0.128\n",
            "[601,     2] loss: 0.128\n",
            "[801,     2] loss: 0.128\n",
            "[1001,     2] loss: 0.128\n",
            "[1201,     2] loss: 0.128\n",
            "[1401,     2] loss: 0.128\n",
            "[1601,     2] loss: 0.128\n",
            "[1801,     2] loss: 0.128\n",
            "[2001,     2] loss: 0.128\n",
            "[2201,     2] loss: 0.128\n",
            "[2401,     2] loss: 0.128\n",
            "[2601,     2] loss: 0.128\n",
            "[2801,     2] loss: 0.128\n",
            "Finished Training\n",
            "0.12786391377449036 99.31 99.05 0.0 0.95 0.0\n",
            "for a value 0.000, b value 5.000, and c value 5.000 \n",
            "**********************************************************************\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 5.0 5.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[5.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([5.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     2] loss: 2.923\n",
            "[201,     2] loss: 0.146\n",
            "[401,     2] loss: 0.146\n",
            "[601,     2] loss: 0.146\n",
            "[801,     2] loss: 0.146\n",
            "[1001,     2] loss: 0.146\n",
            "[1201,     2] loss: 0.146\n",
            "[1401,     2] loss: 0.146\n",
            "[1601,     2] loss: 0.146\n",
            "[1801,     2] loss: 0.146\n",
            "[2001,     2] loss: 0.146\n",
            "[2201,     2] loss: 0.146\n",
            "[2401,     2] loss: 0.146\n",
            "[2601,     2] loss: 0.146\n",
            "[2801,     2] loss: 0.146\n",
            "Finished Training\n",
            "0.1460774838924408 69.93 99.02 0.0 0.98 0.0\n",
            "for a value 0.000, b value 5.000, and c value 10.000 \n",
            "**********************************************************************\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 5.0 10.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[5.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([10.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     2] loss: 0.733\n",
            "[201,     2] loss: 0.403\n",
            "[401,     2] loss: 0.403\n",
            "[601,     2] loss: 0.403\n",
            "[801,     2] loss: 0.403\n",
            "[1001,     2] loss: 0.403\n",
            "[1201,     2] loss: 0.403\n",
            "[1401,     2] loss: 0.403\n",
            "[1601,     2] loss: 0.403\n",
            "[1801,     2] loss: 0.403\n",
            "[2001,     2] loss: 0.403\n",
            "[2201,     2] loss: 0.403\n",
            "[2401,     2] loss: 0.403\n",
            "[2601,     2] loss: 0.403\n",
            "[2801,     2] loss: 0.403\n",
            "Finished Training\n",
            "0.40288591384887695 0.0 84.77 0.0 15.23 0.0\n",
            "for a value 0.000, b value 10.000, and c value -10.000 \n",
            "**********************************************************************\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 10.0 -10.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[10.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([-10.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     2] loss: 15.851\n",
            "[201,     2] loss: 2.755\n",
            "[401,     2] loss: 2.672\n",
            "[601,     2] loss: 2.647\n",
            "[801,     2] loss: 2.634\n",
            "[1001,     2] loss: 2.627\n",
            "[1201,     2] loss: 2.622\n",
            "[1401,     2] loss: 2.618\n",
            "[1601,     2] loss: 2.615\n",
            "[1801,     2] loss: 2.613\n",
            "[2001,     2] loss: 2.612\n",
            "[2201,     2] loss: 2.610\n",
            "[2401,     2] loss: 2.609\n",
            "[2601,     2] loss: 2.608\n",
            "[2801,     2] loss: 2.608\n",
            "Finished Training\n",
            "2.6068813800811768 100.0 49.02 0.0 50.98 0.0\n",
            "for a value 0.000, b value 10.000, and c value -5.000 \n",
            "**********************************************************************\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 10.0 -5.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[10.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([-5.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     2] loss: 13.349\n",
            "[201,     2] loss: 0.887\n",
            "[401,     2] loss: 0.809\n",
            "[601,     2] loss: 0.783\n",
            "[801,     2] loss: 0.770\n",
            "[1001,     2] loss: 0.763\n",
            "[1201,     2] loss: 0.758\n",
            "[1401,     2] loss: 0.754\n",
            "[1601,     2] loss: 0.751\n",
            "[1801,     2] loss: 0.749\n",
            "[2001,     2] loss: 0.748\n",
            "[2201,     2] loss: 0.746\n",
            "[2401,     2] loss: 0.745\n",
            "[2601,     2] loss: 0.744\n",
            "[2801,     2] loss: 0.743\n",
            "Finished Training\n",
            "0.7425566911697388 100.0 72.23 0.0 27.77 0.0\n",
            "for a value 0.000, b value 10.000, and c value -2.000 \n",
            "**********************************************************************\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 10.0 -2.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[10.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([-2.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     2] loss: 11.847\n",
            "[201,     2] loss: 0.310\n",
            "[401,     2] loss: 0.249\n",
            "[601,     2] loss: 0.229\n",
            "[801,     2] loss: 0.219\n",
            "[1001,     2] loss: 0.213\n",
            "[1201,     2] loss: 0.208\n",
            "[1401,     2] loss: 0.205\n",
            "[1601,     2] loss: 0.203\n",
            "[1801,     2] loss: 0.201\n",
            "[2001,     2] loss: 0.200\n",
            "[2201,     2] loss: 0.199\n",
            "[2401,     2] loss: 0.198\n",
            "[2601,     2] loss: 0.197\n",
            "[2801,     2] loss: 0.196\n",
            "Finished Training\n",
            "0.19536016881465912 100.0 88.96 0.0 11.04 0.0\n",
            "for a value 0.000, b value 10.000, and c value 0.000 \n",
            "**********************************************************************\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 10.0 0.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[10.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([0.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     2] loss: 10.846\n",
            "[201,     2] loss: 0.137\n",
            "[401,     2] loss: 0.103\n",
            "[601,     2] loss: 0.093\n",
            "[801,     2] loss: 0.089\n",
            "[1001,     2] loss: 0.086\n",
            "[1201,     2] loss: 0.085\n",
            "[1401,     2] loss: 0.084\n",
            "[1601,     2] loss: 0.083\n",
            "[1801,     2] loss: 0.083\n",
            "[2001,     2] loss: 0.082\n",
            "[2201,     2] loss: 0.082\n",
            "[2401,     2] loss: 0.082\n",
            "[2601,     2] loss: 0.081\n",
            "[2801,     2] loss: 0.081\n",
            "Finished Training\n",
            "0.08099833875894547 100.0 97.99 0.0 2.01 0.0\n",
            "for a value 0.000, b value 10.000, and c value 2.000 \n",
            "**********************************************************************\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 10.0 2.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[10.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([2.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     2] loss: 9.845\n",
            "[201,     2] loss: 0.077\n",
            "[401,     2] loss: 0.068\n",
            "[601,     2] loss: 0.067\n",
            "[801,     2] loss: 0.067\n",
            "[1001,     2] loss: 0.067\n",
            "[1201,     2] loss: 0.067\n",
            "[1401,     2] loss: 0.067\n",
            "[1601,     2] loss: 0.067\n",
            "[1801,     2] loss: 0.067\n",
            "[2001,     2] loss: 0.067\n",
            "[2201,     2] loss: 0.067\n",
            "[2401,     2] loss: 0.067\n",
            "[2601,     2] loss: 0.067\n",
            "[2801,     2] loss: 0.067\n",
            "Finished Training\n",
            "0.06699605286121368 100.0 99.3 0.0 0.7 0.0\n",
            "for a value 0.000, b value 10.000, and c value 5.000 \n",
            "**********************************************************************\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 10.0 5.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[10.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([5.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     2] loss: 8.344\n",
            "[201,     2] loss: 0.064\n",
            "[401,     2] loss: 0.064\n",
            "[601,     2] loss: 0.064\n",
            "[801,     2] loss: 0.064\n",
            "[1001,     2] loss: 0.064\n",
            "[1201,     2] loss: 0.064\n",
            "[1401,     2] loss: 0.064\n",
            "[1601,     2] loss: 0.064\n",
            "[1801,     2] loss: 0.064\n",
            "[2001,     2] loss: 0.064\n",
            "[2201,     2] loss: 0.064\n",
            "[2401,     2] loss: 0.064\n",
            "[2601,     2] loss: 0.064\n",
            "[2801,     2] loss: 0.064\n",
            "Finished Training\n",
            "0.0635363906621933 97.06 99.02 0.0 0.98 0.0\n",
            "for a value 0.000, b value 10.000, and c value 10.000 \n",
            "**********************************************************************\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 10.0 10.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[10.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([10.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     2] loss: 5.841\n",
            "[201,     2] loss: 0.072\n",
            "[401,     2] loss: 0.072\n",
            "[601,     2] loss: 0.072\n",
            "[801,     2] loss: 0.072\n",
            "[1001,     2] loss: 0.072\n",
            "[1201,     2] loss: 0.072\n",
            "[1401,     2] loss: 0.072\n",
            "[1601,     2] loss: 0.072\n",
            "[1801,     2] loss: 0.072\n",
            "[2001,     2] loss: 0.072\n",
            "[2201,     2] loss: 0.072\n",
            "[2401,     2] loss: 0.072\n",
            "[2601,     2] loss: 0.072\n",
            "[2801,     2] loss: 0.072\n",
            "Finished Training\n",
            "0.07189685851335526 68.69 99.36 0.0 0.64 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gQoPST5zW2t"
      },
      "source": [
        "# df_train = pd.DataFrame()\n",
        "df_test = pd.DataFrame()``"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "In76SYH_zZHV"
      },
      "source": [
        "columns = [\"init_a\", \"init_b\", \"init_c\", \"final_a\", \"final_b\", \"final_c\", \"train_loss\", \"argmax > 0.5\" , \"ftpt\", \"ffpt\", \"ftpf\", \"ffpf\" ]"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BS4HtOHEzZ0E"
      },
      "source": [
        "df_test[columns[0]] = init_a\n",
        "df_test[columns[1]] = init_b\n",
        "df_test[columns[2]] = init_c\n",
        "df_test[columns[3]] = final_a\n",
        "df_test[columns[4]] = final_b\n",
        "df_test[columns[5]] = final_c\n",
        "df_test[columns[6]] = all_loss\n",
        "# df_test[columns[7]] = all_alphas_more_than_half\n",
        "df_test[columns[8]] = all_ftpt\n",
        "df_test[columns[9]] = all_ffpt\n",
        "df_test[columns[10]] = all_ftpf\n",
        "df_test[columns[11]] = all_ffpf"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UbTkfLUINTI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a8ee4fdb-420d-4f08-d646-ab03e9c4d793"
      },
      "source": [
        "df_test"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>init_a</th>\n",
              "      <th>init_b</th>\n",
              "      <th>init_c</th>\n",
              "      <th>final_a</th>\n",
              "      <th>final_b</th>\n",
              "      <th>final_c</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>ftpt</th>\n",
              "      <th>ffpt</th>\n",
              "      <th>ftpf</th>\n",
              "      <th>ffpf</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>-10.0</td>\n",
              "      <td>-10.0</td>\n",
              "      <td>0.621</td>\n",
              "      <td>-10.0</td>\n",
              "      <td>-10.0</td>\n",
              "      <td>4.603</td>\n",
              "      <td>35.79</td>\n",
              "      <td>0.00</td>\n",
              "      <td>64.21</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>-10.0</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>0.906</td>\n",
              "      <td>-10.0</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>6.151</td>\n",
              "      <td>29.17</td>\n",
              "      <td>0.00</td>\n",
              "      <td>70.83</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>-10.0</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>4.347</td>\n",
              "      <td>-10.0</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>5.129</td>\n",
              "      <td>10.27</td>\n",
              "      <td>0.00</td>\n",
              "      <td>89.73</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>-10.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.419</td>\n",
              "      <td>-10.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.021</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.00</td>\n",
              "      <td>99.91</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>-10.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4.455</td>\n",
              "      <td>-10.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>5.120</td>\n",
              "      <td>10.65</td>\n",
              "      <td>0.00</td>\n",
              "      <td>89.35</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.0</td>\n",
              "      <td>-10.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4.480</td>\n",
              "      <td>-10.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.663</td>\n",
              "      <td>27.66</td>\n",
              "      <td>0.00</td>\n",
              "      <td>72.34</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.0</td>\n",
              "      <td>-10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>4.492</td>\n",
              "      <td>-10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>7.522</td>\n",
              "      <td>50.98</td>\n",
              "      <td>0.00</td>\n",
              "      <td>49.02</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.0</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>-10.0</td>\n",
              "      <td>0.047</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>-10.0</td>\n",
              "      <td>0.998</td>\n",
              "      <td>46.04</td>\n",
              "      <td>0.00</td>\n",
              "      <td>53.96</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.0</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>0.609</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>2.419</td>\n",
              "      <td>37.18</td>\n",
              "      <td>0.00</td>\n",
              "      <td>62.82</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.0</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>1.013</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>3.296</td>\n",
              "      <td>25.16</td>\n",
              "      <td>0.00</td>\n",
              "      <td>74.84</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.0</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.897</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.653</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.00</td>\n",
              "      <td>99.91</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.0</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.994</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.851</td>\n",
              "      <td>22.42</td>\n",
              "      <td>0.00</td>\n",
              "      <td>77.58</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.0</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4.033</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>3.835</td>\n",
              "      <td>50.98</td>\n",
              "      <td>0.00</td>\n",
              "      <td>49.02</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.0</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>4.038</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>6.254</td>\n",
              "      <td>50.98</td>\n",
              "      <td>0.00</td>\n",
              "      <td>49.02</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.0</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>-10.0</td>\n",
              "      <td>-1.925</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>-10.0</td>\n",
              "      <td>2.378</td>\n",
              "      <td>0.00</td>\n",
              "      <td>49.02</td>\n",
              "      <td>0.00</td>\n",
              "      <td>50.98</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.0</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>-0.714</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>0.691</td>\n",
              "      <td>0.00</td>\n",
              "      <td>53.86</td>\n",
              "      <td>0.00</td>\n",
              "      <td>46.14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.0</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>0.504</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>1.218</td>\n",
              "      <td>50.70</td>\n",
              "      <td>0.00</td>\n",
              "      <td>49.30</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.0</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.002</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.374</td>\n",
              "      <td>0.66</td>\n",
              "      <td>0.00</td>\n",
              "      <td>99.34</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.0</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.357</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.720</td>\n",
              "      <td>50.98</td>\n",
              "      <td>0.00</td>\n",
              "      <td>49.02</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.0</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>3.416</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>3.031</td>\n",
              "      <td>50.98</td>\n",
              "      <td>0.00</td>\n",
              "      <td>49.02</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.0</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>3.419</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>5.517</td>\n",
              "      <td>50.98</td>\n",
              "      <td>0.00</td>\n",
              "      <td>49.02</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-10.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-10.0</td>\n",
              "      <td>5.005</td>\n",
              "      <td>4.94</td>\n",
              "      <td>44.08</td>\n",
              "      <td>5.28</td>\n",
              "      <td>45.70</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>2.509</td>\n",
              "      <td>4.94</td>\n",
              "      <td>44.08</td>\n",
              "      <td>5.28</td>\n",
              "      <td>45.70</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>1.128</td>\n",
              "      <td>4.94</td>\n",
              "      <td>44.08</td>\n",
              "      <td>5.28</td>\n",
              "      <td>45.70</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.693</td>\n",
              "      <td>4.94</td>\n",
              "      <td>44.08</td>\n",
              "      <td>5.28</td>\n",
              "      <td>45.70</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.126</td>\n",
              "      <td>5.28</td>\n",
              "      <td>45.70</td>\n",
              "      <td>4.94</td>\n",
              "      <td>44.08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>2.504</td>\n",
              "      <td>5.28</td>\n",
              "      <td>45.70</td>\n",
              "      <td>4.94</td>\n",
              "      <td>44.08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>4.995</td>\n",
              "      <td>5.28</td>\n",
              "      <td>45.70</td>\n",
              "      <td>4.94</td>\n",
              "      <td>44.08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>-10.0</td>\n",
              "      <td>2.752</td>\n",
              "      <td>2.0</td>\n",
              "      <td>-10.0</td>\n",
              "      <td>4.526</td>\n",
              "      <td>49.02</td>\n",
              "      <td>0.00</td>\n",
              "      <td>50.98</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>2.744</td>\n",
              "      <td>2.0</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>2.035</td>\n",
              "      <td>49.02</td>\n",
              "      <td>0.00</td>\n",
              "      <td>50.98</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>2.602</td>\n",
              "      <td>2.0</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>0.717</td>\n",
              "      <td>49.02</td>\n",
              "      <td>0.00</td>\n",
              "      <td>50.98</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.908</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.319</td>\n",
              "      <td>92.91</td>\n",
              "      <td>0.00</td>\n",
              "      <td>7.09</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.061</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.318</td>\n",
              "      <td>95.82</td>\n",
              "      <td>0.00</td>\n",
              "      <td>4.18</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.110</td>\n",
              "      <td>2.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.681</td>\n",
              "      <td>51.06</td>\n",
              "      <td>0.00</td>\n",
              "      <td>48.94</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>-1.963</td>\n",
              "      <td>2.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>2.377</td>\n",
              "      <td>0.00</td>\n",
              "      <td>50.98</td>\n",
              "      <td>0.00</td>\n",
              "      <td>49.02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>-10.0</td>\n",
              "      <td>3.107</td>\n",
              "      <td>5.0</td>\n",
              "      <td>-10.0</td>\n",
              "      <td>3.792</td>\n",
              "      <td>49.02</td>\n",
              "      <td>0.00</td>\n",
              "      <td>50.98</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>3.079</td>\n",
              "      <td>5.0</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>1.368</td>\n",
              "      <td>49.02</td>\n",
              "      <td>0.00</td>\n",
              "      <td>50.98</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>2.871</td>\n",
              "      <td>5.0</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>0.379</td>\n",
              "      <td>77.01</td>\n",
              "      <td>0.00</td>\n",
              "      <td>22.99</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.258</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.156</td>\n",
              "      <td>96.91</td>\n",
              "      <td>0.00</td>\n",
              "      <td>3.09</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.502</td>\n",
              "      <td>5.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.128</td>\n",
              "      <td>99.05</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.95</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1.000</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.146</td>\n",
              "      <td>99.02</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.98</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.351</td>\n",
              "      <td>5.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.403</td>\n",
              "      <td>84.77</td>\n",
              "      <td>0.00</td>\n",
              "      <td>15.23</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>0.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>-10.0</td>\n",
              "      <td>3.365</td>\n",
              "      <td>10.0</td>\n",
              "      <td>-10.0</td>\n",
              "      <td>2.607</td>\n",
              "      <td>49.02</td>\n",
              "      <td>0.00</td>\n",
              "      <td>50.98</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>0.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>3.256</td>\n",
              "      <td>10.0</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>0.743</td>\n",
              "      <td>72.23</td>\n",
              "      <td>0.00</td>\n",
              "      <td>27.77</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>0.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>3.000</td>\n",
              "      <td>10.0</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>0.195</td>\n",
              "      <td>88.96</td>\n",
              "      <td>0.00</td>\n",
              "      <td>11.04</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>0.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.492</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.081</td>\n",
              "      <td>97.99</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2.01</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>0.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.821</td>\n",
              "      <td>10.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.067</td>\n",
              "      <td>99.30</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.70</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>0.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1.392</td>\n",
              "      <td>10.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.064</td>\n",
              "      <td>99.02</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.98</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>0.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.989</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.072</td>\n",
              "      <td>99.36</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    init_a  init_b  init_c  final_a  ...   ftpt   ffpt   ftpf   ffpf\n",
              "0      0.0   -10.0   -10.0    0.621  ...  35.79   0.00  64.21   0.00\n",
              "1      0.0   -10.0    -5.0    0.906  ...  29.17   0.00  70.83   0.00\n",
              "2      0.0   -10.0    -2.0    4.347  ...  10.27   0.00  89.73   0.00\n",
              "3      0.0   -10.0     0.0    4.419  ...   0.09   0.00  99.91   0.00\n",
              "4      0.0   -10.0     2.0    4.455  ...  10.65   0.00  89.35   0.00\n",
              "5      0.0   -10.0     5.0    4.480  ...  27.66   0.00  72.34   0.00\n",
              "6      0.0   -10.0    10.0    4.492  ...  50.98   0.00  49.02   0.00\n",
              "7      0.0    -5.0   -10.0    0.047  ...  46.04   0.00  53.96   0.00\n",
              "8      0.0    -5.0    -5.0    0.609  ...  37.18   0.00  62.82   0.00\n",
              "9      0.0    -5.0    -2.0    1.013  ...  25.16   0.00  74.84   0.00\n",
              "10     0.0    -5.0     0.0    3.897  ...   0.09   0.00  99.91   0.00\n",
              "11     0.0    -5.0     2.0    3.994  ...  22.42   0.00  77.58   0.00\n",
              "12     0.0    -5.0     5.0    4.033  ...  50.98   0.00  49.02   0.00\n",
              "13     0.0    -5.0    10.0    4.038  ...  50.98   0.00  49.02   0.00\n",
              "14     0.0    -2.0   -10.0   -1.925  ...   0.00  49.02   0.00  50.98\n",
              "15     0.0    -2.0    -5.0   -0.714  ...   0.00  53.86   0.00  46.14\n",
              "16     0.0    -2.0    -2.0    0.504  ...  50.70   0.00  49.30   0.00\n",
              "17     0.0    -2.0     0.0    3.002  ...   0.66   0.00  99.34   0.00\n",
              "18     0.0    -2.0     2.0    3.357  ...  50.98   0.00  49.02   0.00\n",
              "19     0.0    -2.0     5.0    3.416  ...  50.98   0.00  49.02   0.00\n",
              "20     0.0    -2.0    10.0    3.419  ...  50.98   0.00  49.02   0.00\n",
              "21     0.0     0.0   -10.0    0.000  ...   4.94  44.08   5.28  45.70\n",
              "22     0.0     0.0    -5.0    0.000  ...   4.94  44.08   5.28  45.70\n",
              "23     0.0     0.0    -2.0    0.000  ...   4.94  44.08   5.28  45.70\n",
              "24     0.0     0.0     0.0    0.000  ...   4.94  44.08   5.28  45.70\n",
              "25     0.0     0.0     2.0    0.000  ...   5.28  45.70   4.94  44.08\n",
              "26     0.0     0.0     5.0    0.000  ...   5.28  45.70   4.94  44.08\n",
              "27     0.0     0.0    10.0    0.000  ...   5.28  45.70   4.94  44.08\n",
              "28     0.0     2.0   -10.0    2.752  ...  49.02   0.00  50.98   0.00\n",
              "29     0.0     2.0    -5.0    2.744  ...  49.02   0.00  50.98   0.00\n",
              "30     0.0     2.0    -2.0    2.602  ...  49.02   0.00  50.98   0.00\n",
              "31     0.0     2.0     0.0    1.908  ...  92.91   0.00   7.09   0.00\n",
              "32     0.0     2.0     2.0    1.061  ...  95.82   0.00   4.18   0.00\n",
              "33     0.0     2.0     5.0    0.110  ...  51.06   0.00  48.94   0.00\n",
              "34     0.0     2.0    10.0   -1.963  ...   0.00  50.98   0.00  49.02\n",
              "35     0.0     5.0   -10.0    3.107  ...  49.02   0.00  50.98   0.00\n",
              "36     0.0     5.0    -5.0    3.079  ...  49.02   0.00  50.98   0.00\n",
              "37     0.0     5.0    -2.0    2.871  ...  77.01   0.00  22.99   0.00\n",
              "38     0.0     5.0     0.0    2.258  ...  96.91   0.00   3.09   0.00\n",
              "39     0.0     5.0     2.0    1.502  ...  99.05   0.00   0.95   0.00\n",
              "40     0.0     5.0     5.0    1.000  ...  99.02   0.00   0.98   0.00\n",
              "41     0.0     5.0    10.0    0.351  ...  84.77   0.00  15.23   0.00\n",
              "42     0.0    10.0   -10.0    3.365  ...  49.02   0.00  50.98   0.00\n",
              "43     0.0    10.0    -5.0    3.256  ...  72.23   0.00  27.77   0.00\n",
              "44     0.0    10.0    -2.0    3.000  ...  88.96   0.00  11.04   0.00\n",
              "45     0.0    10.0     0.0    2.492  ...  97.99   0.00   2.01   0.00\n",
              "46     0.0    10.0     2.0    1.821  ...  99.30   0.00   0.70   0.00\n",
              "47     0.0    10.0     5.0    1.392  ...  99.02   0.00   0.98   0.00\n",
              "48     0.0    10.0    10.0    0.989  ...  99.36   0.00   0.64   0.00\n",
              "\n",
              "[49 rows x 11 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1m2PyeaJcO7H"
      },
      "source": [
        "# df_test.to_csv(\"linear_linear_altmin_focus_first.csv\")"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mipCcN0cdXO"
      },
      "source": [
        "#df_train.to_csv(\"train_1.csv\")\n",
        "df_test.to_csv(\"test_3.csv\")"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0ETaUo8mIWC"
      },
      "source": [
        "loss_heat_map = np.zeros((7,7))"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-JVi7MbOhok"
      },
      "source": [
        "loss_heat_map[0,:] = df_test[df_test.init_b == -10].train_loss\n",
        "loss_heat_map[1,:] = df_test[df_test.init_b == -5].train_loss\n",
        "loss_heat_map[2,:] = df_test[df_test.init_b == -2].train_loss\n",
        "loss_heat_map[3,:] = df_test[df_test.init_b == 0].train_loss\n",
        "loss_heat_map[4,:] = df_test[df_test.init_b == 2].train_loss\n",
        "loss_heat_map[5,:] = df_test[df_test.init_b == 5].train_loss\n",
        "loss_heat_map[6,:] = df_test[df_test.init_b == 10].train_loss\n"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02VCWCublw_j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdc904f5-279f-4543-9de1-53de727a0436"
      },
      "source": [
        "loss_heat_map"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[4.603, 6.151, 5.129, 5.021, 5.12 , 5.663, 7.522],\n",
              "       [0.998, 2.419, 3.296, 2.653, 2.851, 3.835, 6.254],\n",
              "       [2.378, 0.691, 1.218, 1.374, 1.72 , 3.031, 5.517],\n",
              "       [5.005, 2.509, 1.128, 0.693, 1.126, 2.504, 4.995],\n",
              "       [4.526, 2.035, 0.717, 0.319, 0.318, 0.681, 2.377],\n",
              "       [3.792, 1.368, 0.379, 0.156, 0.128, 0.146, 0.403],\n",
              "       [2.607, 0.743, 0.195, 0.081, 0.067, 0.064, 0.072]])"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mb4xTTxte7rg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        },
        "outputId": "4511d0bd-9669-4173-8355-f7817925cdbb"
      },
      "source": [
        "import seaborn as sns\n",
        "\n",
        "fig = plt.figure(figsize=(8,8))\n",
        "ax = sns.heatmap( loss_heat_map , linewidth = 0.5 , cmap = 'coolwarm' ,annot=True)\n",
        "\n",
        "\n",
        "ax.set_xticks(np.arange(len(c)))\n",
        "ax.set_yticks(np.arange(len(b)))\n",
        "\n",
        "ax.set_xticklabels(c)\n",
        "ax.set_yticklabels(b)\n",
        "# Rotate the tick labels and set their alignment.\n",
        "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
        "         rotation_mode=\"anchor\")\n",
        "\n",
        "plt.setp(ax.get_yticklabels(), rotation=45, ha=\"right\",\n",
        "         rotation_mode=\"anchor\")\n",
        "\n",
        "plt.xlabel(\"c\")\n",
        "plt.ylabel(\"b\")\n",
        "# for i in range(len(b)):\n",
        "#     for j in range(len(c)):\n",
        "#         text = ax.text(j, i, loss_heat_map[i, j],\n",
        "#                         color=\"w\",)\n",
        "fig.tight_layout() \n",
        "plt.title(\"Minimizing a for fix value of b and c,loss values here\")"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Minimizing a for fix value of b and c,loss values here')"
            ]
          },
          "metadata": {},
          "execution_count": 44
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x576 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEV22UlEcvby"
      },
      "source": [
        "a_value_heat_map = np.zeros((7,7))"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ra93yQJJc7rU"
      },
      "source": [
        "a_value_heat_map[0,:] = df_test[df_test.init_b == -10].final_a\n",
        "a_value_heat_map[1,:] = df_test[df_test.init_b == -5].final_a\n",
        "a_value_heat_map[2,:] = df_test[df_test.init_b == -2].final_a\n",
        "a_value_heat_map[3,:] = df_test[df_test.init_b == 0].final_a\n",
        "a_value_heat_map[4,:] = df_test[df_test.init_b == 2].final_a\n",
        "a_value_heat_map[5,:] = df_test[df_test.init_b == 5].final_a\n",
        "a_value_heat_map[6,:] = df_test[df_test.init_b == 10].final_a"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85YAdELIdXsc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f51764ff-410b-4847-eb0f-b2a06c44b734"
      },
      "source": [
        "a_value_heat_map"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.621,  0.906,  4.347,  4.419,  4.455,  4.48 ,  4.492],\n",
              "       [ 0.047,  0.609,  1.013,  3.897,  3.994,  4.033,  4.038],\n",
              "       [-1.925, -0.714,  0.504,  3.002,  3.357,  3.416,  3.419],\n",
              "       [ 0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
              "       [ 2.752,  2.744,  2.602,  1.908,  1.061,  0.11 , -1.963],\n",
              "       [ 3.107,  3.079,  2.871,  2.258,  1.502,  1.   ,  0.351],\n",
              "       [ 3.365,  3.256,  3.   ,  2.492,  1.821,  1.392,  0.989]])"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nu6ai-LdZAU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        },
        "outputId": "e6d4e12c-3370-4816-a5b5-38b1da4886ae"
      },
      "source": [
        "fig = plt.figure(figsize=(8,8))\n",
        "ax = sns.heatmap( a_value_heat_map , linewidth = 0.5 , cmap = 'coolwarm' ,annot=True)\n",
        "\n",
        "\n",
        "ax.set_xticks(np.arange(len(c)))\n",
        "ax.set_yticks(np.arange(len(b)))\n",
        "\n",
        "ax.set_xticklabels(c)\n",
        "ax.set_yticklabels(b)\n",
        "# Rotate the tick labels and set their alignment.\n",
        "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
        "         rotation_mode=\"anchor\")\n",
        "\n",
        "plt.setp(ax.get_yticklabels(), rotation=45, ha=\"right\",\n",
        "         rotation_mode=\"anchor\")\n",
        "plt.xlabel(\"c\")\n",
        "plt.ylabel(\"b\")\n",
        "\n",
        "# for i in range(len(b)):\n",
        "#     for j in range(len(c)):\n",
        "#         text = ax.text(j, i, loss_heat_map[i, j],\n",
        "#                         color=\"w\",)\n",
        "fig.tight_layout() \n",
        "plt.title(\"Minimizing a for fix value of b and c,final a values here\")"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Minimizing a for fix value of b and c,final a values here')"
            ]
          },
          "metadata": {},
          "execution_count": 48
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x576 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yiE4iLhOiRdp"
      },
      "source": [
        " "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
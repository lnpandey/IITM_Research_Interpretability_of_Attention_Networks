{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "linear_linear_adam_simultaneous_27_exp.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2_J4Rw2r0SQ",
        "outputId": "cb647704-5c23-46e6-bbf8-e97b365dea4e"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "from tqdm import tqdm\n",
        "%matplotlib inline\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn import functional as F\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6fjud_Fr0Sa"
      },
      "source": [
        "# Generate dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqdXHO0Cr0Sd",
        "outputId": "87989796-17b1-49cc-c923-292483a9eb96"
      },
      "source": [
        "np.random.seed(12)\n",
        "y = np.random.randint(0,3,500)\n",
        "idx= []\n",
        "for i in range(3):\n",
        "    print(i,sum(y==i))\n",
        "    idx.append(y==i)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 174\n",
            "1 163\n",
            "2 163\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddhXyODwr0Sk"
      },
      "source": [
        "x = np.zeros((500,))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyV3N2DIr0Sp"
      },
      "source": [
        "np.random.seed(12)\n",
        "x[idx[0]] = np.random.uniform(low =-1,high =0,size= sum(idx[0]))\n",
        "x[idx[1]] = np.random.uniform(low =0,high =1,size= sum(idx[1]))\n",
        "x[idx[2]] = np.random.uniform(low =2,high =3,size= sum(idx[2]))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qh1mDScsU07I",
        "outputId": "2f527c4f-4bf1-4c6c-cf2d-adadd615d396"
      },
      "source": [
        "x[idx[0]][0], x[idx[2]][5] "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-0.8458371576203276, 2.2459800877058114)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Vr5ErQ_wSrV",
        "outputId": "df25824a-4cc0-46d3-8745-1ee0c61fb43a"
      },
      "source": [
        "print(x.shape,y.shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(500,) (500,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NG-3RpffwU_i"
      },
      "source": [
        "idx= []\n",
        "for i in range(3):\n",
        "  idx.append(y==i)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "id": "hJ8Jm7YUr0St",
        "outputId": "3a55b4fe-494f-4daa-f0df-e2bb8797abdd"
      },
      "source": [
        "for i in range(3):\n",
        "    y= np.zeros(x[idx[i]].shape[0])\n",
        "    plt.scatter(x[idx[i]],y,label=\"class_\"+str(i))\n",
        "plt.legend()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7efee443b790>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lMBZEHNBlF2",
        "outputId": "cf535b9e-95ef-470a-d534-efedcca9f98e"
      },
      "source": [
        "bg_idx = [ np.where(idx[2] == True)[0]]\n",
        "\n",
        "bg_idx = np.concatenate(bg_idx, axis = 0)\n",
        "bg_idx.shape"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(163,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "blRbGZHeCwXU",
        "outputId": "697d39f1-e39e-4d66-a172-d18c3a9165e7"
      },
      "source": [
        "np.unique(bg_idx).shape"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(163,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y43sWeX7C15F"
      },
      "source": [
        "x = x - np.mean(x[bg_idx], axis = 0, keepdims = True)\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ooII7N6UDWe0",
        "outputId": "0adef6ea-c5ef-456a-cc5f-1793adaf3be6"
      },
      "source": [
        "np.mean(x[bg_idx], axis = 0, keepdims = True), np.mean(x, axis = 0, keepdims = True)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([-1.00805526e-16]), array([-1.7163962]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g21bvPRYDL9k"
      },
      "source": [
        "x = x/np.std(x[bg_idx], axis = 0, keepdims = True)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GtFvIeHsDZJk",
        "outputId": "0c3f5206-e4fb-46cb-ab9e-5a00e83c74f5"
      },
      "source": [
        "np.std(x[bg_idx], axis = 0, keepdims = True), np.std(x, axis = 0, keepdims = True)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([1.]), array([4.37808847]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "id": "8-VLhUfDDeHt",
        "outputId": "6d856e11-4134-4881-92d2-0a36db907555"
      },
      "source": [
        "for i in range(3):\n",
        "    y= np.zeros(x[idx[i]].shape[0])\n",
        "    plt.scatter(x[idx[i]],y,label=\"class_\"+str(i))\n",
        "plt.legend()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7efee3f04450>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfFHcZJOr0Sz"
      },
      "source": [
        "foreground_classes = {'class_0','class_1' }\n",
        "\n",
        "background_classes = {'class_2'}"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OplNpNQVr0S2",
        "outputId": "f3472549-8e24-419c-ead6-ff219b0430a1"
      },
      "source": [
        "fg_class  = np.random.randint(0,2)\n",
        "fg_idx = np.random.randint(0,9)\n",
        "\n",
        "a = []\n",
        "for i in range(9):\n",
        "    if i == fg_idx:\n",
        "        b = np.random.choice(np.where(idx[fg_class]==True)[0],size=1)\n",
        "        a.append(x[b])\n",
        "        print(\"foreground \"+str(fg_class)+\" present at \" + str(fg_idx))\n",
        "    else:\n",
        "        bg_class = np.random.randint(2,3)\n",
        "        b = np.random.choice(np.where(idx[bg_class]==True)[0],size=1)\n",
        "        a.append(x[b])\n",
        "        print(\"background \"+str(bg_class)+\" present at \" + str(i))\n",
        "a = np.concatenate(a,axis=0)\n",
        "print(a.shape)\n",
        "\n",
        "print(fg_class , fg_idx)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "background 2 present at 0\n",
            "background 2 present at 1\n",
            "background 2 present at 2\n",
            "background 2 present at 3\n",
            "background 2 present at 4\n",
            "background 2 present at 5\n",
            "background 2 present at 6\n",
            "background 2 present at 7\n",
            "foreground 1 present at 8\n",
            "(9,)\n",
            "1 8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dwZVmmRBr0S8",
        "outputId": "d66bb5cb-6c8d-406e-f352-266535044788"
      },
      "source": [
        "a.shape"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OoxzYI-ur0S_",
        "outputId": "787c3d1e-ee82-4441-b936-1f1fe4787533"
      },
      "source": [
        "np.reshape(a,(9,1))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.80241043],\n",
              "       [ 1.00228507],\n",
              "       [-0.81132988],\n",
              "       [ 1.2774409 ],\n",
              "       [ 0.57398646],\n",
              "       [-0.91850331],\n",
              "       [ 0.6600648 ],\n",
              "       [ 0.57398646],\n",
              "       [-7.63155537]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4ruI0cxr0TE"
      },
      "source": [
        "a=np.reshape(a,(3,3))"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 288
        },
        "id": "RTUTFhJIr0TI",
        "outputId": "e3d377d9-a4dc-4687-a4d6-e05f6187ba96"
      },
      "source": [
        "plt.imshow(a)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7efee3ea0f10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqbvfbwVr0TN"
      },
      "source": [
        "desired_num = 2000\n",
        "mosaic_list_of_images =[]\n",
        "mosaic_label = []\n",
        "fore_idx=[]\n",
        "for j in range(desired_num):\n",
        "    np.random.seed(j)\n",
        "    fg_class  = np.random.randint(0,2)\n",
        "    fg_idx = 0\n",
        "    a = []\n",
        "    for i in range(9):\n",
        "        if i == fg_idx:\n",
        "            b = np.random.choice(np.where(idx[fg_class]==True)[0],size=1)\n",
        "            a.append(x[b])\n",
        "#             print(\"foreground \"+str(fg_class)+\" present at \" + str(fg_idx))\n",
        "        else:\n",
        "            bg_class = np.random.randint(2,3)\n",
        "            b = np.random.choice(np.where(idx[bg_class]==True)[0],size=1)\n",
        "            a.append(x[b])\n",
        "#             print(\"background \"+str(bg_class)+\" present at \" + str(i))\n",
        "    a = np.concatenate(a,axis=0)\n",
        "    mosaic_list_of_images.append(np.reshape(a,(9,1)))\n",
        "    mosaic_label.append(fg_class)\n",
        "    fore_idx.append(fg_idx)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOsFmWfMr0TR"
      },
      "source": [
        "mosaic_list_of_images = np.concatenate(mosaic_list_of_images,axis=1).T\n"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2aIPMgLXNiXW",
        "outputId": "21983dda-acb3-4863-98e4-42505882e352"
      },
      "source": [
        "mosaic_list_of_images.shape, mosaic_list_of_images[0]"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((2000, 9), array([-10.62460519,   1.43580093,   0.13695373,  -0.4252276 ,\n",
              "         -0.91850331,  -0.12909751,  -1.73154424,   0.75582313,\n",
              "          0.6640638 ]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A3qcsbbzPfRG",
        "outputId": "a1d3e2cb-4065-4341-993c-daade80f3cef"
      },
      "source": [
        "for j in range(9):\n",
        "  print(mosaic_list_of_images[0][j])\n",
        "  "
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-10.624605191016078\n",
            "1.4358009314866034\n",
            "0.13695372724012772\n",
            "-0.4252275951078302\n",
            "-0.9185033095974854\n",
            "-0.12909750858981067\n",
            "-1.7315442430857817\n",
            "0.7558231343879587\n",
            "0.6640638025673503\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPoIwbMHx44n"
      },
      "source": [
        "class MosaicDataset(Dataset):\n",
        "  \"\"\"MosaicDataset dataset.\"\"\"\n",
        "\n",
        "  def __init__(self, mosaic_list_of_images, mosaic_label, fore_idx):\n",
        "    \"\"\"\n",
        "      Args:\n",
        "        csv_file (string): Path to the csv file with annotations.\n",
        "        root_dir (string): Directory with all the images.\n",
        "        transform (callable, optional): Optional transform to be applied\n",
        "            on a sample.\n",
        "    \"\"\"\n",
        "    self.mosaic = mosaic_list_of_images\n",
        "    self.label = mosaic_label\n",
        "    self.fore_idx = fore_idx\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.label)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.mosaic[idx] , self.label[idx], self.fore_idx[idx]\n",
        "\n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOPAJQJeW8Ah"
      },
      "source": [
        "batch = 250\n",
        "msd1 = MosaicDataset(mosaic_list_of_images[0:1000], mosaic_label[0:1000] , fore_idx[0:1000])\n",
        "train_loader = DataLoader( msd1 ,batch_size= batch ,shuffle=True)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjNiQgxZW8bA"
      },
      "source": [
        "batch = 250\n",
        "msd2 = MosaicDataset(mosaic_list_of_images[1000:2000], mosaic_label[1000:2000] , fore_idx[1000:2000])\n",
        "test_loader = DataLoader( msd2 ,batch_size= batch ,shuffle=True)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30ZAjix3x8CM"
      },
      "source": [
        "class Focus(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Focus, self).__init__()\n",
        "\n",
        "    self.fc1 = nn.Linear(1, 1, bias= False)\n",
        "    # self.fc2 = nn.Linear(2, 1)\n",
        "\n",
        "  def forward(self,z):  #y is avg image #z batch of list of 9 images\n",
        "    y = torch.zeros([batch], dtype=torch.float64)\n",
        "    x = torch.zeros([batch,9],dtype=torch.float64)\n",
        "    y = y.to(\"cuda\")\n",
        "    x = x.to(\"cuda\")\n",
        "    # print(x.shape, z.shape)\n",
        "    for i in range(9):\n",
        "      # print(z[:,i].shape)\n",
        "      # print(self.helper(z[:,i])[:,0].shape)\n",
        "      x[:,i] = self.helper(z[:,i])[:,0]\n",
        "    # print(x.shape, z.shape)\n",
        "    x = F.softmax(x,dim=1)\n",
        "    # print(x.shape, z.shape)\n",
        "    # x1 = x[:,0]\n",
        "    # print(torch.mul(x[:,0],z[:,0]).shape)\n",
        "\n",
        "    for i in range(9):            \n",
        "      # x1 = x[:,i]          \n",
        "      y = y + torch.mul(x[:,i],z[:,i])\n",
        "\n",
        "    # print(x.shape, y.shape)\n",
        "    return x, y\n",
        "    \n",
        "  def helper(self, x):\n",
        "    x = x.view(-1, 1)\n",
        "    # x = F.relu(self.fc1(x))\n",
        "    x = (self.fc1(x))\n",
        "\n",
        "    return x"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dYXnywAD-4l"
      },
      "source": [
        "class Classification(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Classification, self).__init__()\n",
        "    self.fc1 = nn.Linear(1, 1)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x.view(-1, 1)\n",
        "    x = self.fc1(x)\n",
        "    # print(x.shape)\n",
        "    return x"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yl41sE8vFERk"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "def train(train_loader, test_loader, focus_net, classify):    \n",
        "  criterion = nn.BCEWithLogitsLoss()\n",
        "  optimizer_classify = optim.Adam(classify.parameters(), lr=0.01 ) #, momentum=0.9)\n",
        "  optimizer_focus = optim.Adam(focus_net.parameters(), lr=0.01 ) #, momentum=0.9)\n",
        "\n",
        "  print('-'*50)\n",
        "  print(focus_net.fc1.weight, classify.fc1.weight, classify.fc1.bias)\n",
        "  nos_epochs = 1000\n",
        "  loss_ret=0.0\n",
        "  for epoch in range(nos_epochs):  # loop over the dataset multiple times\n",
        "    \n",
        "    running_loss = 0.0\n",
        "    epoch_loss = []\n",
        "    cnt=0\n",
        "\n",
        "    iteration = desired_num // batch\n",
        "    \n",
        "    #training data set\n",
        "    \n",
        "    for i, data in  enumerate(train_loader):\n",
        "      inputs , labels , fore_idx = data\n",
        "      inputs, labels = inputs.to(\"cuda\"), labels.to(\"cuda\")\n",
        "      inputs = inputs.double()\n",
        "      labels = labels.float()\n",
        "      # zero the parameter gradients\n",
        "      \n",
        "      optimizer_focus.zero_grad()\n",
        "      optimizer_classify.zero_grad()\n",
        "      \n",
        "      alphas, avg_images = focus_net(inputs)\n",
        "      outputs = classify(avg_images)\n",
        "\n",
        "      predicted = np.round(torch.sigmoid(outputs.data).cpu().numpy())\n",
        "      # print(predicted.shape)\n",
        "      # print(outputs.shape,labels.shape)\n",
        "      # print(outputs)\n",
        "      # print(labels)\n",
        "      loss = criterion(outputs[:,0], labels) \n",
        "      loss.backward()\n",
        "      optimizer_focus.step()\n",
        "      optimizer_classify.step()\n",
        "\n",
        "      running_loss += loss.item()\n",
        "      # mini = 3\n",
        "      # if cnt % mini == mini-1 :    # print every 40 mini-batches\n",
        "      epoch_loss.append(running_loss)\n",
        "      running_loss = 0.0\n",
        "      cnt=cnt+1\n",
        "    loss_ret = np.mean(epoch_loss)\n",
        "    if(epoch%200==0):\n",
        "      print('[%d, %5d] loss: %.3f' %(epoch + 1, cnt + 1, np.mean(epoch_loss)))\n",
        "    if(np.mean(epoch_loss) <= 0.001):\n",
        "        break;\n",
        "\n",
        "  with torch.no_grad():\n",
        "    focus_true_pred_true =0\n",
        "    focus_false_pred_true =0\n",
        "    focus_true_pred_false =0\n",
        "    focus_false_pred_false =0\n",
        "\n",
        "    argmax_more_than_half = 0\n",
        "    argmax_less_than_half =0\n",
        "    for data in test_loader:\n",
        "      inputs, labels , fore_idx = data\n",
        "      inputs = inputs.double()\n",
        "      inputs, labels = inputs.to(\"cuda\"), labels.to(\"cuda\")\n",
        "      alphas, avg_images = focus_net(inputs)\n",
        "      outputs = classify(avg_images)\n",
        "      predicted = np.round(torch.sigmoid(outputs.data).cpu().numpy())\n",
        "\n",
        "      for j in range (batch):\n",
        "        focus = torch.argmax(alphas[j])\n",
        "\n",
        "        if(alphas[j][focus] >= 0.5):\n",
        "          argmax_more_than_half +=1\n",
        "        else:\n",
        "          argmax_less_than_half +=1\n",
        "\n",
        "        if(focus == fore_idx[j] and predicted[j] == labels[j].item()):\n",
        "          focus_true_pred_true += 1\n",
        "\n",
        "        elif(focus != fore_idx[j] and predicted[j] == labels[j].item()):\n",
        "          focus_false_pred_true +=1\n",
        "\n",
        "        elif(focus == fore_idx[j] and predicted[j] != labels[j].item()):\n",
        "          focus_true_pred_false +=1\n",
        "\n",
        "        elif(focus != fore_idx[j] and predicted[j] != labels[j].item()):\n",
        "          focus_false_pred_false +=1\n",
        "\n",
        "  print('Finished Training')  \n",
        "  return loss_ret, argmax_more_than_half/10, focus_true_pred_true/10, focus_false_pred_true/10, focus_true_pred_false/10 , focus_false_pred_false/10, focus_net.fc1.weight.item(), classify.fc1.weight.item(), classify.fc1.bias.item()\n",
        "    \n"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fd7ScJj9g6rl"
      },
      "source": [
        "a = [-0.1, 0.0, 0.1]\n",
        "b = [-0.1, 0.0, 0.1]\n",
        "c = [-0.1, 0.0, 0.1]"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0rXlZZfUK3B"
      },
      "source": [
        "# torch.manual_seed(12)\n",
        "# focus_net = Focus().double()\n",
        "# focus_net.fc1.weight = torch.nn.Parameter(torch.tensor(np.array([[0.0]])))\n",
        "# torch.manual_seed(12)\n",
        "# classify = Classification().double()\n",
        "# classify.fc1.weight = torch.nn.Parameter(torch.tensor(np.array([[0.0]])))\n",
        "# classify.fc1.bias = torch.nn.Parameter(torch.tensor(np.array([0.0])))\n",
        "# focus_net = focus_net.to(\"cuda\")\n",
        "# classify = classify.to(\"cuda\")\n",
        "# # print(\"--\"*40,\"a,b,c = \",a[i],b[j],c[k])\n",
        "# print(focus_net.fc1.weight, classify.fc1.weight, classify.fc1.bias)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIzudMBEuFQl"
      },
      "source": [
        "# np.round(focus_net.fc1.weight.item(),3), np.round(99.3, 3)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEXvq7EWUck2"
      },
      "source": [
        "# train(train_loader, test_loader, focus_net, classify)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zo5QyZobhHpM",
        "outputId": "b3138dd7-1580-4a86-d488-8e6077e9ddff"
      },
      "source": [
        "all_loss=[]\n",
        "all_alphas_more_than_half=[]\n",
        "all_ftpt=[]\n",
        "all_ffpt=[]\n",
        "all_ftpf = []\n",
        "all_ffpf= []\n",
        "init_a = []\n",
        "init_b = []\n",
        "init_c = []\n",
        "final_a=[]\n",
        "final_b = []\n",
        "final_c = []\n",
        "\n",
        "for i in range(3):\n",
        "  for j in range(3):\n",
        "    for k in range(3):\n",
        "      torch.manual_seed(12)\n",
        "      focus_net = Focus().double()\n",
        "      focus_net.fc1.weight = torch.nn.Parameter(torch.tensor(np.array([[a[i]]])))\n",
        "      torch.manual_seed(12)\n",
        "      classify = Classification().double()\n",
        "      classify.fc1.weight = torch.nn.Parameter(torch.tensor(np.array([[b[j]]])))\n",
        "      classify.fc1.bias = torch.nn.Parameter(torch.tensor(np.array([c[k]])))\n",
        "      focus_net = focus_net.to(\"cuda\")\n",
        "      classify = classify.to(\"cuda\")\n",
        "      print(\"--\"*40,\"a,b,c = \",a[i],b[j],c[k])\n",
        "      cost, alpha_per, ftpt, ffpt, ftpf, ffpf, f_a, f_b, f_c = train(train_loader, test_loader, focus_net, classify)\n",
        "      print(cost, alpha_per, ftpt, ffpt, ftpf, ffpf)\n",
        "      init_a.append(a[i])\n",
        "      init_b.append(b[j])\n",
        "      init_c.append(c[k])\n",
        "      final_a.append(np.round(f_a,3))\n",
        "      final_b.append(np.round(f_b,3))\n",
        "      final_c.append(np.round(f_c,3))\n",
        "      all_loss.append(np.round(cost,3))\n",
        "      all_alphas_more_than_half.append(alpha_per)\n",
        "      all_ftpt.append(ftpt)\n",
        "      all_ffpt.append(ffpt)\n",
        "      all_ftpf.append(ftpf)\n",
        "      all_ffpf.append(ffpf)\n",
        "\n"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------------------------------------------------------------------------------- a,b,c =  -0.1 -0.1 -0.1\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[-0.1000]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[-0.1000]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([-0.1000], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     5] loss: 0.718\n",
            "[201,     5] loss: 0.190\n",
            "[401,     5] loss: 0.124\n",
            "[601,     5] loss: 0.096\n",
            "[801,     5] loss: 0.079\n",
            "Finished Training\n",
            "0.06691071949899197 55.6 100.0 0.0 0.0 0.0\n",
            "-------------------------------------------------------------------------------- a,b,c =  -0.1 -0.1 0.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[-0.1000]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[-0.1000]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([0.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     5] loss: 0.720\n",
            "[201,     5] loss: 0.189\n",
            "[401,     5] loss: 0.123\n",
            "[601,     5] loss: 0.095\n",
            "[801,     5] loss: 0.079\n",
            "Finished Training\n",
            "0.06682517938315868 55.6 100.0 0.0 0.0 0.0\n",
            "-------------------------------------------------------------------------------- a,b,c =  -0.1 -0.1 0.1\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[-0.1000]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[-0.1000]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([0.1000], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     5] loss: 0.724\n",
            "[201,     5] loss: 0.190\n",
            "[401,     5] loss: 0.124\n",
            "[601,     5] loss: 0.096\n",
            "[801,     5] loss: 0.079\n",
            "Finished Training\n",
            "0.06686902046203613 55.6 100.0 0.0 0.0 0.0\n",
            "-------------------------------------------------------------------------------- a,b,c =  -0.1 0.0 -0.1\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[-0.1000]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([-0.1000], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     5] loss: 0.691\n",
            "[201,     5] loss: 0.197\n",
            "[401,     5] loss: 0.127\n",
            "[601,     5] loss: 0.098\n",
            "[801,     5] loss: 0.081\n",
            "Finished Training\n",
            "0.06836719438433647 55.6 100.0 0.0 0.0 0.0\n",
            "-------------------------------------------------------------------------------- a,b,c =  -0.1 0.0 0.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[-0.1000]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([0.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     5] loss: 0.688\n",
            "[201,     5] loss: 0.194\n",
            "[401,     5] loss: 0.126\n",
            "[601,     5] loss: 0.097\n",
            "[801,     5] loss: 0.080\n",
            "Finished Training\n",
            "0.06781388074159622 55.6 100.0 0.0 0.0 0.0\n",
            "-------------------------------------------------------------------------------- a,b,c =  -0.1 0.0 0.1\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[-0.1000]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([0.1000], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     5] loss: 0.688\n",
            "[201,     5] loss: 0.193\n",
            "[401,     5] loss: 0.125\n",
            "[601,     5] loss: 0.097\n",
            "[801,     5] loss: 0.080\n",
            "Finished Training\n",
            "0.06761322822421789 55.7 100.0 0.0 0.0 0.0\n",
            "-------------------------------------------------------------------------------- a,b,c =  -0.1 0.1 -0.1\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[-0.1000]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[0.1000]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([-0.1000], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     5] loss: 0.670\n",
            "[201,     5] loss: 0.200\n",
            "[401,     5] loss: 0.129\n",
            "[601,     5] loss: 0.099\n",
            "[801,     5] loss: 0.081\n",
            "Finished Training\n",
            "0.06904213689267635 55.4 100.0 0.0 0.0 0.0\n",
            "-------------------------------------------------------------------------------- a,b,c =  -0.1 0.1 0.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[-0.1000]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[0.1000]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([0.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     5] loss: 0.662\n",
            "[201,     5] loss: 0.196\n",
            "[401,     5] loss: 0.127\n",
            "[601,     5] loss: 0.098\n",
            "[801,     5] loss: 0.081\n",
            "Finished Training\n",
            "0.06836522743105888 55.4 100.0 0.0 0.0 0.0\n",
            "-------------------------------------------------------------------------------- a,b,c =  -0.1 0.1 0.1\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[-0.1000]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[0.1000]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([0.1000], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     5] loss: 0.656\n",
            "[201,     5] loss: 0.193\n",
            "[401,     5] loss: 0.125\n",
            "[601,     5] loss: 0.097\n",
            "[801,     5] loss: 0.080\n",
            "Finished Training\n",
            "0.06769031193107367 55.6 100.0 0.0 0.0 0.0\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 -0.1 -0.1\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[-0.1000]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([-0.1000], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     5] loss: 0.700\n",
            "[201,     5] loss: 0.190\n",
            "[401,     5] loss: 0.124\n",
            "[601,     5] loss: 0.096\n",
            "[801,     5] loss: 0.079\n",
            "Finished Training\n",
            "0.06686716340482235 55.6 100.0 0.0 0.0 0.0\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 -0.1 0.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[-0.1000]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([0.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     5] loss: 0.700\n",
            "[201,     5] loss: 0.190\n",
            "[401,     5] loss: 0.123\n",
            "[601,     5] loss: 0.095\n",
            "[801,     5] loss: 0.079\n",
            "Finished Training\n",
            "0.06679703202098608 55.6 100.0 0.0 0.0 0.0\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 -0.1 0.1\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[-0.1000]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([0.1000], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     5] loss: 0.702\n",
            "[201,     5] loss: 0.190\n",
            "[401,     5] loss: 0.123\n",
            "[601,     5] loss: 0.095\n",
            "[801,     5] loss: 0.079\n",
            "Finished Training\n",
            "0.0667605483904481 55.6 100.0 0.0 0.0 0.0\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 0.0 -0.1\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([-0.1000], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     5] loss: 0.693\n",
            "[201,     5] loss: 0.195\n",
            "[401,     5] loss: 0.127\n",
            "[601,     5] loss: 0.098\n",
            "[801,     5] loss: 0.080\n",
            "Finished Training\n",
            "0.06816288270056248 55.5 100.0 0.0 0.0 0.0\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 0.0 0.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([0.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     5] loss: 0.692\n",
            "[201,     5] loss: 0.193\n",
            "[401,     5] loss: 0.126\n",
            "[601,     5] loss: 0.097\n",
            "[801,     5] loss: 0.080\n",
            "Finished Training\n",
            "0.06776118744164705 55.6 100.0 0.0 0.0 0.0\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 0.0 0.1\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([0.1000], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     5] loss: 0.692\n",
            "[201,     5] loss: 0.193\n",
            "[401,     5] loss: 0.126\n",
            "[601,     5] loss: 0.097\n",
            "[801,     5] loss: 0.080\n",
            "Finished Training\n",
            "0.06772794760763645 55.6 100.0 0.0 0.0 0.0\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 0.1 -0.1\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[0.1000]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([-0.1000], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     5] loss: 0.686\n",
            "[201,     5] loss: 0.198\n",
            "[401,     5] loss: 0.128\n",
            "[601,     5] loss: 0.099\n",
            "[801,     5] loss: 0.081\n",
            "Finished Training\n",
            "0.06863593496382236 55.4 100.0 0.0 0.0 0.0\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 0.1 0.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[0.1000]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([0.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     5] loss: 0.682\n",
            "[201,     5] loss: 0.194\n",
            "[401,     5] loss: 0.126\n",
            "[601,     5] loss: 0.097\n",
            "[801,     5] loss: 0.080\n",
            "Finished Training\n",
            "0.06800359766930342 55.4 100.0 0.0 0.0 0.0\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 0.1 0.1\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[0.1000]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([0.1000], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     5] loss: 0.680\n",
            "[201,     5] loss: 0.192\n",
            "[401,     5] loss: 0.125\n",
            "[601,     5] loss: 0.097\n",
            "[801,     5] loss: 0.080\n",
            "Finished Training\n",
            "0.067543163895607 55.5 100.0 0.0 0.0 0.0\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.1 -0.1 -0.1\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.1000]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[-0.1000]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([-0.1000], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     5] loss: 0.695\n",
            "[201,     5] loss: 0.692\n",
            "[401,     5] loss: 0.692\n",
            "[601,     5] loss: 0.692\n",
            "[801,     5] loss: 0.692\n",
            "Finished Training\n",
            "0.691774994134903 0.0 0.0 53.5 0.0 46.5\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.1 -0.1 0.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.1000]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[-0.1000]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([0.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     5] loss: 0.694\n",
            "[201,     5] loss: 0.692\n",
            "[401,     5] loss: 0.692\n",
            "[601,     5] loss: 0.692\n",
            "[801,     5] loss: 0.692\n",
            "Finished Training\n",
            "0.691774994134903 0.0 0.0 53.5 0.0 46.5\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.1 -0.1 0.1\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.1000]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[-0.1000]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([0.1000], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     5] loss: 0.695\n",
            "[201,     5] loss: 0.692\n",
            "[401,     5] loss: 0.692\n",
            "[601,     5] loss: 0.692\n",
            "[801,     5] loss: 0.692\n",
            "Finished Training\n",
            "0.6917750239372253 0.0 0.0 53.5 0.0 46.5\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.1 0.0 -0.1\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.1000]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([-0.1000], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     5] loss: 0.695\n",
            "[201,     5] loss: 0.692\n",
            "[401,     5] loss: 0.692\n",
            "[601,     5] loss: 0.692\n",
            "[801,     5] loss: 0.692\n",
            "Finished Training\n",
            "0.6917750239372253 0.0 0.0 53.5 0.0 46.5\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.1 0.0 0.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.1000]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([0.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     5] loss: 0.693\n",
            "[201,     5] loss: 0.193\n",
            "[401,     5] loss: 0.125\n",
            "[601,     5] loss: 0.097\n",
            "[801,     5] loss: 0.080\n",
            "Finished Training\n",
            "0.06763469334691763 55.5 100.0 0.0 0.0 0.0\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.1 0.0 0.1\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.1000]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([0.1000], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     5] loss: 0.693\n",
            "[201,     5] loss: 0.193\n",
            "[401,     5] loss: 0.126\n",
            "[601,     5] loss: 0.097\n",
            "[801,     5] loss: 0.080\n",
            "Finished Training\n",
            "0.06770370621234179 55.5 100.0 0.0 0.0 0.0\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.1 0.1 -0.1\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.1000]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[0.1000]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([-0.1000], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     5] loss: 0.694\n",
            "[201,     5] loss: 0.194\n",
            "[401,     5] loss: 0.126\n",
            "[601,     5] loss: 0.097\n",
            "[801,     5] loss: 0.080\n",
            "Finished Training\n",
            "0.06796899624168873 55.4 100.0 0.0 0.0 0.0\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.1 0.1 0.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.1000]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[0.1000]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([0.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     5] loss: 0.692\n",
            "[201,     5] loss: 0.193\n",
            "[401,     5] loss: 0.125\n",
            "[601,     5] loss: 0.097\n",
            "[801,     5] loss: 0.080\n",
            "Finished Training\n",
            "0.06765762995928526 55.4 100.0 0.0 0.0 0.0\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.1 0.1 0.1\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.1000]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[0.1000]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([0.1000], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     5] loss: 0.691\n",
            "[201,     5] loss: 0.192\n",
            "[401,     5] loss: 0.125\n",
            "[601,     5] loss: 0.097\n",
            "[801,     5] loss: 0.080\n",
            "Finished Training\n",
            "0.06762342434376478 55.4 100.0 0.0 0.0 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gQoPST5zW2t"
      },
      "source": [
        "# df_train = pd.DataFrame()\n",
        "df_test = pd.DataFrame()"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "In76SYH_zZHV"
      },
      "source": [
        "columns = [\"init_a\", \"init_b\", \"init_c\", \"final_a\", \"final_b\", \"final_c\", \"train_loss\", \"argmax > 0.5\" , \"ftpt\", \"ffpt\", \"ftpf\", \"ffpf\" ]"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BS4HtOHEzZ0E"
      },
      "source": [
        "df_test[columns[0]] = init_a\n",
        "df_test[columns[1]] = init_b\n",
        "df_test[columns[2]] = init_c\n",
        "df_test[columns[3]] = final_a\n",
        "df_test[columns[4]] = final_b\n",
        "df_test[columns[5]] = final_c\n",
        "df_test[columns[6]] = all_loss\n",
        "df_test[columns[7]] = all_alphas_more_than_half\n",
        "df_test[columns[8]] = all_ftpt\n",
        "df_test[columns[9]] = all_ffpt\n",
        "df_test[columns[10]] = all_ftpf\n",
        "df_test[columns[11]] = all_ffpf"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 896
        },
        "id": "tlaNFbwNGfju",
        "outputId": "34528290-7250-4b5c-8d47-8fadd7180f28"
      },
      "source": [
        "df_test"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>init_a</th>\n",
              "      <th>init_b</th>\n",
              "      <th>init_c</th>\n",
              "      <th>final_a</th>\n",
              "      <th>final_b</th>\n",
              "      <th>final_c</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>argmax &gt; 0.5</th>\n",
              "      <th>ftpt</th>\n",
              "      <th>ffpt</th>\n",
              "      <th>ftpf</th>\n",
              "      <th>ffpf</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.1</td>\n",
              "      <td>-0.1</td>\n",
              "      <td>-0.1</td>\n",
              "      <td>-0.256</td>\n",
              "      <td>3.368</td>\n",
              "      <td>15.120</td>\n",
              "      <td>0.067</td>\n",
              "      <td>55.6</td>\n",
              "      <td>100.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.1</td>\n",
              "      <td>-0.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.256</td>\n",
              "      <td>3.371</td>\n",
              "      <td>15.138</td>\n",
              "      <td>0.067</td>\n",
              "      <td>55.6</td>\n",
              "      <td>100.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.1</td>\n",
              "      <td>-0.1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>-0.256</td>\n",
              "      <td>3.369</td>\n",
              "      <td>15.129</td>\n",
              "      <td>0.067</td>\n",
              "      <td>55.6</td>\n",
              "      <td>100.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.1</td>\n",
              "      <td>-0.256</td>\n",
              "      <td>3.303</td>\n",
              "      <td>14.823</td>\n",
              "      <td>0.068</td>\n",
              "      <td>55.6</td>\n",
              "      <td>100.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.256</td>\n",
              "      <td>3.327</td>\n",
              "      <td>14.935</td>\n",
              "      <td>0.068</td>\n",
              "      <td>55.6</td>\n",
              "      <td>100.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>-0.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>-0.256</td>\n",
              "      <td>3.335</td>\n",
              "      <td>14.976</td>\n",
              "      <td>0.068</td>\n",
              "      <td>55.7</td>\n",
              "      <td>100.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>-0.1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>-0.1</td>\n",
              "      <td>-0.255</td>\n",
              "      <td>3.276</td>\n",
              "      <td>14.688</td>\n",
              "      <td>0.069</td>\n",
              "      <td>55.4</td>\n",
              "      <td>100.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>-0.1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.255</td>\n",
              "      <td>3.305</td>\n",
              "      <td>14.822</td>\n",
              "      <td>0.068</td>\n",
              "      <td>55.4</td>\n",
              "      <td>100.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>-0.1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>-0.255</td>\n",
              "      <td>3.334</td>\n",
              "      <td>14.959</td>\n",
              "      <td>0.068</td>\n",
              "      <td>55.6</td>\n",
              "      <td>100.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.1</td>\n",
              "      <td>-0.1</td>\n",
              "      <td>-0.256</td>\n",
              "      <td>3.370</td>\n",
              "      <td>15.129</td>\n",
              "      <td>0.067</td>\n",
              "      <td>55.6</td>\n",
              "      <td>100.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.256</td>\n",
              "      <td>3.373</td>\n",
              "      <td>15.143</td>\n",
              "      <td>0.067</td>\n",
              "      <td>55.6</td>\n",
              "      <td>100.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>-0.256</td>\n",
              "      <td>3.375</td>\n",
              "      <td>15.151</td>\n",
              "      <td>0.067</td>\n",
              "      <td>55.6</td>\n",
              "      <td>100.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.1</td>\n",
              "      <td>-0.255</td>\n",
              "      <td>3.313</td>\n",
              "      <td>14.863</td>\n",
              "      <td>0.068</td>\n",
              "      <td>55.5</td>\n",
              "      <td>100.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.256</td>\n",
              "      <td>3.330</td>\n",
              "      <td>14.944</td>\n",
              "      <td>0.068</td>\n",
              "      <td>55.6</td>\n",
              "      <td>100.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>-0.256</td>\n",
              "      <td>3.332</td>\n",
              "      <td>14.951</td>\n",
              "      <td>0.068</td>\n",
              "      <td>55.6</td>\n",
              "      <td>100.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>-0.1</td>\n",
              "      <td>-0.255</td>\n",
              "      <td>3.293</td>\n",
              "      <td>14.768</td>\n",
              "      <td>0.069</td>\n",
              "      <td>55.4</td>\n",
              "      <td>100.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.255</td>\n",
              "      <td>3.321</td>\n",
              "      <td>14.895</td>\n",
              "      <td>0.068</td>\n",
              "      <td>55.4</td>\n",
              "      <td>100.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>-0.255</td>\n",
              "      <td>3.341</td>\n",
              "      <td>14.988</td>\n",
              "      <td>0.068</td>\n",
              "      <td>55.5</td>\n",
              "      <td>100.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.1</td>\n",
              "      <td>-0.1</td>\n",
              "      <td>-0.1</td>\n",
              "      <td>0.251</td>\n",
              "      <td>-0.284</td>\n",
              "      <td>0.045</td>\n",
              "      <td>0.692</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>53.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>46.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.1</td>\n",
              "      <td>-0.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.251</td>\n",
              "      <td>-0.284</td>\n",
              "      <td>0.045</td>\n",
              "      <td>0.692</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>53.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>46.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.1</td>\n",
              "      <td>-0.1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.251</td>\n",
              "      <td>-0.284</td>\n",
              "      <td>0.045</td>\n",
              "      <td>0.692</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>53.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>46.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.1</td>\n",
              "      <td>0.251</td>\n",
              "      <td>-0.284</td>\n",
              "      <td>0.045</td>\n",
              "      <td>0.692</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>53.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>46.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.255</td>\n",
              "      <td>3.337</td>\n",
              "      <td>14.970</td>\n",
              "      <td>0.068</td>\n",
              "      <td>55.5</td>\n",
              "      <td>100.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>-0.255</td>\n",
              "      <td>3.334</td>\n",
              "      <td>14.956</td>\n",
              "      <td>0.068</td>\n",
              "      <td>55.5</td>\n",
              "      <td>100.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>-0.1</td>\n",
              "      <td>-0.255</td>\n",
              "      <td>3.322</td>\n",
              "      <td>14.902</td>\n",
              "      <td>0.068</td>\n",
              "      <td>55.4</td>\n",
              "      <td>100.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.255</td>\n",
              "      <td>3.336</td>\n",
              "      <td>14.965</td>\n",
              "      <td>0.068</td>\n",
              "      <td>55.4</td>\n",
              "      <td>100.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>-0.255</td>\n",
              "      <td>3.338</td>\n",
              "      <td>14.972</td>\n",
              "      <td>0.068</td>\n",
              "      <td>55.4</td>\n",
              "      <td>100.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    init_a  init_b  init_c  final_a  ...   ftpt  ffpt  ftpf  ffpf\n",
              "0     -0.1    -0.1    -0.1   -0.256  ...  100.0   0.0   0.0   0.0\n",
              "1     -0.1    -0.1     0.0   -0.256  ...  100.0   0.0   0.0   0.0\n",
              "2     -0.1    -0.1     0.1   -0.256  ...  100.0   0.0   0.0   0.0\n",
              "3     -0.1     0.0    -0.1   -0.256  ...  100.0   0.0   0.0   0.0\n",
              "4     -0.1     0.0     0.0   -0.256  ...  100.0   0.0   0.0   0.0\n",
              "5     -0.1     0.0     0.1   -0.256  ...  100.0   0.0   0.0   0.0\n",
              "6     -0.1     0.1    -0.1   -0.255  ...  100.0   0.0   0.0   0.0\n",
              "7     -0.1     0.1     0.0   -0.255  ...  100.0   0.0   0.0   0.0\n",
              "8     -0.1     0.1     0.1   -0.255  ...  100.0   0.0   0.0   0.0\n",
              "9      0.0    -0.1    -0.1   -0.256  ...  100.0   0.0   0.0   0.0\n",
              "10     0.0    -0.1     0.0   -0.256  ...  100.0   0.0   0.0   0.0\n",
              "11     0.0    -0.1     0.1   -0.256  ...  100.0   0.0   0.0   0.0\n",
              "12     0.0     0.0    -0.1   -0.255  ...  100.0   0.0   0.0   0.0\n",
              "13     0.0     0.0     0.0   -0.256  ...  100.0   0.0   0.0   0.0\n",
              "14     0.0     0.0     0.1   -0.256  ...  100.0   0.0   0.0   0.0\n",
              "15     0.0     0.1    -0.1   -0.255  ...  100.0   0.0   0.0   0.0\n",
              "16     0.0     0.1     0.0   -0.255  ...  100.0   0.0   0.0   0.0\n",
              "17     0.0     0.1     0.1   -0.255  ...  100.0   0.0   0.0   0.0\n",
              "18     0.1    -0.1    -0.1    0.251  ...    0.0  53.5   0.0  46.5\n",
              "19     0.1    -0.1     0.0    0.251  ...    0.0  53.5   0.0  46.5\n",
              "20     0.1    -0.1     0.1    0.251  ...    0.0  53.5   0.0  46.5\n",
              "21     0.1     0.0    -0.1    0.251  ...    0.0  53.5   0.0  46.5\n",
              "22     0.1     0.0     0.0   -0.255  ...  100.0   0.0   0.0   0.0\n",
              "23     0.1     0.0     0.1   -0.255  ...  100.0   0.0   0.0   0.0\n",
              "24     0.1     0.1    -0.1   -0.255  ...  100.0   0.0   0.0   0.0\n",
              "25     0.1     0.1     0.0   -0.255  ...  100.0   0.0   0.0   0.0\n",
              "26     0.1     0.1     0.1   -0.255  ...  100.0   0.0   0.0   0.0\n",
              "\n",
              "[27 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1m2PyeaJcO7H"
      },
      "source": [
        "df_test.to_csv(\"linear_linear_simultaneous.csv\")"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mipCcN0cdXO"
      },
      "source": [
        ""
      ],
      "execution_count": 41,
      "outputs": []
    }
  ]
}
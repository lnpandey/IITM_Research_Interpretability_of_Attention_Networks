{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "linear_linear_adam_simultaneous_27_exp.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2_J4Rw2r0SQ",
        "outputId": "483082fd-0b1c-4dac-c627-8908fa5ce879"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "from tqdm import tqdm\n",
        "%matplotlib inline\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn import functional as F\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6fjud_Fr0Sa"
      },
      "source": [
        "# Generate dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqdXHO0Cr0Sd",
        "outputId": "bc024be5-2645-47db-f01e-0e69c03df20e"
      },
      "source": [
        "y = np.random.randint(0,3,500)\n",
        "idx= []\n",
        "for i in range(3):\n",
        "    print(i,sum(y==i))\n",
        "    idx.append(y==i)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 167\n",
            "1 166\n",
            "2 167\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddhXyODwr0Sk"
      },
      "source": [
        "x = np.zeros((500,))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyV3N2DIr0Sp"
      },
      "source": [
        "np.random.seed(12)\n",
        "x[idx[0]] = np.random.uniform(low =-1,high =0,size= sum(idx[0]))\n",
        "x[idx[1]] = np.random.uniform(low =0,high =1,size= sum(idx[1]))\n",
        "x[idx[2]] = np.random.uniform(low =2,high =3,size= sum(idx[2]))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qh1mDScsU07I",
        "outputId": "e5d0d98a-5a6e-4f41-a88c-de1e821fcdb8"
      },
      "source": [
        "x[idx[0]][0], x[idx[2]][5] "
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-0.8458371576203276, 2.2252632537156383)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Vr5ErQ_wSrV",
        "outputId": "d5ee1c89-d606-4f6d-a55a-77a9a2d10b60"
      },
      "source": [
        "print(x.shape,y.shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(500,) (500,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NG-3RpffwU_i"
      },
      "source": [
        "idx= []\n",
        "for i in range(3):\n",
        "  idx.append(y==i)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "id": "hJ8Jm7YUr0St",
        "outputId": "67878209-0b28-4f53-a658-31f07679b5ce"
      },
      "source": [
        "for i in range(3):\n",
        "    y= np.zeros(x[idx[i]].shape[0])\n",
        "    plt.scatter(x[idx[i]],y,label=\"class_\"+str(i))\n",
        "plt.legend()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f23b42a8710>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lMBZEHNBlF2",
        "outputId": "f6617aac-79a0-48bf-cc63-33968860a8ed"
      },
      "source": [
        "bg_idx = [ np.where(idx[2] == True)[0]]\n",
        "\n",
        "bg_idx = np.concatenate(bg_idx, axis = 0)\n",
        "bg_idx.shape"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(167,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "blRbGZHeCwXU",
        "outputId": "9c8dc17d-e4ca-4f28-833b-f3cdd0f26b48"
      },
      "source": [
        "np.unique(bg_idx).shape"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(167,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y43sWeX7C15F"
      },
      "source": [
        "x = x - np.mean(x[bg_idx], axis = 0, keepdims = True)\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ooII7N6UDWe0",
        "outputId": "3092a01e-60db-4bbc-d36b-7796f257d23a"
      },
      "source": [
        "np.mean(x[bg_idx], axis = 0, keepdims = True), np.mean(x, axis = 0, keepdims = True)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([7.17988543e-17]), array([-1.68308742]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g21bvPRYDL9k"
      },
      "source": [
        "x = x/np.std(x[bg_idx], axis = 0, keepdims = True)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GtFvIeHsDZJk",
        "outputId": "17bc975e-e20d-41da-c1ad-0a04b55d99a6"
      },
      "source": [
        "np.std(x[bg_idx], axis = 0, keepdims = True), np.std(x, axis = 0, keepdims = True)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([1.]), array([4.32711436]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "id": "8-VLhUfDDeHt",
        "outputId": "5db6c574-f747-449f-eb6f-0f6ba9f3abac"
      },
      "source": [
        "for i in range(3):\n",
        "    y= np.zeros(x[idx[i]].shape[0])\n",
        "    plt.scatter(x[idx[i]],y,label=\"class_\"+str(i))\n",
        "plt.legend()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f23b3d75110>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfFHcZJOr0Sz"
      },
      "source": [
        "foreground_classes = {'class_0','class_1' }\n",
        "\n",
        "background_classes = {'class_2'}"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OplNpNQVr0S2",
        "outputId": "4b95d754-be39-4099-e909-eb829a38a7d7"
      },
      "source": [
        "fg_class  = np.random.randint(0,2)\n",
        "fg_idx = np.random.randint(0,9)\n",
        "\n",
        "a = []\n",
        "for i in range(9):\n",
        "    if i == fg_idx:\n",
        "        b = np.random.choice(np.where(idx[fg_class]==True)[0],size=1)\n",
        "        a.append(x[b])\n",
        "        print(\"foreground \"+str(fg_class)+\" present at \" + str(fg_idx))\n",
        "    else:\n",
        "        bg_class = np.random.randint(2,3)\n",
        "        b = np.random.choice(np.where(idx[bg_class]==True)[0],size=1)\n",
        "        a.append(x[b])\n",
        "        print(\"background \"+str(bg_class)+\" present at \" + str(i))\n",
        "a = np.concatenate(a,axis=0)\n",
        "print(a.shape)\n",
        "\n",
        "print(fg_class , fg_idx)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "background 2 present at 0\n",
            "background 2 present at 1\n",
            "background 2 present at 2\n",
            "background 2 present at 3\n",
            "background 2 present at 4\n",
            "background 2 present at 5\n",
            "background 2 present at 6\n",
            "background 2 present at 7\n",
            "foreground 1 present at 8\n",
            "(9,)\n",
            "1 8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dwZVmmRBr0S8",
        "outputId": "d09ac7f0-dd73-4239-c61f-450196446764"
      },
      "source": [
        "a.shape"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OoxzYI-ur0S_",
        "outputId": "24fbbaa1-1765-4f15-f1b5-1d9fb3bf822f"
      },
      "source": [
        "np.reshape(a,(9,1))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.02667998],\n",
              "       [ 0.14684259],\n",
              "       [ 1.37848801],\n",
              "       [ 0.419546  ],\n",
              "       [ 1.27408564],\n",
              "       [-0.90359503],\n",
              "       [-0.68478824],\n",
              "       [ 1.27408564],\n",
              "       [-8.06954111]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4ruI0cxr0TE"
      },
      "source": [
        "a=np.reshape(a,(3,3))"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 288
        },
        "id": "RTUTFhJIr0TI",
        "outputId": "93075cd3-be33-4881-ccd1-95d6d63c3c72"
      },
      "source": [
        "plt.imshow(a)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f23b3d75b50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqbvfbwVr0TN"
      },
      "source": [
        "desired_num = 2000\n",
        "mosaic_list_of_images =[]\n",
        "mosaic_label = []\n",
        "fore_idx=[]\n",
        "for j in range(desired_num):\n",
        "    np.random.seed(j)\n",
        "    fg_class  = np.random.randint(0,2)\n",
        "    fg_idx = 0\n",
        "    a = []\n",
        "    for i in range(9):\n",
        "        if i == fg_idx:\n",
        "            b = np.random.choice(np.where(idx[fg_class]==True)[0],size=1)\n",
        "            a.append(x[b])\n",
        "#             print(\"foreground \"+str(fg_class)+\" present at \" + str(fg_idx))\n",
        "        else:\n",
        "            bg_class = np.random.randint(2,3)\n",
        "            b = np.random.choice(np.where(idx[bg_class]==True)[0],size=1)\n",
        "            a.append(x[b])\n",
        "#             print(\"background \"+str(bg_class)+\" present at \" + str(i))\n",
        "    a = np.concatenate(a,axis=0)\n",
        "    mosaic_list_of_images.append(np.reshape(a,(9,1)))\n",
        "    mosaic_label.append(fg_class)\n",
        "    fore_idx.append(fg_idx)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOsFmWfMr0TR"
      },
      "source": [
        "mosaic_list_of_images = np.concatenate(mosaic_list_of_images,axis=1).T\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2aIPMgLXNiXW",
        "outputId": "fe06a269-be94-4f63-e44a-791f0be94aed"
      },
      "source": [
        "mosaic_list_of_images.shape, mosaic_list_of_images[0]"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((2000, 9), array([-10.52117516,   1.30506863,   0.52441591,   1.04161499,\n",
              "         -0.90359503,   1.39692569,  -0.28550801,  -1.17213902,\n",
              "          0.14909581]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A3qcsbbzPfRG",
        "outputId": "7af54152-a97f-4bc6-fbd8-f193b07ed307"
      },
      "source": [
        "for j in range(9):\n",
        "  print(mosaic_list_of_images[0][j])\n",
        "  "
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-10.52117516172382\n",
            "1.3050686326842567\n",
            "0.5244159096640372\n",
            "1.0416149919208415\n",
            "-0.9035950289622539\n",
            "1.3969256896593936\n",
            "-0.285508009030342\n",
            "-1.172139015736176\n",
            "0.14909580785370755\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPoIwbMHx44n"
      },
      "source": [
        "class MosaicDataset(Dataset):\n",
        "  \"\"\"MosaicDataset dataset.\"\"\"\n",
        "\n",
        "  def __init__(self, mosaic_list_of_images, mosaic_label, fore_idx):\n",
        "    \"\"\"\n",
        "      Args:\n",
        "        csv_file (string): Path to the csv file with annotations.\n",
        "        root_dir (string): Directory with all the images.\n",
        "        transform (callable, optional): Optional transform to be applied\n",
        "            on a sample.\n",
        "    \"\"\"\n",
        "    self.mosaic = mosaic_list_of_images\n",
        "    self.label = mosaic_label\n",
        "    self.fore_idx = fore_idx\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.label)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.mosaic[idx] , self.label[idx], self.fore_idx[idx]\n",
        "\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOPAJQJeW8Ah"
      },
      "source": [
        "batch = 250\n",
        "msd1 = MosaicDataset(mosaic_list_of_images[0:1000], mosaic_label[0:1000] , fore_idx[0:1000])\n",
        "train_loader = DataLoader( msd1 ,batch_size= batch ,shuffle=True)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjNiQgxZW8bA"
      },
      "source": [
        "batch = 250\n",
        "msd2 = MosaicDataset(mosaic_list_of_images[1000:2000], mosaic_label[1000:2000] , fore_idx[1000:2000])\n",
        "test_loader = DataLoader( msd2 ,batch_size= batch ,shuffle=True)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30ZAjix3x8CM"
      },
      "source": [
        "class Focus(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Focus, self).__init__()\n",
        "\n",
        "    self.fc1 = nn.Linear(1, 1, bias= False)\n",
        "    # self.fc2 = nn.Linear(2, 1)\n",
        "\n",
        "  def forward(self,z):  #y is avg image #z batch of list of 9 images\n",
        "    y = torch.zeros([batch], dtype=torch.float64)\n",
        "    x = torch.zeros([batch,9],dtype=torch.float64)\n",
        "    y = y.to(\"cuda\")\n",
        "    x = x.to(\"cuda\")\n",
        "    # print(x.shape, z.shape)\n",
        "    for i in range(9):\n",
        "      # print(z[:,i].shape)\n",
        "      # print(self.helper(z[:,i])[:,0].shape)\n",
        "      x[:,i] = self.helper(z[:,i])[:,0]\n",
        "    # print(x.shape, z.shape)\n",
        "    x = F.softmax(x,dim=1)\n",
        "    # print(x.shape, z.shape)\n",
        "    # x1 = x[:,0]\n",
        "    # print(torch.mul(x[:,0],z[:,0]).shape)\n",
        "\n",
        "    for i in range(9):            \n",
        "      # x1 = x[:,i]          \n",
        "      y = y + torch.mul(x[:,i],z[:,i])\n",
        "\n",
        "    # print(x.shape, y.shape)\n",
        "    return x, y\n",
        "    \n",
        "  def helper(self, x):\n",
        "    x = x.view(-1, 1)\n",
        "    # x = F.relu(self.fc1(x))\n",
        "    x = (self.fc1(x))\n",
        "\n",
        "    return x"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dYXnywAD-4l"
      },
      "source": [
        "class Classification(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Classification, self).__init__()\n",
        "    self.fc1 = nn.Linear(1, 1)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x.view(-1, 1)\n",
        "    x = self.fc1(x)\n",
        "    # print(x.shape)\n",
        "    return x"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yl41sE8vFERk"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "def train(train_loader, test_loader, focus_net, classify):    \n",
        "  criterion = nn.BCEWithLogitsLoss()\n",
        "  optimizer_classify = optim.Adam(classify.parameters(), lr=0.01 ) #, momentum=0.9)\n",
        "  optimizer_focus = optim.Adam(focus_net.parameters(), lr=0.01 ) #, momentum=0.9)\n",
        "\n",
        "  print('-'*50)\n",
        "  print(focus_net.fc1.weight, classify.fc1.weight, classify.fc1.bias)\n",
        "  nos_epochs = 1000\n",
        "  loss_ret=0.0\n",
        "  for epoch in range(nos_epochs):  # loop over the dataset multiple times\n",
        "    \n",
        "    running_loss = 0.0\n",
        "    epoch_loss = []\n",
        "    cnt=0\n",
        "\n",
        "    iteration = desired_num // batch\n",
        "    \n",
        "    #training data set\n",
        "    \n",
        "    for i, data in  enumerate(train_loader):\n",
        "      inputs , labels , fore_idx = data\n",
        "      inputs, labels = inputs.to(\"cuda\"), labels.to(\"cuda\")\n",
        "      inputs = inputs.double()\n",
        "      labels = labels.float()\n",
        "      # zero the parameter gradients\n",
        "      \n",
        "      optimizer_focus.zero_grad()\n",
        "      optimizer_classify.zero_grad()\n",
        "      \n",
        "      alphas, avg_images = focus_net(inputs)\n",
        "      outputs = classify(avg_images)\n",
        "\n",
        "      predicted = np.round(torch.sigmoid(outputs.data).cpu().numpy())\n",
        "      # print(predicted.shape)\n",
        "      # print(outputs.shape,labels.shape)\n",
        "      # print(outputs)\n",
        "      # print(labels)\n",
        "      loss = criterion(outputs[:,0], labels) \n",
        "      loss.backward()\n",
        "      optimizer_focus.step()\n",
        "      optimizer_classify.step()\n",
        "\n",
        "      running_loss += loss.item()\n",
        "      # mini = 3\n",
        "      # if cnt % mini == mini-1 :    # print every 40 mini-batches\n",
        "      epoch_loss.append(running_loss)\n",
        "      running_loss = 0.0\n",
        "      cnt=cnt+1\n",
        "    loss_ret = np.mean(epoch_loss)\n",
        "    if(epoch%200==0):\n",
        "      print('[%d, %5d] loss: %.3f' %(epoch + 1, cnt + 1, np.mean(epoch_loss)))\n",
        "    if(np.mean(epoch_loss) <= 0.001):\n",
        "        break;\n",
        "\n",
        "  with torch.no_grad():\n",
        "    focus_true_pred_true =0\n",
        "    focus_false_pred_true =0\n",
        "    focus_true_pred_false =0\n",
        "    focus_false_pred_false =0\n",
        "\n",
        "    argmax_more_than_half = 0\n",
        "    argmax_less_than_half =0\n",
        "    for data in test_loader:\n",
        "      inputs, labels , fore_idx = data\n",
        "      inputs = inputs.double()\n",
        "      inputs, labels = inputs.to(\"cuda\"), labels.to(\"cuda\")\n",
        "      alphas, avg_images = focus_net(inputs)\n",
        "      outputs = classify(avg_images)\n",
        "      predicted = np.round(torch.sigmoid(outputs.data).cpu().numpy())\n",
        "\n",
        "      for j in range (batch):\n",
        "        focus = torch.argmax(alphas[j])\n",
        "\n",
        "        if(alphas[j][focus] >= 0.5):\n",
        "          argmax_more_than_half +=1\n",
        "        else:\n",
        "          argmax_less_than_half +=1\n",
        "\n",
        "        if(focus == fore_idx[j] and predicted[j] == labels[j].item()):\n",
        "          focus_true_pred_true += 1\n",
        "\n",
        "        elif(focus != fore_idx[j] and predicted[j] == labels[j].item()):\n",
        "          focus_false_pred_true +=1\n",
        "\n",
        "        elif(focus == fore_idx[j] and predicted[j] != labels[j].item()):\n",
        "          focus_true_pred_false +=1\n",
        "\n",
        "        elif(focus != fore_idx[j] and predicted[j] != labels[j].item()):\n",
        "          focus_false_pred_false +=1\n",
        "\n",
        "  print('Finished Training')  \n",
        "  return loss_ret, argmax_more_than_half/10, focus_true_pred_true/10, focus_false_pred_true/10, focus_true_pred_false/10 , focus_false_pred_false/10, focus_net.fc1.weight.item(), classify.fc1.weight.item(), classify.fc1.bias.item()\n",
        "    \n"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fd7ScJj9g6rl"
      },
      "source": [
        "a = [-0.1, 0.0, 0.1]\n",
        "b = [-0.1, 0.0, 0.1]\n",
        "c = [-0.1, 0.0, 0.1]"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0rXlZZfUK3B"
      },
      "source": [
        "# torch.manual_seed(12)\n",
        "# focus_net = Focus().double()\n",
        "# focus_net.fc1.weight = torch.nn.Parameter(torch.tensor(np.array([[0.0]])))\n",
        "# torch.manual_seed(12)\n",
        "# classify = Classification().double()\n",
        "# classify.fc1.weight = torch.nn.Parameter(torch.tensor(np.array([[0.0]])))\n",
        "# classify.fc1.bias = torch.nn.Parameter(torch.tensor(np.array([0.0])))\n",
        "# focus_net = focus_net.to(\"cuda\")\n",
        "# classify = classify.to(\"cuda\")\n",
        "# # print(\"--\"*40,\"a,b,c = \",a[i],b[j],c[k])\n",
        "# print(focus_net.fc1.weight, classify.fc1.weight, classify.fc1.bias)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIzudMBEuFQl"
      },
      "source": [
        "# np.round(focus_net.fc1.weight.item(),3), np.round(99.3, 3)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEXvq7EWUck2"
      },
      "source": [
        "# train(train_loader, test_loader, focus_net, classify)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zo5QyZobhHpM",
        "outputId": "9bfea9e9-94f5-477b-aa7d-5bd05bfdb8d3"
      },
      "source": [
        "all_loss=[]\n",
        "all_alphas_more_than_half=[]\n",
        "all_ftpt=[]\n",
        "all_ffpt=[]\n",
        "all_ftpf = []\n",
        "all_ffpf= []\n",
        "init_a = []\n",
        "init_b = []\n",
        "init_c = []\n",
        "final_a=[]\n",
        "final_b = []\n",
        "final_c = []\n",
        "\n",
        "for i in range(3):\n",
        "  for j in range(3):\n",
        "    for k in range(3):\n",
        "      torch.manual_seed(12)\n",
        "      focus_net = Focus().double()\n",
        "      focus_net.fc1.weight = torch.nn.Parameter(torch.tensor(np.array([[a[i]]])))\n",
        "      torch.manual_seed(12)\n",
        "      classify = Classification().double()\n",
        "      classify.fc1.weight = torch.nn.Parameter(torch.tensor(np.array([[b[j]]])))\n",
        "      classify.fc1.bias = torch.nn.Parameter(torch.tensor(np.array([c[k]])))\n",
        "      focus_net = focus_net.to(\"cuda\")\n",
        "      classify = classify.to(\"cuda\")\n",
        "      print(\"--\"*40,\"a,b,c = \",a[i],b[j],c[k])\n",
        "      cost, alpha_per, ftpt, ffpt, ftpf, ffpf, f_a, f_b, f_c = train(train_loader, test_loader, focus_net, classify)\n",
        "      print(cost, alpha_per, ftpt, ffpt, ftpf, ffpf)\n",
        "      init_a.append(a[i])\n",
        "      init_b.append(b[j])\n",
        "      init_c.append(c[k])\n",
        "      final_a.append(np.round(f_a,3))\n",
        "      final_b.append(np.round(f_b,3))\n",
        "      final_c.append(np.round(f_c,3))\n",
        "      all_loss.append(np.round(cost,3))\n",
        "      all_alphas_more_than_half.append(alpha_per)\n",
        "      all_ftpt.append(ftpt)\n",
        "      all_ffpt.append(ffpt)\n",
        "      all_ftpf.append(ftpf)\n",
        "      all_ffpf.append(ffpf)\n",
        "\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------------------------------------------------------------------------------- a,b,c =  -0.1 -0.1 -0.1\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[-0.1000]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[-0.1000]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([-0.1000], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     5] loss: 0.716\n",
            "[201,     5] loss: 0.198\n",
            "[401,     5] loss: 0.131\n",
            "[601,     5] loss: 0.102\n",
            "[801,     5] loss: 0.085\n",
            "Finished Training\n",
            "0.07343695685267448 65.4 99.6 0.0 0.4 0.0\n",
            "-------------------------------------------------------------------------------- a,b,c =  -0.1 -0.1 0.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[-0.1000]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[-0.1000]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([0.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     5] loss: 0.717\n",
            "[201,     5] loss: 0.198\n",
            "[401,     5] loss: 0.130\n",
            "[601,     5] loss: 0.101\n",
            "[801,     5] loss: 0.085\n",
            "Finished Training\n",
            "0.07335146237164736 65.4 99.6 0.0 0.4 0.0\n",
            "-------------------------------------------------------------------------------- a,b,c =  -0.1 -0.1 0.1\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[-0.1000]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[-0.1000]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([0.1000], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     5] loss: 0.722\n",
            "[201,     5] loss: 0.198\n",
            "[401,     5] loss: 0.131\n",
            "[601,     5] loss: 0.102\n",
            "[801,     5] loss: 0.085\n",
            "Finished Training\n",
            "0.0733950799331069 65.4 99.6 0.0 0.4 0.0\n",
            "-------------------------------------------------------------------------------- a,b,c =  -0.1 0.0 -0.1\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[-0.1000]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([-0.1000], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     5] loss: 0.691\n",
            "[201,     5] loss: 0.205\n",
            "[401,     5] loss: 0.134\n",
            "[601,     5] loss: 0.104\n",
            "[801,     5] loss: 0.087\n",
            "Finished Training\n",
            "0.07493563648313284 65.4 99.6 0.0 0.4 0.0\n",
            "-------------------------------------------------------------------------------- a,b,c =  -0.1 0.0 0.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[-0.1000]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([0.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     5] loss: 0.689\n",
            "[201,     5] loss: 0.202\n",
            "[401,     5] loss: 0.133\n",
            "[601,     5] loss: 0.103\n",
            "[801,     5] loss: 0.086\n",
            "Finished Training\n",
            "0.07438671309500933 65.4 99.6 0.0 0.4 0.0\n",
            "-------------------------------------------------------------------------------- a,b,c =  -0.1 0.0 0.1\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[-0.1000]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([0.1000], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     5] loss: 0.688\n",
            "[201,     5] loss: 0.201\n",
            "[401,     5] loss: 0.133\n",
            "[601,     5] loss: 0.103\n",
            "[801,     5] loss: 0.086\n",
            "Finished Training\n",
            "0.0741899125277996 65.4 99.6 0.0 0.4 0.0\n",
            "-------------------------------------------------------------------------------- a,b,c =  -0.1 0.1 -0.1\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[-0.1000]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[0.1000]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([-0.1000], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     5] loss: 0.672\n",
            "[201,     5] loss: 0.208\n",
            "[401,     5] loss: 0.136\n",
            "[601,     5] loss: 0.105\n",
            "[801,     5] loss: 0.088\n",
            "Finished Training\n",
            "0.07559128105640411 65.3 99.6 0.0 0.4 0.0\n",
            "-------------------------------------------------------------------------------- a,b,c =  -0.1 0.1 0.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[-0.1000]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[0.1000]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([0.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     5] loss: 0.664\n",
            "[201,     5] loss: 0.204\n",
            "[401,     5] loss: 0.134\n",
            "[601,     5] loss: 0.104\n",
            "[801,     5] loss: 0.087\n",
            "Finished Training\n",
            "0.07492445223033428 65.4 99.6 0.0 0.4 0.0\n",
            "-------------------------------------------------------------------------------- a,b,c =  -0.1 0.1 0.1\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[-0.1000]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[0.1000]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([0.1000], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     5] loss: 0.658\n",
            "[201,     5] loss: 0.201\n",
            "[401,     5] loss: 0.133\n",
            "[601,     5] loss: 0.103\n",
            "[801,     5] loss: 0.086\n",
            "Finished Training\n",
            "0.07425812166184187 65.4 99.6 0.0 0.4 0.0\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 -0.1 -0.1\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[-0.1000]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([-0.1000], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     5] loss: 0.699\n",
            "[201,     5] loss: 0.684\n",
            "[401,     5] loss: 0.684\n",
            "[601,     5] loss: 0.684\n",
            "[801,     5] loss: 0.684\n",
            "Finished Training\n",
            "0.6844055950641632 0.0 0.0 54.9 0.0 45.1\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 -0.1 0.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[-0.1000]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([0.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     5] loss: 0.699\n",
            "[201,     5] loss: 0.684\n",
            "[401,     5] loss: 0.684\n",
            "[601,     5] loss: 0.684\n",
            "[801,     5] loss: 0.684\n",
            "Finished Training\n",
            "0.6844056099653244 0.0 0.0 54.9 0.0 45.1\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 -0.1 0.1\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[-0.1000]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([0.1000], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     5] loss: 0.701\n",
            "[201,     5] loss: 0.684\n",
            "[401,     5] loss: 0.684\n",
            "[601,     5] loss: 0.684\n",
            "[801,     5] loss: 0.684\n",
            "Finished Training\n",
            "0.6844055652618408 0.0 0.0 54.9 0.0 45.1\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 0.0 -0.1\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([-0.1000], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     5] loss: 0.694\n",
            "[201,     5] loss: 0.204\n",
            "[401,     5] loss: 0.134\n",
            "[601,     5] loss: 0.104\n",
            "[801,     5] loss: 0.086\n",
            "Finished Training\n",
            "0.07471069786697626 65.4 99.6 0.0 0.4 0.0\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 0.0 0.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([0.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     5] loss: 0.692\n",
            "[201,     5] loss: 0.202\n",
            "[401,     5] loss: 0.133\n",
            "[601,     5] loss: 0.103\n",
            "[801,     5] loss: 0.086\n",
            "Finished Training\n",
            "0.07432696782052517 65.4 99.6 0.0 0.4 0.0\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 0.0 0.1\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([0.1000], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     5] loss: 0.692\n",
            "[201,     5] loss: 0.202\n",
            "[401,     5] loss: 0.133\n",
            "[601,     5] loss: 0.103\n",
            "[801,     5] loss: 0.086\n",
            "Finished Training\n",
            "0.07430021092295647 65.4 99.6 0.0 0.4 0.0\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 0.1 -0.1\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[0.1000]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([-0.1000], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     5] loss: 0.688\n",
            "[201,     5] loss: 0.206\n",
            "[401,     5] loss: 0.135\n",
            "[601,     5] loss: 0.105\n",
            "[801,     5] loss: 0.087\n",
            "Finished Training\n",
            "0.07514900714159012 65.4 99.6 0.0 0.4 0.0\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 0.1 0.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[0.1000]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([0.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     5] loss: 0.684\n",
            "[201,     5] loss: 0.203\n",
            "[401,     5] loss: 0.133\n",
            "[601,     5] loss: 0.103\n",
            "[801,     5] loss: 0.086\n",
            "Finished Training\n",
            "0.0745458509773016 65.4 99.6 0.0 0.4 0.0\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 0.1 0.1\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[0.1000]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([0.1000], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     5] loss: 0.681\n",
            "[201,     5] loss: 0.200\n",
            "[401,     5] loss: 0.132\n",
            "[601,     5] loss: 0.103\n",
            "[801,     5] loss: 0.086\n",
            "Finished Training\n",
            "0.07410846184939146 65.4 99.6 0.0 0.4 0.0\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.1 -0.1 -0.1\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.1000]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[-0.1000]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([-0.1000], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     5] loss: 0.693\n",
            "[201,     5] loss: 0.684\n",
            "[401,     5] loss: 0.684\n",
            "[601,     5] loss: 0.684\n",
            "[801,     5] loss: 0.684\n",
            "Finished Training\n",
            "0.6844056397676468 0.0 0.0 54.9 0.0 45.1\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.1 -0.1 0.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.1000]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[-0.1000]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([0.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     5] loss: 0.693\n",
            "[201,     5] loss: 0.684\n",
            "[401,     5] loss: 0.684\n",
            "[601,     5] loss: 0.684\n",
            "[801,     5] loss: 0.684\n",
            "Finished Training\n",
            "0.6844056397676468 0.0 0.0 54.9 0.0 45.1\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.1 -0.1 0.1\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.1000]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[-0.1000]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([0.1000], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     5] loss: 0.693\n",
            "[201,     5] loss: 0.684\n",
            "[401,     5] loss: 0.684\n",
            "[601,     5] loss: 0.684\n",
            "[801,     5] loss: 0.684\n",
            "Finished Training\n",
            "0.6844055950641632 0.0 0.0 54.9 0.0 45.1\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.1 0.0 -0.1\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.1000]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([-0.1000], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     5] loss: 0.695\n",
            "[201,     5] loss: 0.684\n",
            "[401,     5] loss: 0.684\n",
            "[601,     5] loss: 0.684\n",
            "[801,     5] loss: 0.684\n",
            "Finished Training\n",
            "0.6844056099653244 0.0 0.0 54.9 0.0 45.1\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.1 0.0 0.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.1000]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([0.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     5] loss: 0.693\n",
            "[201,     5] loss: 0.684\n",
            "[401,     5] loss: 0.684\n",
            "[601,     5] loss: 0.684\n",
            "[801,     5] loss: 0.684\n",
            "Finished Training\n",
            "0.6844056099653244 0.0 0.0 54.9 0.0 45.1\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.1 0.0 0.1\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.1000]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([0.1000], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     5] loss: 0.694\n",
            "[201,     5] loss: 0.684\n",
            "[401,     5] loss: 0.684\n",
            "[601,     5] loss: 0.684\n",
            "[801,     5] loss: 0.684\n",
            "Finished Training\n",
            "0.6844056099653244 0.0 0.0 54.9 0.0 45.1\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.1 0.1 -0.1\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.1000]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[0.1000]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([-0.1000], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     5] loss: 0.695\n",
            "[201,     5] loss: 0.202\n",
            "[401,     5] loss: 0.133\n",
            "[601,     5] loss: 0.103\n",
            "[801,     5] loss: 0.086\n",
            "Finished Training\n",
            "0.0743880569934845 65.4 99.6 0.0 0.4 0.0\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.1 0.1 0.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.1000]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[0.1000]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([0.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     5] loss: 0.693\n",
            "[201,     5] loss: 0.201\n",
            "[401,     5] loss: 0.132\n",
            "[601,     5] loss: 0.103\n",
            "[801,     5] loss: 0.086\n",
            "Finished Training\n",
            "0.074157590046525 65.4 99.6 0.0 0.4 0.0\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.1 0.1 0.1\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.1000]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[0.1000]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([0.1000], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     5] loss: 0.693\n",
            "[201,     5] loss: 0.201\n",
            "[401,     5] loss: 0.132\n",
            "[601,     5] loss: 0.103\n",
            "[801,     5] loss: 0.086\n",
            "Finished Training\n",
            "0.07417884282767773 65.4 99.6 0.0 0.4 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gQoPST5zW2t"
      },
      "source": [
        "# df_train = pd.DataFrame()\n",
        "df_test = pd.DataFrame()"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "In76SYH_zZHV"
      },
      "source": [
        "columns = [\"init_a\", \"init_b\", \"init_c\", \"final_a\", \"final_b\", \"final_c\", \"train_loss\", \"argmax > 0.5\" , \"ftpt\", \"ffpt\", \"ftpf\", \"ffpf\" ]"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BS4HtOHEzZ0E"
      },
      "source": [
        "df_test[columns[0]] = init_a\n",
        "df_test[columns[1]] = init_b\n",
        "df_test[columns[2]] = init_c\n",
        "df_test[columns[3]] = final_a\n",
        "df_test[columns[4]] = final_b\n",
        "df_test[columns[5]] = final_c\n",
        "df_test[columns[6]] = all_loss\n",
        "df_test[columns[7]] = all_alphas_more_than_half\n",
        "df_test[columns[8]] = all_ftpt\n",
        "df_test[columns[9]] = all_ffpt\n",
        "df_test[columns[10]] = all_ftpf\n",
        "df_test[columns[11]] = all_ffpf"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UbTkfLUINTI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 896
        },
        "outputId": "6ff79360-2163-4518-e95e-061d6fbec425"
      },
      "source": [
        "df_test"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>init_a</th>\n",
              "      <th>init_b</th>\n",
              "      <th>init_c</th>\n",
              "      <th>final_a</th>\n",
              "      <th>final_b</th>\n",
              "      <th>final_c</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>argmax &gt; 0.5</th>\n",
              "      <th>ftpt</th>\n",
              "      <th>ffpt</th>\n",
              "      <th>ftpf</th>\n",
              "      <th>ffpf</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.1</td>\n",
              "      <td>-0.1</td>\n",
              "      <td>-0.1</td>\n",
              "      <td>-0.279</td>\n",
              "      <td>3.184</td>\n",
              "      <td>15.309</td>\n",
              "      <td>0.073</td>\n",
              "      <td>65.4</td>\n",
              "      <td>99.6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.1</td>\n",
              "      <td>-0.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.279</td>\n",
              "      <td>3.187</td>\n",
              "      <td>15.327</td>\n",
              "      <td>0.073</td>\n",
              "      <td>65.4</td>\n",
              "      <td>99.6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.1</td>\n",
              "      <td>-0.1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>-0.279</td>\n",
              "      <td>3.185</td>\n",
              "      <td>15.318</td>\n",
              "      <td>0.073</td>\n",
              "      <td>65.4</td>\n",
              "      <td>99.6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.1</td>\n",
              "      <td>-0.278</td>\n",
              "      <td>3.124</td>\n",
              "      <td>15.006</td>\n",
              "      <td>0.075</td>\n",
              "      <td>65.4</td>\n",
              "      <td>99.6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.279</td>\n",
              "      <td>3.145</td>\n",
              "      <td>15.116</td>\n",
              "      <td>0.074</td>\n",
              "      <td>65.4</td>\n",
              "      <td>99.6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>-0.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>-0.279</td>\n",
              "      <td>3.153</td>\n",
              "      <td>15.156</td>\n",
              "      <td>0.074</td>\n",
              "      <td>65.4</td>\n",
              "      <td>99.6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>-0.1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>-0.1</td>\n",
              "      <td>-0.278</td>\n",
              "      <td>3.099</td>\n",
              "      <td>14.876</td>\n",
              "      <td>0.076</td>\n",
              "      <td>65.3</td>\n",
              "      <td>99.6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>-0.1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.278</td>\n",
              "      <td>3.125</td>\n",
              "      <td>15.007</td>\n",
              "      <td>0.075</td>\n",
              "      <td>65.4</td>\n",
              "      <td>99.6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>-0.1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>-0.279</td>\n",
              "      <td>3.151</td>\n",
              "      <td>15.141</td>\n",
              "      <td>0.074</td>\n",
              "      <td>65.4</td>\n",
              "      <td>99.6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.1</td>\n",
              "      <td>-0.1</td>\n",
              "      <td>0.252</td>\n",
              "      <td>-0.766</td>\n",
              "      <td>0.095</td>\n",
              "      <td>0.684</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>54.9</td>\n",
              "      <td>0.0</td>\n",
              "      <td>45.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.252</td>\n",
              "      <td>-0.766</td>\n",
              "      <td>0.095</td>\n",
              "      <td>0.684</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>54.9</td>\n",
              "      <td>0.0</td>\n",
              "      <td>45.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.252</td>\n",
              "      <td>-0.766</td>\n",
              "      <td>0.095</td>\n",
              "      <td>0.684</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>54.9</td>\n",
              "      <td>0.0</td>\n",
              "      <td>45.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.1</td>\n",
              "      <td>-0.278</td>\n",
              "      <td>3.133</td>\n",
              "      <td>15.050</td>\n",
              "      <td>0.075</td>\n",
              "      <td>65.4</td>\n",
              "      <td>99.6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.279</td>\n",
              "      <td>3.148</td>\n",
              "      <td>15.127</td>\n",
              "      <td>0.074</td>\n",
              "      <td>65.4</td>\n",
              "      <td>99.6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>-0.279</td>\n",
              "      <td>3.149</td>\n",
              "      <td>15.133</td>\n",
              "      <td>0.074</td>\n",
              "      <td>65.4</td>\n",
              "      <td>99.6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>-0.1</td>\n",
              "      <td>-0.278</td>\n",
              "      <td>3.116</td>\n",
              "      <td>14.963</td>\n",
              "      <td>0.075</td>\n",
              "      <td>65.4</td>\n",
              "      <td>99.6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.278</td>\n",
              "      <td>3.140</td>\n",
              "      <td>15.083</td>\n",
              "      <td>0.075</td>\n",
              "      <td>65.4</td>\n",
              "      <td>99.6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>-0.279</td>\n",
              "      <td>3.157</td>\n",
              "      <td>15.171</td>\n",
              "      <td>0.074</td>\n",
              "      <td>65.4</td>\n",
              "      <td>99.6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.1</td>\n",
              "      <td>-0.1</td>\n",
              "      <td>-0.1</td>\n",
              "      <td>0.252</td>\n",
              "      <td>-0.766</td>\n",
              "      <td>0.095</td>\n",
              "      <td>0.684</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>54.9</td>\n",
              "      <td>0.0</td>\n",
              "      <td>45.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.1</td>\n",
              "      <td>-0.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.252</td>\n",
              "      <td>-0.766</td>\n",
              "      <td>0.095</td>\n",
              "      <td>0.684</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>54.9</td>\n",
              "      <td>0.0</td>\n",
              "      <td>45.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.1</td>\n",
              "      <td>-0.1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.252</td>\n",
              "      <td>-0.766</td>\n",
              "      <td>0.095</td>\n",
              "      <td>0.684</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>54.9</td>\n",
              "      <td>0.0</td>\n",
              "      <td>45.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.1</td>\n",
              "      <td>0.252</td>\n",
              "      <td>-0.766</td>\n",
              "      <td>0.095</td>\n",
              "      <td>0.684</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>54.9</td>\n",
              "      <td>0.0</td>\n",
              "      <td>45.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.252</td>\n",
              "      <td>-0.766</td>\n",
              "      <td>0.095</td>\n",
              "      <td>0.684</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>54.9</td>\n",
              "      <td>0.0</td>\n",
              "      <td>45.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.252</td>\n",
              "      <td>-0.766</td>\n",
              "      <td>0.095</td>\n",
              "      <td>0.684</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>54.9</td>\n",
              "      <td>0.0</td>\n",
              "      <td>45.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>-0.1</td>\n",
              "      <td>-0.279</td>\n",
              "      <td>3.146</td>\n",
              "      <td>15.115</td>\n",
              "      <td>0.074</td>\n",
              "      <td>65.4</td>\n",
              "      <td>99.6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.279</td>\n",
              "      <td>3.155</td>\n",
              "      <td>15.161</td>\n",
              "      <td>0.074</td>\n",
              "      <td>65.4</td>\n",
              "      <td>99.6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>-0.279</td>\n",
              "      <td>3.155</td>\n",
              "      <td>15.157</td>\n",
              "      <td>0.074</td>\n",
              "      <td>65.4</td>\n",
              "      <td>99.6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    init_a  init_b  init_c  final_a  ...  ftpt  ffpt  ftpf  ffpf\n",
              "0     -0.1    -0.1    -0.1   -0.279  ...  99.6   0.0   0.4   0.0\n",
              "1     -0.1    -0.1     0.0   -0.279  ...  99.6   0.0   0.4   0.0\n",
              "2     -0.1    -0.1     0.1   -0.279  ...  99.6   0.0   0.4   0.0\n",
              "3     -0.1     0.0    -0.1   -0.278  ...  99.6   0.0   0.4   0.0\n",
              "4     -0.1     0.0     0.0   -0.279  ...  99.6   0.0   0.4   0.0\n",
              "5     -0.1     0.0     0.1   -0.279  ...  99.6   0.0   0.4   0.0\n",
              "6     -0.1     0.1    -0.1   -0.278  ...  99.6   0.0   0.4   0.0\n",
              "7     -0.1     0.1     0.0   -0.278  ...  99.6   0.0   0.4   0.0\n",
              "8     -0.1     0.1     0.1   -0.279  ...  99.6   0.0   0.4   0.0\n",
              "9      0.0    -0.1    -0.1    0.252  ...   0.0  54.9   0.0  45.1\n",
              "10     0.0    -0.1     0.0    0.252  ...   0.0  54.9   0.0  45.1\n",
              "11     0.0    -0.1     0.1    0.252  ...   0.0  54.9   0.0  45.1\n",
              "12     0.0     0.0    -0.1   -0.278  ...  99.6   0.0   0.4   0.0\n",
              "13     0.0     0.0     0.0   -0.279  ...  99.6   0.0   0.4   0.0\n",
              "14     0.0     0.0     0.1   -0.279  ...  99.6   0.0   0.4   0.0\n",
              "15     0.0     0.1    -0.1   -0.278  ...  99.6   0.0   0.4   0.0\n",
              "16     0.0     0.1     0.0   -0.278  ...  99.6   0.0   0.4   0.0\n",
              "17     0.0     0.1     0.1   -0.279  ...  99.6   0.0   0.4   0.0\n",
              "18     0.1    -0.1    -0.1    0.252  ...   0.0  54.9   0.0  45.1\n",
              "19     0.1    -0.1     0.0    0.252  ...   0.0  54.9   0.0  45.1\n",
              "20     0.1    -0.1     0.1    0.252  ...   0.0  54.9   0.0  45.1\n",
              "21     0.1     0.0    -0.1    0.252  ...   0.0  54.9   0.0  45.1\n",
              "22     0.1     0.0     0.0    0.252  ...   0.0  54.9   0.0  45.1\n",
              "23     0.1     0.0     0.1    0.252  ...   0.0  54.9   0.0  45.1\n",
              "24     0.1     0.1    -0.1   -0.279  ...  99.6   0.0   0.4   0.0\n",
              "25     0.1     0.1     0.0   -0.279  ...  99.6   0.0   0.4   0.0\n",
              "26     0.1     0.1     0.1   -0.279  ...  99.6   0.0   0.4   0.0\n",
              "\n",
              "[27 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1m2PyeaJcO7H"
      },
      "source": [
        "df_test.to_csv(\"linear_linear_simultaneous.csv\")"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mipCcN0cdXO"
      },
      "source": [
        ""
      ],
      "execution_count": 41,
      "outputs": []
    }
  ]
}
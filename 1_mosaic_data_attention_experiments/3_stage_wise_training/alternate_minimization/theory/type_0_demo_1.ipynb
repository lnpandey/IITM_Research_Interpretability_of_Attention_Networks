{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "type_0_demo.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2_J4Rw2r0SQ",
        "outputId": "f68bdd59-cceb-498a-f941-cf21083b00bd"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "from tqdm import tqdm\n",
        "%matplotlib inline\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn import functional as F\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6fjud_Fr0Sa"
      },
      "source": [
        "# Generate dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqdXHO0Cr0Sd",
        "outputId": "6689b1eb-22ba-450a-830d-cd1f449aff52"
      },
      "source": [
        "np.random.seed(12)\n",
        "y = np.random.randint(0,3,5000)\n",
        "idx= []\n",
        "for i in range(3):\n",
        "    print(i,sum(y==i))\n",
        "    idx.append(y==i)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 1703\n",
            "1 1677\n",
            "2 1620\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddhXyODwr0Sk"
      },
      "source": [
        "x = np.zeros((5000,))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyV3N2DIr0Sp"
      },
      "source": [
        "np.random.seed(12)\n",
        "x[idx[0]] = np.random.uniform(low =-1,high =0,size= sum(idx[0]))\n",
        "x[idx[1]] = np.random.uniform(low =0,high =1,size= sum(idx[1]))\n",
        "x[idx[2]] = np.random.uniform(low =2,high =3,size= sum(idx[2]))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qh1mDScsU07I",
        "outputId": "e26e5ea7-9be2-4a53-9724-38e91dc52353"
      },
      "source": [
        "x[idx[0]][0], x[idx[2]][5] "
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-0.8458371576203276, 2.536741188595152)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Vr5ErQ_wSrV",
        "outputId": "b6973e1e-0e78-44aa-b2ac-7265c4e39a75"
      },
      "source": [
        "print(x.shape,y.shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5000,) (5000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NG-3RpffwU_i"
      },
      "source": [
        "idx= []\n",
        "for i in range(3):\n",
        "  idx.append(y==i)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "hJ8Jm7YUr0St",
        "outputId": "04836b8c-2b8d-49c0-a6ca-df6f6f1a95c1"
      },
      "source": [
        "for i in range(3):\n",
        "    y= np.zeros(x[idx[i]].shape[0])\n",
        "    plt.scatter(x[idx[i]],y,label=\"class_\"+str(i))\n",
        "plt.legend()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7ff9116efdd0>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lMBZEHNBlF2",
        "outputId": "7d94c78a-68ec-4344-a68e-55d1d909dc27"
      },
      "source": [
        "bg_idx = [ np.where(idx[2] == True)[0]]\n",
        "\n",
        "bg_idx = np.concatenate(bg_idx, axis = 0)\n",
        "bg_idx.shape"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1620,)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "blRbGZHeCwXU",
        "outputId": "1e893bb6-9367-4107-a35a-58cfc216baf4"
      },
      "source": [
        "np.unique(bg_idx).shape"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1620,)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y43sWeX7C15F"
      },
      "source": [
        "# x = x - np.mean(x[bg_idx], axis = 0, keepdims = True)\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ooII7N6UDWe0"
      },
      "source": [
        "# np.mean(x[bg_idx], axis = 0, keepdims = True), np.mean(x, axis = 0, keepdims = True)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g21bvPRYDL9k"
      },
      "source": [
        "# x = x/np.std(x[bg_idx], axis = 0, keepdims = True)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtFvIeHsDZJk"
      },
      "source": [
        "# np.std(x[bg_idx], axis = 0, keepdims = True), np.std(x, axis = 0, keepdims = True)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "8-VLhUfDDeHt",
        "outputId": "dc4bda3f-5edd-4add-a749-cf39c75eda4d"
      },
      "source": [
        "for i in range(3):\n",
        "    y= np.zeros(x[idx[i]].shape[0])\n",
        "    plt.scatter(x[idx[i]],y,label=\"class_\"+str(i))\n",
        "plt.legend()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7ff9116bbad0>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfFHcZJOr0Sz"
      },
      "source": [
        "foreground_classes = {'class_0','class_1' }\n",
        "\n",
        "background_classes = {'class_2'}"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OplNpNQVr0S2"
      },
      "source": [
        "# fg_class  = np.random.randint(0,2)\n",
        "# fg_idx = np.random.randint(0,9)\n",
        "\n",
        "# a = []\n",
        "# for i in range(9):\n",
        "#     if i == fg_idx:\n",
        "#         b = np.random.choice(np.where(idx[fg_class]==True)[0],size=1)\n",
        "#         a.append(x[b])\n",
        "#         print(\"foreground \"+str(fg_class)+\" present at \" + str(fg_idx))\n",
        "#     else:\n",
        "#         bg_class = np.random.randint(2,3)\n",
        "#         b = np.random.choice(np.where(idx[bg_class]==True)[0],size=1)\n",
        "#         a.append(x[b])\n",
        "#         print(\"background \"+str(bg_class)+\" present at \" + str(i))\n",
        "# a = np.concatenate(a,axis=0)\n",
        "# print(a.shape)\n",
        "\n",
        "# print(fg_class , fg_idx)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwZVmmRBr0S8"
      },
      "source": [
        "# a.shape"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OoxzYI-ur0S_"
      },
      "source": [
        "# np.reshape(a,(9,1))"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4ruI0cxr0TE"
      },
      "source": [
        "# a=np.reshape(a,(3,3))"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTUTFhJIr0TI"
      },
      "source": [
        "# plt.imshow(a)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqbvfbwVr0TN"
      },
      "source": [
        "desired_num = 20000\n",
        "mosaic_list_of_images =[]\n",
        "mosaic_label = []\n",
        "fore_idx=[]\n",
        "for j in range(desired_num):\n",
        "    np.random.seed(j)\n",
        "    fg_class  = np.random.randint(0,2)\n",
        "    fg_idx = np.random.randint(0,9)\n",
        "    a = []\n",
        "    for i in range(9):\n",
        "        if i == fg_idx:\n",
        "            b = np.random.choice(np.where(idx[fg_class]==True)[0],size=1)\n",
        "            a.append(x[b])\n",
        "#             print(\"foreground \"+str(fg_class)+\" present at \" + str(fg_idx))\n",
        "        else:\n",
        "            bg_class = np.random.randint(2,3)\n",
        "            b = np.random.choice(np.where(idx[bg_class]==True)[0],size=1)\n",
        "            a.append(x[b])\n",
        "#             print(\"background \"+str(bg_class)+\" present at \" + str(i))\n",
        "    a = np.concatenate(a,axis=0)\n",
        "    mosaic_list_of_images.append(np.reshape(a,(9,1)))\n",
        "    mosaic_label.append(fg_class)\n",
        "    fore_idx.append(fg_idx)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOsFmWfMr0TR"
      },
      "source": [
        "mosaic_list_of_images = np.concatenate(mosaic_list_of_images,axis=1).T\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2aIPMgLXNiXW",
        "outputId": "3c53f0d3-58ed-4ee0-a650-c903eb9bef68"
      },
      "source": [
        "mosaic_list_of_images.shape, mosaic_list_of_images[0]"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((20000, 9),\n",
              " array([ 2.57383607,  2.74792999,  2.64740665,  2.68100263,  2.25277891,\n",
              "        -0.39131949,  2.17435623,  2.40378039,  2.02468417]))"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A3qcsbbzPfRG",
        "outputId": "56a7aa85-12dc-49fc-b92b-92900b3f14d0"
      },
      "source": [
        "for j in range(9):\n",
        "  print(mosaic_list_of_images[1][j])\n",
        "  "
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.388687880796558\n",
            "2.3807168997738155\n",
            "2.018097375943883\n",
            "2.7039373167343776\n",
            "2.197069764484894\n",
            "2.075070857999611\n",
            "2.1596484472843214\n",
            "2.569785999638043\n",
            "0.6042359616682469\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOzkdRYvsQrY",
        "outputId": "f19be997-bc6b-4eb3-f9bf-7f70fcdd33e9"
      },
      "source": [
        "fore_idx[1],mosaic_label[1]"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPoIwbMHx44n"
      },
      "source": [
        "class MosaicDataset(Dataset):\n",
        "  \"\"\"MosaicDataset dataset.\"\"\"\n",
        "\n",
        "  def __init__(self, mosaic_list_of_images, mosaic_label, fore_idx):\n",
        "    \"\"\"\n",
        "      Args:\n",
        "        csv_file (string): Path to the csv file with annotations.\n",
        "        root_dir (string): Directory with all the images.\n",
        "        transform (callable, optional): Optional transform to be applied\n",
        "            on a sample.\n",
        "    \"\"\"\n",
        "    self.mosaic = mosaic_list_of_images\n",
        "    self.label = mosaic_label\n",
        "    self.fore_idx = fore_idx\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.label)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.mosaic[idx] , self.label[idx], self.fore_idx[idx]\n",
        "\n"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOPAJQJeW8Ah"
      },
      "source": [
        "batch = 10000\n",
        "msd1 = MosaicDataset(mosaic_list_of_images[0:10000], mosaic_label[0:10000] , fore_idx[0:10000])\n",
        "train_loader = DataLoader( msd1 ,batch_size= batch ,shuffle=True)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjNiQgxZW8bA"
      },
      "source": [
        "batch = 10000\n",
        "msd2 = MosaicDataset(mosaic_list_of_images[10000:20000], mosaic_label[10000:20000] , fore_idx[10000:20000])\n",
        "test_loader = DataLoader( msd2 ,batch_size= batch ,shuffle=True)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30ZAjix3x8CM"
      },
      "source": [
        "class Focus(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Focus, self).__init__()\n",
        "\n",
        "    self.fc1 = nn.Linear(1, 1, bias= False)\n",
        "    # self.fc2 = nn.Linear(2, 1)\n",
        "\n",
        "  def forward(self,z):  #y is avg image #z batch of list of 9 images\n",
        "    y = torch.zeros([batch], dtype=torch.float64)\n",
        "    x = torch.zeros([batch,9],dtype=torch.float64)\n",
        "    y = y.to(\"cuda\")\n",
        "    x = x.to(\"cuda\")\n",
        "    for i in range(9):\n",
        "      x[:,i] = self.helper(z[:,i])[:,0]\n",
        "\n",
        "    x = F.softmax(x,dim=1)\n",
        "\n",
        "    for i in range(9):            \n",
        "      # x1 = x[:,i]          \n",
        "      y = y + torch.mul(x[:,i],z[:,i])\n",
        "\n",
        "    # print(x.shape, y.shape)\n",
        "    return x, y\n",
        "    \n",
        "  def helper(self, x):\n",
        "    x = x.view(-1, 1)\n",
        "    # x = F.relu(self.fc1(x))\n",
        "    x = (self.fc1(x))\n",
        "\n",
        "    return x"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dYXnywAD-4l"
      },
      "source": [
        "class Classification(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Classification, self).__init__()\n",
        "    self.fc1 = nn.Linear(1, 1)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x.view(-1, 1)\n",
        "    x = self.fc1(x)\n",
        "    # print(x.shape)\n",
        "    return x"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQfSQzYjtat-"
      },
      "source": [
        ""
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yl41sE8vFERk"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "def train(train_loader, test_loader, focus_net, classify):    \n",
        "  criterion = nn.BCEWithLogitsLoss()\n",
        "  #optimizer_classify = optim.SGD(classify.parameters(), lr=0.01 ) #, momentum=0.9)\n",
        "  optimizer_focus = optim.SGD(focus_net.parameters(), lr=0.01 ) #, momentum=0.9)\n",
        "\n",
        "  print('-'*50)\n",
        "  print(focus_net.fc1.weight, classify.fc1.weight, classify.fc1.bias)\n",
        "  nos_epochs = 3000\n",
        "  loss_ret=0.0\n",
        "  every_what_epoch = 1\n",
        "\n",
        "  for epoch in range(nos_epochs):  # loop over the dataset multiple times\n",
        "    \n",
        "    running_loss = 0.0\n",
        "    epoch_loss = []\n",
        "    cnt=0\n",
        "\n",
        "    iteration = desired_num // batch\n",
        "    \n",
        "    #training data set\n",
        "    #if ((epoch) % (every_what_epoch*2) ) <= every_what_epoch-1 :\n",
        "      #print(epoch+1,\"updating focus_net, classify_net is freezed\")\n",
        "      #print(\"--\"*40)\n",
        "    # elif ((epoch) % (every_what_epoch*2)) > every_what_epoch-1 :\n",
        "    #   print(epoch+1,\"updating classify_net, focus_net is freezed\")\n",
        "    #   print(\"--\"*40)    \n",
        "    for i, data in  enumerate(train_loader):\n",
        "      #print(i)\n",
        "      inputs , labels , fore_idx = data\n",
        "      inputs, labels = inputs.to(\"cuda\"), labels.to(\"cuda\")\n",
        "      inputs = inputs.double()\n",
        "      labels = labels.float()\n",
        "      # zero the parameter gradients\n",
        "      \n",
        "      optimizer_focus.zero_grad()\n",
        "      #optimizer_classify.zero_grad()\n",
        "      \n",
        "      alphas, avg_images = focus_net(inputs)\n",
        "      outputs = classify(avg_images)\n",
        "\n",
        "      predicted = np.round(torch.sigmoid(outputs.data).cpu().numpy())\n",
        "\n",
        "      loss = criterion(outputs[:,0], labels) \n",
        "      loss.backward()\n",
        "      if ((epoch) % (every_what_epoch*2) ) <= every_what_epoch-1 :\n",
        "        optimizer_focus.step()\n",
        "      # elif ( (epoch) % (every_what_epoch*2)) > every_what_epoch-1 :\n",
        "      #   optimizer_classify.step()\n",
        "\n",
        "      running_loss += loss.item()\n",
        "      # mini = 3\n",
        "      # if cnt % mini == mini-1 :    # print every 40 mini-batches\n",
        "      epoch_loss.append(running_loss)\n",
        "      running_loss = 0.0\n",
        "      cnt=cnt+1\n",
        "    loss_ret = np.mean(epoch_loss)\n",
        "    if(epoch%200==0):\n",
        "      print('[%d, %5d] loss: %.3f' %(epoch + 1, cnt + 1, np.mean(epoch_loss)))\n",
        "    if(np.mean(epoch_loss) <= 0.01):\n",
        "        break;\n",
        "\n",
        "  with torch.no_grad():\n",
        "    focus_true_pred_true =0\n",
        "    focus_false_pred_true =0\n",
        "    focus_true_pred_false =0\n",
        "    focus_false_pred_false =0\n",
        "\n",
        "    argmax_more_than_half = 0\n",
        "    argmax_less_than_half =0\n",
        "    for data in test_loader:\n",
        "      inputs, labels , fore_idx = data\n",
        "      inputs = inputs.double()\n",
        "      inputs, labels = inputs.to(\"cuda\"), labels.to(\"cuda\")\n",
        "      alphas, avg_images = focus_net(inputs)\n",
        "      outputs = classify(avg_images)\n",
        "      predicted = np.round(torch.sigmoid(outputs.data).cpu().numpy())\n",
        "\n",
        "      for j in range (batch):\n",
        "        focus = torch.argmax(alphas[j])\n",
        "\n",
        "        if(alphas[j][focus] >= 0.5):\n",
        "          argmax_more_than_half +=1\n",
        "        else:\n",
        "          argmax_less_than_half +=1\n",
        "\n",
        "        if(focus == fore_idx[j] and predicted[j] == labels[j].item()):\n",
        "          focus_true_pred_true += 1\n",
        "\n",
        "        elif(focus != fore_idx[j] and predicted[j] == labels[j].item()):\n",
        "          focus_false_pred_true +=1\n",
        "\n",
        "        elif(focus == fore_idx[j] and predicted[j] != labels[j].item()):\n",
        "          focus_true_pred_false +=1\n",
        "\n",
        "        elif(focus != fore_idx[j] and predicted[j] != labels[j].item()):\n",
        "          focus_false_pred_false +=1\n",
        "\n",
        "  print('Finished Training')  \n",
        "  return loss_ret, argmax_more_than_half/100, focus_true_pred_true/100, focus_false_pred_true/100, focus_true_pred_false/100 , focus_false_pred_false/100, focus_net.fc1.weight.item(), classify.fc1.weight.item(), classify.fc1.bias.item()\n",
        "    \n"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fd7ScJj9g6rl"
      },
      "source": [
        "a =  [0.]\n",
        "b = [-10.,-5.,-2.,0.,2.,5.,10.]\n",
        "c = [-10.,-5.,-2.,0.,2.,5.,10.]"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zo5QyZobhHpM",
        "outputId": "082b799d-bd98-4446-f131-2dd5b894bb4d"
      },
      "source": [
        "all_loss=[]\n",
        "# all_alphas_more_than_half=[]\n",
        "all_ftpt=[]\n",
        "all_ffpt=[]\n",
        "all_ftpf = []\n",
        "all_ffpf= []\n",
        "init_a = []\n",
        "init_b = []\n",
        "init_c = []\n",
        "final_a=[]\n",
        "final_b = []\n",
        "final_c = []\n",
        "\n",
        "for a1 in a:\n",
        "  for b1 in b:\n",
        "    for c1 in c:\n",
        "      print(\"for a value %.3f, b value %.3f, and c value %.3f \" %(a1,b1,c1))\n",
        "      print(\"*\"*70)\n",
        "      torch.manual_seed(12)\n",
        "      focus_net = Focus().double()\n",
        "      focus_net.fc1.weight = torch.nn.Parameter(torch.tensor(np.array([[a1]])))\n",
        "      torch.manual_seed(12)\n",
        "      classify = Classification().double()\n",
        "      classify.fc1.weight = torch.nn.Parameter(torch.tensor(np.array([[b1]])))\n",
        "      classify.fc1.bias = torch.nn.Parameter(torch.tensor(np.array([c1])))\n",
        "      focus_net = focus_net.to(\"cuda\")\n",
        "      classify = classify.to(\"cuda\")\n",
        "      print(\"--\"*40,\"a,b,c = \",a1,b1,c1)\n",
        "      cost, alpha_per, ftpt, ffpt, ftpf, ffpf, f_a, f_b, f_c = train(train_loader, test_loader, focus_net, classify)\n",
        "      print(cost, alpha_per, ftpt, ffpt, ftpf, ffpf)\n",
        "      init_a.append(a1)\n",
        "      init_b.append(b1)\n",
        "      init_c.append(c1)\n",
        "      final_a.append(np.round(f_a,3))\n",
        "      final_b.append(np.round(f_b,3))\n",
        "      final_c.append(np.round(f_c,3))\n",
        "      all_loss.append(np.round(cost,3))\n",
        "      # all_alphas_more_than_half.append(alpha_per)\n",
        "      all_ftpt.append(ftpt)\n",
        "      all_ffpt.append(ffpt)\n",
        "      all_ftpf.append(ftpf)\n",
        "      all_ffpf.append(ffpf)\n",
        "\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "for a value 0.000, b value -10.000, and c value -10.000 \n",
            "**********************************************************************\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 -10.0 -10.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[-10.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([-10.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     2] loss: 16.403\n",
            "[201,     2] loss: 8.049\n",
            "[401,     2] loss: 7.726\n",
            "[601,     2] loss: 7.641\n",
            "[801,     2] loss: 7.601\n",
            "[1001,     2] loss: 7.579\n",
            "[1201,     2] loss: 7.564\n",
            "[1401,     2] loss: 7.553\n",
            "[1601,     2] loss: 7.545\n",
            "[1801,     2] loss: 7.539\n",
            "[2001,     2] loss: 7.535\n",
            "[2201,     2] loss: 7.531\n",
            "[2401,     2] loss: 7.527\n",
            "[2601,     2] loss: 7.525\n",
            "[2801,     2] loss: 7.522\n",
            "Finished Training\n",
            "7.5201311111450195 100.0 49.02 0.0 50.98 0.0\n",
            "for a value 0.000, b value -10.000, and c value -5.000 \n",
            "**********************************************************************\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 -10.0 -5.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[-10.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([-5.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     2] loss: 13.901\n",
            "[201,     2] loss: 6.249\n",
            "[401,     2] loss: 5.885\n",
            "[601,     2] loss: 5.791\n",
            "[801,     2] loss: 5.748\n",
            "[1001,     2] loss: 5.724\n",
            "[1201,     2] loss: 5.708\n",
            "[1401,     2] loss: 5.697\n",
            "[1601,     2] loss: 5.689\n",
            "[1801,     2] loss: 5.683\n",
            "[2001,     2] loss: 5.678\n",
            "[2201,     2] loss: 5.674\n",
            "[2401,     2] loss: 5.670\n",
            "[2601,     2] loss: 5.667\n",
            "[2801,     2] loss: 5.665\n",
            "Finished Training\n",
            "5.6629462242126465 100.0 24.68 0.0 75.32 0.0\n",
            "for a value 0.000, b value -10.000, and c value -2.000 \n",
            "**********************************************************************\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 -10.0 -2.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[-10.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([-2.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     2] loss: 12.399\n",
            "[201,     2] loss: 5.819\n",
            "[401,     2] loss: 5.375\n",
            "[601,     2] loss: 5.266\n",
            "[801,     2] loss: 5.218\n",
            "[1001,     2] loss: 5.191\n",
            "[1201,     2] loss: 5.174\n",
            "[1401,     2] loss: 5.162\n",
            "[1601,     2] loss: 5.153\n",
            "[1801,     2] loss: 5.147\n",
            "[2001,     2] loss: 5.141\n",
            "[2201,     2] loss: 5.137\n",
            "[2401,     2] loss: 5.133\n",
            "[2601,     2] loss: 5.130\n",
            "[2801,     2] loss: 5.128\n",
            "Finished Training\n",
            "5.1255950927734375 100.0 10.42 0.0 89.58 0.0\n",
            "for a value 0.000, b value -10.000, and c value 0.000 \n",
            "**********************************************************************\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 -10.0 0.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[-10.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([0.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     2] loss: 11.398\n",
            "[201,     2] loss: 5.878\n",
            "[401,     2] loss: 5.312\n",
            "[601,     2] loss: 5.179\n",
            "[801,     2] loss: 5.123\n",
            "[1001,     2] loss: 5.092\n",
            "[1201,     2] loss: 5.073\n",
            "[1401,     2] loss: 5.060\n",
            "[1601,     2] loss: 5.050\n",
            "[1801,     2] loss: 5.043\n",
            "[2001,     2] loss: 5.037\n",
            "[2201,     2] loss: 5.033\n",
            "[2401,     2] loss: 5.029\n",
            "[2601,     2] loss: 5.026\n",
            "[2801,     2] loss: 5.023\n",
            "Finished Training\n",
            "5.020803928375244 100.0 0.04 0.0 99.96 0.0\n",
            "for a value 0.000, b value -10.000, and c value 2.000 \n",
            "**********************************************************************\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 -10.0 2.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[-10.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([2.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     2] loss: 10.397\n",
            "[201,     2] loss: 6.375\n",
            "[401,     2] loss: 5.552\n",
            "[601,     2] loss: 5.333\n",
            "[801,     2] loss: 5.252\n",
            "[1001,     2] loss: 5.210\n",
            "[1201,     2] loss: 5.186\n",
            "[1401,     2] loss: 5.170\n",
            "[1601,     2] loss: 5.158\n",
            "[1801,     2] loss: 5.149\n",
            "[2001,     2] loss: 5.143\n",
            "[2201,     2] loss: 5.137\n",
            "[2401,     2] loss: 5.133\n",
            "[2601,     2] loss: 5.129\n",
            "[2801,     2] loss: 5.126\n",
            "Finished Training\n",
            "5.123537063598633 100.0 10.55 0.0 89.45 0.0\n",
            "for a value 0.000, b value -10.000, and c value 5.000 \n",
            "**********************************************************************\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 -10.0 5.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[-10.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([5.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     2] loss: 8.896\n",
            "[201,     2] loss: 6.154\n",
            "[401,     2] loss: 6.154\n",
            "[601,     2] loss: 6.154\n",
            "[801,     2] loss: 6.154\n",
            "[1001,     2] loss: 6.154\n",
            "[1201,     2] loss: 6.154\n",
            "[1401,     2] loss: 6.154\n",
            "[1601,     2] loss: 6.154\n",
            "[1801,     2] loss: 6.154\n",
            "[2001,     2] loss: 6.154\n",
            "[2201,     2] loss: 6.154\n",
            "[2401,     2] loss: 6.154\n",
            "[2601,     2] loss: 6.154\n",
            "[2801,     2] loss: 6.154\n",
            "Finished Training\n",
            "6.153514862060547 57.16 26.46 0.0 73.54 0.0\n",
            "for a value 0.000, b value -10.000, and c value 10.000 \n",
            "**********************************************************************\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 -10.0 10.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[-10.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([10.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     2] loss: 6.393\n",
            "[201,     2] loss: 4.606\n",
            "[401,     2] loss: 4.606\n",
            "[601,     2] loss: 4.606\n",
            "[801,     2] loss: 4.606\n",
            "[1001,     2] loss: 4.606\n",
            "[1201,     2] loss: 4.606\n",
            "[1401,     2] loss: 4.606\n",
            "[1601,     2] loss: 4.606\n",
            "[1801,     2] loss: 4.606\n",
            "[2001,     2] loss: 4.606\n",
            "[2201,     2] loss: 4.606\n",
            "[2401,     2] loss: 4.606\n",
            "[2601,     2] loss: 4.606\n",
            "[2801,     2] loss: 4.606\n",
            "Finished Training\n",
            "4.606182098388672 6.29 32.47 0.0 67.53 0.0\n",
            "for a value 0.000, b value -5.000, and c value -10.000 \n",
            "**********************************************************************\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 -5.0 -10.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[-5.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([-10.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     2] loss: 10.704\n",
            "[201,     2] loss: 7.061\n",
            "[401,     2] loss: 6.508\n",
            "[601,     2] loss: 6.395\n",
            "[801,     2] loss: 6.347\n",
            "[1001,     2] loss: 6.321\n",
            "[1201,     2] loss: 6.304\n",
            "[1401,     2] loss: 6.293\n",
            "[1601,     2] loss: 6.285\n",
            "[1801,     2] loss: 6.278\n",
            "[2001,     2] loss: 6.273\n",
            "[2201,     2] loss: 6.269\n",
            "[2401,     2] loss: 6.266\n",
            "[2601,     2] loss: 6.263\n",
            "[2801,     2] loss: 6.261\n",
            "Finished Training\n",
            "6.258599281311035 100.0 49.02 0.0 50.98 0.0\n",
            "for a value 0.000, b value -5.000, and c value -5.000 \n",
            "**********************************************************************\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 -5.0 -5.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[-5.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([-5.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     2] loss: 8.202\n",
            "[201,     2] loss: 4.644\n",
            "[401,     2] loss: 4.090\n",
            "[601,     2] loss: 3.973\n",
            "[801,     2] loss: 3.925\n",
            "[1001,     2] loss: 3.898\n",
            "[1201,     2] loss: 3.881\n",
            "[1401,     2] loss: 3.870\n",
            "[1601,     2] loss: 3.861\n",
            "[1801,     2] loss: 3.855\n",
            "[2001,     2] loss: 3.850\n",
            "[2201,     2] loss: 3.845\n",
            "[2401,     2] loss: 3.842\n",
            "[2601,     2] loss: 3.839\n",
            "[2801,     2] loss: 3.837\n",
            "Finished Training\n",
            "3.8346385955810547 100.0 49.02 0.0 50.98 0.0\n",
            "for a value 0.000, b value -5.000, and c value -2.000 \n",
            "**********************************************************************\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 -5.0 -2.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[-5.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([-2.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     2] loss: 6.700\n",
            "[201,     2] loss: 3.726\n",
            "[401,     2] loss: 3.150\n",
            "[601,     2] loss: 3.010\n",
            "[801,     2] loss: 2.953\n",
            "[1001,     2] loss: 2.922\n",
            "[1201,     2] loss: 2.903\n",
            "[1401,     2] loss: 2.890\n",
            "[1601,     2] loss: 2.880\n",
            "[1801,     2] loss: 2.873\n",
            "[2001,     2] loss: 2.868\n",
            "[2201,     2] loss: 2.863\n",
            "[2401,     2] loss: 2.860\n",
            "[2601,     2] loss: 2.856\n",
            "[2801,     2] loss: 2.854\n",
            "Finished Training\n",
            "2.8515944480895996 100.0 20.0 0.0 80.0 0.0\n",
            "for a value 0.000, b value -5.000, and c value 0.000 \n",
            "**********************************************************************\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 -5.0 0.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[-5.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([0.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     2] loss: 5.699\n",
            "[201,     2] loss: 3.563\n",
            "[401,     2] loss: 3.070\n",
            "[601,     2] loss: 2.867\n",
            "[801,     2] loss: 2.784\n",
            "[1001,     2] loss: 2.741\n",
            "[1201,     2] loss: 2.716\n",
            "[1401,     2] loss: 2.699\n",
            "[1601,     2] loss: 2.688\n",
            "[1801,     2] loss: 2.679\n",
            "[2001,     2] loss: 2.672\n",
            "[2201,     2] loss: 2.667\n",
            "[2401,     2] loss: 2.662\n",
            "[2601,     2] loss: 2.659\n",
            "[2801,     2] loss: 2.656\n",
            "Finished Training\n",
            "2.6530845165252686 100.0 0.23 0.0 99.77 0.0\n",
            "for a value 0.000, b value -5.000, and c value 2.000 \n",
            "**********************************************************************\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 -5.0 2.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[-5.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([2.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     2] loss: 4.698\n",
            "[201,     2] loss: 3.300\n",
            "[401,     2] loss: 3.298\n",
            "[601,     2] loss: 3.298\n",
            "[801,     2] loss: 3.298\n",
            "[1001,     2] loss: 3.298\n",
            "[1201,     2] loss: 3.298\n",
            "[1401,     2] loss: 3.298\n",
            "[1601,     2] loss: 3.298\n",
            "[1801,     2] loss: 3.298\n",
            "[2001,     2] loss: 3.298\n",
            "[2201,     2] loss: 3.298\n",
            "[2401,     2] loss: 3.298\n",
            "[2601,     2] loss: 3.298\n",
            "[2801,     2] loss: 3.298\n",
            "Finished Training\n",
            "3.298400402069092 71.1 22.68 0.0 77.32 0.0\n",
            "for a value 0.000, b value -5.000, and c value 5.000 \n",
            "**********************************************************************\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 -5.0 5.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[-5.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([5.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     2] loss: 3.199\n",
            "[201,     2] loss: 2.421\n",
            "[401,     2] loss: 2.421\n",
            "[601,     2] loss: 2.421\n",
            "[801,     2] loss: 2.421\n",
            "[1001,     2] loss: 2.421\n",
            "[1201,     2] loss: 2.421\n",
            "[1401,     2] loss: 2.421\n",
            "[1601,     2] loss: 2.421\n",
            "[1801,     2] loss: 2.421\n",
            "[2001,     2] loss: 2.421\n",
            "[2201,     2] loss: 2.421\n",
            "[2401,     2] loss: 2.421\n",
            "[2601,     2] loss: 2.421\n",
            "[2801,     2] loss: 2.421\n",
            "Finished Training\n",
            "2.4212918281555176 3.66 34.18 0.0 65.82 0.0\n",
            "for a value 0.000, b value -5.000, and c value 10.000 \n",
            "**********************************************************************\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 -5.0 10.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[-5.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([10.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     2] loss: 1.006\n",
            "[201,     2] loss: 1.001\n",
            "[401,     2] loss: 1.001\n",
            "[601,     2] loss: 1.001\n",
            "[801,     2] loss: 1.001\n",
            "[1001,     2] loss: 1.001\n",
            "[1201,     2] loss: 1.001\n",
            "[1401,     2] loss: 1.001\n",
            "[1601,     2] loss: 1.001\n",
            "[1801,     2] loss: 1.001\n",
            "[2001,     2] loss: 1.001\n",
            "[2201,     2] loss: 1.001\n",
            "[2401,     2] loss: 1.001\n",
            "[2601,     2] loss: 1.001\n",
            "[2801,     2] loss: 1.001\n",
            "Finished Training\n",
            "1.0007737874984741 0.0 43.46 0.0 56.54 0.0\n",
            "for a value 0.000, b value -2.000, and c value -10.000 \n",
            "**********************************************************************\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 -2.0 -10.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[-2.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([-10.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     2] loss: 7.285\n",
            "[201,     2] loss: 6.801\n",
            "[401,     2] loss: 5.998\n",
            "[601,     2] loss: 5.737\n",
            "[801,     2] loss: 5.648\n",
            "[1001,     2] loss: 5.606\n",
            "[1201,     2] loss: 5.582\n",
            "[1401,     2] loss: 5.567\n",
            "[1601,     2] loss: 5.556\n",
            "[1801,     2] loss: 5.548\n",
            "[2001,     2] loss: 5.542\n",
            "[2201,     2] loss: 5.537\n",
            "[2401,     2] loss: 5.533\n",
            "[2601,     2] loss: 5.530\n",
            "[2801,     2] loss: 5.527\n",
            "Finished Training\n",
            "5.524612903594971 100.0 49.02 0.0 50.98 0.0\n",
            "for a value 0.000, b value -2.000, and c value -5.000 \n",
            "**********************************************************************\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 -2.0 -5.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[-2.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([-5.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     2] loss: 4.782\n",
            "[201,     2] loss: 4.300\n",
            "[401,     2] loss: 3.508\n",
            "[601,     2] loss: 3.247\n",
            "[801,     2] loss: 3.158\n",
            "[1001,     2] loss: 3.116\n",
            "[1201,     2] loss: 3.092\n",
            "[1401,     2] loss: 3.076\n",
            "[1601,     2] loss: 3.065\n",
            "[1801,     2] loss: 3.057\n",
            "[2001,     2] loss: 3.051\n",
            "[2201,     2] loss: 3.046\n",
            "[2401,     2] loss: 3.042\n",
            "[2601,     2] loss: 3.039\n",
            "[2801,     2] loss: 3.036\n",
            "Finished Training\n",
            "3.034012794494629 100.0 49.02 0.0 50.98 0.0\n",
            "for a value 0.000, b value -2.000, and c value -2.000 \n",
            "**********************************************************************\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 -2.0 -2.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[-2.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([-2.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     2] loss: 3.282\n",
            "[201,     2] loss: 2.823\n",
            "[401,     2] loss: 2.203\n",
            "[601,     2] loss: 1.957\n",
            "[801,     2] loss: 1.860\n",
            "[1001,     2] loss: 1.812\n",
            "[1201,     2] loss: 1.785\n",
            "[1401,     2] loss: 1.767\n",
            "[1601,     2] loss: 1.755\n",
            "[1801,     2] loss: 1.746\n",
            "[2001,     2] loss: 1.740\n",
            "[2201,     2] loss: 1.734\n",
            "[2401,     2] loss: 1.730\n",
            "[2601,     2] loss: 1.726\n",
            "[2801,     2] loss: 1.723\n",
            "Finished Training\n",
            "1.7209542989730835 100.0 49.02 0.0 50.98 0.0\n",
            "for a value 0.000, b value -2.000, and c value 0.000 \n",
            "**********************************************************************\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 -2.0 0.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[-2.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([0.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     2] loss: 2.292\n",
            "[201,     2] loss: 1.938\n",
            "[401,     2] loss: 1.736\n",
            "[601,     2] loss: 1.675\n",
            "[801,     2] loss: 1.616\n",
            "[1001,     2] loss: 1.554\n",
            "[1201,     2] loss: 1.502\n",
            "[1401,     2] loss: 1.465\n",
            "[1601,     2] loss: 1.439\n",
            "[1801,     2] loss: 1.421\n",
            "[2001,     2] loss: 1.407\n",
            "[2201,     2] loss: 1.397\n",
            "[2401,     2] loss: 1.389\n",
            "[2601,     2] loss: 1.383\n",
            "[2801,     2] loss: 1.378\n",
            "Finished Training\n",
            "1.3736835718154907 100.0 0.75 0.0 99.25 0.0\n",
            "for a value 0.000, b value -2.000, and c value 2.000 \n",
            "**********************************************************************\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 -2.0 2.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[-2.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([2.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     2] loss: 1.364\n",
            "[201,     2] loss: 1.244\n",
            "[401,     2] loss: 1.221\n",
            "[601,     2] loss: 1.220\n",
            "[801,     2] loss: 1.220\n",
            "[1001,     2] loss: 1.220\n",
            "[1201,     2] loss: 1.220\n",
            "[1401,     2] loss: 1.220\n",
            "[1601,     2] loss: 1.220\n",
            "[1801,     2] loss: 1.220\n",
            "[2001,     2] loss: 1.220\n",
            "[2201,     2] loss: 1.220\n",
            "[2401,     2] loss: 1.220\n",
            "[2601,     2] loss: 1.220\n",
            "[2801,     2] loss: 1.220\n",
            "Finished Training\n",
            "1.2201908826828003 0.0 48.67 0.0 51.33 0.0\n",
            "for a value 0.000, b value -2.000, and c value 5.000 \n",
            "**********************************************************************\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 -2.0 5.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[-2.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([5.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     2] loss: 0.790\n",
            "[201,     2] loss: 0.718\n",
            "[401,     2] loss: 0.704\n",
            "[601,     2] loss: 0.699\n",
            "[801,     2] loss: 0.696\n",
            "[1001,     2] loss: 0.695\n",
            "[1201,     2] loss: 0.694\n",
            "[1401,     2] loss: 0.693\n",
            "[1601,     2] loss: 0.692\n",
            "[1801,     2] loss: 0.692\n",
            "[2001,     2] loss: 0.692\n",
            "[2201,     2] loss: 0.692\n",
            "[2401,     2] loss: 0.692\n",
            "[2601,     2] loss: 0.691\n",
            "[2801,     2] loss: 0.691\n",
            "Finished Training\n",
            "0.6913507580757141 0.0 0.0 53.81 0.0 46.19\n",
            "for a value 0.000, b value -2.000, and c value 10.000 \n",
            "**********************************************************************\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 -2.0 10.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[-2.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([10.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     2] loss: 2.832\n",
            "[201,     2] loss: 2.544\n",
            "[401,     2] loss: 2.488\n",
            "[601,     2] loss: 2.462\n",
            "[801,     2] loss: 2.445\n",
            "[1001,     2] loss: 2.433\n",
            "[1201,     2] loss: 2.424\n",
            "[1401,     2] loss: 2.416\n",
            "[1601,     2] loss: 2.409\n",
            "[1801,     2] loss: 2.402\n",
            "[2001,     2] loss: 2.397\n",
            "[2201,     2] loss: 2.391\n",
            "[2401,     2] loss: 2.386\n",
            "[2601,     2] loss: 2.382\n",
            "[2801,     2] loss: 2.377\n",
            "Finished Training\n",
            "2.373033046722412 0.0 0.0 50.98 0.0 49.02\n",
            "for a value 0.000, b value 0.000, and c value -10.000 \n",
            "**********************************************************************\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 0.0 -10.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([-10.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     2] loss: 5.005\n",
            "[201,     2] loss: 5.005\n",
            "[401,     2] loss: 5.005\n",
            "[601,     2] loss: 5.005\n",
            "[801,     2] loss: 5.005\n",
            "[1001,     2] loss: 5.005\n",
            "[1201,     2] loss: 5.005\n",
            "[1401,     2] loss: 5.005\n",
            "[1601,     2] loss: 5.005\n",
            "[1801,     2] loss: 5.005\n",
            "[2001,     2] loss: 5.005\n",
            "[2201,     2] loss: 5.005\n",
            "[2401,     2] loss: 5.005\n",
            "[2601,     2] loss: 5.005\n",
            "[2801,     2] loss: 5.005\n",
            "Finished Training\n",
            "5.0050458908081055 0.0 4.94 44.08 5.28 45.7\n",
            "for a value 0.000, b value 0.000, and c value -5.000 \n",
            "**********************************************************************\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 0.0 -5.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([-5.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     2] loss: 2.509\n",
            "[201,     2] loss: 2.509\n",
            "[401,     2] loss: 2.509\n",
            "[601,     2] loss: 2.509\n",
            "[801,     2] loss: 2.509\n",
            "[1001,     2] loss: 2.509\n",
            "[1201,     2] loss: 2.509\n",
            "[1401,     2] loss: 2.509\n",
            "[1601,     2] loss: 2.509\n",
            "[1801,     2] loss: 2.509\n",
            "[2001,     2] loss: 2.509\n",
            "[2201,     2] loss: 2.509\n",
            "[2401,     2] loss: 2.509\n",
            "[2601,     2] loss: 2.509\n",
            "[2801,     2] loss: 2.509\n",
            "Finished Training\n",
            "2.5092153549194336 0.0 4.94 44.08 5.28 45.7\n",
            "for a value 0.000, b value 0.000, and c value -2.000 \n",
            "**********************************************************************\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 0.0 -2.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([-2.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     2] loss: 1.128\n",
            "[201,     2] loss: 1.128\n",
            "[401,     2] loss: 1.128\n",
            "[601,     2] loss: 1.128\n",
            "[801,     2] loss: 1.128\n",
            "[1001,     2] loss: 1.128\n",
            "[1201,     2] loss: 1.128\n",
            "[1401,     2] loss: 1.128\n",
            "[1601,     2] loss: 1.128\n",
            "[1801,     2] loss: 1.128\n",
            "[2001,     2] loss: 1.128\n",
            "[2201,     2] loss: 1.128\n",
            "[2401,     2] loss: 1.128\n",
            "[2601,     2] loss: 1.128\n",
            "[2801,     2] loss: 1.128\n",
            "Finished Training\n",
            "1.1279281377792358 0.0 4.94 44.08 5.28 45.7\n",
            "for a value 0.000, b value 0.000, and c value 0.000 \n",
            "**********************************************************************\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 0.0 0.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([0.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     2] loss: 0.693\n",
            "[201,     2] loss: 0.693\n",
            "[401,     2] loss: 0.693\n",
            "[601,     2] loss: 0.693\n",
            "[801,     2] loss: 0.693\n",
            "[1001,     2] loss: 0.693\n",
            "[1201,     2] loss: 0.693\n",
            "[1401,     2] loss: 0.693\n",
            "[1601,     2] loss: 0.693\n",
            "[1801,     2] loss: 0.693\n",
            "[2001,     2] loss: 0.693\n",
            "[2201,     2] loss: 0.693\n",
            "[2401,     2] loss: 0.693\n",
            "[2601,     2] loss: 0.693\n",
            "[2801,     2] loss: 0.693\n",
            "Finished Training\n",
            "0.6931471228599548 0.0 4.94 44.08 5.28 45.7\n",
            "for a value 0.000, b value 0.000, and c value 2.000 \n",
            "**********************************************************************\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 0.0 2.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([2.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     2] loss: 1.126\n",
            "[201,     2] loss: 1.126\n",
            "[401,     2] loss: 1.126\n",
            "[601,     2] loss: 1.126\n",
            "[801,     2] loss: 1.126\n",
            "[1001,     2] loss: 1.126\n",
            "[1201,     2] loss: 1.126\n",
            "[1401,     2] loss: 1.126\n",
            "[1601,     2] loss: 1.126\n",
            "[1801,     2] loss: 1.126\n",
            "[2001,     2] loss: 1.126\n",
            "[2201,     2] loss: 1.126\n",
            "[2401,     2] loss: 1.126\n",
            "[2601,     2] loss: 1.126\n",
            "[2801,     2] loss: 1.126\n",
            "Finished Training\n",
            "1.1259280443191528 0.0 5.28 45.7 4.94 44.08\n",
            "for a value 0.000, b value 0.000, and c value 5.000 \n",
            "**********************************************************************\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 0.0 5.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([5.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     2] loss: 2.504\n",
            "[201,     2] loss: 2.504\n",
            "[401,     2] loss: 2.504\n",
            "[601,     2] loss: 2.504\n",
            "[801,     2] loss: 2.504\n",
            "[1001,     2] loss: 2.504\n",
            "[1201,     2] loss: 2.504\n",
            "[1401,     2] loss: 2.504\n",
            "[1601,     2] loss: 2.504\n",
            "[1801,     2] loss: 2.504\n",
            "[2001,     2] loss: 2.504\n",
            "[2201,     2] loss: 2.504\n",
            "[2401,     2] loss: 2.504\n",
            "[2601,     2] loss: 2.504\n",
            "[2801,     2] loss: 2.504\n",
            "Finished Training\n",
            "2.5042154788970947 0.0 5.28 45.7 4.94 44.08\n",
            "for a value 0.000, b value 0.000, and c value 10.000 \n",
            "**********************************************************************\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 0.0 10.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([10.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     2] loss: 4.995\n",
            "[201,     2] loss: 4.995\n",
            "[401,     2] loss: 4.995\n",
            "[601,     2] loss: 4.995\n",
            "[801,     2] loss: 4.995\n",
            "[1001,     2] loss: 4.995\n",
            "[1201,     2] loss: 4.995\n",
            "[1401,     2] loss: 4.995\n",
            "[1601,     2] loss: 4.995\n",
            "[1801,     2] loss: 4.995\n",
            "[2001,     2] loss: 4.995\n",
            "[2201,     2] loss: 4.995\n",
            "[2401,     2] loss: 4.995\n",
            "[2601,     2] loss: 4.995\n",
            "[2801,     2] loss: 4.995\n",
            "Finished Training\n",
            "4.995046138763428 0.0 5.28 45.7 4.94 44.08\n",
            "for a value 0.000, b value 2.000, and c value -10.000 \n",
            "**********************************************************************\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 2.0 -10.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[2.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([-10.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     2] loss: 2.729\n",
            "[201,     2] loss: 2.594\n",
            "[401,     2] loss: 2.535\n",
            "[601,     2] loss: 2.500\n",
            "[801,     2] loss: 2.476\n",
            "[1001,     2] loss: 2.459\n",
            "[1201,     2] loss: 2.445\n",
            "[1401,     2] loss: 2.433\n",
            "[1601,     2] loss: 2.424\n",
            "[1801,     2] loss: 2.415\n",
            "[2001,     2] loss: 2.407\n",
            "[2201,     2] loss: 2.401\n",
            "[2401,     2] loss: 2.394\n",
            "[2601,     2] loss: 2.388\n",
            "[2801,     2] loss: 2.383\n",
            "Finished Training\n",
            "2.3778209686279297 0.0 0.0 49.02 0.0 50.98\n",
            "for a value 0.000, b value 2.000, and c value -5.000 \n",
            "**********************************************************************\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 2.0 -5.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[2.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([-5.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     2] loss: 0.683\n",
            "[201,     2] loss: 0.682\n",
            "[401,     2] loss: 0.681\n",
            "[601,     2] loss: 0.681\n",
            "[801,     2] loss: 0.681\n",
            "[1001,     2] loss: 0.681\n",
            "[1201,     2] loss: 0.681\n",
            "[1401,     2] loss: 0.681\n",
            "[1601,     2] loss: 0.681\n",
            "[1801,     2] loss: 0.681\n",
            "[2001,     2] loss: 0.681\n",
            "[2201,     2] loss: 0.681\n",
            "[2401,     2] loss: 0.681\n",
            "[2601,     2] loss: 0.681\n",
            "[2801,     2] loss: 0.681\n",
            "Finished Training\n",
            "0.6805455684661865 0.0 49.11 0.0 50.89 0.0\n",
            "for a value 0.000, b value 2.000, and c value -2.000 \n",
            "**********************************************************************\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 2.0 -2.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[2.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([-2.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     2] loss: 1.253\n",
            "[201,     2] loss: 0.341\n",
            "[401,     2] loss: 0.319\n",
            "[601,     2] loss: 0.319\n",
            "[801,     2] loss: 0.319\n",
            "[1001,     2] loss: 0.319\n",
            "[1201,     2] loss: 0.319\n",
            "[1401,     2] loss: 0.319\n",
            "[1601,     2] loss: 0.319\n",
            "[1801,     2] loss: 0.319\n",
            "[2001,     2] loss: 0.319\n",
            "[2201,     2] loss: 0.319\n",
            "[2401,     2] loss: 0.319\n",
            "[2601,     2] loss: 0.319\n",
            "[2801,     2] loss: 0.319\n",
            "Finished Training\n",
            "0.31864026188850403 76.37 96.37 0.0 3.63 0.0\n",
            "for a value 0.000, b value 2.000, and c value 0.000 \n",
            "**********************************************************************\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 2.0 0.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[2.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([0.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     2] loss: 2.179\n",
            "[201,     2] loss: 0.430\n",
            "[401,     2] loss: 0.343\n",
            "[601,     2] loss: 0.329\n",
            "[801,     2] loss: 0.324\n",
            "[1001,     2] loss: 0.322\n",
            "[1201,     2] loss: 0.321\n",
            "[1401,     2] loss: 0.320\n",
            "[1601,     2] loss: 0.320\n",
            "[1801,     2] loss: 0.319\n",
            "[2001,     2] loss: 0.319\n",
            "[2201,     2] loss: 0.319\n",
            "[2401,     2] loss: 0.319\n",
            "[2601,     2] loss: 0.319\n",
            "[2801,     2] loss: 0.319\n",
            "Finished Training\n",
            "0.31894996762275696 100.0 92.71 0.0 7.29 0.0\n",
            "for a value 0.000, b value 2.000, and c value 2.000 \n",
            "**********************************************************************\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 2.0 2.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[2.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([2.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     2] loss: 3.168\n",
            "[201,     2] loss: 0.948\n",
            "[401,     2] loss: 0.789\n",
            "[601,     2] loss: 0.755\n",
            "[801,     2] loss: 0.740\n",
            "[1001,     2] loss: 0.733\n",
            "[1201,     2] loss: 0.728\n",
            "[1401,     2] loss: 0.724\n",
            "[1601,     2] loss: 0.722\n",
            "[1801,     2] loss: 0.720\n",
            "[2001,     2] loss: 0.718\n",
            "[2201,     2] loss: 0.717\n",
            "[2401,     2] loss: 0.716\n",
            "[2601,     2] loss: 0.715\n",
            "[2801,     2] loss: 0.714\n",
            "Finished Training\n",
            "0.7138403654098511 100.0 50.98 0.0 49.02 0.0\n",
            "for a value 0.000, b value 2.000, and c value 5.000 \n",
            "**********************************************************************\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 2.0 5.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[2.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([5.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     2] loss: 4.665\n",
            "[201,     2] loss: 2.300\n",
            "[401,     2] loss: 2.114\n",
            "[601,     2] loss: 2.075\n",
            "[801,     2] loss: 2.058\n",
            "[1001,     2] loss: 2.049\n",
            "[1201,     2] loss: 2.044\n",
            "[1401,     2] loss: 2.040\n",
            "[1601,     2] loss: 2.037\n",
            "[1801,     2] loss: 2.034\n",
            "[2001,     2] loss: 2.033\n",
            "[2201,     2] loss: 2.031\n",
            "[2401,     2] loss: 2.030\n",
            "[2601,     2] loss: 2.029\n",
            "[2801,     2] loss: 2.028\n",
            "Finished Training\n",
            "2.0275845527648926 100.0 50.98 0.0 49.02 0.0\n",
            "for a value 0.000, b value 2.000, and c value 10.000 \n",
            "**********************************************************************\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 2.0 10.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[2.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([10.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     2] loss: 7.162\n",
            "[201,     2] loss: 4.789\n",
            "[401,     2] loss: 4.600\n",
            "[601,     2] loss: 4.561\n",
            "[801,     2] loss: 4.544\n",
            "[1001,     2] loss: 4.535\n",
            "[1201,     2] loss: 4.529\n",
            "[1401,     2] loss: 4.525\n",
            "[1601,     2] loss: 4.523\n",
            "[1801,     2] loss: 4.520\n",
            "[2001,     2] loss: 4.519\n",
            "[2201,     2] loss: 4.517\n",
            "[2401,     2] loss: 4.516\n",
            "[2601,     2] loss: 4.515\n",
            "[2801,     2] loss: 4.514\n",
            "Finished Training\n",
            "4.513360023498535 100.0 50.98 0.0 49.02 0.0\n",
            "for a value 0.000, b value 5.000, and c value -10.000 \n",
            "**********************************************************************\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 5.0 -10.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[5.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([-10.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     2] loss: 0.735\n",
            "[201,     2] loss: 0.402\n",
            "[401,     2] loss: 0.402\n",
            "[601,     2] loss: 0.402\n",
            "[801,     2] loss: 0.402\n",
            "[1001,     2] loss: 0.402\n",
            "[1201,     2] loss: 0.402\n",
            "[1401,     2] loss: 0.402\n",
            "[1601,     2] loss: 0.402\n",
            "[1801,     2] loss: 0.402\n",
            "[2001,     2] loss: 0.402\n",
            "[2201,     2] loss: 0.402\n",
            "[2401,     2] loss: 0.402\n",
            "[2601,     2] loss: 0.402\n",
            "[2801,     2] loss: 0.402\n",
            "Finished Training\n",
            "0.4023108184337616 0.0 83.99 0.0 16.01 0.0\n",
            "for a value 0.000, b value 5.000, and c value -5.000 \n",
            "**********************************************************************\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 5.0 -5.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[5.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([-5.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     2] loss: 2.923\n",
            "[201,     2] loss: 0.146\n",
            "[401,     2] loss: 0.146\n",
            "[601,     2] loss: 0.146\n",
            "[801,     2] loss: 0.146\n",
            "[1001,     2] loss: 0.146\n",
            "[1201,     2] loss: 0.146\n",
            "[1401,     2] loss: 0.146\n",
            "[1601,     2] loss: 0.146\n",
            "[1801,     2] loss: 0.146\n",
            "[2001,     2] loss: 0.146\n",
            "[2201,     2] loss: 0.146\n",
            "[2401,     2] loss: 0.146\n",
            "[2601,     2] loss: 0.146\n",
            "[2801,     2] loss: 0.146\n",
            "Finished Training\n",
            "0.14608508348464966 70.12 99.18 0.0 0.82 0.0\n",
            "for a value 0.000, b value 5.000, and c value -2.000 \n",
            "**********************************************************************\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 5.0 -2.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[5.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([-2.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     2] loss: 4.419\n",
            "[201,     2] loss: 0.135\n",
            "[401,     2] loss: 0.128\n",
            "[601,     2] loss: 0.128\n",
            "[801,     2] loss: 0.128\n",
            "[1001,     2] loss: 0.128\n",
            "[1201,     2] loss: 0.128\n",
            "[1401,     2] loss: 0.128\n",
            "[1601,     2] loss: 0.128\n",
            "[1801,     2] loss: 0.128\n",
            "[2001,     2] loss: 0.128\n",
            "[2201,     2] loss: 0.128\n",
            "[2401,     2] loss: 0.128\n",
            "[2601,     2] loss: 0.128\n",
            "[2801,     2] loss: 0.128\n",
            "Finished Training\n",
            "0.12786410748958588 99.44 98.89 0.0 1.11 0.0\n",
            "for a value 0.000, b value 5.000, and c value 0.000 \n",
            "**********************************************************************\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 5.0 0.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[5.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([0.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     2] loss: 5.418\n",
            "[201,     2] loss: 0.219\n",
            "[401,     2] loss: 0.176\n",
            "[601,     2] loss: 0.166\n",
            "[801,     2] loss: 0.162\n",
            "[1001,     2] loss: 0.160\n",
            "[1201,     2] loss: 0.158\n",
            "[1401,     2] loss: 0.157\n",
            "[1601,     2] loss: 0.157\n",
            "[1801,     2] loss: 0.156\n",
            "[2001,     2] loss: 0.156\n",
            "[2201,     2] loss: 0.156\n",
            "[2401,     2] loss: 0.156\n",
            "[2601,     2] loss: 0.156\n",
            "[2801,     2] loss: 0.156\n",
            "Finished Training\n",
            "0.15552149713039398 100.0 96.6 0.0 3.4 0.0\n",
            "for a value 0.000, b value 5.000, and c value 2.000 \n",
            "**********************************************************************\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 5.0 2.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[5.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([2.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     2] loss: 6.417\n",
            "[201,     2] loss: 0.514\n",
            "[401,     2] loss: 0.435\n",
            "[601,     2] loss: 0.411\n",
            "[801,     2] loss: 0.400\n",
            "[1001,     2] loss: 0.393\n",
            "[1201,     2] loss: 0.389\n",
            "[1401,     2] loss: 0.386\n",
            "[1601,     2] loss: 0.383\n",
            "[1801,     2] loss: 0.381\n",
            "[2001,     2] loss: 0.380\n",
            "[2201,     2] loss: 0.379\n",
            "[2401,     2] loss: 0.378\n",
            "[2601,     2] loss: 0.377\n",
            "[2801,     2] loss: 0.376\n",
            "Finished Training\n",
            "0.37526535987854004 100.0 79.58 0.0 20.42 0.0\n",
            "for a value 0.000, b value 5.000, and c value 5.000 \n",
            "**********************************************************************\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 5.0 5.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[5.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([5.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     2] loss: 7.915\n",
            "[201,     2] loss: 1.524\n",
            "[401,     2] loss: 1.426\n",
            "[601,     2] loss: 1.398\n",
            "[801,     2] loss: 1.385\n",
            "[1001,     2] loss: 1.377\n",
            "[1201,     2] loss: 1.372\n",
            "[1401,     2] loss: 1.369\n",
            "[1601,     2] loss: 1.366\n",
            "[1801,     2] loss: 1.364\n",
            "[2001,     2] loss: 1.362\n",
            "[2201,     2] loss: 1.361\n",
            "[2401,     2] loss: 1.360\n",
            "[2601,     2] loss: 1.359\n",
            "[2801,     2] loss: 1.358\n",
            "Finished Training\n",
            "1.3573598861694336 100.0 50.98 0.0 49.02 0.0\n",
            "for a value 0.000, b value 5.000, and c value 10.000 \n",
            "**********************************************************************\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 5.0 10.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[5.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([10.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     2] loss: 10.413\n",
            "[201,     2] loss: 3.946\n",
            "[401,     2] loss: 3.846\n",
            "[601,     2] loss: 3.817\n",
            "[801,     2] loss: 3.804\n",
            "[1001,     2] loss: 3.796\n",
            "[1201,     2] loss: 3.791\n",
            "[1401,     2] loss: 3.788\n",
            "[1601,     2] loss: 3.785\n",
            "[1801,     2] loss: 3.783\n",
            "[2001,     2] loss: 3.781\n",
            "[2201,     2] loss: 3.780\n",
            "[2401,     2] loss: 3.779\n",
            "[2601,     2] loss: 3.778\n",
            "[2801,     2] loss: 3.777\n",
            "Finished Training\n",
            "3.776435136795044 100.0 50.98 0.0 49.02 0.0\n",
            "for a value 0.000, b value 10.000, and c value -10.000 \n",
            "**********************************************************************\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 10.0 -10.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[10.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([-10.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     2] loss: 5.841\n",
            "[201,     2] loss: 0.072\n",
            "[401,     2] loss: 0.072\n",
            "[601,     2] loss: 0.072\n",
            "[801,     2] loss: 0.072\n",
            "[1001,     2] loss: 0.072\n",
            "[1201,     2] loss: 0.072\n",
            "[1401,     2] loss: 0.072\n",
            "[1601,     2] loss: 0.072\n",
            "[1801,     2] loss: 0.072\n",
            "[2001,     2] loss: 0.072\n",
            "[2201,     2] loss: 0.072\n",
            "[2401,     2] loss: 0.072\n",
            "[2601,     2] loss: 0.072\n",
            "[2801,     2] loss: 0.072\n",
            "Finished Training\n",
            "0.07181482017040253 69.22 99.42 0.0 0.58 0.0\n",
            "for a value 0.000, b value 10.000, and c value -5.000 \n",
            "**********************************************************************\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 10.0 -5.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[10.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([-5.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     2] loss: 8.338\n",
            "[201,     2] loss: 0.064\n",
            "[401,     2] loss: 0.064\n",
            "[601,     2] loss: 0.064\n",
            "[801,     2] loss: 0.064\n",
            "[1001,     2] loss: 0.064\n",
            "[1201,     2] loss: 0.064\n",
            "[1401,     2] loss: 0.064\n",
            "[1601,     2] loss: 0.064\n",
            "[1801,     2] loss: 0.064\n",
            "[2001,     2] loss: 0.064\n",
            "[2201,     2] loss: 0.064\n",
            "[2401,     2] loss: 0.064\n",
            "[2601,     2] loss: 0.064\n",
            "[2801,     2] loss: 0.064\n",
            "Finished Training\n",
            "0.0635688453912735 97.19 99.02 0.0 0.98 0.0\n",
            "for a value 0.000, b value 10.000, and c value -2.000 \n",
            "**********************************************************************\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 10.0 -2.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[10.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([-2.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     2] loss: 9.837\n",
            "[201,     2] loss: 0.077\n",
            "[401,     2] loss: 0.068\n",
            "[601,     2] loss: 0.067\n",
            "[801,     2] loss: 0.067\n",
            "[1001,     2] loss: 0.067\n",
            "[1201,     2] loss: 0.067\n",
            "[1401,     2] loss: 0.067\n",
            "[1601,     2] loss: 0.067\n",
            "[1801,     2] loss: 0.067\n",
            "[2001,     2] loss: 0.067\n",
            "[2201,     2] loss: 0.067\n",
            "[2401,     2] loss: 0.067\n",
            "[2601,     2] loss: 0.067\n",
            "[2801,     2] loss: 0.067\n",
            "Finished Training\n",
            "0.06710799038410187 100.0 99.16 0.0 0.84 0.0\n",
            "for a value 0.000, b value 10.000, and c value 0.000 \n",
            "**********************************************************************\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 10.0 0.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[10.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([0.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     2] loss: 10.836\n",
            "[201,     2] loss: 0.140\n",
            "[401,     2] loss: 0.104\n",
            "[601,     2] loss: 0.094\n",
            "[801,     2] loss: 0.090\n",
            "[1001,     2] loss: 0.087\n",
            "[1201,     2] loss: 0.086\n",
            "[1401,     2] loss: 0.084\n",
            "[1601,     2] loss: 0.084\n",
            "[1801,     2] loss: 0.083\n",
            "[2001,     2] loss: 0.083\n",
            "[2201,     2] loss: 0.082\n",
            "[2401,     2] loss: 0.082\n",
            "[2601,     2] loss: 0.082\n",
            "[2801,     2] loss: 0.082\n",
            "Finished Training\n",
            "0.08148099482059479 100.0 98.14 0.0 1.86 0.0\n",
            "for a value 0.000, b value 10.000, and c value 2.000 \n",
            "**********************************************************************\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 10.0 2.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[10.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([2.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     2] loss: 11.835\n",
            "[201,     2] loss: 0.313\n",
            "[401,     2] loss: 0.251\n",
            "[601,     2] loss: 0.231\n",
            "[801,     2] loss: 0.221\n",
            "[1001,     2] loss: 0.214\n",
            "[1201,     2] loss: 0.210\n",
            "[1401,     2] loss: 0.207\n",
            "[1601,     2] loss: 0.205\n",
            "[1801,     2] loss: 0.203\n",
            "[2001,     2] loss: 0.201\n",
            "[2201,     2] loss: 0.200\n",
            "[2401,     2] loss: 0.199\n",
            "[2601,     2] loss: 0.198\n",
            "[2801,     2] loss: 0.197\n",
            "Finished Training\n",
            "0.19683218002319336 100.0 89.2 0.0 10.8 0.0\n",
            "for a value 0.000, b value 10.000, and c value 5.000 \n",
            "**********************************************************************\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 10.0 5.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[10.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([5.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     2] loss: 13.333\n",
            "[201,     2] loss: 0.876\n",
            "[401,     2] loss: 0.798\n",
            "[601,     2] loss: 0.773\n",
            "[801,     2] loss: 0.761\n",
            "[1001,     2] loss: 0.753\n",
            "[1201,     2] loss: 0.748\n",
            "[1401,     2] loss: 0.744\n",
            "[1601,     2] loss: 0.742\n",
            "[1801,     2] loss: 0.740\n",
            "[2001,     2] loss: 0.738\n",
            "[2201,     2] loss: 0.736\n",
            "[2401,     2] loss: 0.735\n",
            "[2601,     2] loss: 0.734\n",
            "[2801,     2] loss: 0.733\n",
            "Finished Training\n",
            "0.732725977897644 100.0 75.17 0.0 24.83 0.0\n",
            "for a value 0.000, b value 10.000, and c value 10.000 \n",
            "**********************************************************************\n",
            "-------------------------------------------------------------------------------- a,b,c =  0.0 10.0 10.0\n",
            "--------------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[0.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([[10.]], device='cuda:0', dtype=torch.float64, requires_grad=True) Parameter containing:\n",
            "tensor([10.], device='cuda:0', dtype=torch.float64, requires_grad=True)\n",
            "[1,     2] loss: 15.831\n",
            "[201,     2] loss: 2.732\n",
            "[401,     2] loss: 2.650\n",
            "[601,     2] loss: 2.624\n",
            "[801,     2] loss: 2.612\n",
            "[1001,     2] loss: 2.605\n",
            "[1201,     2] loss: 2.600\n",
            "[1401,     2] loss: 2.596\n",
            "[1601,     2] loss: 2.593\n",
            "[1801,     2] loss: 2.591\n",
            "[2001,     2] loss: 2.590\n",
            "[2201,     2] loss: 2.588\n",
            "[2401,     2] loss: 2.587\n",
            "[2601,     2] loss: 2.586\n",
            "[2801,     2] loss: 2.586\n",
            "Finished Training\n",
            "2.584892749786377 100.0 50.98 0.0 49.02 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gQoPST5zW2t"
      },
      "source": [
        "# df_train = pd.DataFrame()\n",
        "df_test = pd.DataFrame()"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "In76SYH_zZHV"
      },
      "source": [
        "columns = [\"init_a\", \"init_b\", \"init_c\", \"final_a\", \"final_b\", \"final_c\", \"train_loss\", \"argmax > 0.5\" , \"ftpt\", \"ffpt\", \"ftpf\", \"ffpf\" ]"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BS4HtOHEzZ0E"
      },
      "source": [
        "df_test[columns[0]] = init_a\n",
        "df_test[columns[1]] = init_b\n",
        "df_test[columns[2]] = init_c\n",
        "df_test[columns[3]] = final_a\n",
        "df_test[columns[4]] = final_b\n",
        "df_test[columns[5]] = final_c\n",
        "df_test[columns[6]] = all_loss\n",
        "# df_test[columns[7]] = all_alphas_more_than_half\n",
        "df_test[columns[8]] = all_ftpt\n",
        "df_test[columns[9]] = all_ffpt\n",
        "df_test[columns[10]] = all_ftpf\n",
        "df_test[columns[11]] = all_ffpf"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UbTkfLUINTI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "86fc1bce-b074-436e-ffb8-88e955de6ea4"
      },
      "source": [
        "df_test"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>init_a</th>\n",
              "      <th>init_b</th>\n",
              "      <th>init_c</th>\n",
              "      <th>final_a</th>\n",
              "      <th>final_b</th>\n",
              "      <th>final_c</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>ftpt</th>\n",
              "      <th>ffpt</th>\n",
              "      <th>ftpf</th>\n",
              "      <th>ffpf</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>-10.0</td>\n",
              "      <td>-10.0</td>\n",
              "      <td>-4.490</td>\n",
              "      <td>-10.0</td>\n",
              "      <td>-10.0</td>\n",
              "      <td>7.520</td>\n",
              "      <td>49.02</td>\n",
              "      <td>0.00</td>\n",
              "      <td>50.98</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>-10.0</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>-4.477</td>\n",
              "      <td>-10.0</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>5.663</td>\n",
              "      <td>24.68</td>\n",
              "      <td>0.00</td>\n",
              "      <td>75.32</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>-10.0</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>-4.453</td>\n",
              "      <td>-10.0</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>5.126</td>\n",
              "      <td>10.42</td>\n",
              "      <td>0.00</td>\n",
              "      <td>89.58</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>-10.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-4.418</td>\n",
              "      <td>-10.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.021</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.00</td>\n",
              "      <td>99.96</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>-10.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>-4.345</td>\n",
              "      <td>-10.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>5.124</td>\n",
              "      <td>10.55</td>\n",
              "      <td>0.00</td>\n",
              "      <td>89.45</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.0</td>\n",
              "      <td>-10.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>-0.898</td>\n",
              "      <td>-10.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>6.154</td>\n",
              "      <td>26.46</td>\n",
              "      <td>0.00</td>\n",
              "      <td>73.54</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.0</td>\n",
              "      <td>-10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>-0.621</td>\n",
              "      <td>-10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>4.606</td>\n",
              "      <td>32.47</td>\n",
              "      <td>0.00</td>\n",
              "      <td>67.53</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.0</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>-10.0</td>\n",
              "      <td>-4.035</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>-10.0</td>\n",
              "      <td>6.259</td>\n",
              "      <td>49.02</td>\n",
              "      <td>0.00</td>\n",
              "      <td>50.98</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.0</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>-4.030</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>3.835</td>\n",
              "      <td>49.02</td>\n",
              "      <td>0.00</td>\n",
              "      <td>50.98</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.0</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>-3.991</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>2.852</td>\n",
              "      <td>20.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>80.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.0</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-3.894</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.653</td>\n",
              "      <td>0.23</td>\n",
              "      <td>0.00</td>\n",
              "      <td>99.77</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.0</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>-1.006</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.298</td>\n",
              "      <td>22.68</td>\n",
              "      <td>0.00</td>\n",
              "      <td>77.32</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.0</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>-0.609</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>2.421</td>\n",
              "      <td>34.18</td>\n",
              "      <td>0.00</td>\n",
              "      <td>65.82</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.0</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>-0.050</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>1.001</td>\n",
              "      <td>43.46</td>\n",
              "      <td>0.00</td>\n",
              "      <td>56.54</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.0</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>-10.0</td>\n",
              "      <td>-3.417</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>-10.0</td>\n",
              "      <td>5.525</td>\n",
              "      <td>49.02</td>\n",
              "      <td>0.00</td>\n",
              "      <td>50.98</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.0</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>-3.413</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>3.034</td>\n",
              "      <td>49.02</td>\n",
              "      <td>0.00</td>\n",
              "      <td>50.98</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.0</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>-3.354</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>1.721</td>\n",
              "      <td>49.02</td>\n",
              "      <td>0.00</td>\n",
              "      <td>50.98</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.0</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-3.002</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.374</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.00</td>\n",
              "      <td>99.25</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.0</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>-0.506</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.220</td>\n",
              "      <td>48.67</td>\n",
              "      <td>0.00</td>\n",
              "      <td>51.33</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.0</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.702</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.691</td>\n",
              "      <td>0.00</td>\n",
              "      <td>53.81</td>\n",
              "      <td>0.00</td>\n",
              "      <td>46.19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.0</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>1.913</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>2.373</td>\n",
              "      <td>0.00</td>\n",
              "      <td>50.98</td>\n",
              "      <td>0.00</td>\n",
              "      <td>49.02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-10.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-10.0</td>\n",
              "      <td>5.005</td>\n",
              "      <td>4.94</td>\n",
              "      <td>44.08</td>\n",
              "      <td>5.28</td>\n",
              "      <td>45.70</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>2.509</td>\n",
              "      <td>4.94</td>\n",
              "      <td>44.08</td>\n",
              "      <td>5.28</td>\n",
              "      <td>45.70</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>1.128</td>\n",
              "      <td>4.94</td>\n",
              "      <td>44.08</td>\n",
              "      <td>5.28</td>\n",
              "      <td>45.70</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.693</td>\n",
              "      <td>4.94</td>\n",
              "      <td>44.08</td>\n",
              "      <td>5.28</td>\n",
              "      <td>45.70</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.126</td>\n",
              "      <td>5.28</td>\n",
              "      <td>45.70</td>\n",
              "      <td>4.94</td>\n",
              "      <td>44.08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>2.504</td>\n",
              "      <td>5.28</td>\n",
              "      <td>45.70</td>\n",
              "      <td>4.94</td>\n",
              "      <td>44.08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>4.995</td>\n",
              "      <td>5.28</td>\n",
              "      <td>45.70</td>\n",
              "      <td>4.94</td>\n",
              "      <td>44.08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>-10.0</td>\n",
              "      <td>1.967</td>\n",
              "      <td>2.0</td>\n",
              "      <td>-10.0</td>\n",
              "      <td>2.378</td>\n",
              "      <td>0.00</td>\n",
              "      <td>49.02</td>\n",
              "      <td>0.00</td>\n",
              "      <td>50.98</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>-0.108</td>\n",
              "      <td>2.0</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>0.681</td>\n",
              "      <td>49.11</td>\n",
              "      <td>0.00</td>\n",
              "      <td>50.89</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>-1.056</td>\n",
              "      <td>2.0</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>0.319</td>\n",
              "      <td>96.37</td>\n",
              "      <td>0.00</td>\n",
              "      <td>3.63</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.898</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.319</td>\n",
              "      <td>92.71</td>\n",
              "      <td>0.00</td>\n",
              "      <td>7.29</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>-2.596</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.714</td>\n",
              "      <td>50.98</td>\n",
              "      <td>0.00</td>\n",
              "      <td>49.02</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>-2.740</td>\n",
              "      <td>2.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>2.028</td>\n",
              "      <td>50.98</td>\n",
              "      <td>0.00</td>\n",
              "      <td>49.02</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>-2.748</td>\n",
              "      <td>2.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>4.513</td>\n",
              "      <td>50.98</td>\n",
              "      <td>0.00</td>\n",
              "      <td>49.02</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>-10.0</td>\n",
              "      <td>-0.351</td>\n",
              "      <td>5.0</td>\n",
              "      <td>-10.0</td>\n",
              "      <td>0.402</td>\n",
              "      <td>83.99</td>\n",
              "      <td>0.00</td>\n",
              "      <td>16.01</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>-0.999</td>\n",
              "      <td>5.0</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>0.146</td>\n",
              "      <td>99.18</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.82</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>-1.500</td>\n",
              "      <td>5.0</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>0.128</td>\n",
              "      <td>98.89</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.11</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-2.252</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.156</td>\n",
              "      <td>96.60</td>\n",
              "      <td>0.00</td>\n",
              "      <td>3.40</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>-2.864</td>\n",
              "      <td>5.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.375</td>\n",
              "      <td>79.58</td>\n",
              "      <td>0.00</td>\n",
              "      <td>20.42</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>-3.073</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1.357</td>\n",
              "      <td>50.98</td>\n",
              "      <td>0.00</td>\n",
              "      <td>49.02</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>-3.101</td>\n",
              "      <td>5.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>3.776</td>\n",
              "      <td>50.98</td>\n",
              "      <td>0.00</td>\n",
              "      <td>49.02</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>0.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>-10.0</td>\n",
              "      <td>-0.991</td>\n",
              "      <td>10.0</td>\n",
              "      <td>-10.0</td>\n",
              "      <td>0.072</td>\n",
              "      <td>99.42</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>0.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>-1.396</td>\n",
              "      <td>10.0</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>0.064</td>\n",
              "      <td>99.02</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.98</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>0.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>-1.828</td>\n",
              "      <td>10.0</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>0.067</td>\n",
              "      <td>99.16</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.84</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>0.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-2.503</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.081</td>\n",
              "      <td>98.14</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.86</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>0.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>-2.997</td>\n",
              "      <td>10.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.197</td>\n",
              "      <td>89.20</td>\n",
              "      <td>0.00</td>\n",
              "      <td>10.80</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>0.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>-3.247</td>\n",
              "      <td>10.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.733</td>\n",
              "      <td>75.17</td>\n",
              "      <td>0.00</td>\n",
              "      <td>24.83</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>0.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>-3.359</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>2.585</td>\n",
              "      <td>50.98</td>\n",
              "      <td>0.00</td>\n",
              "      <td>49.02</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    init_a  init_b  init_c  final_a  ...   ftpt   ffpt   ftpf   ffpf\n",
              "0      0.0   -10.0   -10.0   -4.490  ...  49.02   0.00  50.98   0.00\n",
              "1      0.0   -10.0    -5.0   -4.477  ...  24.68   0.00  75.32   0.00\n",
              "2      0.0   -10.0    -2.0   -4.453  ...  10.42   0.00  89.58   0.00\n",
              "3      0.0   -10.0     0.0   -4.418  ...   0.04   0.00  99.96   0.00\n",
              "4      0.0   -10.0     2.0   -4.345  ...  10.55   0.00  89.45   0.00\n",
              "5      0.0   -10.0     5.0   -0.898  ...  26.46   0.00  73.54   0.00\n",
              "6      0.0   -10.0    10.0   -0.621  ...  32.47   0.00  67.53   0.00\n",
              "7      0.0    -5.0   -10.0   -4.035  ...  49.02   0.00  50.98   0.00\n",
              "8      0.0    -5.0    -5.0   -4.030  ...  49.02   0.00  50.98   0.00\n",
              "9      0.0    -5.0    -2.0   -3.991  ...  20.00   0.00  80.00   0.00\n",
              "10     0.0    -5.0     0.0   -3.894  ...   0.23   0.00  99.77   0.00\n",
              "11     0.0    -5.0     2.0   -1.006  ...  22.68   0.00  77.32   0.00\n",
              "12     0.0    -5.0     5.0   -0.609  ...  34.18   0.00  65.82   0.00\n",
              "13     0.0    -5.0    10.0   -0.050  ...  43.46   0.00  56.54   0.00\n",
              "14     0.0    -2.0   -10.0   -3.417  ...  49.02   0.00  50.98   0.00\n",
              "15     0.0    -2.0    -5.0   -3.413  ...  49.02   0.00  50.98   0.00\n",
              "16     0.0    -2.0    -2.0   -3.354  ...  49.02   0.00  50.98   0.00\n",
              "17     0.0    -2.0     0.0   -3.002  ...   0.75   0.00  99.25   0.00\n",
              "18     0.0    -2.0     2.0   -0.506  ...  48.67   0.00  51.33   0.00\n",
              "19     0.0    -2.0     5.0    0.702  ...   0.00  53.81   0.00  46.19\n",
              "20     0.0    -2.0    10.0    1.913  ...   0.00  50.98   0.00  49.02\n",
              "21     0.0     0.0   -10.0    0.000  ...   4.94  44.08   5.28  45.70\n",
              "22     0.0     0.0    -5.0    0.000  ...   4.94  44.08   5.28  45.70\n",
              "23     0.0     0.0    -2.0    0.000  ...   4.94  44.08   5.28  45.70\n",
              "24     0.0     0.0     0.0    0.000  ...   4.94  44.08   5.28  45.70\n",
              "25     0.0     0.0     2.0    0.000  ...   5.28  45.70   4.94  44.08\n",
              "26     0.0     0.0     5.0    0.000  ...   5.28  45.70   4.94  44.08\n",
              "27     0.0     0.0    10.0    0.000  ...   5.28  45.70   4.94  44.08\n",
              "28     0.0     2.0   -10.0    1.967  ...   0.00  49.02   0.00  50.98\n",
              "29     0.0     2.0    -5.0   -0.108  ...  49.11   0.00  50.89   0.00\n",
              "30     0.0     2.0    -2.0   -1.056  ...  96.37   0.00   3.63   0.00\n",
              "31     0.0     2.0     0.0   -1.898  ...  92.71   0.00   7.29   0.00\n",
              "32     0.0     2.0     2.0   -2.596  ...  50.98   0.00  49.02   0.00\n",
              "33     0.0     2.0     5.0   -2.740  ...  50.98   0.00  49.02   0.00\n",
              "34     0.0     2.0    10.0   -2.748  ...  50.98   0.00  49.02   0.00\n",
              "35     0.0     5.0   -10.0   -0.351  ...  83.99   0.00  16.01   0.00\n",
              "36     0.0     5.0    -5.0   -0.999  ...  99.18   0.00   0.82   0.00\n",
              "37     0.0     5.0    -2.0   -1.500  ...  98.89   0.00   1.11   0.00\n",
              "38     0.0     5.0     0.0   -2.252  ...  96.60   0.00   3.40   0.00\n",
              "39     0.0     5.0     2.0   -2.864  ...  79.58   0.00  20.42   0.00\n",
              "40     0.0     5.0     5.0   -3.073  ...  50.98   0.00  49.02   0.00\n",
              "41     0.0     5.0    10.0   -3.101  ...  50.98   0.00  49.02   0.00\n",
              "42     0.0    10.0   -10.0   -0.991  ...  99.42   0.00   0.58   0.00\n",
              "43     0.0    10.0    -5.0   -1.396  ...  99.02   0.00   0.98   0.00\n",
              "44     0.0    10.0    -2.0   -1.828  ...  99.16   0.00   0.84   0.00\n",
              "45     0.0    10.0     0.0   -2.503  ...  98.14   0.00   1.86   0.00\n",
              "46     0.0    10.0     2.0   -2.997  ...  89.20   0.00  10.80   0.00\n",
              "47     0.0    10.0     5.0   -3.247  ...  75.17   0.00  24.83   0.00\n",
              "48     0.0    10.0    10.0   -3.359  ...  50.98   0.00  49.02   0.00\n",
              "\n",
              "[49 rows x 11 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1m2PyeaJcO7H"
      },
      "source": [
        "# df_test.to_csv(\"linear_linear_altmin_focus_first.csv\")"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mipCcN0cdXO"
      },
      "source": [
        "#df_train.to_csv(\"train_1.csv\")\n",
        "df_test.to_csv(\"test_2.csv\")"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0ETaUo8mIWC"
      },
      "source": [
        "loss_heat_map = np.zeros((7,7))"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-JVi7MbOhok"
      },
      "source": [
        "loss_heat_map[0,:] = df_test[df_test.init_b == -10].train_loss\n",
        "loss_heat_map[1,:] = df_test[df_test.init_b == -5].train_loss\n",
        "loss_heat_map[2,:] = df_test[df_test.init_b == -2].train_loss\n",
        "loss_heat_map[3,:] = df_test[df_test.init_b == 0].train_loss\n",
        "loss_heat_map[4,:] = df_test[df_test.init_b == 2].train_loss\n",
        "loss_heat_map[5,:] = df_test[df_test.init_b == 5].train_loss\n",
        "loss_heat_map[6,:] = df_test[df_test.init_b == 10].train_loss\n"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02VCWCublw_j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd1d0406-35f3-4630-c9ca-a981d5ed7be1"
      },
      "source": [
        "loss_heat_map"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[7.52 , 5.663, 5.126, 5.021, 5.124, 6.154, 4.606],\n",
              "       [6.259, 3.835, 2.852, 2.653, 3.298, 2.421, 1.001],\n",
              "       [5.525, 3.034, 1.721, 1.374, 1.22 , 0.691, 2.373],\n",
              "       [5.005, 2.509, 1.128, 0.693, 1.126, 2.504, 4.995],\n",
              "       [2.378, 0.681, 0.319, 0.319, 0.714, 2.028, 4.513],\n",
              "       [0.402, 0.146, 0.128, 0.156, 0.375, 1.357, 3.776],\n",
              "       [0.072, 0.064, 0.067, 0.081, 0.197, 0.733, 2.585]])"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mb4xTTxte7rg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        },
        "outputId": "ae788f98-25f7-4d26-aee5-05c3932901b0"
      },
      "source": [
        "import seaborn as sns\n",
        "\n",
        "fig = plt.figure(figsize=(8,8))\n",
        "ax = sns.heatmap( loss_heat_map , linewidth = 0.5 , cmap = 'coolwarm' ,annot=True)\n",
        "\n",
        "\n",
        "ax.set_xticks(np.arange(len(c)))\n",
        "ax.set_yticks(np.arange(len(b)))\n",
        "\n",
        "ax.set_xticklabels(c)\n",
        "ax.set_yticklabels(b)\n",
        "# Rotate the tick labels and set their alignment.\n",
        "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
        "         rotation_mode=\"anchor\")\n",
        "\n",
        "plt.setp(ax.get_yticklabels(), rotation=45, ha=\"right\",\n",
        "         rotation_mode=\"anchor\")\n",
        "\n",
        "plt.xlabel(\"c\")\n",
        "plt.ylabel(\"b\")\n",
        "# for i in range(len(b)):\n",
        "#     for j in range(len(c)):\n",
        "#         text = ax.text(j, i, loss_heat_map[i, j],\n",
        "#                         color=\"w\",)\n",
        "fig.tight_layout() \n",
        "plt.title(\"Minimizing a for fix value of b and c,loss values here\")"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Minimizing a for fix value of b and c,loss values here')"
            ]
          },
          "metadata": {},
          "execution_count": 50
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x576 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEV22UlEcvby"
      },
      "source": [
        "a_value_heat_map = np.zeros((7,7))"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ra93yQJJc7rU"
      },
      "source": [
        "a_value_heat_map[0,:] = df_test[df_test.init_b == -10].final_a\n",
        "a_value_heat_map[1,:] = df_test[df_test.init_b == -5].final_a\n",
        "a_value_heat_map[2,:] = df_test[df_test.init_b == -2].final_a\n",
        "a_value_heat_map[3,:] = df_test[df_test.init_b == 0].final_a\n",
        "a_value_heat_map[4,:] = df_test[df_test.init_b == 2].final_a\n",
        "a_value_heat_map[5,:] = df_test[df_test.init_b == 5].final_a\n",
        "a_value_heat_map[6,:] = df_test[df_test.init_b == 10].final_a"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85YAdELIdXsc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d737815c-097d-477e-a610-355c01c0fd6b"
      },
      "source": [
        "a_value_heat_map"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-4.49 , -4.477, -4.453, -4.418, -4.345, -0.898, -0.621],\n",
              "       [-4.035, -4.03 , -3.991, -3.894, -1.006, -0.609, -0.05 ],\n",
              "       [-3.417, -3.413, -3.354, -3.002, -0.506,  0.702,  1.913],\n",
              "       [ 0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
              "       [ 1.967, -0.108, -1.056, -1.898, -2.596, -2.74 , -2.748],\n",
              "       [-0.351, -0.999, -1.5  , -2.252, -2.864, -3.073, -3.101],\n",
              "       [-0.991, -1.396, -1.828, -2.503, -2.997, -3.247, -3.359]])"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nu6ai-LdZAU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        },
        "outputId": "2a63fddb-a3d1-48de-cbdb-848edfe002fe"
      },
      "source": [
        "fig = plt.figure(figsize=(8,8))\n",
        "ax = sns.heatmap( a_value_heat_map , linewidth = 0.5 , cmap = 'coolwarm' ,annot=True)\n",
        "\n",
        "\n",
        "ax.set_xticks(np.arange(len(c)))\n",
        "ax.set_yticks(np.arange(len(b)))\n",
        "\n",
        "ax.set_xticklabels(c)\n",
        "ax.set_yticklabels(b)\n",
        "# Rotate the tick labels and set their alignment.\n",
        "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
        "         rotation_mode=\"anchor\")\n",
        "\n",
        "plt.setp(ax.get_yticklabels(), rotation=45, ha=\"right\",\n",
        "         rotation_mode=\"anchor\")\n",
        "plt.xlabel(\"c\")\n",
        "plt.ylabel(\"b\")\n",
        "\n",
        "# for i in range(len(b)):\n",
        "#     for j in range(len(c)):\n",
        "#         text = ax.text(j, i, loss_heat_map[i, j],\n",
        "#                         color=\"w\",)\n",
        "fig.tight_layout() \n",
        "plt.title(\"Minimizing a for fix value of b and c,final a values here\")"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Minimizing a for fix value of b and c,final a values here')"
            ]
          },
          "metadata": {},
          "execution_count": 54
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x576 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yiE4iLhOiRdp"
      },
      "source": [
        ""
      ],
      "execution_count": 48,
      "outputs": []
    }
  ]
}
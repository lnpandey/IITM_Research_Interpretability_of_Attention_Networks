{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "blob_only_atttention_weights_lr_10.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWIyC9Ip_bcq"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGVy-1EllAc_"
      },
      "source": [
        "train_data = np.load(\"train_blob_data.npy\",allow_pickle=True)\n",
        "\n",
        "test_data = np.load(\"test_blob_data.npy\",allow_pickle=True)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uL771xuGZC5Q"
      },
      "source": [
        "mosaic_list_of_images = train_data[0][\"mosaic_list\"]\n",
        "mosaic_label = train_data[0][\"mosaic_label\"]\n",
        "fore_idx = train_data[0][\"fore_idx\"]\n",
        "\n",
        "\n",
        "test_mosaic_list_of_images = test_data[0][\"mosaic_list\"]\n",
        "test_mosaic_label = test_data[0][\"mosaic_label\"]\n",
        "test_fore_idx = test_data[0][\"fore_idx\"]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2qfRXfNZCao"
      },
      "source": [
        "class MosaicDataset1(Dataset):\n",
        "  \"\"\"MosaicDataset dataset.\"\"\"\n",
        "\n",
        "  def __init__(self, mosaic_list, mosaic_label,fore_idx):\n",
        "    \"\"\"\n",
        "      Args:\n",
        "        csv_file (string): Path to the csv file with annotations.\n",
        "        root_dir (string): Directory with all the images.\n",
        "        transform (callable, optional): Optional transform to be applied\n",
        "            on a sample.\n",
        "    \"\"\"\n",
        "    self.mosaic = mosaic_list\n",
        "    self.label = mosaic_label\n",
        "    self.fore_idx = fore_idx\n",
        "    \n",
        "  def __len__(self):\n",
        "    return len(self.label)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.mosaic[idx] , self.label[idx] , self.fore_idx[idx]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uf76JwkxZCT0"
      },
      "source": [
        "batch = 250\n",
        "train_dataset = MosaicDataset1(mosaic_list_of_images, mosaic_label, fore_idx)\n",
        "train_loader = DataLoader( train_dataset,batch_size= batch ,shuffle=False)\n",
        "test_dataset = MosaicDataset1(test_mosaic_list_of_images, test_mosaic_label, test_fore_idx)\n",
        "test_loader = DataLoader(test_dataset,batch_size= batch ,shuffle=False)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOpZfj1bq7wN"
      },
      "source": [
        "bg = []\n",
        "for i in range(12):\n",
        "  torch.manual_seed(i)\n",
        "  betag = torch.randn(250,9)#torch.ones((250,9))/9\n",
        "  bg.append( betag.requires_grad_() )"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzb3ii4drXpu",
        "outputId": "3cc65cc7-8723-4a83-cd29-2a7acd768fb9"
      },
      "source": [
        "bg"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([[-1.1258, -1.1524, -0.2506,  ..., -0.3160, -2.1152,  0.3223],\n",
              "         [-1.2633,  0.3500,  0.3081,  ..., -0.2473, -1.3527, -1.6959],\n",
              "         [ 0.5667,  0.7935,  0.5988,  ...,  0.7502, -0.5855, -0.1734],\n",
              "         ...,\n",
              "         [ 0.8374, -0.7942, -0.3622,  ...,  0.0121,  0.8032, -0.6962],\n",
              "         [-1.0645,  0.2384, -0.3385,  ...,  0.9635, -1.0340,  0.1894],\n",
              "         [ 0.8253,  1.1038, -1.2491,  ..., -0.5940, -1.7125,  0.3617]],\n",
              "        requires_grad=True),\n",
              " tensor([[-1.5256, -0.7502, -0.6540,  ..., -0.9798, -1.6091, -0.7121],\n",
              "         [ 0.3037, -0.7773, -0.2515,  ...,  0.4676, -0.6970, -1.1608],\n",
              "         [ 0.6995,  0.1991,  0.8657,  ...,  1.1017, -0.1759, -2.2456],\n",
              "         ...,\n",
              "         [-0.4302,  0.1508,  0.6937,  ...,  0.0314,  2.6645,  0.1189],\n",
              "         [ 1.4484, -0.0213, -1.3367,  ...,  0.6279, -1.4719, -1.0291],\n",
              "         [ 0.9081, -1.2433,  1.6062,  ..., -0.1177, -0.5548, -0.0595]],\n",
              "        requires_grad=True),\n",
              " tensor([[-1.0408,  0.9166, -1.3042,  ..., -1.0574, -0.1188, -0.9078],\n",
              "         [ 0.3452, -0.5713, -0.2351,  ..., -0.4327, -1.5071, -0.4586],\n",
              "         [-0.8480,  0.5266,  0.0299,  ...,  0.4640, -0.4986,  0.1289],\n",
              "         ...,\n",
              "         [ 1.5719,  1.0154, -2.1620,  ..., -1.0790,  1.5801, -1.6557],\n",
              "         [-1.1613,  0.3672, -0.3078,  ..., -1.2456, -0.1125,  0.6222],\n",
              "         [ 0.4521, -0.2505,  2.3728,  ..., -0.1377, -0.8815, -0.1671]],\n",
              "        requires_grad=True),\n",
              " tensor([[-0.0766,  0.3599, -0.7820,  ...,  1.6206, -1.5967, -0.0517],\n",
              "         [-0.3060,  0.2485, -0.2226,  ...,  0.4163,  0.2615,  0.9311],\n",
              "         [-0.5145, -1.6517,  1.0460,  ...,  0.5638,  2.2566,  1.8693],\n",
              "         ...,\n",
              "         [ 2.1181,  0.1464, -0.0447,  ...,  1.3816,  0.4975,  0.2814],\n",
              "         [-0.7639, -1.4938, -1.1430,  ...,  0.6355,  0.6700,  1.5335],\n",
              "         [-0.0191, -0.3568,  0.4536,  ..., -0.9493,  2.0439, -0.3827]],\n",
              "        requires_grad=True),\n",
              " tensor([[-0.9414,  1.2632, -0.1838,  ..., -2.6021,  0.6245, -0.8684],\n",
              "         [-0.2051,  0.3976,  0.6699,  ..., -2.1205,  1.5191, -0.6682],\n",
              "         [ 0.0031, -0.1535,  1.1396,  ..., -0.7588, -0.1853, -0.8558],\n",
              "         ...,\n",
              "         [ 1.6794, -0.5509,  0.4118,  ...,  0.9084, -0.8626, -0.6553],\n",
              "         [ 0.6058, -0.5888,  0.9448,  ...,  0.0072, -0.2579,  1.7659],\n",
              "         [-1.2965,  0.2970, -0.5833,  ...,  1.7838, -0.4794,  0.5579]],\n",
              "        requires_grad=True),\n",
              " tensor([[ 1.8423,  0.5189, -1.7119,  ..., -0.1307, -1.4374,  0.3908],\n",
              "         [-0.0190, -1.3527, -0.7308,  ..., -0.7823,  2.7799,  1.2220],\n",
              "         [-0.3364, -0.9651, -0.1297,  ..., -0.4374,  0.7792, -0.0583],\n",
              "         ...,\n",
              "         [ 0.6700, -0.5400,  0.2353,  ..., -1.0840, -0.6141, -0.0155],\n",
              "         [ 0.4779, -0.4648, -0.1366,  ...,  0.1162,  3.0351, -0.2885],\n",
              "         [-0.6777, -0.1373, -0.7330,  ...,  0.6185, -0.3036, -1.0850]],\n",
              "        requires_grad=True),\n",
              " tensor([[-1.2113,  0.6304, -1.4713,  ...,  0.3295,  0.3264, -0.4806],\n",
              "         [ 1.1032,  2.5485,  0.3006,  ..., -1.6279, -1.4801, -1.0631],\n",
              "         [ 0.3630,  0.3995,  0.1457,  ..., -1.3437,  0.8535,  0.8811],\n",
              "         ...,\n",
              "         [-0.5519,  0.2253,  0.4891,  ..., -0.0110, -0.6023, -0.7230],\n",
              "         [-1.1593, -0.6551,  1.6578,  ...,  0.4795, -1.3562,  0.2920],\n",
              "         [ 0.3474, -0.9874, -0.0130,  ...,  0.6061,  0.8639, -0.9552]],\n",
              "        requires_grad=True),\n",
              " tensor([[-0.8201,  0.3956,  0.8989,  ..., -0.6411, -0.8937,  0.9265],\n",
              "         [-0.5355, -1.1597, -0.4602,  ...,  1.0902, -1.5827, -0.3246],\n",
              "         [ 1.9264, -0.3300,  0.1984,  ..., -0.2093, -0.2153, -1.8157],\n",
              "         ...,\n",
              "         [-0.6910,  0.3328,  2.2102,  ..., -0.0383,  0.4400, -0.8350],\n",
              "         [-0.2194, -0.7611, -0.0921,  ..., -0.3143, -0.4196,  1.1570],\n",
              "         [-0.8934, -1.7705,  0.3805,  ...,  0.1963, -0.7307,  1.3581]],\n",
              "        requires_grad=True),\n",
              " tensor([[-1.1892,  1.3932,  2.1059,  ...,  2.1414,  0.1317, -0.6388],\n",
              "         [ 1.3384, -1.1908, -0.7601,  ..., -0.1051,  0.4414,  0.6590],\n",
              "         [-0.7585, -0.6001, -0.3948,  ..., -1.7526,  0.3920,  0.8295],\n",
              "         ...,\n",
              "         [-0.0557, -0.1032, -0.4624,  ..., -0.1339, -1.6662, -0.4955],\n",
              "         [ 1.0884, -0.4479, -0.0847,  ...,  1.7487, -1.6152, -1.8258],\n",
              "         [ 1.7062,  1.1041, -1.3736,  ..., -1.5244,  0.4869, -1.7420]],\n",
              "        requires_grad=True),\n",
              " tensor([[-1.0674, -0.7172,  1.0897,  ..., -0.7737, -2.4656,  0.9968],\n",
              "         [ 0.4524, -0.3464, -0.7245,  ...,  0.2331, -1.1433,  0.8289],\n",
              "         [ 0.9534,  0.2948,  1.5159,  ...,  0.3971,  0.4058, -0.5274],\n",
              "         ...,\n",
              "         [-0.3297, -0.3700,  1.9490,  ..., -0.0443,  1.8073, -0.6388],\n",
              "         [ 0.0977,  0.1862,  1.4303,  ..., -1.9735, -1.1663,  1.7066],\n",
              "         [-0.8396, -2.5271, -1.0791,  ...,  0.1053,  1.2463, -0.7709]],\n",
              "        requires_grad=True),\n",
              " tensor([[-0.8173, -0.5556, -0.8267,  ..., -0.5133,  2.6278, -0.7465],\n",
              "         [ 1.0051, -0.2568,  0.4765,  ..., -0.2496,  0.8298,  1.1209],\n",
              "         [ 0.9999,  1.1167,  1.0763,  ...,  0.0562,  0.2456,  0.9535],\n",
              "         ...,\n",
              "         [-1.0042, -0.7732,  0.9129,  ..., -0.4342,  1.3256, -0.6357],\n",
              "         [-0.5979,  1.2285,  1.0288,  ..., -1.4067,  0.2403,  0.5257],\n",
              "         [-1.7332, -0.2443,  0.1425,  ..., -0.9291,  1.4324, -0.2338]],\n",
              "        requires_grad=True),\n",
              " tensor([[-0.5108,  1.0283, -0.3532,  ...,  0.1421, -0.5243, -0.2487],\n",
              "         [-0.5252,  2.8922, -0.5947,  ..., -0.0080,  0.2479,  1.5727],\n",
              "         [-1.6395, -1.5925, -0.1546,  ..., -0.3935,  0.6171,  0.7528],\n",
              "         ...,\n",
              "         [-0.3538,  0.1294,  1.1873,  ..., -0.2866, -0.3111,  0.2674],\n",
              "         [ 1.7757, -0.1730,  0.6679,  ..., -0.2519,  0.8360, -0.4348],\n",
              "         [ 0.4242,  0.7649, -0.5807,  ..., -0.7654, -0.1086,  0.4636]],\n",
              "        requires_grad=True)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HbrMidFCla6h"
      },
      "source": [
        "class Module2(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Module2, self).__init__()\n",
        "    self.linear1 = nn.Linear(5,100)\n",
        "    self.linear2 = nn.Linear(100,3)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = F.relu(self.linear1(x))\n",
        "    x = self.linear2(x)\n",
        "    return x"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRqj2VELllkX"
      },
      "source": [
        "torch.manual_seed(1234)\n",
        "what_net = Module2().double()\n",
        "\n",
        "what_net.load_state_dict(torch.load(\"blob_what_net.pt\"))\n",
        "what_net = what_net.to(\"cuda\")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6d8Wch99l4yB"
      },
      "source": [
        "def attn_avg(x,beta):\n",
        "  y = torch.zeros([batch,5], dtype=torch.float64)\n",
        "  y = y.to(\"cuda\")\n",
        "  alpha = F.softmax(beta,dim=1)   # alphas\n",
        "  #print(alpha[0],x[0,:])\n",
        "  for i in range(9):            \n",
        "    alpha1 = alpha[:,i]      \n",
        "    y = y + torch.mul(alpha1[:,None],x[:,i])\n",
        "  return y,alpha\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rz1Kpw12loV6"
      },
      "source": [
        "def calculate_attn_loss(dataloader,what,criter):\n",
        "  what.eval()\n",
        "  r_loss = 0\n",
        "  alphas = []\n",
        "  lbls = []\n",
        "  pred = []\n",
        "  fidices = []\n",
        "  correct = 0\n",
        "  tot = 0\n",
        "  with torch.no_grad():\n",
        "    for i, data in enumerate(dataloader, 0):\n",
        "      inputs, labels,fidx= data\n",
        "      lbls.append(labels)\n",
        "      fidices.append(fidx)\n",
        "      inputs = inputs.double()\n",
        "      beta = bg[i]  # beta for ith batch\n",
        "      inputs, labels,beta = inputs.to(\"cuda\"),labels.to(\"cuda\"),beta.to(\"cuda\")\n",
        "      avg,alpha = attn_avg(inputs,beta)\n",
        "      alpha = alpha.to(\"cuda\")\n",
        "      outputs = what(avg)\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      correct += sum(predicted == labels)\n",
        "      tot += len(predicted)\n",
        "      pred.append(predicted.cpu().numpy())\n",
        "      alphas.append(alpha.cpu().numpy())\n",
        "      loss = criter(outputs, labels)\n",
        "      r_loss += loss.item()\n",
        "  alphas = np.concatenate(alphas,axis=0)\n",
        "  pred = np.concatenate(pred,axis=0)\n",
        "  lbls = np.concatenate(lbls,axis=0)\n",
        "  fidices = np.concatenate(fidices,axis=0)\n",
        "  #print(alphas.shape,pred.shape,lbls.shape,fidices.shape) \n",
        "  analysis = analyse_data(alphas,lbls,pred,fidices)\n",
        "  return r_loss/i,analysis,correct.item(),tot,correct.item()/tot"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAY-x6UAwrwE"
      },
      "source": [
        "for param in what_net.parameters():\n",
        "    param.requires_grad = False"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_toCktPanH0S"
      },
      "source": [
        "\n",
        "def analyse_data(alphas,lbls,predicted,f_idx):\n",
        "    '''\n",
        "       analysis data is created here\n",
        "    '''\n",
        "    batch = len(predicted)\n",
        "    amth,alth,ftpt,ffpt,ftpf,ffpf = 0,0,0,0,0,0\n",
        "    for j in range (batch):\n",
        "      focus = np.argmax(alphas[j])\n",
        "      if(alphas[j][focus] >= 0.5):\n",
        "        amth +=1\n",
        "      else:\n",
        "        alth +=1\n",
        "      if(focus == f_idx[j] and predicted[j] == lbls[j]):\n",
        "        ftpt += 1\n",
        "      elif(focus != f_idx[j] and predicted[j] == lbls[j]):\n",
        "        ffpt +=1\n",
        "      elif(focus == f_idx[j] and predicted[j] != lbls[j]):\n",
        "        ftpf +=1\n",
        "      elif(focus != f_idx[j] and predicted[j] != lbls[j]):\n",
        "        ffpf +=1\n",
        "    #print(sum(predicted==lbls),ftpt+ffpt)\n",
        "    return [ftpt,ffpt,ftpf,ffpf,amth,alth]"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S633XgMToeN3"
      },
      "source": [
        "optim1 = []\n",
        "for i in range(12):\n",
        "  optim1.append(optim.RMSprop([bg[i]], lr=10))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qPaYaojinMTA",
        "outputId": "96b520c0-5415-419c-a03c-7af3d66b40ca"
      },
      "source": [
        "# instantiate optimizer\n",
        "#optimizer_what = optim.RMSprop(what_net.parameters(), lr=0.001)#, momentum=0.9)#,nesterov=True)\n",
        "\n",
        "\n",
        " \n",
        "\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "acti = []\n",
        "analysis_data_tr = []\n",
        "analysis_data_tst = []\n",
        "loss_curi_tr = []\n",
        "loss_curi_tst = []\n",
        "epochs = 200\n",
        "\n",
        "\n",
        "# calculate zeroth epoch loss and FTPT values\n",
        "running_loss,anlys_data,correct,total,accuracy = calculate_attn_loss(train_loader,what_net,criterion)\n",
        "print('training epoch: [%d ] loss: %.3f correct: %.3f, total: %.3f, accuracy: %.3f' %(0,running_loss,correct,total,accuracy)) \n",
        "loss_curi_tr.append(running_loss)\n",
        "analysis_data_tr.append(anlys_data)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# training starts \n",
        "for epoch in range(epochs): # loop over the dataset multiple times\n",
        "  ep_lossi = []\n",
        "  running_loss = 0.0\n",
        "  #what_net.train()\n",
        "  for i, data in enumerate(train_loader, 0):\n",
        "    # get the inputs\n",
        "    inputs, labels,_  = data\n",
        "    inputs = inputs.double()\n",
        "    beta = bg[i] # alpha for ith batch\n",
        "    #print(labels)\n",
        "    inputs, labels,beta = inputs.to(\"cuda\"),labels.to(\"cuda\"),beta.to(\"cuda\")\n",
        "        \n",
        "    # zero the parameter gradients\n",
        "    #optimizer_what.zero_grad()\n",
        "    optim1[i].zero_grad()\n",
        "      \n",
        "    # forward + backward + optimize\n",
        "    avg,alpha = attn_avg(inputs,beta)\n",
        "    outputs = what_net(avg)     \n",
        "    loss = criterion(outputs, labels)\n",
        "\n",
        "    # print statistics\n",
        "    running_loss += loss.item()\n",
        "    #alpha.retain_grad()\n",
        "    loss.backward(retain_graph=False)\n",
        "    #optimizer_what.step()\n",
        "    optim1[i].step()\n",
        "\n",
        "\n",
        "  running_loss_tr,anls_data,correct,total,accuracy = calculate_attn_loss(train_loader,what_net,criterion)\n",
        "  analysis_data_tr.append(anls_data)\n",
        "  loss_curi_tr.append(running_loss_tr)   #loss per epoch\n",
        "  print('training epoch: [%d ] loss: %.3f correct: %.3f, total: %.3f, accuracy: %.3f' %(epoch+1,running_loss_tr,correct,total,accuracy)) \n",
        "\n",
        "\n",
        "  \n",
        "  if running_loss_tr<=0.08:\n",
        "    break\n",
        "print('Finished Training run ')\n",
        "analysis_data_tr = np.array(analysis_data_tr)\n",
        "\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training epoch: [0 ] loss: 12.050 correct: 1071.000, total: 3000.000, accuracy: 0.357\n",
            "training epoch: [1 ] loss: 3.583 correct: 2270.000, total: 3000.000, accuracy: 0.757\n",
            "training epoch: [2 ] loss: 1.850 correct: 2721.000, total: 3000.000, accuracy: 0.907\n",
            "training epoch: [3 ] loss: 1.186 correct: 2815.000, total: 3000.000, accuracy: 0.938\n",
            "training epoch: [4 ] loss: 1.152 correct: 2822.000, total: 3000.000, accuracy: 0.941\n",
            "training epoch: [5 ] loss: 1.118 correct: 2826.000, total: 3000.000, accuracy: 0.942\n",
            "training epoch: [6 ] loss: 1.118 correct: 2826.000, total: 3000.000, accuracy: 0.942\n",
            "training epoch: [7 ] loss: 1.118 correct: 2826.000, total: 3000.000, accuracy: 0.942\n",
            "training epoch: [8 ] loss: 1.117 correct: 2826.000, total: 3000.000, accuracy: 0.942\n",
            "training epoch: [9 ] loss: 1.105 correct: 2827.000, total: 3000.000, accuracy: 0.942\n",
            "training epoch: [10 ] loss: 1.094 correct: 2829.000, total: 3000.000, accuracy: 0.943\n",
            "training epoch: [11 ] loss: 1.094 correct: 2830.000, total: 3000.000, accuracy: 0.943\n",
            "training epoch: [12 ] loss: 1.103 correct: 2830.000, total: 3000.000, accuracy: 0.943\n",
            "training epoch: [13 ] loss: 1.089 correct: 2831.000, total: 3000.000, accuracy: 0.944\n",
            "training epoch: [14 ] loss: 1.089 correct: 2831.000, total: 3000.000, accuracy: 0.944\n",
            "training epoch: [15 ] loss: 1.089 correct: 2831.000, total: 3000.000, accuracy: 0.944\n",
            "training epoch: [16 ] loss: 1.089 correct: 2831.000, total: 3000.000, accuracy: 0.944\n",
            "training epoch: [17 ] loss: 1.089 correct: 2831.000, total: 3000.000, accuracy: 0.944\n",
            "training epoch: [18 ] loss: 1.088 correct: 2832.000, total: 3000.000, accuracy: 0.944\n",
            "training epoch: [19 ] loss: 1.088 correct: 2832.000, total: 3000.000, accuracy: 0.944\n",
            "training epoch: [20 ] loss: 1.084 correct: 2832.000, total: 3000.000, accuracy: 0.944\n",
            "training epoch: [21 ] loss: 1.074 correct: 2835.000, total: 3000.000, accuracy: 0.945\n",
            "training epoch: [22 ] loss: 1.074 correct: 2835.000, total: 3000.000, accuracy: 0.945\n",
            "training epoch: [23 ] loss: 1.074 correct: 2836.000, total: 3000.000, accuracy: 0.945\n",
            "training epoch: [24 ] loss: 1.065 correct: 2837.000, total: 3000.000, accuracy: 0.946\n",
            "training epoch: [25 ] loss: 1.061 correct: 2838.000, total: 3000.000, accuracy: 0.946\n",
            "training epoch: [26 ] loss: 1.061 correct: 2838.000, total: 3000.000, accuracy: 0.946\n",
            "training epoch: [27 ] loss: 1.061 correct: 2838.000, total: 3000.000, accuracy: 0.946\n",
            "training epoch: [28 ] loss: 1.061 correct: 2839.000, total: 3000.000, accuracy: 0.946\n",
            "training epoch: [29 ] loss: 1.061 correct: 2839.000, total: 3000.000, accuracy: 0.946\n",
            "training epoch: [30 ] loss: 1.061 correct: 2839.000, total: 3000.000, accuracy: 0.946\n",
            "training epoch: [31 ] loss: 1.061 correct: 2839.000, total: 3000.000, accuracy: 0.946\n",
            "training epoch: [32 ] loss: 1.062 correct: 2839.000, total: 3000.000, accuracy: 0.946\n",
            "training epoch: [33 ] loss: 1.062 correct: 2839.000, total: 3000.000, accuracy: 0.946\n",
            "training epoch: [34 ] loss: 1.061 correct: 2839.000, total: 3000.000, accuracy: 0.946\n",
            "training epoch: [35 ] loss: 1.061 correct: 2839.000, total: 3000.000, accuracy: 0.946\n",
            "training epoch: [36 ] loss: 1.061 correct: 2839.000, total: 3000.000, accuracy: 0.946\n",
            "training epoch: [37 ] loss: 1.061 correct: 2839.000, total: 3000.000, accuracy: 0.946\n",
            "training epoch: [38 ] loss: 1.061 correct: 2839.000, total: 3000.000, accuracy: 0.946\n",
            "training epoch: [39 ] loss: 1.061 correct: 2839.000, total: 3000.000, accuracy: 0.946\n",
            "training epoch: [40 ] loss: 1.061 correct: 2839.000, total: 3000.000, accuracy: 0.946\n",
            "training epoch: [41 ] loss: 1.061 correct: 2839.000, total: 3000.000, accuracy: 0.946\n",
            "training epoch: [42 ] loss: 1.061 correct: 2839.000, total: 3000.000, accuracy: 0.946\n",
            "training epoch: [43 ] loss: 1.061 correct: 2839.000, total: 3000.000, accuracy: 0.946\n",
            "training epoch: [44 ] loss: 1.061 correct: 2839.000, total: 3000.000, accuracy: 0.946\n",
            "training epoch: [45 ] loss: 1.061 correct: 2839.000, total: 3000.000, accuracy: 0.946\n",
            "training epoch: [46 ] loss: 1.061 correct: 2839.000, total: 3000.000, accuracy: 0.946\n",
            "training epoch: [47 ] loss: 1.061 correct: 2839.000, total: 3000.000, accuracy: 0.946\n",
            "training epoch: [48 ] loss: 1.061 correct: 2839.000, total: 3000.000, accuracy: 0.946\n",
            "training epoch: [49 ] loss: 1.061 correct: 2839.000, total: 3000.000, accuracy: 0.946\n",
            "training epoch: [50 ] loss: 1.061 correct: 2839.000, total: 3000.000, accuracy: 0.946\n",
            "training epoch: [51 ] loss: 1.061 correct: 2839.000, total: 3000.000, accuracy: 0.946\n",
            "training epoch: [52 ] loss: 1.061 correct: 2839.000, total: 3000.000, accuracy: 0.946\n",
            "training epoch: [53 ] loss: 1.061 correct: 2839.000, total: 3000.000, accuracy: 0.946\n",
            "training epoch: [54 ] loss: 1.061 correct: 2839.000, total: 3000.000, accuracy: 0.946\n",
            "training epoch: [55 ] loss: 1.061 correct: 2839.000, total: 3000.000, accuracy: 0.946\n",
            "training epoch: [56 ] loss: 1.061 correct: 2839.000, total: 3000.000, accuracy: 0.946\n",
            "training epoch: [57 ] loss: 1.061 correct: 2839.000, total: 3000.000, accuracy: 0.946\n",
            "training epoch: [58 ] loss: 1.061 correct: 2839.000, total: 3000.000, accuracy: 0.946\n",
            "training epoch: [59 ] loss: 1.061 correct: 2839.000, total: 3000.000, accuracy: 0.946\n",
            "training epoch: [60 ] loss: 1.061 correct: 2839.000, total: 3000.000, accuracy: 0.946\n",
            "training epoch: [61 ] loss: 1.061 correct: 2839.000, total: 3000.000, accuracy: 0.946\n",
            "training epoch: [62 ] loss: 1.061 correct: 2839.000, total: 3000.000, accuracy: 0.946\n",
            "training epoch: [63 ] loss: 1.061 correct: 2839.000, total: 3000.000, accuracy: 0.946\n",
            "training epoch: [64 ] loss: 1.061 correct: 2839.000, total: 3000.000, accuracy: 0.946\n",
            "training epoch: [65 ] loss: 1.061 correct: 2839.000, total: 3000.000, accuracy: 0.946\n",
            "training epoch: [66 ] loss: 1.061 correct: 2839.000, total: 3000.000, accuracy: 0.946\n",
            "training epoch: [67 ] loss: 1.061 correct: 2839.000, total: 3000.000, accuracy: 0.946\n",
            "training epoch: [68 ] loss: 1.061 correct: 2839.000, total: 3000.000, accuracy: 0.946\n",
            "training epoch: [69 ] loss: 1.061 correct: 2839.000, total: 3000.000, accuracy: 0.946\n",
            "training epoch: [70 ] loss: 1.061 correct: 2839.000, total: 3000.000, accuracy: 0.946\n",
            "training epoch: [71 ] loss: 1.061 correct: 2839.000, total: 3000.000, accuracy: 0.946\n",
            "training epoch: [72 ] loss: 1.061 correct: 2839.000, total: 3000.000, accuracy: 0.946\n",
            "training epoch: [73 ] loss: 1.061 correct: 2839.000, total: 3000.000, accuracy: 0.946\n",
            "training epoch: [74 ] loss: 1.061 correct: 2839.000, total: 3000.000, accuracy: 0.946\n",
            "training epoch: [75 ] loss: 1.061 correct: 2839.000, total: 3000.000, accuracy: 0.946\n",
            "training epoch: [76 ] loss: 1.061 correct: 2839.000, total: 3000.000, accuracy: 0.946\n",
            "training epoch: [77 ] loss: 1.061 correct: 2839.000, total: 3000.000, accuracy: 0.946\n",
            "training epoch: [78 ] loss: 1.061 correct: 2839.000, total: 3000.000, accuracy: 0.946\n",
            "training epoch: [79 ] loss: 1.054 correct: 2839.000, total: 3000.000, accuracy: 0.946\n",
            "training epoch: [80 ] loss: 1.053 correct: 2840.000, total: 3000.000, accuracy: 0.947\n",
            "training epoch: [81 ] loss: 1.053 correct: 2840.000, total: 3000.000, accuracy: 0.947\n",
            "training epoch: [82 ] loss: 1.053 correct: 2840.000, total: 3000.000, accuracy: 0.947\n",
            "training epoch: [83 ] loss: 1.053 correct: 2840.000, total: 3000.000, accuracy: 0.947\n",
            "training epoch: [84 ] loss: 1.053 correct: 2840.000, total: 3000.000, accuracy: 0.947\n",
            "training epoch: [85 ] loss: 1.053 correct: 2840.000, total: 3000.000, accuracy: 0.947\n",
            "training epoch: [86 ] loss: 1.051 correct: 2840.000, total: 3000.000, accuracy: 0.947\n",
            "training epoch: [87 ] loss: 1.051 correct: 2840.000, total: 3000.000, accuracy: 0.947\n",
            "training epoch: [88 ] loss: 1.051 correct: 2840.000, total: 3000.000, accuracy: 0.947\n",
            "training epoch: [89 ] loss: 1.051 correct: 2840.000, total: 3000.000, accuracy: 0.947\n",
            "training epoch: [90 ] loss: 1.051 correct: 2840.000, total: 3000.000, accuracy: 0.947\n",
            "training epoch: [91 ] loss: 1.051 correct: 2840.000, total: 3000.000, accuracy: 0.947\n",
            "training epoch: [92 ] loss: 1.051 correct: 2840.000, total: 3000.000, accuracy: 0.947\n",
            "training epoch: [93 ] loss: 1.051 correct: 2840.000, total: 3000.000, accuracy: 0.947\n",
            "training epoch: [94 ] loss: 1.051 correct: 2840.000, total: 3000.000, accuracy: 0.947\n",
            "training epoch: [95 ] loss: 1.051 correct: 2840.000, total: 3000.000, accuracy: 0.947\n",
            "training epoch: [96 ] loss: 1.051 correct: 2840.000, total: 3000.000, accuracy: 0.947\n",
            "training epoch: [97 ] loss: 1.051 correct: 2840.000, total: 3000.000, accuracy: 0.947\n",
            "training epoch: [98 ] loss: 1.051 correct: 2840.000, total: 3000.000, accuracy: 0.947\n",
            "training epoch: [99 ] loss: 1.051 correct: 2840.000, total: 3000.000, accuracy: 0.947\n",
            "training epoch: [100 ] loss: 1.047 correct: 2841.000, total: 3000.000, accuracy: 0.947\n",
            "training epoch: [101 ] loss: 1.047 correct: 2841.000, total: 3000.000, accuracy: 0.947\n",
            "training epoch: [102 ] loss: 1.047 correct: 2841.000, total: 3000.000, accuracy: 0.947\n",
            "training epoch: [103 ] loss: 1.047 correct: 2841.000, total: 3000.000, accuracy: 0.947\n",
            "training epoch: [104 ] loss: 1.047 correct: 2841.000, total: 3000.000, accuracy: 0.947\n",
            "training epoch: [105 ] loss: 1.047 correct: 2841.000, total: 3000.000, accuracy: 0.947\n",
            "training epoch: [106 ] loss: 1.047 correct: 2841.000, total: 3000.000, accuracy: 0.947\n",
            "training epoch: [107 ] loss: 1.047 correct: 2841.000, total: 3000.000, accuracy: 0.947\n",
            "training epoch: [108 ] loss: 1.044 correct: 2842.000, total: 3000.000, accuracy: 0.947\n",
            "training epoch: [109 ] loss: 1.044 correct: 2842.000, total: 3000.000, accuracy: 0.947\n",
            "training epoch: [110 ] loss: 1.044 correct: 2842.000, total: 3000.000, accuracy: 0.947\n",
            "training epoch: [111 ] loss: 1.044 correct: 2842.000, total: 3000.000, accuracy: 0.947\n",
            "training epoch: [112 ] loss: 1.044 correct: 2842.000, total: 3000.000, accuracy: 0.947\n",
            "training epoch: [113 ] loss: 1.044 correct: 2842.000, total: 3000.000, accuracy: 0.947\n",
            "training epoch: [114 ] loss: 1.037 correct: 2843.000, total: 3000.000, accuracy: 0.948\n",
            "training epoch: [115 ] loss: 1.037 correct: 2843.000, total: 3000.000, accuracy: 0.948\n",
            "training epoch: [116 ] loss: 1.037 correct: 2843.000, total: 3000.000, accuracy: 0.948\n",
            "training epoch: [117 ] loss: 1.037 correct: 2843.000, total: 3000.000, accuracy: 0.948\n",
            "training epoch: [118 ] loss: 1.037 correct: 2843.000, total: 3000.000, accuracy: 0.948\n",
            "training epoch: [119 ] loss: 1.037 correct: 2843.000, total: 3000.000, accuracy: 0.948\n",
            "training epoch: [120 ] loss: 1.035 correct: 2844.000, total: 3000.000, accuracy: 0.948\n",
            "training epoch: [121 ] loss: 1.035 correct: 2844.000, total: 3000.000, accuracy: 0.948\n",
            "training epoch: [122 ] loss: 1.035 correct: 2844.000, total: 3000.000, accuracy: 0.948\n",
            "training epoch: [123 ] loss: 1.035 correct: 2844.000, total: 3000.000, accuracy: 0.948\n",
            "training epoch: [124 ] loss: 1.035 correct: 2844.000, total: 3000.000, accuracy: 0.948\n",
            "training epoch: [125 ] loss: 1.035 correct: 2844.000, total: 3000.000, accuracy: 0.948\n",
            "training epoch: [126 ] loss: 1.035 correct: 2844.000, total: 3000.000, accuracy: 0.948\n",
            "training epoch: [127 ] loss: 1.028 correct: 2845.000, total: 3000.000, accuracy: 0.948\n",
            "training epoch: [128 ] loss: 1.028 correct: 2845.000, total: 3000.000, accuracy: 0.948\n",
            "training epoch: [129 ] loss: 1.028 correct: 2845.000, total: 3000.000, accuracy: 0.948\n",
            "training epoch: [130 ] loss: 1.028 correct: 2845.000, total: 3000.000, accuracy: 0.948\n",
            "training epoch: [131 ] loss: 1.028 correct: 2845.000, total: 3000.000, accuracy: 0.948\n",
            "training epoch: [132 ] loss: 1.028 correct: 2845.000, total: 3000.000, accuracy: 0.948\n",
            "training epoch: [133 ] loss: 1.028 correct: 2845.000, total: 3000.000, accuracy: 0.948\n",
            "training epoch: [134 ] loss: 1.028 correct: 2845.000, total: 3000.000, accuracy: 0.948\n",
            "training epoch: [135 ] loss: 1.028 correct: 2845.000, total: 3000.000, accuracy: 0.948\n",
            "training epoch: [136 ] loss: 1.028 correct: 2845.000, total: 3000.000, accuracy: 0.948\n",
            "training epoch: [137 ] loss: 1.028 correct: 2845.000, total: 3000.000, accuracy: 0.948\n",
            "training epoch: [138 ] loss: 1.028 correct: 2845.000, total: 3000.000, accuracy: 0.948\n",
            "training epoch: [139 ] loss: 1.028 correct: 2845.000, total: 3000.000, accuracy: 0.948\n",
            "training epoch: [140 ] loss: 1.028 correct: 2845.000, total: 3000.000, accuracy: 0.948\n",
            "training epoch: [141 ] loss: 1.028 correct: 2845.000, total: 3000.000, accuracy: 0.948\n",
            "training epoch: [142 ] loss: 1.028 correct: 2845.000, total: 3000.000, accuracy: 0.948\n",
            "training epoch: [143 ] loss: 1.028 correct: 2845.000, total: 3000.000, accuracy: 0.948\n",
            "training epoch: [144 ] loss: 1.028 correct: 2845.000, total: 3000.000, accuracy: 0.948\n",
            "training epoch: [145 ] loss: 1.028 correct: 2845.000, total: 3000.000, accuracy: 0.948\n",
            "training epoch: [146 ] loss: 1.028 correct: 2845.000, total: 3000.000, accuracy: 0.948\n",
            "training epoch: [147 ] loss: 1.028 correct: 2845.000, total: 3000.000, accuracy: 0.948\n",
            "training epoch: [148 ] loss: 1.028 correct: 2845.000, total: 3000.000, accuracy: 0.948\n",
            "training epoch: [149 ] loss: 1.028 correct: 2845.000, total: 3000.000, accuracy: 0.948\n",
            "training epoch: [150 ] loss: 1.028 correct: 2845.000, total: 3000.000, accuracy: 0.948\n",
            "training epoch: [151 ] loss: 1.028 correct: 2845.000, total: 3000.000, accuracy: 0.948\n",
            "training epoch: [152 ] loss: 1.028 correct: 2845.000, total: 3000.000, accuracy: 0.948\n",
            "training epoch: [153 ] loss: 1.028 correct: 2845.000, total: 3000.000, accuracy: 0.948\n",
            "training epoch: [154 ] loss: 1.028 correct: 2845.000, total: 3000.000, accuracy: 0.948\n",
            "training epoch: [155 ] loss: 1.028 correct: 2845.000, total: 3000.000, accuracy: 0.948\n",
            "training epoch: [156 ] loss: 1.028 correct: 2845.000, total: 3000.000, accuracy: 0.948\n",
            "training epoch: [157 ] loss: 1.028 correct: 2845.000, total: 3000.000, accuracy: 0.948\n",
            "training epoch: [158 ] loss: 1.028 correct: 2845.000, total: 3000.000, accuracy: 0.948\n",
            "training epoch: [159 ] loss: 1.028 correct: 2845.000, total: 3000.000, accuracy: 0.948\n",
            "training epoch: [160 ] loss: 1.028 correct: 2845.000, total: 3000.000, accuracy: 0.948\n",
            "training epoch: [161 ] loss: 1.028 correct: 2845.000, total: 3000.000, accuracy: 0.948\n",
            "training epoch: [162 ] loss: 1.028 correct: 2845.000, total: 3000.000, accuracy: 0.948\n",
            "training epoch: [163 ] loss: 1.028 correct: 2845.000, total: 3000.000, accuracy: 0.948\n",
            "training epoch: [164 ] loss: 1.028 correct: 2845.000, total: 3000.000, accuracy: 0.948\n",
            "training epoch: [165 ] loss: 1.028 correct: 2845.000, total: 3000.000, accuracy: 0.948\n",
            "training epoch: [166 ] loss: 1.028 correct: 2845.000, total: 3000.000, accuracy: 0.948\n",
            "training epoch: [167 ] loss: 1.015 correct: 2846.000, total: 3000.000, accuracy: 0.949\n",
            "training epoch: [168 ] loss: 1.018 correct: 2845.000, total: 3000.000, accuracy: 0.948\n",
            "training epoch: [169 ] loss: 1.015 correct: 2846.000, total: 3000.000, accuracy: 0.949\n",
            "training epoch: [170 ] loss: 1.015 correct: 2846.000, total: 3000.000, accuracy: 0.949\n",
            "training epoch: [171 ] loss: 1.015 correct: 2846.000, total: 3000.000, accuracy: 0.949\n",
            "training epoch: [172 ] loss: 1.015 correct: 2846.000, total: 3000.000, accuracy: 0.949\n",
            "training epoch: [173 ] loss: 1.015 correct: 2846.000, total: 3000.000, accuracy: 0.949\n",
            "training epoch: [174 ] loss: 1.015 correct: 2846.000, total: 3000.000, accuracy: 0.949\n",
            "training epoch: [175 ] loss: 1.015 correct: 2846.000, total: 3000.000, accuracy: 0.949\n",
            "training epoch: [176 ] loss: 1.015 correct: 2846.000, total: 3000.000, accuracy: 0.949\n",
            "training epoch: [177 ] loss: 1.015 correct: 2846.000, total: 3000.000, accuracy: 0.949\n",
            "training epoch: [178 ] loss: 1.015 correct: 2846.000, total: 3000.000, accuracy: 0.949\n",
            "training epoch: [179 ] loss: 1.015 correct: 2846.000, total: 3000.000, accuracy: 0.949\n",
            "training epoch: [180 ] loss: 1.015 correct: 2846.000, total: 3000.000, accuracy: 0.949\n",
            "training epoch: [181 ] loss: 1.015 correct: 2846.000, total: 3000.000, accuracy: 0.949\n",
            "training epoch: [182 ] loss: 1.015 correct: 2846.000, total: 3000.000, accuracy: 0.949\n",
            "training epoch: [183 ] loss: 1.015 correct: 2846.000, total: 3000.000, accuracy: 0.949\n",
            "training epoch: [184 ] loss: 1.015 correct: 2846.000, total: 3000.000, accuracy: 0.949\n",
            "training epoch: [185 ] loss: 1.015 correct: 2846.000, total: 3000.000, accuracy: 0.949\n",
            "training epoch: [186 ] loss: 1.015 correct: 2846.000, total: 3000.000, accuracy: 0.949\n",
            "training epoch: [187 ] loss: 1.015 correct: 2846.000, total: 3000.000, accuracy: 0.949\n",
            "training epoch: [188 ] loss: 1.015 correct: 2846.000, total: 3000.000, accuracy: 0.949\n",
            "training epoch: [189 ] loss: 1.015 correct: 2846.000, total: 3000.000, accuracy: 0.949\n",
            "training epoch: [190 ] loss: 1.015 correct: 2846.000, total: 3000.000, accuracy: 0.949\n",
            "training epoch: [191 ] loss: 1.015 correct: 2846.000, total: 3000.000, accuracy: 0.949\n",
            "training epoch: [192 ] loss: 1.015 correct: 2846.000, total: 3000.000, accuracy: 0.949\n",
            "training epoch: [193 ] loss: 1.015 correct: 2846.000, total: 3000.000, accuracy: 0.949\n",
            "training epoch: [194 ] loss: 1.015 correct: 2846.000, total: 3000.000, accuracy: 0.949\n",
            "training epoch: [195 ] loss: 1.015 correct: 2846.000, total: 3000.000, accuracy: 0.949\n",
            "training epoch: [196 ] loss: 1.013 correct: 2846.000, total: 3000.000, accuracy: 0.949\n",
            "training epoch: [197 ] loss: 1.010 correct: 2847.000, total: 3000.000, accuracy: 0.949\n",
            "training epoch: [198 ] loss: 1.010 correct: 2847.000, total: 3000.000, accuracy: 0.949\n",
            "training epoch: [199 ] loss: 1.010 correct: 2847.000, total: 3000.000, accuracy: 0.949\n",
            "training epoch: [200 ] loss: 1.010 correct: 2847.000, total: 3000.000, accuracy: 0.949\n",
            "Finished Training run \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AciJnAh5nfug"
      },
      "source": [
        "columns = [\"epochs\", \"argmax > 0.5\" ,\"argmax < 0.5\", \"focus_true_pred_true\", \"focus_false_pred_true\", \"focus_true_pred_false\", \"focus_false_pred_false\" ]\n",
        "df_train = pd.DataFrame()\n",
        "df_test = pd.DataFrame()\n",
        "df_train[columns[0]] = np.arange(0,epoch+2)\n",
        "df_train[columns[1]] = analysis_data_tr[:,-2]\n",
        "df_train[columns[2]] = analysis_data_tr[:,-1]\n",
        "df_train[columns[3]] = analysis_data_tr[:,0]\n",
        "df_train[columns[4]] = analysis_data_tr[:,1]\n",
        "df_train[columns[5]] = analysis_data_tr[:,2]\n",
        "df_train[columns[6]] = analysis_data_tr[:,3]"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "NoQpS_6scRsC",
        "outputId": "3e4b8472-7eae-4a2e-d3e7-a3be3309847e"
      },
      "source": [
        "df_train"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>epochs</th>\n",
              "      <th>argmax &gt; 0.5</th>\n",
              "      <th>argmax &lt; 0.5</th>\n",
              "      <th>focus_true_pred_true</th>\n",
              "      <th>focus_false_pred_true</th>\n",
              "      <th>focus_true_pred_false</th>\n",
              "      <th>focus_false_pred_false</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>302</td>\n",
              "      <td>2698</td>\n",
              "      <td>135</td>\n",
              "      <td>936</td>\n",
              "      <td>179</td>\n",
              "      <td>1750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1318</td>\n",
              "      <td>1682</td>\n",
              "      <td>403</td>\n",
              "      <td>1867</td>\n",
              "      <td>121</td>\n",
              "      <td>609</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>1935</td>\n",
              "      <td>1065</td>\n",
              "      <td>574</td>\n",
              "      <td>2147</td>\n",
              "      <td>8</td>\n",
              "      <td>271</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>1952</td>\n",
              "      <td>1048</td>\n",
              "      <td>597</td>\n",
              "      <td>2218</td>\n",
              "      <td>0</td>\n",
              "      <td>185</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>1960</td>\n",
              "      <td>1040</td>\n",
              "      <td>602</td>\n",
              "      <td>2220</td>\n",
              "      <td>0</td>\n",
              "      <td>178</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>196</th>\n",
              "      <td>196</td>\n",
              "      <td>2010</td>\n",
              "      <td>990</td>\n",
              "      <td>596</td>\n",
              "      <td>2250</td>\n",
              "      <td>0</td>\n",
              "      <td>154</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>197</th>\n",
              "      <td>197</td>\n",
              "      <td>2010</td>\n",
              "      <td>990</td>\n",
              "      <td>597</td>\n",
              "      <td>2250</td>\n",
              "      <td>0</td>\n",
              "      <td>153</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>198</th>\n",
              "      <td>198</td>\n",
              "      <td>2010</td>\n",
              "      <td>990</td>\n",
              "      <td>597</td>\n",
              "      <td>2250</td>\n",
              "      <td>0</td>\n",
              "      <td>153</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199</th>\n",
              "      <td>199</td>\n",
              "      <td>2012</td>\n",
              "      <td>988</td>\n",
              "      <td>597</td>\n",
              "      <td>2250</td>\n",
              "      <td>0</td>\n",
              "      <td>153</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>200</th>\n",
              "      <td>200</td>\n",
              "      <td>2011</td>\n",
              "      <td>989</td>\n",
              "      <td>597</td>\n",
              "      <td>2250</td>\n",
              "      <td>0</td>\n",
              "      <td>153</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>201 rows Ã— 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     epochs  argmax > 0.5  ...  focus_true_pred_false  focus_false_pred_false\n",
              "0         0           302  ...                    179                    1750\n",
              "1         1          1318  ...                    121                     609\n",
              "2         2          1935  ...                      8                     271\n",
              "3         3          1952  ...                      0                     185\n",
              "4         4          1960  ...                      0                     178\n",
              "..      ...           ...  ...                    ...                     ...\n",
              "196     196          2010  ...                      0                     154\n",
              "197     197          2010  ...                      0                     153\n",
              "198     198          2010  ...                      0                     153\n",
              "199     199          2012  ...                      0                     153\n",
              "200     200          2011  ...                      0                     153\n",
              "\n",
              "[201 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "id": "IMAhRdxOcVf6",
        "outputId": "2c9d90e5-7f4a-4929-d53f-36ff79998466"
      },
      "source": [
        "fig= plt.figure(figsize=(6,6))\n",
        "plt.plot(df_train[columns[0]],df_train[columns[3]]/30, label =\"focus_true_pred_true \")\n",
        "plt.plot(df_train[columns[0]],df_train[columns[4]]/30, label =\"focus_false_pred_true \")\n",
        "plt.plot(df_train[columns[0]],df_train[columns[5]]/30, label =\"focus_true_pred_false \")\n",
        "plt.plot(df_train[columns[0]],df_train[columns[6]]/30, label =\"focus_false_pred_false \")\n",
        "plt.title(\"On Train set\")\n",
        "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"percentage of data\")\n",
        "plt.xticks([0,5,10])\n",
        "#plt.vlines(vline_list,min(min(df_train[columns[3]]/300),min(df_train[columns[4]]/300),min(df_train[columns[5]]/300),min(df_train[columns[6]]/300)), max(max(df_train[columns[3]]/300),max(df_train[columns[4]]/300),max(df_train[columns[5]]/300),max(df_train[columns[6]]/300)),linestyles='dotted')\n",
        "plt.show()\n",
        "fig.savefig(\"train_analysis.pdf\")\n",
        "fig.savefig(\"train_analysis.png\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCnS6r2_3WdU"
      },
      "source": [
        "aph = []\n",
        "for i in bg:\n",
        "  aph.append(F.softmax(i,dim=1).detach().numpy())\n",
        "  \n",
        "aph = np.concatenate(aph,axis=0)\n",
        "torch.save({\n",
        "            'epoch': 500,\n",
        "            'model_state_dict': what_net.state_dict(),\n",
        "            #'optimizer_state_dict': optimizer_what.state_dict(),\n",
        "            \"optimizer_alpha\":optim1,\n",
        "            \"FTPT_analysis\":analysis_data_tr,\n",
        "            \"alpha\":aph\n",
        "\n",
        "            }, \"type4_what_net_500.pt\")"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVzrDOGS4UxU",
        "outputId": "f2b9f8f6-dd98-46ca-f16a-f78eb6e059c5"
      },
      "source": [
        "aph[0]"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.24095091, 0.11138774, 0.11505604, 0.        , 0.        ,\n",
              "       0.43773946, 0.        , 0.        , 0.0948659 ], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BwTDpx6STIPh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
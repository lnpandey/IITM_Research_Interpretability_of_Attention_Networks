{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "type4_attn_ewts_NTK.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7pI7PJ8XATdT",
        "outputId": "a28f9b14-831c-420d-ae8c-b83af86df429"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9qVDQd_BBqS",
        "outputId": "00c06a07-c283-4943-c82b-81b9fedb2bd9"
      },
      "source": [
        "%cd /content/drive/MyDrive/Neural_Tangent_Kernel/"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Neural_Tangent_Kernel\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWIyC9Ip_bcq"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "\n",
        "from myrmsprop import MyRmsprop\n",
        "from utils import plot_decision_boundary,attn_avg,plot_analysis\n",
        "from synthetic_dataset import MosaicDataset1\n",
        "from eval_model import calculate_attn_loss,analyse_data\n",
        "\n",
        "\n",
        "\n",
        "%matplotlib inline\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGVy-1EllAc_"
      },
      "source": [
        "train_data = np.load(\"train_type4_data.npy\",allow_pickle=True)\n",
        "\n",
        "test_data = np.load(\"test_type4_data.npy\",allow_pickle=True)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uL771xuGZC5Q"
      },
      "source": [
        "mosaic_list_of_images = train_data[0][\"mosaic_list\"]\n",
        "mosaic_label = train_data[0][\"mosaic_label\"]\n",
        "fore_idx = train_data[0][\"fore_idx\"]\n",
        "\n",
        "\n",
        "test_mosaic_list_of_images = test_data[0][\"mosaic_list\"]\n",
        "test_mosaic_label = test_data[0][\"mosaic_label\"]\n",
        "test_fore_idx = test_data[0][\"fore_idx\"]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uf76JwkxZCT0"
      },
      "source": [
        "batch = 3000\n",
        "train_dataset = MosaicDataset1(mosaic_list_of_images, mosaic_label, fore_idx)\n",
        "train_loader = DataLoader( train_dataset,batch_size= batch ,shuffle=False)\n",
        "#batch = 2000\n",
        "#test_dataset = MosaicDataset1(test_mosaic_list_of_images, test_mosaic_label, test_fore_idx)\n",
        "#test_loader = DataLoader(test_dataset,batch_size= batch ,shuffle=False)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-Lv8nHoB8z-"
      },
      "source": [
        "# NTK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmGjlMfTBp3F"
      },
      "source": [
        "data = np.load(\"NTK_1.npy\",allow_pickle=True)\n",
        "# H = data[0]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beyk_-qYB_Ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fb5da9e-57e4-46c4-a54d-2383897a3236"
      },
      "source": [
        "print(data[0].keys())\n",
        "H = torch.tensor(data[0][\"NTK\"])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['NTK'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ULTAsyF6G6a"
      },
      "source": [
        "lr_1 = 1/1470559.2\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnqbuxdO2U4j"
      },
      "source": [
        "# p_vec = nn.utils.parameters_to_vector(where_func.parameters())\n",
        "# p, = p_vec.shape\n",
        "# n_m, n_obj,_ = inputs.shape  # number of mosaic images x number of objects in each mosaic  x d\n",
        "# # this is the transpose jacobian (grad y(w))^T)\n",
        "# features = torch.zeros(n_m*n_obj, p, requires_grad=False)\n",
        " \n",
        "# k = 0 \n",
        "\n",
        "\n",
        "# for i in range(27000):\n",
        "#     out = where_func(inpp[i])\n",
        "#     where_func.zero_grad()\n",
        "#     out.backward(retain_graph=False)\n",
        "#     p_grad = torch.tensor([], requires_grad=False)\n",
        "#     for p in where_func.parameters():\n",
        "#       p_grad = torch.cat((p_grad, p.grad.reshape(-1)))\n",
        "#     features[k,:] = p_grad\n",
        "#     k = k+1\n",
        "# tangent_kernel =  features@features.T"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SInPc5gk9XDH"
      },
      "source": [
        "# class Module1(nn.Module):\n",
        "#   def __init__(self):\n",
        "#     super(Module1, self).__init__()\n",
        "#     self.linear1 = nn.Linear(2,100)\n",
        "#     self.linear2 = nn.Linear(100,1)\n",
        "\n",
        "#   def forward(self,x):\n",
        "#     x = F.relu(self.linear1(x))\n",
        "#     x = self.linear2(x)\n",
        "#     return x"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VW0lzy6i9wk0"
      },
      "source": [
        "# from tqdm import tqdm as tqdm"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cti_LAbE8-dn"
      },
      "source": [
        "# inputs,_,_ = iter(train_loader).next()\n",
        "# inputs = torch.reshape(inputs,(27000,2))\n",
        "# inputs = (inputs - torch.mean(inputs,dim=0,keepdims=True) )/torch.std(inputs,dim=0,keepdims=True)\n",
        "# where_net = Module1()\n",
        "# outputs = where_net(inputs)\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-03FnsNP5bk"
      },
      "source": [
        "# feature1 = torch.zeros((27000,200))\n",
        "# feature2  = torch.zeros((27000,100))\n",
        "# for i in tqdm(range(27000)):\n",
        "#   where_net.zero_grad()\n",
        "#   outputs[i].backward(retain_graph=True)\n",
        "#   par = []\n",
        "#   j = 0\n",
        "#   for p in where_net.parameters():\n",
        "#     if j%2 == 0:\n",
        "#       vec = torch.nn.utils.parameters_to_vector(p)\n",
        "#       p_grad = p.grad.reshape(-1)\n",
        "#       par.append(p_grad)\n",
        "#     j = j+1\n",
        "#   feature1[i,:] = par[0]\n",
        "#   feature2[i,:] = par[1]"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eI20WxiR-zCi"
      },
      "source": [
        "# H = feature1@feature1.T + feature2@feature2.T"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWIBQfQly25h"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HbrMidFCla6h"
      },
      "source": [
        "class Module2(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Module2, self).__init__()\n",
        "    self.linear1 = nn.Linear(2,100)\n",
        "    self.linear2 = nn.Linear(100,3)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = F.relu(self.linear1(x))\n",
        "    x = self.linear2(x)\n",
        "    return x"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXpnLkMoCocj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c9008e2-2c80-469c-b785-c6965e680e7f"
      },
      "source": [
        "print(H)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[75.1031, 35.6146, 79.1224,  ..., 63.2880, 38.2985, 72.4251],\n",
            "        [35.6146, 21.1840, 42.9770,  ..., 33.6579, 25.0675, 44.2191],\n",
            "        [79.1224, 42.9770, 92.8314,  ..., 73.0225, 48.9726, 88.7758],\n",
            "        ...,\n",
            "        [63.2880, 33.6579, 73.0225,  ..., 58.0862, 38.0167, 69.3583],\n",
            "        [38.2985, 25.0675, 48.9726,  ..., 38.0167, 34.9229, 54.7437],\n",
            "        [72.4251, 44.2191, 88.7758,  ..., 69.3583, 54.7437, 95.1200]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRDhoG3rEp_w"
      },
      "source": [
        ""
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRqj2VELllkX"
      },
      "source": [
        "torch.manual_seed(1234)\n",
        "what_net = Module2().double()\n",
        "\n",
        "#what_net.load_state_dict(torch.load(\"type4_what_net.pt\"))\n",
        "what_net = what_net.to(\"cuda\")"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOpZfj1bq7wN"
      },
      "source": [
        "n_batches = 3000//batch\n",
        "bg = []\n",
        "for i in range(n_batches):\n",
        "  torch.manual_seed(i)\n",
        "  betag = torch.randn(3000,9)#torch.ones((250,9))/9\n",
        "  bg.append( betag.requires_grad_() )"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76PwzSMACDDj"
      },
      "source": [
        "# training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S633XgMToeN3"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lrDkUUaDFCR"
      },
      "source": [
        "optim1 = []\n",
        "H= H.to(\"cpu\")\n",
        "for i in range(n_batches):\n",
        "  optim1.append(MyRmsprop([bg[i]],H=H,lr=0.01))\n",
        "# instantiate what net optimizer\n",
        "optimizer_what = optim.RMSprop(what_net.parameters(), lr=0.0001)#, momentum=0.9)#,nesterov=True)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPaYaojinMTA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20688822-6a09-4fdb-d7cf-3a15a8d554b7"
      },
      "source": [
        "\n",
        "acti = []\n",
        "analysis_data_tr = []\n",
        "analysis_data_tst = []\n",
        "loss_curi_tr = []\n",
        "loss_curi_tst = []\n",
        "epochs = 2500\n",
        "\n",
        "\n",
        "# calculate zeroth epoch loss and FTPT values\n",
        "running_loss,anlys_data,correct,total,accuracy = calculate_attn_loss(train_loader,bg,what_net,criterion)\n",
        "print('training epoch: [%d ] loss: %.3f correct: %.3f, total: %.3f, accuracy: %.3f' %(0,running_loss,correct,total,accuracy)) \n",
        "loss_curi_tr.append(running_loss)\n",
        "analysis_data_tr.append(anlys_data)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# training starts \n",
        "for epoch in range(epochs): # loop over the dataset multiple times\n",
        "  ep_lossi = []\n",
        "  running_loss = 0.0\n",
        "  what_net.train()\n",
        "  for i, data in enumerate(train_loader, 0):\n",
        "    # get the inputs\n",
        "    inputs, labels,_  = data\n",
        "    inputs = inputs.double()\n",
        "    beta = bg[i] # alpha for ith batch\n",
        "    #print(labels)\n",
        "    inputs, labels,beta = inputs.to(\"cuda\"),labels.to(\"cuda\"),beta.to(\"cuda\")\n",
        "        \n",
        "    # zero the parameter gradients\n",
        "    optimizer_what.zero_grad()\n",
        "    optim1[i].zero_grad()\n",
        "      \n",
        "    # forward + backward + optimize\n",
        "    avg,alpha = attn_avg(inputs,beta)\n",
        "    outputs = what_net(avg)     \n",
        "    loss = criterion(outputs, labels)\n",
        "\n",
        "    # print statistics\n",
        "    running_loss += loss.item()\n",
        "    #alpha.retain_grad()\n",
        "    loss.backward(retain_graph=False)\n",
        "    optimizer_what.step()\n",
        "    optim1[i].step()\n",
        "\n",
        "\n",
        "  running_loss_tr,anls_data,correct,total,accuracy = calculate_attn_loss(train_loader,bg,what_net,criterion)\n",
        "  analysis_data_tr.append(anls_data)\n",
        "  loss_curi_tr.append(running_loss_tr)   #loss per epoch\n",
        "  print('training epoch: [%d ] loss: %.3f correct: %.3f, total: %.3f, accuracy: %.3f' %(epoch+1,running_loss_tr,correct,total,accuracy)) \n",
        "\n",
        "\n",
        "  \n",
        "  if running_loss_tr<=0.08:\n",
        "    break\n",
        "print('Finished Training run ')\n",
        "analysis_data_tr = np.array(analysis_data_tr)\n",
        "\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training epoch: [0 ] loss: 1.478 correct: 971.000, total: 3000.000, accuracy: 0.324\n",
            "training epoch: [1 ] loss: 1.403 correct: 969.000, total: 3000.000, accuracy: 0.323\n",
            "training epoch: [2 ] loss: 1.354 correct: 970.000, total: 3000.000, accuracy: 0.323\n",
            "training epoch: [3 ] loss: 1.317 correct: 973.000, total: 3000.000, accuracy: 0.324\n",
            "training epoch: [4 ] loss: 1.287 correct: 971.000, total: 3000.000, accuracy: 0.324\n",
            "training epoch: [5 ] loss: 1.262 correct: 972.000, total: 3000.000, accuracy: 0.324\n",
            "training epoch: [6 ] loss: 1.241 correct: 969.000, total: 3000.000, accuracy: 0.323\n",
            "training epoch: [7 ] loss: 1.223 correct: 970.000, total: 3000.000, accuracy: 0.323\n",
            "training epoch: [8 ] loss: 1.207 correct: 971.000, total: 3000.000, accuracy: 0.324\n",
            "training epoch: [9 ] loss: 1.193 correct: 973.000, total: 3000.000, accuracy: 0.324\n",
            "training epoch: [10 ] loss: 1.181 correct: 982.000, total: 3000.000, accuracy: 0.327\n",
            "training epoch: [11 ] loss: 1.171 correct: 991.000, total: 3000.000, accuracy: 0.330\n",
            "training epoch: [12 ] loss: 1.162 correct: 994.000, total: 3000.000, accuracy: 0.331\n",
            "training epoch: [13 ] loss: 1.154 correct: 1007.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [14 ] loss: 1.147 correct: 1011.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [15 ] loss: 1.140 correct: 1023.000, total: 3000.000, accuracy: 0.341\n",
            "training epoch: [16 ] loss: 1.135 correct: 1028.000, total: 3000.000, accuracy: 0.343\n",
            "training epoch: [17 ] loss: 1.130 correct: 1030.000, total: 3000.000, accuracy: 0.343\n",
            "training epoch: [18 ] loss: 1.126 correct: 1038.000, total: 3000.000, accuracy: 0.346\n",
            "training epoch: [19 ] loss: 1.122 correct: 1042.000, total: 3000.000, accuracy: 0.347\n",
            "training epoch: [20 ] loss: 1.119 correct: 1047.000, total: 3000.000, accuracy: 0.349\n",
            "training epoch: [21 ] loss: 1.116 correct: 1066.000, total: 3000.000, accuracy: 0.355\n",
            "training epoch: [22 ] loss: 1.113 correct: 1083.000, total: 3000.000, accuracy: 0.361\n",
            "training epoch: [23 ] loss: 1.111 correct: 1093.000, total: 3000.000, accuracy: 0.364\n",
            "training epoch: [24 ] loss: 1.109 correct: 1091.000, total: 3000.000, accuracy: 0.364\n",
            "training epoch: [25 ] loss: 1.108 correct: 1095.000, total: 3000.000, accuracy: 0.365\n",
            "training epoch: [26 ] loss: 1.106 correct: 1095.000, total: 3000.000, accuracy: 0.365\n",
            "training epoch: [27 ] loss: 1.105 correct: 1092.000, total: 3000.000, accuracy: 0.364\n",
            "training epoch: [28 ] loss: 1.104 correct: 1103.000, total: 3000.000, accuracy: 0.368\n",
            "training epoch: [29 ] loss: 1.103 correct: 1103.000, total: 3000.000, accuracy: 0.368\n",
            "training epoch: [30 ] loss: 1.102 correct: 1099.000, total: 3000.000, accuracy: 0.366\n",
            "training epoch: [31 ] loss: 1.101 correct: 1098.000, total: 3000.000, accuracy: 0.366\n",
            "training epoch: [32 ] loss: 1.100 correct: 1097.000, total: 3000.000, accuracy: 0.366\n",
            "training epoch: [33 ] loss: 1.100 correct: 1094.000, total: 3000.000, accuracy: 0.365\n",
            "training epoch: [34 ] loss: 1.099 correct: 1095.000, total: 3000.000, accuracy: 0.365\n",
            "training epoch: [35 ] loss: 1.099 correct: 1092.000, total: 3000.000, accuracy: 0.364\n",
            "training epoch: [36 ] loss: 1.098 correct: 1083.000, total: 3000.000, accuracy: 0.361\n",
            "training epoch: [37 ] loss: 1.098 correct: 1092.000, total: 3000.000, accuracy: 0.364\n",
            "training epoch: [38 ] loss: 1.098 correct: 1081.000, total: 3000.000, accuracy: 0.360\n",
            "training epoch: [39 ] loss: 1.097 correct: 1076.000, total: 3000.000, accuracy: 0.359\n",
            "training epoch: [40 ] loss: 1.097 correct: 1082.000, total: 3000.000, accuracy: 0.361\n",
            "training epoch: [41 ] loss: 1.097 correct: 1080.000, total: 3000.000, accuracy: 0.360\n",
            "training epoch: [42 ] loss: 1.097 correct: 1080.000, total: 3000.000, accuracy: 0.360\n",
            "training epoch: [43 ] loss: 1.097 correct: 1079.000, total: 3000.000, accuracy: 0.360\n",
            "training epoch: [44 ] loss: 1.096 correct: 1075.000, total: 3000.000, accuracy: 0.358\n",
            "training epoch: [45 ] loss: 1.096 correct: 1077.000, total: 3000.000, accuracy: 0.359\n",
            "training epoch: [46 ] loss: 1.096 correct: 1074.000, total: 3000.000, accuracy: 0.358\n",
            "training epoch: [47 ] loss: 1.096 correct: 1070.000, total: 3000.000, accuracy: 0.357\n",
            "training epoch: [48 ] loss: 1.096 correct: 1069.000, total: 3000.000, accuracy: 0.356\n",
            "training epoch: [49 ] loss: 1.096 correct: 1072.000, total: 3000.000, accuracy: 0.357\n",
            "training epoch: [50 ] loss: 1.096 correct: 1070.000, total: 3000.000, accuracy: 0.357\n",
            "training epoch: [51 ] loss: 1.096 correct: 1073.000, total: 3000.000, accuracy: 0.358\n",
            "training epoch: [52 ] loss: 1.096 correct: 1076.000, total: 3000.000, accuracy: 0.359\n",
            "training epoch: [53 ] loss: 1.096 correct: 1072.000, total: 3000.000, accuracy: 0.357\n",
            "training epoch: [54 ] loss: 1.095 correct: 1068.000, total: 3000.000, accuracy: 0.356\n",
            "training epoch: [55 ] loss: 1.095 correct: 1068.000, total: 3000.000, accuracy: 0.356\n",
            "training epoch: [56 ] loss: 1.095 correct: 1066.000, total: 3000.000, accuracy: 0.355\n",
            "training epoch: [57 ] loss: 1.095 correct: 1063.000, total: 3000.000, accuracy: 0.354\n",
            "training epoch: [58 ] loss: 1.095 correct: 1057.000, total: 3000.000, accuracy: 0.352\n",
            "training epoch: [59 ] loss: 1.095 correct: 1057.000, total: 3000.000, accuracy: 0.352\n",
            "training epoch: [60 ] loss: 1.095 correct: 1058.000, total: 3000.000, accuracy: 0.353\n",
            "training epoch: [61 ] loss: 1.095 correct: 1061.000, total: 3000.000, accuracy: 0.354\n",
            "training epoch: [62 ] loss: 1.095 correct: 1063.000, total: 3000.000, accuracy: 0.354\n",
            "training epoch: [63 ] loss: 1.095 correct: 1056.000, total: 3000.000, accuracy: 0.352\n",
            "training epoch: [64 ] loss: 1.095 correct: 1054.000, total: 3000.000, accuracy: 0.351\n",
            "training epoch: [65 ] loss: 1.095 correct: 1052.000, total: 3000.000, accuracy: 0.351\n",
            "training epoch: [66 ] loss: 1.095 correct: 1050.000, total: 3000.000, accuracy: 0.350\n",
            "training epoch: [67 ] loss: 1.095 correct: 1049.000, total: 3000.000, accuracy: 0.350\n",
            "training epoch: [68 ] loss: 1.095 correct: 1050.000, total: 3000.000, accuracy: 0.350\n",
            "training epoch: [69 ] loss: 1.095 correct: 1052.000, total: 3000.000, accuracy: 0.351\n",
            "training epoch: [70 ] loss: 1.095 correct: 1052.000, total: 3000.000, accuracy: 0.351\n",
            "training epoch: [71 ] loss: 1.095 correct: 1050.000, total: 3000.000, accuracy: 0.350\n",
            "training epoch: [72 ] loss: 1.095 correct: 1048.000, total: 3000.000, accuracy: 0.349\n",
            "training epoch: [73 ] loss: 1.095 correct: 1049.000, total: 3000.000, accuracy: 0.350\n",
            "training epoch: [74 ] loss: 1.095 correct: 1049.000, total: 3000.000, accuracy: 0.350\n",
            "training epoch: [75 ] loss: 1.095 correct: 1053.000, total: 3000.000, accuracy: 0.351\n",
            "training epoch: [76 ] loss: 1.095 correct: 1057.000, total: 3000.000, accuracy: 0.352\n",
            "training epoch: [77 ] loss: 1.095 correct: 1056.000, total: 3000.000, accuracy: 0.352\n",
            "training epoch: [78 ] loss: 1.095 correct: 1055.000, total: 3000.000, accuracy: 0.352\n",
            "training epoch: [79 ] loss: 1.094 correct: 1055.000, total: 3000.000, accuracy: 0.352\n",
            "training epoch: [80 ] loss: 1.094 correct: 1055.000, total: 3000.000, accuracy: 0.352\n",
            "training epoch: [81 ] loss: 1.094 correct: 1056.000, total: 3000.000, accuracy: 0.352\n",
            "training epoch: [82 ] loss: 1.094 correct: 1056.000, total: 3000.000, accuracy: 0.352\n",
            "training epoch: [83 ] loss: 1.094 correct: 1058.000, total: 3000.000, accuracy: 0.353\n",
            "training epoch: [84 ] loss: 1.094 correct: 1055.000, total: 3000.000, accuracy: 0.352\n",
            "training epoch: [85 ] loss: 1.094 correct: 1056.000, total: 3000.000, accuracy: 0.352\n",
            "training epoch: [86 ] loss: 1.094 correct: 1057.000, total: 3000.000, accuracy: 0.352\n",
            "training epoch: [87 ] loss: 1.094 correct: 1057.000, total: 3000.000, accuracy: 0.352\n",
            "training epoch: [88 ] loss: 1.094 correct: 1060.000, total: 3000.000, accuracy: 0.353\n",
            "training epoch: [89 ] loss: 1.094 correct: 1060.000, total: 3000.000, accuracy: 0.353\n",
            "training epoch: [90 ] loss: 1.094 correct: 1058.000, total: 3000.000, accuracy: 0.353\n",
            "training epoch: [91 ] loss: 1.094 correct: 1059.000, total: 3000.000, accuracy: 0.353\n",
            "training epoch: [92 ] loss: 1.094 correct: 1057.000, total: 3000.000, accuracy: 0.352\n",
            "training epoch: [93 ] loss: 1.094 correct: 1055.000, total: 3000.000, accuracy: 0.352\n",
            "training epoch: [94 ] loss: 1.094 correct: 1054.000, total: 3000.000, accuracy: 0.351\n",
            "training epoch: [95 ] loss: 1.094 correct: 1054.000, total: 3000.000, accuracy: 0.351\n",
            "training epoch: [96 ] loss: 1.094 correct: 1054.000, total: 3000.000, accuracy: 0.351\n",
            "training epoch: [97 ] loss: 1.094 correct: 1055.000, total: 3000.000, accuracy: 0.352\n",
            "training epoch: [98 ] loss: 1.094 correct: 1053.000, total: 3000.000, accuracy: 0.351\n",
            "training epoch: [99 ] loss: 1.094 correct: 1056.000, total: 3000.000, accuracy: 0.352\n",
            "training epoch: [100 ] loss: 1.094 correct: 1055.000, total: 3000.000, accuracy: 0.352\n",
            "training epoch: [101 ] loss: 1.094 correct: 1057.000, total: 3000.000, accuracy: 0.352\n",
            "training epoch: [102 ] loss: 1.094 correct: 1056.000, total: 3000.000, accuracy: 0.352\n",
            "training epoch: [103 ] loss: 1.094 correct: 1058.000, total: 3000.000, accuracy: 0.353\n",
            "training epoch: [104 ] loss: 1.094 correct: 1058.000, total: 3000.000, accuracy: 0.353\n",
            "training epoch: [105 ] loss: 1.094 correct: 1057.000, total: 3000.000, accuracy: 0.352\n",
            "training epoch: [106 ] loss: 1.094 correct: 1058.000, total: 3000.000, accuracy: 0.353\n",
            "training epoch: [107 ] loss: 1.094 correct: 1059.000, total: 3000.000, accuracy: 0.353\n",
            "training epoch: [108 ] loss: 1.094 correct: 1061.000, total: 3000.000, accuracy: 0.354\n",
            "training epoch: [109 ] loss: 1.094 correct: 1063.000, total: 3000.000, accuracy: 0.354\n",
            "training epoch: [110 ] loss: 1.094 correct: 1063.000, total: 3000.000, accuracy: 0.354\n",
            "training epoch: [111 ] loss: 1.094 correct: 1064.000, total: 3000.000, accuracy: 0.355\n",
            "training epoch: [112 ] loss: 1.094 correct: 1064.000, total: 3000.000, accuracy: 0.355\n",
            "training epoch: [113 ] loss: 1.094 correct: 1063.000, total: 3000.000, accuracy: 0.354\n",
            "training epoch: [114 ] loss: 1.093 correct: 1060.000, total: 3000.000, accuracy: 0.353\n",
            "training epoch: [115 ] loss: 1.093 correct: 1059.000, total: 3000.000, accuracy: 0.353\n",
            "training epoch: [116 ] loss: 1.093 correct: 1058.000, total: 3000.000, accuracy: 0.353\n",
            "training epoch: [117 ] loss: 1.093 correct: 1058.000, total: 3000.000, accuracy: 0.353\n",
            "training epoch: [118 ] loss: 1.093 correct: 1057.000, total: 3000.000, accuracy: 0.352\n",
            "training epoch: [119 ] loss: 1.093 correct: 1054.000, total: 3000.000, accuracy: 0.351\n",
            "training epoch: [120 ] loss: 1.093 correct: 1057.000, total: 3000.000, accuracy: 0.352\n",
            "training epoch: [121 ] loss: 1.093 correct: 1058.000, total: 3000.000, accuracy: 0.353\n",
            "training epoch: [122 ] loss: 1.093 correct: 1057.000, total: 3000.000, accuracy: 0.352\n",
            "training epoch: [123 ] loss: 1.093 correct: 1056.000, total: 3000.000, accuracy: 0.352\n",
            "training epoch: [124 ] loss: 1.093 correct: 1055.000, total: 3000.000, accuracy: 0.352\n",
            "training epoch: [125 ] loss: 1.093 correct: 1055.000, total: 3000.000, accuracy: 0.352\n",
            "training epoch: [126 ] loss: 1.093 correct: 1056.000, total: 3000.000, accuracy: 0.352\n",
            "training epoch: [127 ] loss: 1.093 correct: 1057.000, total: 3000.000, accuracy: 0.352\n",
            "training epoch: [128 ] loss: 1.093 correct: 1057.000, total: 3000.000, accuracy: 0.352\n",
            "training epoch: [129 ] loss: 1.093 correct: 1054.000, total: 3000.000, accuracy: 0.351\n",
            "training epoch: [130 ] loss: 1.093 correct: 1051.000, total: 3000.000, accuracy: 0.350\n",
            "training epoch: [131 ] loss: 1.093 correct: 1051.000, total: 3000.000, accuracy: 0.350\n",
            "training epoch: [132 ] loss: 1.093 correct: 1050.000, total: 3000.000, accuracy: 0.350\n",
            "training epoch: [133 ] loss: 1.093 correct: 1048.000, total: 3000.000, accuracy: 0.349\n",
            "training epoch: [134 ] loss: 1.093 correct: 1048.000, total: 3000.000, accuracy: 0.349\n",
            "training epoch: [135 ] loss: 1.093 correct: 1046.000, total: 3000.000, accuracy: 0.349\n",
            "training epoch: [136 ] loss: 1.093 correct: 1044.000, total: 3000.000, accuracy: 0.348\n",
            "training epoch: [137 ] loss: 1.093 correct: 1041.000, total: 3000.000, accuracy: 0.347\n",
            "training epoch: [138 ] loss: 1.093 correct: 1044.000, total: 3000.000, accuracy: 0.348\n",
            "training epoch: [139 ] loss: 1.093 correct: 1043.000, total: 3000.000, accuracy: 0.348\n",
            "training epoch: [140 ] loss: 1.093 correct: 1045.000, total: 3000.000, accuracy: 0.348\n",
            "training epoch: [141 ] loss: 1.093 correct: 1042.000, total: 3000.000, accuracy: 0.347\n",
            "training epoch: [142 ] loss: 1.093 correct: 1041.000, total: 3000.000, accuracy: 0.347\n",
            "training epoch: [143 ] loss: 1.093 correct: 1043.000, total: 3000.000, accuracy: 0.348\n",
            "training epoch: [144 ] loss: 1.093 correct: 1040.000, total: 3000.000, accuracy: 0.347\n",
            "training epoch: [145 ] loss: 1.093 correct: 1039.000, total: 3000.000, accuracy: 0.346\n",
            "training epoch: [146 ] loss: 1.093 correct: 1039.000, total: 3000.000, accuracy: 0.346\n",
            "training epoch: [147 ] loss: 1.093 correct: 1043.000, total: 3000.000, accuracy: 0.348\n",
            "training epoch: [148 ] loss: 1.093 correct: 1043.000, total: 3000.000, accuracy: 0.348\n",
            "training epoch: [149 ] loss: 1.093 correct: 1042.000, total: 3000.000, accuracy: 0.347\n",
            "training epoch: [150 ] loss: 1.092 correct: 1042.000, total: 3000.000, accuracy: 0.347\n",
            "training epoch: [151 ] loss: 1.092 correct: 1043.000, total: 3000.000, accuracy: 0.348\n",
            "training epoch: [152 ] loss: 1.092 correct: 1043.000, total: 3000.000, accuracy: 0.348\n",
            "training epoch: [153 ] loss: 1.092 correct: 1045.000, total: 3000.000, accuracy: 0.348\n",
            "training epoch: [154 ] loss: 1.092 correct: 1049.000, total: 3000.000, accuracy: 0.350\n",
            "training epoch: [155 ] loss: 1.092 correct: 1051.000, total: 3000.000, accuracy: 0.350\n",
            "training epoch: [156 ] loss: 1.092 correct: 1049.000, total: 3000.000, accuracy: 0.350\n",
            "training epoch: [157 ] loss: 1.092 correct: 1049.000, total: 3000.000, accuracy: 0.350\n",
            "training epoch: [158 ] loss: 1.092 correct: 1050.000, total: 3000.000, accuracy: 0.350\n",
            "training epoch: [159 ] loss: 1.092 correct: 1048.000, total: 3000.000, accuracy: 0.349\n",
            "training epoch: [160 ] loss: 1.092 correct: 1046.000, total: 3000.000, accuracy: 0.349\n",
            "training epoch: [161 ] loss: 1.092 correct: 1047.000, total: 3000.000, accuracy: 0.349\n",
            "training epoch: [162 ] loss: 1.092 correct: 1045.000, total: 3000.000, accuracy: 0.348\n",
            "training epoch: [163 ] loss: 1.092 correct: 1044.000, total: 3000.000, accuracy: 0.348\n",
            "training epoch: [164 ] loss: 1.092 correct: 1045.000, total: 3000.000, accuracy: 0.348\n",
            "training epoch: [165 ] loss: 1.092 correct: 1045.000, total: 3000.000, accuracy: 0.348\n",
            "training epoch: [166 ] loss: 1.092 correct: 1047.000, total: 3000.000, accuracy: 0.349\n",
            "training epoch: [167 ] loss: 1.092 correct: 1046.000, total: 3000.000, accuracy: 0.349\n",
            "training epoch: [168 ] loss: 1.092 correct: 1045.000, total: 3000.000, accuracy: 0.348\n",
            "training epoch: [169 ] loss: 1.092 correct: 1045.000, total: 3000.000, accuracy: 0.348\n",
            "training epoch: [170 ] loss: 1.092 correct: 1040.000, total: 3000.000, accuracy: 0.347\n",
            "training epoch: [171 ] loss: 1.092 correct: 1040.000, total: 3000.000, accuracy: 0.347\n",
            "training epoch: [172 ] loss: 1.092 correct: 1043.000, total: 3000.000, accuracy: 0.348\n",
            "training epoch: [173 ] loss: 1.092 correct: 1040.000, total: 3000.000, accuracy: 0.347\n",
            "training epoch: [174 ] loss: 1.092 correct: 1043.000, total: 3000.000, accuracy: 0.348\n",
            "training epoch: [175 ] loss: 1.092 correct: 1043.000, total: 3000.000, accuracy: 0.348\n",
            "training epoch: [176 ] loss: 1.092 correct: 1042.000, total: 3000.000, accuracy: 0.347\n",
            "training epoch: [177 ] loss: 1.092 correct: 1041.000, total: 3000.000, accuracy: 0.347\n",
            "training epoch: [178 ] loss: 1.092 correct: 1038.000, total: 3000.000, accuracy: 0.346\n",
            "training epoch: [179 ] loss: 1.092 correct: 1041.000, total: 3000.000, accuracy: 0.347\n",
            "training epoch: [180 ] loss: 1.092 correct: 1045.000, total: 3000.000, accuracy: 0.348\n",
            "training epoch: [181 ] loss: 1.092 correct: 1042.000, total: 3000.000, accuracy: 0.347\n",
            "training epoch: [182 ] loss: 1.092 correct: 1043.000, total: 3000.000, accuracy: 0.348\n",
            "training epoch: [183 ] loss: 1.092 correct: 1043.000, total: 3000.000, accuracy: 0.348\n",
            "training epoch: [184 ] loss: 1.092 correct: 1041.000, total: 3000.000, accuracy: 0.347\n",
            "training epoch: [185 ] loss: 1.092 correct: 1039.000, total: 3000.000, accuracy: 0.346\n",
            "training epoch: [186 ] loss: 1.092 correct: 1040.000, total: 3000.000, accuracy: 0.347\n",
            "training epoch: [187 ] loss: 1.091 correct: 1044.000, total: 3000.000, accuracy: 0.348\n",
            "training epoch: [188 ] loss: 1.091 correct: 1047.000, total: 3000.000, accuracy: 0.349\n",
            "training epoch: [189 ] loss: 1.091 correct: 1046.000, total: 3000.000, accuracy: 0.349\n",
            "training epoch: [190 ] loss: 1.091 correct: 1047.000, total: 3000.000, accuracy: 0.349\n",
            "training epoch: [191 ] loss: 1.091 correct: 1048.000, total: 3000.000, accuracy: 0.349\n",
            "training epoch: [192 ] loss: 1.091 correct: 1051.000, total: 3000.000, accuracy: 0.350\n",
            "training epoch: [193 ] loss: 1.091 correct: 1049.000, total: 3000.000, accuracy: 0.350\n",
            "training epoch: [194 ] loss: 1.091 correct: 1047.000, total: 3000.000, accuracy: 0.349\n",
            "training epoch: [195 ] loss: 1.091 correct: 1049.000, total: 3000.000, accuracy: 0.350\n",
            "training epoch: [196 ] loss: 1.091 correct: 1047.000, total: 3000.000, accuracy: 0.349\n",
            "training epoch: [197 ] loss: 1.091 correct: 1050.000, total: 3000.000, accuracy: 0.350\n",
            "training epoch: [198 ] loss: 1.091 correct: 1048.000, total: 3000.000, accuracy: 0.349\n",
            "training epoch: [199 ] loss: 1.091 correct: 1044.000, total: 3000.000, accuracy: 0.348\n",
            "training epoch: [200 ] loss: 1.091 correct: 1043.000, total: 3000.000, accuracy: 0.348\n",
            "training epoch: [201 ] loss: 1.091 correct: 1046.000, total: 3000.000, accuracy: 0.349\n",
            "training epoch: [202 ] loss: 1.091 correct: 1045.000, total: 3000.000, accuracy: 0.348\n",
            "training epoch: [203 ] loss: 1.091 correct: 1047.000, total: 3000.000, accuracy: 0.349\n",
            "training epoch: [204 ] loss: 1.091 correct: 1048.000, total: 3000.000, accuracy: 0.349\n",
            "training epoch: [205 ] loss: 1.091 correct: 1052.000, total: 3000.000, accuracy: 0.351\n",
            "training epoch: [206 ] loss: 1.091 correct: 1051.000, total: 3000.000, accuracy: 0.350\n",
            "training epoch: [207 ] loss: 1.091 correct: 1049.000, total: 3000.000, accuracy: 0.350\n",
            "training epoch: [208 ] loss: 1.091 correct: 1049.000, total: 3000.000, accuracy: 0.350\n",
            "training epoch: [209 ] loss: 1.091 correct: 1049.000, total: 3000.000, accuracy: 0.350\n",
            "training epoch: [210 ] loss: 1.091 correct: 1048.000, total: 3000.000, accuracy: 0.349\n",
            "training epoch: [211 ] loss: 1.091 correct: 1048.000, total: 3000.000, accuracy: 0.349\n",
            "training epoch: [212 ] loss: 1.091 correct: 1047.000, total: 3000.000, accuracy: 0.349\n",
            "training epoch: [213 ] loss: 1.091 correct: 1048.000, total: 3000.000, accuracy: 0.349\n",
            "training epoch: [214 ] loss: 1.091 correct: 1048.000, total: 3000.000, accuracy: 0.349\n",
            "training epoch: [215 ] loss: 1.091 correct: 1048.000, total: 3000.000, accuracy: 0.349\n",
            "training epoch: [216 ] loss: 1.091 correct: 1049.000, total: 3000.000, accuracy: 0.350\n",
            "training epoch: [217 ] loss: 1.091 correct: 1053.000, total: 3000.000, accuracy: 0.351\n",
            "training epoch: [218 ] loss: 1.091 correct: 1056.000, total: 3000.000, accuracy: 0.352\n",
            "training epoch: [219 ] loss: 1.091 correct: 1057.000, total: 3000.000, accuracy: 0.352\n",
            "training epoch: [220 ] loss: 1.091 correct: 1059.000, total: 3000.000, accuracy: 0.353\n",
            "training epoch: [221 ] loss: 1.091 correct: 1060.000, total: 3000.000, accuracy: 0.353\n",
            "training epoch: [222 ] loss: 1.091 correct: 1058.000, total: 3000.000, accuracy: 0.353\n",
            "training epoch: [223 ] loss: 1.091 correct: 1059.000, total: 3000.000, accuracy: 0.353\n",
            "training epoch: [224 ] loss: 1.091 correct: 1060.000, total: 3000.000, accuracy: 0.353\n",
            "training epoch: [225 ] loss: 1.091 correct: 1063.000, total: 3000.000, accuracy: 0.354\n",
            "training epoch: [226 ] loss: 1.090 correct: 1060.000, total: 3000.000, accuracy: 0.353\n",
            "training epoch: [227 ] loss: 1.090 correct: 1058.000, total: 3000.000, accuracy: 0.353\n",
            "training epoch: [228 ] loss: 1.090 correct: 1057.000, total: 3000.000, accuracy: 0.352\n",
            "training epoch: [229 ] loss: 1.090 correct: 1057.000, total: 3000.000, accuracy: 0.352\n",
            "training epoch: [230 ] loss: 1.090 correct: 1059.000, total: 3000.000, accuracy: 0.353\n",
            "training epoch: [231 ] loss: 1.090 correct: 1061.000, total: 3000.000, accuracy: 0.354\n",
            "training epoch: [232 ] loss: 1.090 correct: 1062.000, total: 3000.000, accuracy: 0.354\n",
            "training epoch: [233 ] loss: 1.090 correct: 1061.000, total: 3000.000, accuracy: 0.354\n",
            "training epoch: [234 ] loss: 1.090 correct: 1060.000, total: 3000.000, accuracy: 0.353\n",
            "training epoch: [235 ] loss: 1.090 correct: 1060.000, total: 3000.000, accuracy: 0.353\n",
            "training epoch: [236 ] loss: 1.090 correct: 1057.000, total: 3000.000, accuracy: 0.352\n",
            "training epoch: [237 ] loss: 1.090 correct: 1056.000, total: 3000.000, accuracy: 0.352\n",
            "training epoch: [238 ] loss: 1.090 correct: 1058.000, total: 3000.000, accuracy: 0.353\n",
            "training epoch: [239 ] loss: 1.090 correct: 1059.000, total: 3000.000, accuracy: 0.353\n",
            "training epoch: [240 ] loss: 1.090 correct: 1058.000, total: 3000.000, accuracy: 0.353\n",
            "training epoch: [241 ] loss: 1.090 correct: 1058.000, total: 3000.000, accuracy: 0.353\n",
            "training epoch: [242 ] loss: 1.090 correct: 1059.000, total: 3000.000, accuracy: 0.353\n",
            "training epoch: [243 ] loss: 1.090 correct: 1057.000, total: 3000.000, accuracy: 0.352\n",
            "training epoch: [244 ] loss: 1.090 correct: 1060.000, total: 3000.000, accuracy: 0.353\n",
            "training epoch: [245 ] loss: 1.090 correct: 1058.000, total: 3000.000, accuracy: 0.353\n",
            "training epoch: [246 ] loss: 1.090 correct: 1062.000, total: 3000.000, accuracy: 0.354\n",
            "training epoch: [247 ] loss: 1.090 correct: 1058.000, total: 3000.000, accuracy: 0.353\n",
            "training epoch: [248 ] loss: 1.090 correct: 1058.000, total: 3000.000, accuracy: 0.353\n",
            "training epoch: [249 ] loss: 1.090 correct: 1061.000, total: 3000.000, accuracy: 0.354\n",
            "training epoch: [250 ] loss: 1.090 correct: 1061.000, total: 3000.000, accuracy: 0.354\n",
            "training epoch: [251 ] loss: 1.090 correct: 1057.000, total: 3000.000, accuracy: 0.352\n",
            "training epoch: [252 ] loss: 1.090 correct: 1058.000, total: 3000.000, accuracy: 0.353\n",
            "training epoch: [253 ] loss: 1.090 correct: 1057.000, total: 3000.000, accuracy: 0.352\n",
            "training epoch: [254 ] loss: 1.090 correct: 1057.000, total: 3000.000, accuracy: 0.352\n",
            "training epoch: [255 ] loss: 1.090 correct: 1057.000, total: 3000.000, accuracy: 0.352\n",
            "training epoch: [256 ] loss: 1.090 correct: 1054.000, total: 3000.000, accuracy: 0.351\n",
            "training epoch: [257 ] loss: 1.090 correct: 1058.000, total: 3000.000, accuracy: 0.353\n",
            "training epoch: [258 ] loss: 1.090 correct: 1061.000, total: 3000.000, accuracy: 0.354\n",
            "training epoch: [259 ] loss: 1.090 correct: 1061.000, total: 3000.000, accuracy: 0.354\n",
            "training epoch: [260 ] loss: 1.090 correct: 1060.000, total: 3000.000, accuracy: 0.353\n",
            "training epoch: [261 ] loss: 1.090 correct: 1060.000, total: 3000.000, accuracy: 0.353\n",
            "training epoch: [262 ] loss: 1.090 correct: 1061.000, total: 3000.000, accuracy: 0.354\n",
            "training epoch: [263 ] loss: 1.090 correct: 1063.000, total: 3000.000, accuracy: 0.354\n",
            "training epoch: [264 ] loss: 1.089 correct: 1061.000, total: 3000.000, accuracy: 0.354\n",
            "training epoch: [265 ] loss: 1.089 correct: 1064.000, total: 3000.000, accuracy: 0.355\n",
            "training epoch: [266 ] loss: 1.089 correct: 1068.000, total: 3000.000, accuracy: 0.356\n",
            "training epoch: [267 ] loss: 1.089 correct: 1071.000, total: 3000.000, accuracy: 0.357\n",
            "training epoch: [268 ] loss: 1.089 correct: 1074.000, total: 3000.000, accuracy: 0.358\n",
            "training epoch: [269 ] loss: 1.089 correct: 1074.000, total: 3000.000, accuracy: 0.358\n",
            "training epoch: [270 ] loss: 1.089 correct: 1076.000, total: 3000.000, accuracy: 0.359\n",
            "training epoch: [271 ] loss: 1.089 correct: 1081.000, total: 3000.000, accuracy: 0.360\n",
            "training epoch: [272 ] loss: 1.089 correct: 1081.000, total: 3000.000, accuracy: 0.360\n",
            "training epoch: [273 ] loss: 1.089 correct: 1083.000, total: 3000.000, accuracy: 0.361\n",
            "training epoch: [274 ] loss: 1.089 correct: 1087.000, total: 3000.000, accuracy: 0.362\n",
            "training epoch: [275 ] loss: 1.089 correct: 1088.000, total: 3000.000, accuracy: 0.363\n",
            "training epoch: [276 ] loss: 1.089 correct: 1088.000, total: 3000.000, accuracy: 0.363\n",
            "training epoch: [277 ] loss: 1.089 correct: 1090.000, total: 3000.000, accuracy: 0.363\n",
            "training epoch: [278 ] loss: 1.089 correct: 1088.000, total: 3000.000, accuracy: 0.363\n",
            "training epoch: [279 ] loss: 1.089 correct: 1093.000, total: 3000.000, accuracy: 0.364\n",
            "training epoch: [280 ] loss: 1.089 correct: 1091.000, total: 3000.000, accuracy: 0.364\n",
            "training epoch: [281 ] loss: 1.089 correct: 1092.000, total: 3000.000, accuracy: 0.364\n",
            "training epoch: [282 ] loss: 1.089 correct: 1088.000, total: 3000.000, accuracy: 0.363\n",
            "training epoch: [283 ] loss: 1.089 correct: 1087.000, total: 3000.000, accuracy: 0.362\n",
            "training epoch: [284 ] loss: 1.089 correct: 1089.000, total: 3000.000, accuracy: 0.363\n",
            "training epoch: [285 ] loss: 1.089 correct: 1089.000, total: 3000.000, accuracy: 0.363\n",
            "training epoch: [286 ] loss: 1.089 correct: 1089.000, total: 3000.000, accuracy: 0.363\n",
            "training epoch: [287 ] loss: 1.089 correct: 1094.000, total: 3000.000, accuracy: 0.365\n",
            "training epoch: [288 ] loss: 1.089 correct: 1096.000, total: 3000.000, accuracy: 0.365\n",
            "training epoch: [289 ] loss: 1.089 correct: 1095.000, total: 3000.000, accuracy: 0.365\n",
            "training epoch: [290 ] loss: 1.089 correct: 1100.000, total: 3000.000, accuracy: 0.367\n",
            "training epoch: [291 ] loss: 1.089 correct: 1101.000, total: 3000.000, accuracy: 0.367\n",
            "training epoch: [292 ] loss: 1.089 correct: 1099.000, total: 3000.000, accuracy: 0.366\n",
            "training epoch: [293 ] loss: 1.089 correct: 1105.000, total: 3000.000, accuracy: 0.368\n",
            "training epoch: [294 ] loss: 1.089 correct: 1106.000, total: 3000.000, accuracy: 0.369\n",
            "training epoch: [295 ] loss: 1.088 correct: 1113.000, total: 3000.000, accuracy: 0.371\n",
            "training epoch: [296 ] loss: 1.088 correct: 1116.000, total: 3000.000, accuracy: 0.372\n",
            "training epoch: [297 ] loss: 1.088 correct: 1117.000, total: 3000.000, accuracy: 0.372\n",
            "training epoch: [298 ] loss: 1.088 correct: 1117.000, total: 3000.000, accuracy: 0.372\n",
            "training epoch: [299 ] loss: 1.088 correct: 1118.000, total: 3000.000, accuracy: 0.373\n",
            "training epoch: [300 ] loss: 1.088 correct: 1114.000, total: 3000.000, accuracy: 0.371\n",
            "training epoch: [301 ] loss: 1.088 correct: 1119.000, total: 3000.000, accuracy: 0.373\n",
            "training epoch: [302 ] loss: 1.088 correct: 1121.000, total: 3000.000, accuracy: 0.374\n",
            "training epoch: [303 ] loss: 1.088 correct: 1120.000, total: 3000.000, accuracy: 0.373\n",
            "training epoch: [304 ] loss: 1.088 correct: 1122.000, total: 3000.000, accuracy: 0.374\n",
            "training epoch: [305 ] loss: 1.088 correct: 1127.000, total: 3000.000, accuracy: 0.376\n",
            "training epoch: [306 ] loss: 1.088 correct: 1126.000, total: 3000.000, accuracy: 0.375\n",
            "training epoch: [307 ] loss: 1.088 correct: 1126.000, total: 3000.000, accuracy: 0.375\n",
            "training epoch: [308 ] loss: 1.088 correct: 1129.000, total: 3000.000, accuracy: 0.376\n",
            "training epoch: [309 ] loss: 1.088 correct: 1136.000, total: 3000.000, accuracy: 0.379\n",
            "training epoch: [310 ] loss: 1.088 correct: 1139.000, total: 3000.000, accuracy: 0.380\n",
            "training epoch: [311 ] loss: 1.088 correct: 1141.000, total: 3000.000, accuracy: 0.380\n",
            "training epoch: [312 ] loss: 1.088 correct: 1141.000, total: 3000.000, accuracy: 0.380\n",
            "training epoch: [313 ] loss: 1.088 correct: 1143.000, total: 3000.000, accuracy: 0.381\n",
            "training epoch: [314 ] loss: 1.088 correct: 1145.000, total: 3000.000, accuracy: 0.382\n",
            "training epoch: [315 ] loss: 1.088 correct: 1144.000, total: 3000.000, accuracy: 0.381\n",
            "training epoch: [316 ] loss: 1.088 correct: 1145.000, total: 3000.000, accuracy: 0.382\n",
            "training epoch: [317 ] loss: 1.088 correct: 1145.000, total: 3000.000, accuracy: 0.382\n",
            "training epoch: [318 ] loss: 1.088 correct: 1144.000, total: 3000.000, accuracy: 0.381\n",
            "training epoch: [319 ] loss: 1.088 correct: 1145.000, total: 3000.000, accuracy: 0.382\n",
            "training epoch: [320 ] loss: 1.087 correct: 1150.000, total: 3000.000, accuracy: 0.383\n",
            "training epoch: [321 ] loss: 1.087 correct: 1150.000, total: 3000.000, accuracy: 0.383\n",
            "training epoch: [322 ] loss: 1.087 correct: 1147.000, total: 3000.000, accuracy: 0.382\n",
            "training epoch: [323 ] loss: 1.087 correct: 1147.000, total: 3000.000, accuracy: 0.382\n",
            "training epoch: [324 ] loss: 1.087 correct: 1145.000, total: 3000.000, accuracy: 0.382\n",
            "training epoch: [325 ] loss: 1.087 correct: 1146.000, total: 3000.000, accuracy: 0.382\n",
            "training epoch: [326 ] loss: 1.087 correct: 1144.000, total: 3000.000, accuracy: 0.381\n",
            "training epoch: [327 ] loss: 1.087 correct: 1145.000, total: 3000.000, accuracy: 0.382\n",
            "training epoch: [328 ] loss: 1.087 correct: 1147.000, total: 3000.000, accuracy: 0.382\n",
            "training epoch: [329 ] loss: 1.087 correct: 1151.000, total: 3000.000, accuracy: 0.384\n",
            "training epoch: [330 ] loss: 1.087 correct: 1151.000, total: 3000.000, accuracy: 0.384\n",
            "training epoch: [331 ] loss: 1.087 correct: 1151.000, total: 3000.000, accuracy: 0.384\n",
            "training epoch: [332 ] loss: 1.087 correct: 1148.000, total: 3000.000, accuracy: 0.383\n",
            "training epoch: [333 ] loss: 1.087 correct: 1144.000, total: 3000.000, accuracy: 0.381\n",
            "training epoch: [334 ] loss: 1.087 correct: 1147.000, total: 3000.000, accuracy: 0.382\n",
            "training epoch: [335 ] loss: 1.087 correct: 1148.000, total: 3000.000, accuracy: 0.383\n",
            "training epoch: [336 ] loss: 1.087 correct: 1148.000, total: 3000.000, accuracy: 0.383\n",
            "training epoch: [337 ] loss: 1.087 correct: 1148.000, total: 3000.000, accuracy: 0.383\n",
            "training epoch: [338 ] loss: 1.087 correct: 1151.000, total: 3000.000, accuracy: 0.384\n",
            "training epoch: [339 ] loss: 1.087 correct: 1151.000, total: 3000.000, accuracy: 0.384\n",
            "training epoch: [340 ] loss: 1.087 correct: 1147.000, total: 3000.000, accuracy: 0.382\n",
            "training epoch: [341 ] loss: 1.087 correct: 1146.000, total: 3000.000, accuracy: 0.382\n",
            "training epoch: [342 ] loss: 1.087 correct: 1142.000, total: 3000.000, accuracy: 0.381\n",
            "training epoch: [343 ] loss: 1.086 correct: 1140.000, total: 3000.000, accuracy: 0.380\n",
            "training epoch: [344 ] loss: 1.086 correct: 1137.000, total: 3000.000, accuracy: 0.379\n",
            "training epoch: [345 ] loss: 1.086 correct: 1136.000, total: 3000.000, accuracy: 0.379\n",
            "training epoch: [346 ] loss: 1.086 correct: 1134.000, total: 3000.000, accuracy: 0.378\n",
            "training epoch: [347 ] loss: 1.086 correct: 1136.000, total: 3000.000, accuracy: 0.379\n",
            "training epoch: [348 ] loss: 1.086 correct: 1135.000, total: 3000.000, accuracy: 0.378\n",
            "training epoch: [349 ] loss: 1.086 correct: 1139.000, total: 3000.000, accuracy: 0.380\n",
            "training epoch: [350 ] loss: 1.086 correct: 1139.000, total: 3000.000, accuracy: 0.380\n",
            "training epoch: [351 ] loss: 1.086 correct: 1140.000, total: 3000.000, accuracy: 0.380\n",
            "training epoch: [352 ] loss: 1.086 correct: 1139.000, total: 3000.000, accuracy: 0.380\n",
            "training epoch: [353 ] loss: 1.086 correct: 1144.000, total: 3000.000, accuracy: 0.381\n",
            "training epoch: [354 ] loss: 1.086 correct: 1144.000, total: 3000.000, accuracy: 0.381\n",
            "training epoch: [355 ] loss: 1.086 correct: 1143.000, total: 3000.000, accuracy: 0.381\n",
            "training epoch: [356 ] loss: 1.086 correct: 1142.000, total: 3000.000, accuracy: 0.381\n",
            "training epoch: [357 ] loss: 1.086 correct: 1142.000, total: 3000.000, accuracy: 0.381\n",
            "training epoch: [358 ] loss: 1.086 correct: 1142.000, total: 3000.000, accuracy: 0.381\n",
            "training epoch: [359 ] loss: 1.086 correct: 1144.000, total: 3000.000, accuracy: 0.381\n",
            "training epoch: [360 ] loss: 1.086 correct: 1142.000, total: 3000.000, accuracy: 0.381\n",
            "training epoch: [361 ] loss: 1.086 correct: 1141.000, total: 3000.000, accuracy: 0.380\n",
            "training epoch: [362 ] loss: 1.086 correct: 1142.000, total: 3000.000, accuracy: 0.381\n",
            "training epoch: [363 ] loss: 1.086 correct: 1141.000, total: 3000.000, accuracy: 0.380\n",
            "training epoch: [364 ] loss: 1.086 correct: 1143.000, total: 3000.000, accuracy: 0.381\n",
            "training epoch: [365 ] loss: 1.086 correct: 1143.000, total: 3000.000, accuracy: 0.381\n",
            "training epoch: [366 ] loss: 1.085 correct: 1144.000, total: 3000.000, accuracy: 0.381\n",
            "training epoch: [367 ] loss: 1.085 correct: 1143.000, total: 3000.000, accuracy: 0.381\n",
            "training epoch: [368 ] loss: 1.085 correct: 1143.000, total: 3000.000, accuracy: 0.381\n",
            "training epoch: [369 ] loss: 1.085 correct: 1146.000, total: 3000.000, accuracy: 0.382\n",
            "training epoch: [370 ] loss: 1.085 correct: 1151.000, total: 3000.000, accuracy: 0.384\n",
            "training epoch: [371 ] loss: 1.085 correct: 1152.000, total: 3000.000, accuracy: 0.384\n",
            "training epoch: [372 ] loss: 1.085 correct: 1152.000, total: 3000.000, accuracy: 0.384\n",
            "training epoch: [373 ] loss: 1.085 correct: 1155.000, total: 3000.000, accuracy: 0.385\n",
            "training epoch: [374 ] loss: 1.085 correct: 1154.000, total: 3000.000, accuracy: 0.385\n",
            "training epoch: [375 ] loss: 1.085 correct: 1155.000, total: 3000.000, accuracy: 0.385\n",
            "training epoch: [376 ] loss: 1.085 correct: 1158.000, total: 3000.000, accuracy: 0.386\n",
            "training epoch: [377 ] loss: 1.085 correct: 1160.000, total: 3000.000, accuracy: 0.387\n",
            "training epoch: [378 ] loss: 1.085 correct: 1159.000, total: 3000.000, accuracy: 0.386\n",
            "training epoch: [379 ] loss: 1.085 correct: 1157.000, total: 3000.000, accuracy: 0.386\n",
            "training epoch: [380 ] loss: 1.085 correct: 1158.000, total: 3000.000, accuracy: 0.386\n",
            "training epoch: [381 ] loss: 1.085 correct: 1160.000, total: 3000.000, accuracy: 0.387\n",
            "training epoch: [382 ] loss: 1.085 correct: 1158.000, total: 3000.000, accuracy: 0.386\n",
            "training epoch: [383 ] loss: 1.085 correct: 1158.000, total: 3000.000, accuracy: 0.386\n",
            "training epoch: [384 ] loss: 1.085 correct: 1162.000, total: 3000.000, accuracy: 0.387\n",
            "training epoch: [385 ] loss: 1.085 correct: 1159.000, total: 3000.000, accuracy: 0.386\n",
            "training epoch: [386 ] loss: 1.085 correct: 1156.000, total: 3000.000, accuracy: 0.385\n",
            "training epoch: [387 ] loss: 1.084 correct: 1156.000, total: 3000.000, accuracy: 0.385\n",
            "training epoch: [388 ] loss: 1.084 correct: 1158.000, total: 3000.000, accuracy: 0.386\n",
            "training epoch: [389 ] loss: 1.084 correct: 1157.000, total: 3000.000, accuracy: 0.386\n",
            "training epoch: [390 ] loss: 1.084 correct: 1157.000, total: 3000.000, accuracy: 0.386\n",
            "training epoch: [391 ] loss: 1.084 correct: 1158.000, total: 3000.000, accuracy: 0.386\n",
            "training epoch: [392 ] loss: 1.084 correct: 1158.000, total: 3000.000, accuracy: 0.386\n",
            "training epoch: [393 ] loss: 1.084 correct: 1159.000, total: 3000.000, accuracy: 0.386\n",
            "training epoch: [394 ] loss: 1.084 correct: 1158.000, total: 3000.000, accuracy: 0.386\n",
            "training epoch: [395 ] loss: 1.084 correct: 1158.000, total: 3000.000, accuracy: 0.386\n",
            "training epoch: [396 ] loss: 1.084 correct: 1160.000, total: 3000.000, accuracy: 0.387\n",
            "training epoch: [397 ] loss: 1.084 correct: 1161.000, total: 3000.000, accuracy: 0.387\n",
            "training epoch: [398 ] loss: 1.084 correct: 1161.000, total: 3000.000, accuracy: 0.387\n",
            "training epoch: [399 ] loss: 1.084 correct: 1161.000, total: 3000.000, accuracy: 0.387\n",
            "training epoch: [400 ] loss: 1.084 correct: 1162.000, total: 3000.000, accuracy: 0.387\n",
            "training epoch: [401 ] loss: 1.084 correct: 1160.000, total: 3000.000, accuracy: 0.387\n",
            "training epoch: [402 ] loss: 1.084 correct: 1161.000, total: 3000.000, accuracy: 0.387\n",
            "training epoch: [403 ] loss: 1.084 correct: 1162.000, total: 3000.000, accuracy: 0.387\n",
            "training epoch: [404 ] loss: 1.084 correct: 1160.000, total: 3000.000, accuracy: 0.387\n",
            "training epoch: [405 ] loss: 1.083 correct: 1162.000, total: 3000.000, accuracy: 0.387\n",
            "training epoch: [406 ] loss: 1.083 correct: 1162.000, total: 3000.000, accuracy: 0.387\n",
            "training epoch: [407 ] loss: 1.083 correct: 1162.000, total: 3000.000, accuracy: 0.387\n",
            "training epoch: [408 ] loss: 1.083 correct: 1164.000, total: 3000.000, accuracy: 0.388\n",
            "training epoch: [409 ] loss: 1.083 correct: 1166.000, total: 3000.000, accuracy: 0.389\n",
            "training epoch: [410 ] loss: 1.083 correct: 1164.000, total: 3000.000, accuracy: 0.388\n",
            "training epoch: [411 ] loss: 1.083 correct: 1166.000, total: 3000.000, accuracy: 0.389\n",
            "training epoch: [412 ] loss: 1.083 correct: 1168.000, total: 3000.000, accuracy: 0.389\n",
            "training epoch: [413 ] loss: 1.083 correct: 1169.000, total: 3000.000, accuracy: 0.390\n",
            "training epoch: [414 ] loss: 1.083 correct: 1170.000, total: 3000.000, accuracy: 0.390\n",
            "training epoch: [415 ] loss: 1.083 correct: 1171.000, total: 3000.000, accuracy: 0.390\n",
            "training epoch: [416 ] loss: 1.083 correct: 1170.000, total: 3000.000, accuracy: 0.390\n",
            "training epoch: [417 ] loss: 1.083 correct: 1167.000, total: 3000.000, accuracy: 0.389\n",
            "training epoch: [418 ] loss: 1.083 correct: 1169.000, total: 3000.000, accuracy: 0.390\n",
            "training epoch: [419 ] loss: 1.083 correct: 1168.000, total: 3000.000, accuracy: 0.389\n",
            "training epoch: [420 ] loss: 1.083 correct: 1169.000, total: 3000.000, accuracy: 0.390\n",
            "training epoch: [421 ] loss: 1.083 correct: 1169.000, total: 3000.000, accuracy: 0.390\n",
            "training epoch: [422 ] loss: 1.082 correct: 1170.000, total: 3000.000, accuracy: 0.390\n",
            "training epoch: [423 ] loss: 1.082 correct: 1172.000, total: 3000.000, accuracy: 0.391\n",
            "training epoch: [424 ] loss: 1.082 correct: 1173.000, total: 3000.000, accuracy: 0.391\n",
            "training epoch: [425 ] loss: 1.082 correct: 1174.000, total: 3000.000, accuracy: 0.391\n",
            "training epoch: [426 ] loss: 1.082 correct: 1173.000, total: 3000.000, accuracy: 0.391\n",
            "training epoch: [427 ] loss: 1.082 correct: 1176.000, total: 3000.000, accuracy: 0.392\n",
            "training epoch: [428 ] loss: 1.082 correct: 1177.000, total: 3000.000, accuracy: 0.392\n",
            "training epoch: [429 ] loss: 1.082 correct: 1177.000, total: 3000.000, accuracy: 0.392\n",
            "training epoch: [430 ] loss: 1.082 correct: 1179.000, total: 3000.000, accuracy: 0.393\n",
            "training epoch: [431 ] loss: 1.082 correct: 1177.000, total: 3000.000, accuracy: 0.392\n",
            "training epoch: [432 ] loss: 1.082 correct: 1175.000, total: 3000.000, accuracy: 0.392\n",
            "training epoch: [433 ] loss: 1.082 correct: 1176.000, total: 3000.000, accuracy: 0.392\n",
            "training epoch: [434 ] loss: 1.082 correct: 1177.000, total: 3000.000, accuracy: 0.392\n",
            "training epoch: [435 ] loss: 1.082 correct: 1177.000, total: 3000.000, accuracy: 0.392\n",
            "training epoch: [436 ] loss: 1.082 correct: 1175.000, total: 3000.000, accuracy: 0.392\n",
            "training epoch: [437 ] loss: 1.082 correct: 1177.000, total: 3000.000, accuracy: 0.392\n",
            "training epoch: [438 ] loss: 1.081 correct: 1177.000, total: 3000.000, accuracy: 0.392\n",
            "training epoch: [439 ] loss: 1.081 correct: 1178.000, total: 3000.000, accuracy: 0.393\n",
            "training epoch: [440 ] loss: 1.081 correct: 1177.000, total: 3000.000, accuracy: 0.392\n",
            "training epoch: [441 ] loss: 1.081 correct: 1177.000, total: 3000.000, accuracy: 0.392\n",
            "training epoch: [442 ] loss: 1.081 correct: 1177.000, total: 3000.000, accuracy: 0.392\n",
            "training epoch: [443 ] loss: 1.081 correct: 1178.000, total: 3000.000, accuracy: 0.393\n",
            "training epoch: [444 ] loss: 1.081 correct: 1180.000, total: 3000.000, accuracy: 0.393\n",
            "training epoch: [445 ] loss: 1.081 correct: 1181.000, total: 3000.000, accuracy: 0.394\n",
            "training epoch: [446 ] loss: 1.081 correct: 1181.000, total: 3000.000, accuracy: 0.394\n",
            "training epoch: [447 ] loss: 1.081 correct: 1181.000, total: 3000.000, accuracy: 0.394\n",
            "training epoch: [448 ] loss: 1.081 correct: 1181.000, total: 3000.000, accuracy: 0.394\n",
            "training epoch: [449 ] loss: 1.081 correct: 1184.000, total: 3000.000, accuracy: 0.395\n",
            "training epoch: [450 ] loss: 1.081 correct: 1185.000, total: 3000.000, accuracy: 0.395\n",
            "training epoch: [451 ] loss: 1.081 correct: 1187.000, total: 3000.000, accuracy: 0.396\n",
            "training epoch: [452 ] loss: 1.080 correct: 1188.000, total: 3000.000, accuracy: 0.396\n",
            "training epoch: [453 ] loss: 1.080 correct: 1188.000, total: 3000.000, accuracy: 0.396\n",
            "training epoch: [454 ] loss: 1.080 correct: 1191.000, total: 3000.000, accuracy: 0.397\n",
            "training epoch: [455 ] loss: 1.080 correct: 1192.000, total: 3000.000, accuracy: 0.397\n",
            "training epoch: [456 ] loss: 1.080 correct: 1194.000, total: 3000.000, accuracy: 0.398\n",
            "training epoch: [457 ] loss: 1.080 correct: 1195.000, total: 3000.000, accuracy: 0.398\n",
            "training epoch: [458 ] loss: 1.080 correct: 1198.000, total: 3000.000, accuracy: 0.399\n",
            "training epoch: [459 ] loss: 1.080 correct: 1200.000, total: 3000.000, accuracy: 0.400\n",
            "training epoch: [460 ] loss: 1.080 correct: 1200.000, total: 3000.000, accuracy: 0.400\n",
            "training epoch: [461 ] loss: 1.080 correct: 1202.000, total: 3000.000, accuracy: 0.401\n",
            "training epoch: [462 ] loss: 1.080 correct: 1204.000, total: 3000.000, accuracy: 0.401\n",
            "training epoch: [463 ] loss: 1.080 correct: 1204.000, total: 3000.000, accuracy: 0.401\n",
            "training epoch: [464 ] loss: 1.080 correct: 1205.000, total: 3000.000, accuracy: 0.402\n",
            "training epoch: [465 ] loss: 1.080 correct: 1205.000, total: 3000.000, accuracy: 0.402\n",
            "training epoch: [466 ] loss: 1.079 correct: 1205.000, total: 3000.000, accuracy: 0.402\n",
            "training epoch: [467 ] loss: 1.079 correct: 1205.000, total: 3000.000, accuracy: 0.402\n",
            "training epoch: [468 ] loss: 1.079 correct: 1204.000, total: 3000.000, accuracy: 0.401\n",
            "training epoch: [469 ] loss: 1.079 correct: 1206.000, total: 3000.000, accuracy: 0.402\n",
            "training epoch: [470 ] loss: 1.079 correct: 1206.000, total: 3000.000, accuracy: 0.402\n",
            "training epoch: [471 ] loss: 1.079 correct: 1205.000, total: 3000.000, accuracy: 0.402\n",
            "training epoch: [472 ] loss: 1.079 correct: 1207.000, total: 3000.000, accuracy: 0.402\n",
            "training epoch: [473 ] loss: 1.079 correct: 1209.000, total: 3000.000, accuracy: 0.403\n",
            "training epoch: [474 ] loss: 1.079 correct: 1209.000, total: 3000.000, accuracy: 0.403\n",
            "training epoch: [475 ] loss: 1.079 correct: 1209.000, total: 3000.000, accuracy: 0.403\n",
            "training epoch: [476 ] loss: 1.079 correct: 1211.000, total: 3000.000, accuracy: 0.404\n",
            "training epoch: [477 ] loss: 1.079 correct: 1212.000, total: 3000.000, accuracy: 0.404\n",
            "training epoch: [478 ] loss: 1.079 correct: 1213.000, total: 3000.000, accuracy: 0.404\n",
            "training epoch: [479 ] loss: 1.079 correct: 1214.000, total: 3000.000, accuracy: 0.405\n",
            "training epoch: [480 ] loss: 1.078 correct: 1216.000, total: 3000.000, accuracy: 0.405\n",
            "training epoch: [481 ] loss: 1.078 correct: 1216.000, total: 3000.000, accuracy: 0.405\n",
            "training epoch: [482 ] loss: 1.078 correct: 1220.000, total: 3000.000, accuracy: 0.407\n",
            "training epoch: [483 ] loss: 1.078 correct: 1220.000, total: 3000.000, accuracy: 0.407\n",
            "training epoch: [484 ] loss: 1.078 correct: 1221.000, total: 3000.000, accuracy: 0.407\n",
            "training epoch: [485 ] loss: 1.078 correct: 1220.000, total: 3000.000, accuracy: 0.407\n",
            "training epoch: [486 ] loss: 1.078 correct: 1217.000, total: 3000.000, accuracy: 0.406\n",
            "training epoch: [487 ] loss: 1.078 correct: 1215.000, total: 3000.000, accuracy: 0.405\n",
            "training epoch: [488 ] loss: 1.078 correct: 1220.000, total: 3000.000, accuracy: 0.407\n",
            "training epoch: [489 ] loss: 1.078 correct: 1216.000, total: 3000.000, accuracy: 0.405\n",
            "training epoch: [490 ] loss: 1.078 correct: 1222.000, total: 3000.000, accuracy: 0.407\n",
            "training epoch: [491 ] loss: 1.078 correct: 1217.000, total: 3000.000, accuracy: 0.406\n",
            "training epoch: [492 ] loss: 1.078 correct: 1222.000, total: 3000.000, accuracy: 0.407\n",
            "training epoch: [493 ] loss: 1.078 correct: 1218.000, total: 3000.000, accuracy: 0.406\n",
            "training epoch: [494 ] loss: 1.077 correct: 1227.000, total: 3000.000, accuracy: 0.409\n",
            "training epoch: [495 ] loss: 1.077 correct: 1223.000, total: 3000.000, accuracy: 0.408\n",
            "training epoch: [496 ] loss: 1.077 correct: 1227.000, total: 3000.000, accuracy: 0.409\n",
            "training epoch: [497 ] loss: 1.077 correct: 1226.000, total: 3000.000, accuracy: 0.409\n",
            "training epoch: [498 ] loss: 1.077 correct: 1224.000, total: 3000.000, accuracy: 0.408\n",
            "training epoch: [499 ] loss: 1.077 correct: 1228.000, total: 3000.000, accuracy: 0.409\n",
            "training epoch: [500 ] loss: 1.077 correct: 1222.000, total: 3000.000, accuracy: 0.407\n",
            "training epoch: [501 ] loss: 1.077 correct: 1227.000, total: 3000.000, accuracy: 0.409\n",
            "training epoch: [502 ] loss: 1.077 correct: 1223.000, total: 3000.000, accuracy: 0.408\n",
            "training epoch: [503 ] loss: 1.077 correct: 1225.000, total: 3000.000, accuracy: 0.408\n",
            "training epoch: [504 ] loss: 1.077 correct: 1229.000, total: 3000.000, accuracy: 0.410\n",
            "training epoch: [505 ] loss: 1.077 correct: 1222.000, total: 3000.000, accuracy: 0.407\n",
            "training epoch: [506 ] loss: 1.077 correct: 1232.000, total: 3000.000, accuracy: 0.411\n",
            "training epoch: [507 ] loss: 1.077 correct: 1227.000, total: 3000.000, accuracy: 0.409\n",
            "training epoch: [508 ] loss: 1.077 correct: 1231.000, total: 3000.000, accuracy: 0.410\n",
            "training epoch: [509 ] loss: 1.077 correct: 1232.000, total: 3000.000, accuracy: 0.411\n",
            "training epoch: [510 ] loss: 1.076 correct: 1235.000, total: 3000.000, accuracy: 0.412\n",
            "training epoch: [511 ] loss: 1.076 correct: 1228.000, total: 3000.000, accuracy: 0.409\n",
            "training epoch: [512 ] loss: 1.076 correct: 1242.000, total: 3000.000, accuracy: 0.414\n",
            "training epoch: [513 ] loss: 1.076 correct: 1225.000, total: 3000.000, accuracy: 0.408\n",
            "training epoch: [514 ] loss: 1.076 correct: 1236.000, total: 3000.000, accuracy: 0.412\n",
            "training epoch: [515 ] loss: 1.076 correct: 1222.000, total: 3000.000, accuracy: 0.407\n",
            "training epoch: [516 ] loss: 1.076 correct: 1238.000, total: 3000.000, accuracy: 0.413\n",
            "training epoch: [517 ] loss: 1.076 correct: 1228.000, total: 3000.000, accuracy: 0.409\n",
            "training epoch: [518 ] loss: 1.076 correct: 1243.000, total: 3000.000, accuracy: 0.414\n",
            "training epoch: [519 ] loss: 1.076 correct: 1235.000, total: 3000.000, accuracy: 0.412\n",
            "training epoch: [520 ] loss: 1.076 correct: 1245.000, total: 3000.000, accuracy: 0.415\n",
            "training epoch: [521 ] loss: 1.076 correct: 1233.000, total: 3000.000, accuracy: 0.411\n",
            "training epoch: [522 ] loss: 1.076 correct: 1244.000, total: 3000.000, accuracy: 0.415\n",
            "training epoch: [523 ] loss: 1.076 correct: 1229.000, total: 3000.000, accuracy: 0.410\n",
            "training epoch: [524 ] loss: 1.076 correct: 1247.000, total: 3000.000, accuracy: 0.416\n",
            "training epoch: [525 ] loss: 1.076 correct: 1231.000, total: 3000.000, accuracy: 0.410\n",
            "training epoch: [526 ] loss: 1.076 correct: 1244.000, total: 3000.000, accuracy: 0.415\n",
            "training epoch: [527 ] loss: 1.076 correct: 1234.000, total: 3000.000, accuracy: 0.411\n",
            "training epoch: [528 ] loss: 1.076 correct: 1244.000, total: 3000.000, accuracy: 0.415\n",
            "training epoch: [529 ] loss: 1.075 correct: 1234.000, total: 3000.000, accuracy: 0.411\n",
            "training epoch: [530 ] loss: 1.075 correct: 1245.000, total: 3000.000, accuracy: 0.415\n",
            "training epoch: [531 ] loss: 1.075 correct: 1236.000, total: 3000.000, accuracy: 0.412\n",
            "training epoch: [532 ] loss: 1.075 correct: 1246.000, total: 3000.000, accuracy: 0.415\n",
            "training epoch: [533 ] loss: 1.075 correct: 1235.000, total: 3000.000, accuracy: 0.412\n",
            "training epoch: [534 ] loss: 1.075 correct: 1247.000, total: 3000.000, accuracy: 0.416\n",
            "training epoch: [535 ] loss: 1.075 correct: 1234.000, total: 3000.000, accuracy: 0.411\n",
            "training epoch: [536 ] loss: 1.075 correct: 1250.000, total: 3000.000, accuracy: 0.417\n",
            "training epoch: [537 ] loss: 1.075 correct: 1239.000, total: 3000.000, accuracy: 0.413\n",
            "training epoch: [538 ] loss: 1.075 correct: 1251.000, total: 3000.000, accuracy: 0.417\n",
            "training epoch: [539 ] loss: 1.075 correct: 1244.000, total: 3000.000, accuracy: 0.415\n",
            "training epoch: [540 ] loss: 1.075 correct: 1253.000, total: 3000.000, accuracy: 0.418\n",
            "training epoch: [541 ] loss: 1.075 correct: 1245.000, total: 3000.000, accuracy: 0.415\n",
            "training epoch: [542 ] loss: 1.075 correct: 1251.000, total: 3000.000, accuracy: 0.417\n",
            "training epoch: [543 ] loss: 1.075 correct: 1249.000, total: 3000.000, accuracy: 0.416\n",
            "training epoch: [544 ] loss: 1.075 correct: 1251.000, total: 3000.000, accuracy: 0.417\n",
            "training epoch: [545 ] loss: 1.075 correct: 1250.000, total: 3000.000, accuracy: 0.417\n",
            "training epoch: [546 ] loss: 1.075 correct: 1251.000, total: 3000.000, accuracy: 0.417\n",
            "training epoch: [547 ] loss: 1.075 correct: 1250.000, total: 3000.000, accuracy: 0.417\n",
            "training epoch: [548 ] loss: 1.075 correct: 1253.000, total: 3000.000, accuracy: 0.418\n",
            "training epoch: [549 ] loss: 1.075 correct: 1250.000, total: 3000.000, accuracy: 0.417\n",
            "training epoch: [550 ] loss: 1.075 correct: 1253.000, total: 3000.000, accuracy: 0.418\n",
            "training epoch: [551 ] loss: 1.075 correct: 1245.000, total: 3000.000, accuracy: 0.415\n",
            "training epoch: [552 ] loss: 1.075 correct: 1253.000, total: 3000.000, accuracy: 0.418\n",
            "training epoch: [553 ] loss: 1.075 correct: 1247.000, total: 3000.000, accuracy: 0.416\n",
            "training epoch: [554 ] loss: 1.074 correct: 1252.000, total: 3000.000, accuracy: 0.417\n",
            "training epoch: [555 ] loss: 1.074 correct: 1248.000, total: 3000.000, accuracy: 0.416\n",
            "training epoch: [556 ] loss: 1.074 correct: 1250.000, total: 3000.000, accuracy: 0.417\n",
            "training epoch: [557 ] loss: 1.074 correct: 1246.000, total: 3000.000, accuracy: 0.415\n",
            "training epoch: [558 ] loss: 1.074 correct: 1259.000, total: 3000.000, accuracy: 0.420\n",
            "training epoch: [559 ] loss: 1.074 correct: 1240.000, total: 3000.000, accuracy: 0.413\n",
            "training epoch: [560 ] loss: 1.074 correct: 1259.000, total: 3000.000, accuracy: 0.420\n",
            "training epoch: [561 ] loss: 1.074 correct: 1237.000, total: 3000.000, accuracy: 0.412\n",
            "training epoch: [562 ] loss: 1.074 correct: 1259.000, total: 3000.000, accuracy: 0.420\n",
            "training epoch: [563 ] loss: 1.074 correct: 1235.000, total: 3000.000, accuracy: 0.412\n",
            "training epoch: [564 ] loss: 1.074 correct: 1252.000, total: 3000.000, accuracy: 0.417\n",
            "training epoch: [565 ] loss: 1.074 correct: 1233.000, total: 3000.000, accuracy: 0.411\n",
            "training epoch: [566 ] loss: 1.074 correct: 1251.000, total: 3000.000, accuracy: 0.417\n",
            "training epoch: [567 ] loss: 1.074 correct: 1238.000, total: 3000.000, accuracy: 0.413\n",
            "training epoch: [568 ] loss: 1.074 correct: 1251.000, total: 3000.000, accuracy: 0.417\n",
            "training epoch: [569 ] loss: 1.074 correct: 1238.000, total: 3000.000, accuracy: 0.413\n",
            "training epoch: [570 ] loss: 1.074 correct: 1252.000, total: 3000.000, accuracy: 0.417\n",
            "training epoch: [571 ] loss: 1.074 correct: 1238.000, total: 3000.000, accuracy: 0.413\n",
            "training epoch: [572 ] loss: 1.074 correct: 1258.000, total: 3000.000, accuracy: 0.419\n",
            "training epoch: [573 ] loss: 1.074 correct: 1251.000, total: 3000.000, accuracy: 0.417\n",
            "training epoch: [574 ] loss: 1.074 correct: 1261.000, total: 3000.000, accuracy: 0.420\n",
            "training epoch: [575 ] loss: 1.074 correct: 1252.000, total: 3000.000, accuracy: 0.417\n",
            "training epoch: [576 ] loss: 1.074 correct: 1262.000, total: 3000.000, accuracy: 0.421\n",
            "training epoch: [577 ] loss: 1.074 correct: 1248.000, total: 3000.000, accuracy: 0.416\n",
            "training epoch: [578 ] loss: 1.074 correct: 1259.000, total: 3000.000, accuracy: 0.420\n",
            "training epoch: [579 ] loss: 1.074 correct: 1247.000, total: 3000.000, accuracy: 0.416\n",
            "training epoch: [580 ] loss: 1.074 correct: 1253.000, total: 3000.000, accuracy: 0.418\n",
            "training epoch: [581 ] loss: 1.074 correct: 1246.000, total: 3000.000, accuracy: 0.415\n",
            "training epoch: [582 ] loss: 1.074 correct: 1249.000, total: 3000.000, accuracy: 0.416\n",
            "training epoch: [583 ] loss: 1.074 correct: 1246.000, total: 3000.000, accuracy: 0.415\n",
            "training epoch: [584 ] loss: 1.074 correct: 1248.000, total: 3000.000, accuracy: 0.416\n",
            "training epoch: [585 ] loss: 1.074 correct: 1245.000, total: 3000.000, accuracy: 0.415\n",
            "training epoch: [586 ] loss: 1.074 correct: 1247.000, total: 3000.000, accuracy: 0.416\n",
            "training epoch: [587 ] loss: 1.074 correct: 1245.000, total: 3000.000, accuracy: 0.415\n",
            "training epoch: [588 ] loss: 1.074 correct: 1248.000, total: 3000.000, accuracy: 0.416\n",
            "training epoch: [589 ] loss: 1.073 correct: 1244.000, total: 3000.000, accuracy: 0.415\n",
            "training epoch: [590 ] loss: 1.073 correct: 1249.000, total: 3000.000, accuracy: 0.416\n",
            "training epoch: [591 ] loss: 1.073 correct: 1244.000, total: 3000.000, accuracy: 0.415\n",
            "training epoch: [592 ] loss: 1.073 correct: 1251.000, total: 3000.000, accuracy: 0.417\n",
            "training epoch: [593 ] loss: 1.073 correct: 1246.000, total: 3000.000, accuracy: 0.415\n",
            "training epoch: [594 ] loss: 1.073 correct: 1252.000, total: 3000.000, accuracy: 0.417\n",
            "training epoch: [595 ] loss: 1.073 correct: 1245.000, total: 3000.000, accuracy: 0.415\n",
            "training epoch: [596 ] loss: 1.073 correct: 1253.000, total: 3000.000, accuracy: 0.418\n",
            "training epoch: [597 ] loss: 1.073 correct: 1243.000, total: 3000.000, accuracy: 0.414\n",
            "training epoch: [598 ] loss: 1.073 correct: 1251.000, total: 3000.000, accuracy: 0.417\n",
            "training epoch: [599 ] loss: 1.073 correct: 1245.000, total: 3000.000, accuracy: 0.415\n",
            "training epoch: [600 ] loss: 1.073 correct: 1255.000, total: 3000.000, accuracy: 0.418\n",
            "training epoch: [601 ] loss: 1.073 correct: 1246.000, total: 3000.000, accuracy: 0.415\n",
            "training epoch: [602 ] loss: 1.073 correct: 1254.000, total: 3000.000, accuracy: 0.418\n",
            "training epoch: [603 ] loss: 1.073 correct: 1250.000, total: 3000.000, accuracy: 0.417\n",
            "training epoch: [604 ] loss: 1.073 correct: 1254.000, total: 3000.000, accuracy: 0.418\n",
            "training epoch: [605 ] loss: 1.073 correct: 1252.000, total: 3000.000, accuracy: 0.417\n",
            "training epoch: [606 ] loss: 1.073 correct: 1258.000, total: 3000.000, accuracy: 0.419\n",
            "training epoch: [607 ] loss: 1.073 correct: 1252.000, total: 3000.000, accuracy: 0.417\n",
            "training epoch: [608 ] loss: 1.073 correct: 1257.000, total: 3000.000, accuracy: 0.419\n",
            "training epoch: [609 ] loss: 1.073 correct: 1252.000, total: 3000.000, accuracy: 0.417\n",
            "training epoch: [610 ] loss: 1.073 correct: 1261.000, total: 3000.000, accuracy: 0.420\n",
            "training epoch: [611 ] loss: 1.073 correct: 1255.000, total: 3000.000, accuracy: 0.418\n",
            "training epoch: [612 ] loss: 1.073 correct: 1258.000, total: 3000.000, accuracy: 0.419\n",
            "training epoch: [613 ] loss: 1.073 correct: 1255.000, total: 3000.000, accuracy: 0.418\n",
            "training epoch: [614 ] loss: 1.073 correct: 1257.000, total: 3000.000, accuracy: 0.419\n",
            "training epoch: [615 ] loss: 1.073 correct: 1254.000, total: 3000.000, accuracy: 0.418\n",
            "training epoch: [616 ] loss: 1.073 correct: 1258.000, total: 3000.000, accuracy: 0.419\n",
            "training epoch: [617 ] loss: 1.073 correct: 1254.000, total: 3000.000, accuracy: 0.418\n",
            "training epoch: [618 ] loss: 1.073 correct: 1257.000, total: 3000.000, accuracy: 0.419\n",
            "training epoch: [619 ] loss: 1.073 correct: 1254.000, total: 3000.000, accuracy: 0.418\n",
            "training epoch: [620 ] loss: 1.073 correct: 1257.000, total: 3000.000, accuracy: 0.419\n",
            "training epoch: [621 ] loss: 1.073 correct: 1256.000, total: 3000.000, accuracy: 0.419\n",
            "training epoch: [622 ] loss: 1.073 correct: 1256.000, total: 3000.000, accuracy: 0.419\n",
            "training epoch: [623 ] loss: 1.073 correct: 1254.000, total: 3000.000, accuracy: 0.418\n",
            "training epoch: [624 ] loss: 1.073 correct: 1257.000, total: 3000.000, accuracy: 0.419\n",
            "training epoch: [625 ] loss: 1.073 correct: 1257.000, total: 3000.000, accuracy: 0.419\n",
            "training epoch: [626 ] loss: 1.073 correct: 1257.000, total: 3000.000, accuracy: 0.419\n",
            "training epoch: [627 ] loss: 1.073 correct: 1254.000, total: 3000.000, accuracy: 0.418\n",
            "training epoch: [628 ] loss: 1.073 correct: 1255.000, total: 3000.000, accuracy: 0.418\n",
            "training epoch: [629 ] loss: 1.073 correct: 1254.000, total: 3000.000, accuracy: 0.418\n",
            "training epoch: [630 ] loss: 1.073 correct: 1256.000, total: 3000.000, accuracy: 0.419\n",
            "training epoch: [631 ] loss: 1.073 correct: 1253.000, total: 3000.000, accuracy: 0.418\n",
            "training epoch: [632 ] loss: 1.073 correct: 1255.000, total: 3000.000, accuracy: 0.418\n",
            "training epoch: [633 ] loss: 1.073 correct: 1250.000, total: 3000.000, accuracy: 0.417\n",
            "training epoch: [634 ] loss: 1.073 correct: 1255.000, total: 3000.000, accuracy: 0.418\n",
            "training epoch: [635 ] loss: 1.072 correct: 1251.000, total: 3000.000, accuracy: 0.417\n",
            "training epoch: [636 ] loss: 1.072 correct: 1253.000, total: 3000.000, accuracy: 0.418\n",
            "training epoch: [637 ] loss: 1.072 correct: 1253.000, total: 3000.000, accuracy: 0.418\n",
            "training epoch: [638 ] loss: 1.072 correct: 1250.000, total: 3000.000, accuracy: 0.417\n",
            "training epoch: [639 ] loss: 1.072 correct: 1253.000, total: 3000.000, accuracy: 0.418\n",
            "training epoch: [640 ] loss: 1.072 correct: 1249.000, total: 3000.000, accuracy: 0.416\n",
            "training epoch: [641 ] loss: 1.072 correct: 1254.000, total: 3000.000, accuracy: 0.418\n",
            "training epoch: [642 ] loss: 1.072 correct: 1250.000, total: 3000.000, accuracy: 0.417\n",
            "training epoch: [643 ] loss: 1.072 correct: 1258.000, total: 3000.000, accuracy: 0.419\n",
            "training epoch: [644 ] loss: 1.072 correct: 1248.000, total: 3000.000, accuracy: 0.416\n",
            "training epoch: [645 ] loss: 1.072 correct: 1258.000, total: 3000.000, accuracy: 0.419\n",
            "training epoch: [646 ] loss: 1.072 correct: 1251.000, total: 3000.000, accuracy: 0.417\n",
            "training epoch: [647 ] loss: 1.072 correct: 1259.000, total: 3000.000, accuracy: 0.420\n",
            "training epoch: [648 ] loss: 1.072 correct: 1250.000, total: 3000.000, accuracy: 0.417\n",
            "training epoch: [649 ] loss: 1.072 correct: 1260.000, total: 3000.000, accuracy: 0.420\n",
            "training epoch: [650 ] loss: 1.072 correct: 1250.000, total: 3000.000, accuracy: 0.417\n",
            "training epoch: [651 ] loss: 1.072 correct: 1262.000, total: 3000.000, accuracy: 0.421\n",
            "training epoch: [652 ] loss: 1.072 correct: 1251.000, total: 3000.000, accuracy: 0.417\n",
            "training epoch: [653 ] loss: 1.072 correct: 1261.000, total: 3000.000, accuracy: 0.420\n",
            "training epoch: [654 ] loss: 1.072 correct: 1253.000, total: 3000.000, accuracy: 0.418\n",
            "training epoch: [655 ] loss: 1.072 correct: 1260.000, total: 3000.000, accuracy: 0.420\n",
            "training epoch: [656 ] loss: 1.072 correct: 1257.000, total: 3000.000, accuracy: 0.419\n",
            "training epoch: [657 ] loss: 1.072 correct: 1259.000, total: 3000.000, accuracy: 0.420\n",
            "training epoch: [658 ] loss: 1.072 correct: 1260.000, total: 3000.000, accuracy: 0.420\n",
            "training epoch: [659 ] loss: 1.072 correct: 1259.000, total: 3000.000, accuracy: 0.420\n",
            "training epoch: [660 ] loss: 1.072 correct: 1260.000, total: 3000.000, accuracy: 0.420\n",
            "training epoch: [661 ] loss: 1.072 correct: 1259.000, total: 3000.000, accuracy: 0.420\n",
            "training epoch: [662 ] loss: 1.072 correct: 1259.000, total: 3000.000, accuracy: 0.420\n",
            "training epoch: [663 ] loss: 1.072 correct: 1260.000, total: 3000.000, accuracy: 0.420\n",
            "training epoch: [664 ] loss: 1.072 correct: 1257.000, total: 3000.000, accuracy: 0.419\n",
            "training epoch: [665 ] loss: 1.072 correct: 1259.000, total: 3000.000, accuracy: 0.420\n",
            "training epoch: [666 ] loss: 1.072 correct: 1253.000, total: 3000.000, accuracy: 0.418\n",
            "training epoch: [667 ] loss: 1.072 correct: 1260.000, total: 3000.000, accuracy: 0.420\n",
            "training epoch: [668 ] loss: 1.072 correct: 1251.000, total: 3000.000, accuracy: 0.417\n",
            "training epoch: [669 ] loss: 1.072 correct: 1262.000, total: 3000.000, accuracy: 0.421\n",
            "training epoch: [670 ] loss: 1.072 correct: 1252.000, total: 3000.000, accuracy: 0.417\n",
            "training epoch: [671 ] loss: 1.072 correct: 1262.000, total: 3000.000, accuracy: 0.421\n",
            "training epoch: [672 ] loss: 1.072 correct: 1250.000, total: 3000.000, accuracy: 0.417\n",
            "training epoch: [673 ] loss: 1.072 correct: 1263.000, total: 3000.000, accuracy: 0.421\n",
            "training epoch: [674 ] loss: 1.072 correct: 1249.000, total: 3000.000, accuracy: 0.416\n",
            "training epoch: [675 ] loss: 1.072 correct: 1262.000, total: 3000.000, accuracy: 0.421\n",
            "training epoch: [676 ] loss: 1.072 correct: 1247.000, total: 3000.000, accuracy: 0.416\n",
            "training epoch: [677 ] loss: 1.072 correct: 1261.000, total: 3000.000, accuracy: 0.420\n",
            "training epoch: [678 ] loss: 1.072 correct: 1246.000, total: 3000.000, accuracy: 0.415\n",
            "training epoch: [679 ] loss: 1.072 correct: 1261.000, total: 3000.000, accuracy: 0.420\n",
            "training epoch: [680 ] loss: 1.072 correct: 1247.000, total: 3000.000, accuracy: 0.416\n",
            "training epoch: [681 ] loss: 1.072 correct: 1260.000, total: 3000.000, accuracy: 0.420\n",
            "training epoch: [682 ] loss: 1.072 correct: 1249.000, total: 3000.000, accuracy: 0.416\n",
            "training epoch: [683 ] loss: 1.072 correct: 1261.000, total: 3000.000, accuracy: 0.420\n",
            "training epoch: [684 ] loss: 1.072 correct: 1248.000, total: 3000.000, accuracy: 0.416\n",
            "training epoch: [685 ] loss: 1.072 correct: 1262.000, total: 3000.000, accuracy: 0.421\n",
            "training epoch: [686 ] loss: 1.072 correct: 1247.000, total: 3000.000, accuracy: 0.416\n",
            "training epoch: [687 ] loss: 1.072 correct: 1261.000, total: 3000.000, accuracy: 0.420\n",
            "training epoch: [688 ] loss: 1.072 correct: 1247.000, total: 3000.000, accuracy: 0.416\n",
            "training epoch: [689 ] loss: 1.072 correct: 1261.000, total: 3000.000, accuracy: 0.420\n",
            "training epoch: [690 ] loss: 1.072 correct: 1248.000, total: 3000.000, accuracy: 0.416\n",
            "training epoch: [691 ] loss: 1.072 correct: 1264.000, total: 3000.000, accuracy: 0.421\n",
            "training epoch: [692 ] loss: 1.072 correct: 1254.000, total: 3000.000, accuracy: 0.418\n",
            "training epoch: [693 ] loss: 1.072 correct: 1264.000, total: 3000.000, accuracy: 0.421\n",
            "training epoch: [694 ] loss: 1.072 correct: 1251.000, total: 3000.000, accuracy: 0.417\n",
            "training epoch: [695 ] loss: 1.072 correct: 1263.000, total: 3000.000, accuracy: 0.421\n",
            "training epoch: [696 ] loss: 1.072 correct: 1249.000, total: 3000.000, accuracy: 0.416\n",
            "training epoch: [697 ] loss: 1.072 correct: 1262.000, total: 3000.000, accuracy: 0.421\n",
            "training epoch: [698 ] loss: 1.072 correct: 1247.000, total: 3000.000, accuracy: 0.416\n",
            "training epoch: [699 ] loss: 1.072 correct: 1261.000, total: 3000.000, accuracy: 0.420\n",
            "training epoch: [700 ] loss: 1.072 correct: 1251.000, total: 3000.000, accuracy: 0.417\n",
            "training epoch: [701 ] loss: 1.072 correct: 1262.000, total: 3000.000, accuracy: 0.421\n",
            "training epoch: [702 ] loss: 1.072 correct: 1250.000, total: 3000.000, accuracy: 0.417\n",
            "training epoch: [703 ] loss: 1.072 correct: 1262.000, total: 3000.000, accuracy: 0.421\n",
            "training epoch: [704 ] loss: 1.072 correct: 1251.000, total: 3000.000, accuracy: 0.417\n",
            "training epoch: [705 ] loss: 1.072 correct: 1262.000, total: 3000.000, accuracy: 0.421\n",
            "training epoch: [706 ] loss: 1.072 correct: 1252.000, total: 3000.000, accuracy: 0.417\n",
            "training epoch: [707 ] loss: 1.072 correct: 1262.000, total: 3000.000, accuracy: 0.421\n",
            "training epoch: [708 ] loss: 1.072 correct: 1251.000, total: 3000.000, accuracy: 0.417\n",
            "training epoch: [709 ] loss: 1.071 correct: 1265.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [710 ] loss: 1.071 correct: 1248.000, total: 3000.000, accuracy: 0.416\n",
            "training epoch: [711 ] loss: 1.071 correct: 1265.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [712 ] loss: 1.071 correct: 1249.000, total: 3000.000, accuracy: 0.416\n",
            "training epoch: [713 ] loss: 1.071 correct: 1264.000, total: 3000.000, accuracy: 0.421\n",
            "training epoch: [714 ] loss: 1.071 correct: 1246.000, total: 3000.000, accuracy: 0.415\n",
            "training epoch: [715 ] loss: 1.071 correct: 1266.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [716 ] loss: 1.071 correct: 1246.000, total: 3000.000, accuracy: 0.415\n",
            "training epoch: [717 ] loss: 1.071 correct: 1265.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [718 ] loss: 1.071 correct: 1246.000, total: 3000.000, accuracy: 0.415\n",
            "training epoch: [719 ] loss: 1.071 correct: 1263.000, total: 3000.000, accuracy: 0.421\n",
            "training epoch: [720 ] loss: 1.071 correct: 1248.000, total: 3000.000, accuracy: 0.416\n",
            "training epoch: [721 ] loss: 1.071 correct: 1261.000, total: 3000.000, accuracy: 0.420\n",
            "training epoch: [722 ] loss: 1.071 correct: 1250.000, total: 3000.000, accuracy: 0.417\n",
            "training epoch: [723 ] loss: 1.071 correct: 1259.000, total: 3000.000, accuracy: 0.420\n",
            "training epoch: [724 ] loss: 1.071 correct: 1249.000, total: 3000.000, accuracy: 0.416\n",
            "training epoch: [725 ] loss: 1.071 correct: 1263.000, total: 3000.000, accuracy: 0.421\n",
            "training epoch: [726 ] loss: 1.071 correct: 1249.000, total: 3000.000, accuracy: 0.416\n",
            "training epoch: [727 ] loss: 1.071 correct: 1263.000, total: 3000.000, accuracy: 0.421\n",
            "training epoch: [728 ] loss: 1.071 correct: 1249.000, total: 3000.000, accuracy: 0.416\n",
            "training epoch: [729 ] loss: 1.071 correct: 1264.000, total: 3000.000, accuracy: 0.421\n",
            "training epoch: [730 ] loss: 1.071 correct: 1249.000, total: 3000.000, accuracy: 0.416\n",
            "training epoch: [731 ] loss: 1.071 correct: 1266.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [732 ] loss: 1.071 correct: 1246.000, total: 3000.000, accuracy: 0.415\n",
            "training epoch: [733 ] loss: 1.071 correct: 1265.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [734 ] loss: 1.071 correct: 1246.000, total: 3000.000, accuracy: 0.415\n",
            "training epoch: [735 ] loss: 1.071 correct: 1265.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [736 ] loss: 1.071 correct: 1247.000, total: 3000.000, accuracy: 0.416\n",
            "training epoch: [737 ] loss: 1.071 correct: 1266.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [738 ] loss: 1.071 correct: 1249.000, total: 3000.000, accuracy: 0.416\n",
            "training epoch: [739 ] loss: 1.071 correct: 1269.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [740 ] loss: 1.071 correct: 1248.000, total: 3000.000, accuracy: 0.416\n",
            "training epoch: [741 ] loss: 1.071 correct: 1269.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [742 ] loss: 1.071 correct: 1250.000, total: 3000.000, accuracy: 0.417\n",
            "training epoch: [743 ] loss: 1.071 correct: 1267.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [744 ] loss: 1.071 correct: 1253.000, total: 3000.000, accuracy: 0.418\n",
            "training epoch: [745 ] loss: 1.071 correct: 1265.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [746 ] loss: 1.071 correct: 1254.000, total: 3000.000, accuracy: 0.418\n",
            "training epoch: [747 ] loss: 1.071 correct: 1264.000, total: 3000.000, accuracy: 0.421\n",
            "training epoch: [748 ] loss: 1.071 correct: 1252.000, total: 3000.000, accuracy: 0.417\n",
            "training epoch: [749 ] loss: 1.071 correct: 1265.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [750 ] loss: 1.071 correct: 1251.000, total: 3000.000, accuracy: 0.417\n",
            "training epoch: [751 ] loss: 1.071 correct: 1267.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [752 ] loss: 1.071 correct: 1249.000, total: 3000.000, accuracy: 0.416\n",
            "training epoch: [753 ] loss: 1.071 correct: 1266.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [754 ] loss: 1.071 correct: 1251.000, total: 3000.000, accuracy: 0.417\n",
            "training epoch: [755 ] loss: 1.071 correct: 1266.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [756 ] loss: 1.071 correct: 1249.000, total: 3000.000, accuracy: 0.416\n",
            "training epoch: [757 ] loss: 1.071 correct: 1265.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [758 ] loss: 1.071 correct: 1249.000, total: 3000.000, accuracy: 0.416\n",
            "training epoch: [759 ] loss: 1.071 correct: 1263.000, total: 3000.000, accuracy: 0.421\n",
            "training epoch: [760 ] loss: 1.071 correct: 1249.000, total: 3000.000, accuracy: 0.416\n",
            "training epoch: [761 ] loss: 1.071 correct: 1266.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [762 ] loss: 1.071 correct: 1249.000, total: 3000.000, accuracy: 0.416\n",
            "training epoch: [763 ] loss: 1.071 correct: 1265.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [764 ] loss: 1.071 correct: 1249.000, total: 3000.000, accuracy: 0.416\n",
            "training epoch: [765 ] loss: 1.071 correct: 1265.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [766 ] loss: 1.071 correct: 1249.000, total: 3000.000, accuracy: 0.416\n",
            "training epoch: [767 ] loss: 1.071 correct: 1266.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [768 ] loss: 1.071 correct: 1245.000, total: 3000.000, accuracy: 0.415\n",
            "training epoch: [769 ] loss: 1.071 correct: 1265.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [770 ] loss: 1.071 correct: 1242.000, total: 3000.000, accuracy: 0.414\n",
            "training epoch: [771 ] loss: 1.071 correct: 1262.000, total: 3000.000, accuracy: 0.421\n",
            "training epoch: [772 ] loss: 1.071 correct: 1245.000, total: 3000.000, accuracy: 0.415\n",
            "training epoch: [773 ] loss: 1.071 correct: 1266.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [774 ] loss: 1.071 correct: 1243.000, total: 3000.000, accuracy: 0.414\n",
            "training epoch: [775 ] loss: 1.071 correct: 1266.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [776 ] loss: 1.071 correct: 1240.000, total: 3000.000, accuracy: 0.413\n",
            "training epoch: [777 ] loss: 1.071 correct: 1264.000, total: 3000.000, accuracy: 0.421\n",
            "training epoch: [778 ] loss: 1.071 correct: 1242.000, total: 3000.000, accuracy: 0.414\n",
            "training epoch: [779 ] loss: 1.071 correct: 1264.000, total: 3000.000, accuracy: 0.421\n",
            "training epoch: [780 ] loss: 1.071 correct: 1241.000, total: 3000.000, accuracy: 0.414\n",
            "training epoch: [781 ] loss: 1.071 correct: 1269.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [782 ] loss: 1.071 correct: 1243.000, total: 3000.000, accuracy: 0.414\n",
            "training epoch: [783 ] loss: 1.071 correct: 1271.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [784 ] loss: 1.071 correct: 1247.000, total: 3000.000, accuracy: 0.416\n",
            "training epoch: [785 ] loss: 1.071 correct: 1272.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [786 ] loss: 1.070 correct: 1254.000, total: 3000.000, accuracy: 0.418\n",
            "training epoch: [787 ] loss: 1.070 correct: 1269.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [788 ] loss: 1.070 correct: 1257.000, total: 3000.000, accuracy: 0.419\n",
            "training epoch: [789 ] loss: 1.070 correct: 1265.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [790 ] loss: 1.070 correct: 1261.000, total: 3000.000, accuracy: 0.420\n",
            "training epoch: [791 ] loss: 1.070 correct: 1266.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [792 ] loss: 1.070 correct: 1264.000, total: 3000.000, accuracy: 0.421\n",
            "training epoch: [793 ] loss: 1.070 correct: 1266.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [794 ] loss: 1.070 correct: 1264.000, total: 3000.000, accuracy: 0.421\n",
            "training epoch: [795 ] loss: 1.070 correct: 1266.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [796 ] loss: 1.070 correct: 1267.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [797 ] loss: 1.070 correct: 1265.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [798 ] loss: 1.070 correct: 1269.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [799 ] loss: 1.070 correct: 1264.000, total: 3000.000, accuracy: 0.421\n",
            "training epoch: [800 ] loss: 1.070 correct: 1269.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [801 ] loss: 1.070 correct: 1262.000, total: 3000.000, accuracy: 0.421\n",
            "training epoch: [802 ] loss: 1.070 correct: 1266.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [803 ] loss: 1.070 correct: 1263.000, total: 3000.000, accuracy: 0.421\n",
            "training epoch: [804 ] loss: 1.070 correct: 1267.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [805 ] loss: 1.070 correct: 1259.000, total: 3000.000, accuracy: 0.420\n",
            "training epoch: [806 ] loss: 1.070 correct: 1271.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [807 ] loss: 1.070 correct: 1256.000, total: 3000.000, accuracy: 0.419\n",
            "training epoch: [808 ] loss: 1.070 correct: 1272.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [809 ] loss: 1.070 correct: 1254.000, total: 3000.000, accuracy: 0.418\n",
            "training epoch: [810 ] loss: 1.070 correct: 1273.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [811 ] loss: 1.070 correct: 1255.000, total: 3000.000, accuracy: 0.418\n",
            "training epoch: [812 ] loss: 1.070 correct: 1267.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [813 ] loss: 1.070 correct: 1253.000, total: 3000.000, accuracy: 0.418\n",
            "training epoch: [814 ] loss: 1.070 correct: 1268.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [815 ] loss: 1.070 correct: 1251.000, total: 3000.000, accuracy: 0.417\n",
            "training epoch: [816 ] loss: 1.070 correct: 1270.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [817 ] loss: 1.070 correct: 1253.000, total: 3000.000, accuracy: 0.418\n",
            "training epoch: [818 ] loss: 1.070 correct: 1272.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [819 ] loss: 1.070 correct: 1250.000, total: 3000.000, accuracy: 0.417\n",
            "training epoch: [820 ] loss: 1.070 correct: 1272.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [821 ] loss: 1.070 correct: 1250.000, total: 3000.000, accuracy: 0.417\n",
            "training epoch: [822 ] loss: 1.070 correct: 1274.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [823 ] loss: 1.070 correct: 1251.000, total: 3000.000, accuracy: 0.417\n",
            "training epoch: [824 ] loss: 1.070 correct: 1271.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [825 ] loss: 1.070 correct: 1248.000, total: 3000.000, accuracy: 0.416\n",
            "training epoch: [826 ] loss: 1.070 correct: 1271.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [827 ] loss: 1.070 correct: 1250.000, total: 3000.000, accuracy: 0.417\n",
            "training epoch: [828 ] loss: 1.070 correct: 1275.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [829 ] loss: 1.070 correct: 1260.000, total: 3000.000, accuracy: 0.420\n",
            "training epoch: [830 ] loss: 1.070 correct: 1275.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [831 ] loss: 1.070 correct: 1251.000, total: 3000.000, accuracy: 0.417\n",
            "training epoch: [832 ] loss: 1.070 correct: 1274.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [833 ] loss: 1.070 correct: 1257.000, total: 3000.000, accuracy: 0.419\n",
            "training epoch: [834 ] loss: 1.070 correct: 1273.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [835 ] loss: 1.070 correct: 1254.000, total: 3000.000, accuracy: 0.418\n",
            "training epoch: [836 ] loss: 1.070 correct: 1274.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [837 ] loss: 1.070 correct: 1254.000, total: 3000.000, accuracy: 0.418\n",
            "training epoch: [838 ] loss: 1.070 correct: 1275.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [839 ] loss: 1.070 correct: 1254.000, total: 3000.000, accuracy: 0.418\n",
            "training epoch: [840 ] loss: 1.070 correct: 1274.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [841 ] loss: 1.070 correct: 1256.000, total: 3000.000, accuracy: 0.419\n",
            "training epoch: [842 ] loss: 1.070 correct: 1275.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [843 ] loss: 1.070 correct: 1255.000, total: 3000.000, accuracy: 0.418\n",
            "training epoch: [844 ] loss: 1.070 correct: 1276.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [845 ] loss: 1.070 correct: 1253.000, total: 3000.000, accuracy: 0.418\n",
            "training epoch: [846 ] loss: 1.070 correct: 1270.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [847 ] loss: 1.070 correct: 1252.000, total: 3000.000, accuracy: 0.417\n",
            "training epoch: [848 ] loss: 1.070 correct: 1269.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [849 ] loss: 1.070 correct: 1247.000, total: 3000.000, accuracy: 0.416\n",
            "training epoch: [850 ] loss: 1.070 correct: 1269.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [851 ] loss: 1.070 correct: 1246.000, total: 3000.000, accuracy: 0.415\n",
            "training epoch: [852 ] loss: 1.070 correct: 1269.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [853 ] loss: 1.070 correct: 1246.000, total: 3000.000, accuracy: 0.415\n",
            "training epoch: [854 ] loss: 1.070 correct: 1270.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [855 ] loss: 1.070 correct: 1248.000, total: 3000.000, accuracy: 0.416\n",
            "training epoch: [856 ] loss: 1.070 correct: 1274.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [857 ] loss: 1.070 correct: 1250.000, total: 3000.000, accuracy: 0.417\n",
            "training epoch: [858 ] loss: 1.070 correct: 1273.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [859 ] loss: 1.070 correct: 1250.000, total: 3000.000, accuracy: 0.417\n",
            "training epoch: [860 ] loss: 1.070 correct: 1276.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [861 ] loss: 1.070 correct: 1253.000, total: 3000.000, accuracy: 0.418\n",
            "training epoch: [862 ] loss: 1.070 correct: 1276.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [863 ] loss: 1.070 correct: 1255.000, total: 3000.000, accuracy: 0.418\n",
            "training epoch: [864 ] loss: 1.070 correct: 1276.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [865 ] loss: 1.070 correct: 1257.000, total: 3000.000, accuracy: 0.419\n",
            "training epoch: [866 ] loss: 1.070 correct: 1272.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [867 ] loss: 1.070 correct: 1258.000, total: 3000.000, accuracy: 0.419\n",
            "training epoch: [868 ] loss: 1.070 correct: 1271.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [869 ] loss: 1.070 correct: 1261.000, total: 3000.000, accuracy: 0.420\n",
            "training epoch: [870 ] loss: 1.070 correct: 1272.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [871 ] loss: 1.070 correct: 1261.000, total: 3000.000, accuracy: 0.420\n",
            "training epoch: [872 ] loss: 1.070 correct: 1272.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [873 ] loss: 1.070 correct: 1262.000, total: 3000.000, accuracy: 0.421\n",
            "training epoch: [874 ] loss: 1.070 correct: 1272.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [875 ] loss: 1.070 correct: 1262.000, total: 3000.000, accuracy: 0.421\n",
            "training epoch: [876 ] loss: 1.070 correct: 1271.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [877 ] loss: 1.070 correct: 1265.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [878 ] loss: 1.070 correct: 1271.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [879 ] loss: 1.070 correct: 1268.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [880 ] loss: 1.070 correct: 1276.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [881 ] loss: 1.070 correct: 1261.000, total: 3000.000, accuracy: 0.420\n",
            "training epoch: [882 ] loss: 1.070 correct: 1279.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [883 ] loss: 1.070 correct: 1261.000, total: 3000.000, accuracy: 0.420\n",
            "training epoch: [884 ] loss: 1.070 correct: 1275.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [885 ] loss: 1.070 correct: 1256.000, total: 3000.000, accuracy: 0.419\n",
            "training epoch: [886 ] loss: 1.070 correct: 1269.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [887 ] loss: 1.070 correct: 1253.000, total: 3000.000, accuracy: 0.418\n",
            "training epoch: [888 ] loss: 1.070 correct: 1274.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [889 ] loss: 1.070 correct: 1258.000, total: 3000.000, accuracy: 0.419\n",
            "training epoch: [890 ] loss: 1.070 correct: 1281.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [891 ] loss: 1.070 correct: 1261.000, total: 3000.000, accuracy: 0.420\n",
            "training epoch: [892 ] loss: 1.070 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [893 ] loss: 1.070 correct: 1262.000, total: 3000.000, accuracy: 0.421\n",
            "training epoch: [894 ] loss: 1.070 correct: 1283.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [895 ] loss: 1.070 correct: 1263.000, total: 3000.000, accuracy: 0.421\n",
            "training epoch: [896 ] loss: 1.070 correct: 1282.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [897 ] loss: 1.070 correct: 1259.000, total: 3000.000, accuracy: 0.420\n",
            "training epoch: [898 ] loss: 1.069 correct: 1280.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [899 ] loss: 1.069 correct: 1258.000, total: 3000.000, accuracy: 0.419\n",
            "training epoch: [900 ] loss: 1.069 correct: 1277.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [901 ] loss: 1.069 correct: 1265.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [902 ] loss: 1.069 correct: 1272.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [903 ] loss: 1.069 correct: 1261.000, total: 3000.000, accuracy: 0.420\n",
            "training epoch: [904 ] loss: 1.069 correct: 1269.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [905 ] loss: 1.069 correct: 1266.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [906 ] loss: 1.069 correct: 1267.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [907 ] loss: 1.069 correct: 1271.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [908 ] loss: 1.069 correct: 1266.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [909 ] loss: 1.069 correct: 1270.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [910 ] loss: 1.069 correct: 1265.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [911 ] loss: 1.069 correct: 1270.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [912 ] loss: 1.069 correct: 1265.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [913 ] loss: 1.069 correct: 1268.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [914 ] loss: 1.069 correct: 1264.000, total: 3000.000, accuracy: 0.421\n",
            "training epoch: [915 ] loss: 1.069 correct: 1267.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [916 ] loss: 1.069 correct: 1266.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [917 ] loss: 1.069 correct: 1268.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [918 ] loss: 1.069 correct: 1265.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [919 ] loss: 1.069 correct: 1269.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [920 ] loss: 1.069 correct: 1265.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [921 ] loss: 1.069 correct: 1270.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [922 ] loss: 1.069 correct: 1262.000, total: 3000.000, accuracy: 0.421\n",
            "training epoch: [923 ] loss: 1.069 correct: 1269.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [924 ] loss: 1.069 correct: 1261.000, total: 3000.000, accuracy: 0.420\n",
            "training epoch: [925 ] loss: 1.069 correct: 1269.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [926 ] loss: 1.069 correct: 1260.000, total: 3000.000, accuracy: 0.420\n",
            "training epoch: [927 ] loss: 1.069 correct: 1270.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [928 ] loss: 1.069 correct: 1260.000, total: 3000.000, accuracy: 0.420\n",
            "training epoch: [929 ] loss: 1.069 correct: 1271.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [930 ] loss: 1.069 correct: 1259.000, total: 3000.000, accuracy: 0.420\n",
            "training epoch: [931 ] loss: 1.069 correct: 1272.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [932 ] loss: 1.069 correct: 1259.000, total: 3000.000, accuracy: 0.420\n",
            "training epoch: [933 ] loss: 1.069 correct: 1273.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [934 ] loss: 1.069 correct: 1259.000, total: 3000.000, accuracy: 0.420\n",
            "training epoch: [935 ] loss: 1.069 correct: 1271.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [936 ] loss: 1.069 correct: 1257.000, total: 3000.000, accuracy: 0.419\n",
            "training epoch: [937 ] loss: 1.069 correct: 1271.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [938 ] loss: 1.069 correct: 1256.000, total: 3000.000, accuracy: 0.419\n",
            "training epoch: [939 ] loss: 1.069 correct: 1272.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [940 ] loss: 1.069 correct: 1258.000, total: 3000.000, accuracy: 0.419\n",
            "training epoch: [941 ] loss: 1.069 correct: 1273.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [942 ] loss: 1.069 correct: 1258.000, total: 3000.000, accuracy: 0.419\n",
            "training epoch: [943 ] loss: 1.069 correct: 1274.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [944 ] loss: 1.069 correct: 1257.000, total: 3000.000, accuracy: 0.419\n",
            "training epoch: [945 ] loss: 1.069 correct: 1275.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [946 ] loss: 1.069 correct: 1258.000, total: 3000.000, accuracy: 0.419\n",
            "training epoch: [947 ] loss: 1.069 correct: 1273.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [948 ] loss: 1.069 correct: 1258.000, total: 3000.000, accuracy: 0.419\n",
            "training epoch: [949 ] loss: 1.069 correct: 1272.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [950 ] loss: 1.069 correct: 1256.000, total: 3000.000, accuracy: 0.419\n",
            "training epoch: [951 ] loss: 1.069 correct: 1275.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [952 ] loss: 1.069 correct: 1261.000, total: 3000.000, accuracy: 0.420\n",
            "training epoch: [953 ] loss: 1.069 correct: 1275.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [954 ] loss: 1.069 correct: 1261.000, total: 3000.000, accuracy: 0.420\n",
            "training epoch: [955 ] loss: 1.069 correct: 1277.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [956 ] loss: 1.069 correct: 1261.000, total: 3000.000, accuracy: 0.420\n",
            "training epoch: [957 ] loss: 1.069 correct: 1275.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [958 ] loss: 1.069 correct: 1258.000, total: 3000.000, accuracy: 0.419\n",
            "training epoch: [959 ] loss: 1.069 correct: 1278.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [960 ] loss: 1.069 correct: 1261.000, total: 3000.000, accuracy: 0.420\n",
            "training epoch: [961 ] loss: 1.069 correct: 1277.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [962 ] loss: 1.069 correct: 1261.000, total: 3000.000, accuracy: 0.420\n",
            "training epoch: [963 ] loss: 1.069 correct: 1274.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [964 ] loss: 1.069 correct: 1262.000, total: 3000.000, accuracy: 0.421\n",
            "training epoch: [965 ] loss: 1.069 correct: 1276.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [966 ] loss: 1.069 correct: 1264.000, total: 3000.000, accuracy: 0.421\n",
            "training epoch: [967 ] loss: 1.069 correct: 1276.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [968 ] loss: 1.069 correct: 1264.000, total: 3000.000, accuracy: 0.421\n",
            "training epoch: [969 ] loss: 1.069 correct: 1274.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [970 ] loss: 1.069 correct: 1265.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [971 ] loss: 1.069 correct: 1277.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [972 ] loss: 1.069 correct: 1264.000, total: 3000.000, accuracy: 0.421\n",
            "training epoch: [973 ] loss: 1.069 correct: 1278.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [974 ] loss: 1.069 correct: 1265.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [975 ] loss: 1.069 correct: 1280.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [976 ] loss: 1.069 correct: 1263.000, total: 3000.000, accuracy: 0.421\n",
            "training epoch: [977 ] loss: 1.069 correct: 1280.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [978 ] loss: 1.069 correct: 1261.000, total: 3000.000, accuracy: 0.420\n",
            "training epoch: [979 ] loss: 1.069 correct: 1283.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [980 ] loss: 1.069 correct: 1264.000, total: 3000.000, accuracy: 0.421\n",
            "training epoch: [981 ] loss: 1.069 correct: 1282.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [982 ] loss: 1.069 correct: 1266.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [983 ] loss: 1.069 correct: 1278.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [984 ] loss: 1.069 correct: 1266.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [985 ] loss: 1.069 correct: 1278.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [986 ] loss: 1.069 correct: 1267.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [987 ] loss: 1.069 correct: 1277.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [988 ] loss: 1.069 correct: 1267.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [989 ] loss: 1.069 correct: 1274.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [990 ] loss: 1.069 correct: 1267.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [991 ] loss: 1.069 correct: 1272.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [992 ] loss: 1.069 correct: 1268.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [993 ] loss: 1.069 correct: 1269.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [994 ] loss: 1.069 correct: 1270.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [995 ] loss: 1.069 correct: 1269.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [996 ] loss: 1.069 correct: 1270.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [997 ] loss: 1.069 correct: 1265.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [998 ] loss: 1.069 correct: 1272.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [999 ] loss: 1.069 correct: 1267.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [1000 ] loss: 1.069 correct: 1275.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1001 ] loss: 1.069 correct: 1267.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [1002 ] loss: 1.069 correct: 1273.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [1003 ] loss: 1.069 correct: 1261.000, total: 3000.000, accuracy: 0.420\n",
            "training epoch: [1004 ] loss: 1.069 correct: 1270.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [1005 ] loss: 1.069 correct: 1255.000, total: 3000.000, accuracy: 0.418\n",
            "training epoch: [1006 ] loss: 1.069 correct: 1272.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [1007 ] loss: 1.069 correct: 1258.000, total: 3000.000, accuracy: 0.419\n",
            "training epoch: [1008 ] loss: 1.069 correct: 1272.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [1009 ] loss: 1.069 correct: 1260.000, total: 3000.000, accuracy: 0.420\n",
            "training epoch: [1010 ] loss: 1.069 correct: 1274.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1011 ] loss: 1.069 correct: 1258.000, total: 3000.000, accuracy: 0.419\n",
            "training epoch: [1012 ] loss: 1.069 correct: 1277.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [1013 ] loss: 1.069 correct: 1261.000, total: 3000.000, accuracy: 0.420\n",
            "training epoch: [1014 ] loss: 1.069 correct: 1277.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [1015 ] loss: 1.069 correct: 1263.000, total: 3000.000, accuracy: 0.421\n",
            "training epoch: [1016 ] loss: 1.069 correct: 1274.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1017 ] loss: 1.069 correct: 1264.000, total: 3000.000, accuracy: 0.421\n",
            "training epoch: [1018 ] loss: 1.069 correct: 1276.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1019 ] loss: 1.069 correct: 1259.000, total: 3000.000, accuracy: 0.420\n",
            "training epoch: [1020 ] loss: 1.069 correct: 1274.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1021 ] loss: 1.069 correct: 1264.000, total: 3000.000, accuracy: 0.421\n",
            "training epoch: [1022 ] loss: 1.068 correct: 1275.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1023 ] loss: 1.068 correct: 1263.000, total: 3000.000, accuracy: 0.421\n",
            "training epoch: [1024 ] loss: 1.068 correct: 1274.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1025 ] loss: 1.068 correct: 1264.000, total: 3000.000, accuracy: 0.421\n",
            "training epoch: [1026 ] loss: 1.068 correct: 1277.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [1027 ] loss: 1.068 correct: 1264.000, total: 3000.000, accuracy: 0.421\n",
            "training epoch: [1028 ] loss: 1.068 correct: 1277.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [1029 ] loss: 1.068 correct: 1265.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [1030 ] loss: 1.068 correct: 1277.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [1031 ] loss: 1.068 correct: 1262.000, total: 3000.000, accuracy: 0.421\n",
            "training epoch: [1032 ] loss: 1.068 correct: 1277.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [1033 ] loss: 1.068 correct: 1262.000, total: 3000.000, accuracy: 0.421\n",
            "training epoch: [1034 ] loss: 1.068 correct: 1276.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1035 ] loss: 1.068 correct: 1262.000, total: 3000.000, accuracy: 0.421\n",
            "training epoch: [1036 ] loss: 1.068 correct: 1278.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [1037 ] loss: 1.068 correct: 1264.000, total: 3000.000, accuracy: 0.421\n",
            "training epoch: [1038 ] loss: 1.068 correct: 1281.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [1039 ] loss: 1.068 correct: 1266.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [1040 ] loss: 1.068 correct: 1282.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [1041 ] loss: 1.068 correct: 1268.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [1042 ] loss: 1.068 correct: 1280.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [1043 ] loss: 1.068 correct: 1263.000, total: 3000.000, accuracy: 0.421\n",
            "training epoch: [1044 ] loss: 1.068 correct: 1281.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [1045 ] loss: 1.068 correct: 1265.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [1046 ] loss: 1.068 correct: 1281.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [1047 ] loss: 1.068 correct: 1264.000, total: 3000.000, accuracy: 0.421\n",
            "training epoch: [1048 ] loss: 1.068 correct: 1282.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [1049 ] loss: 1.068 correct: 1266.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [1050 ] loss: 1.068 correct: 1281.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [1051 ] loss: 1.068 correct: 1265.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [1052 ] loss: 1.068 correct: 1281.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [1053 ] loss: 1.068 correct: 1266.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [1054 ] loss: 1.068 correct: 1280.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [1055 ] loss: 1.068 correct: 1266.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [1056 ] loss: 1.068 correct: 1282.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [1057 ] loss: 1.068 correct: 1265.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [1058 ] loss: 1.068 correct: 1281.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [1059 ] loss: 1.068 correct: 1264.000, total: 3000.000, accuracy: 0.421\n",
            "training epoch: [1060 ] loss: 1.068 correct: 1279.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [1061 ] loss: 1.068 correct: 1264.000, total: 3000.000, accuracy: 0.421\n",
            "training epoch: [1062 ] loss: 1.068 correct: 1278.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [1063 ] loss: 1.068 correct: 1264.000, total: 3000.000, accuracy: 0.421\n",
            "training epoch: [1064 ] loss: 1.068 correct: 1277.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [1065 ] loss: 1.068 correct: 1265.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [1066 ] loss: 1.068 correct: 1278.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [1067 ] loss: 1.068 correct: 1265.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [1068 ] loss: 1.068 correct: 1276.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1069 ] loss: 1.068 correct: 1266.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [1070 ] loss: 1.068 correct: 1277.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [1071 ] loss: 1.068 correct: 1267.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [1072 ] loss: 1.068 correct: 1278.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [1073 ] loss: 1.068 correct: 1265.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [1074 ] loss: 1.068 correct: 1278.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [1075 ] loss: 1.068 correct: 1262.000, total: 3000.000, accuracy: 0.421\n",
            "training epoch: [1076 ] loss: 1.068 correct: 1274.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1077 ] loss: 1.068 correct: 1252.000, total: 3000.000, accuracy: 0.417\n",
            "training epoch: [1078 ] loss: 1.068 correct: 1277.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [1079 ] loss: 1.068 correct: 1247.000, total: 3000.000, accuracy: 0.416\n",
            "training epoch: [1080 ] loss: 1.068 correct: 1275.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1081 ] loss: 1.068 correct: 1251.000, total: 3000.000, accuracy: 0.417\n",
            "training epoch: [1082 ] loss: 1.068 correct: 1273.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [1083 ] loss: 1.068 correct: 1253.000, total: 3000.000, accuracy: 0.418\n",
            "training epoch: [1084 ] loss: 1.068 correct: 1274.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1085 ] loss: 1.068 correct: 1264.000, total: 3000.000, accuracy: 0.421\n",
            "training epoch: [1086 ] loss: 1.068 correct: 1275.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1087 ] loss: 1.068 correct: 1268.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [1088 ] loss: 1.068 correct: 1272.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [1089 ] loss: 1.068 correct: 1267.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [1090 ] loss: 1.068 correct: 1270.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [1091 ] loss: 1.068 correct: 1267.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [1092 ] loss: 1.068 correct: 1270.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [1093 ] loss: 1.068 correct: 1268.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [1094 ] loss: 1.068 correct: 1266.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [1095 ] loss: 1.068 correct: 1267.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [1096 ] loss: 1.068 correct: 1264.000, total: 3000.000, accuracy: 0.421\n",
            "training epoch: [1097 ] loss: 1.068 correct: 1270.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [1098 ] loss: 1.068 correct: 1264.000, total: 3000.000, accuracy: 0.421\n",
            "training epoch: [1099 ] loss: 1.068 correct: 1271.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [1100 ] loss: 1.068 correct: 1266.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [1101 ] loss: 1.068 correct: 1272.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [1102 ] loss: 1.068 correct: 1266.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [1103 ] loss: 1.068 correct: 1274.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1104 ] loss: 1.068 correct: 1266.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [1105 ] loss: 1.068 correct: 1275.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1106 ] loss: 1.068 correct: 1267.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [1107 ] loss: 1.068 correct: 1274.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1108 ] loss: 1.068 correct: 1264.000, total: 3000.000, accuracy: 0.421\n",
            "training epoch: [1109 ] loss: 1.068 correct: 1273.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [1110 ] loss: 1.068 correct: 1263.000, total: 3000.000, accuracy: 0.421\n",
            "training epoch: [1111 ] loss: 1.068 correct: 1275.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1112 ] loss: 1.068 correct: 1263.000, total: 3000.000, accuracy: 0.421\n",
            "training epoch: [1113 ] loss: 1.068 correct: 1274.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1114 ] loss: 1.068 correct: 1265.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [1115 ] loss: 1.068 correct: 1278.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [1116 ] loss: 1.068 correct: 1265.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [1117 ] loss: 1.068 correct: 1281.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [1118 ] loss: 1.068 correct: 1268.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [1119 ] loss: 1.068 correct: 1279.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [1120 ] loss: 1.068 correct: 1268.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [1121 ] loss: 1.068 correct: 1281.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [1122 ] loss: 1.068 correct: 1268.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [1123 ] loss: 1.068 correct: 1282.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [1124 ] loss: 1.068 correct: 1266.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [1125 ] loss: 1.068 correct: 1284.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [1126 ] loss: 1.068 correct: 1263.000, total: 3000.000, accuracy: 0.421\n",
            "training epoch: [1127 ] loss: 1.068 correct: 1283.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [1128 ] loss: 1.068 correct: 1262.000, total: 3000.000, accuracy: 0.421\n",
            "training epoch: [1129 ] loss: 1.068 correct: 1283.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [1130 ] loss: 1.068 correct: 1261.000, total: 3000.000, accuracy: 0.420\n",
            "training epoch: [1131 ] loss: 1.068 correct: 1282.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [1132 ] loss: 1.068 correct: 1263.000, total: 3000.000, accuracy: 0.421\n",
            "training epoch: [1133 ] loss: 1.068 correct: 1283.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [1134 ] loss: 1.068 correct: 1262.000, total: 3000.000, accuracy: 0.421\n",
            "training epoch: [1135 ] loss: 1.068 correct: 1282.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [1136 ] loss: 1.068 correct: 1264.000, total: 3000.000, accuracy: 0.421\n",
            "training epoch: [1137 ] loss: 1.068 correct: 1281.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [1138 ] loss: 1.068 correct: 1263.000, total: 3000.000, accuracy: 0.421\n",
            "training epoch: [1139 ] loss: 1.068 correct: 1277.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [1140 ] loss: 1.068 correct: 1260.000, total: 3000.000, accuracy: 0.420\n",
            "training epoch: [1141 ] loss: 1.068 correct: 1278.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [1142 ] loss: 1.068 correct: 1261.000, total: 3000.000, accuracy: 0.420\n",
            "training epoch: [1143 ] loss: 1.068 correct: 1280.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [1144 ] loss: 1.068 correct: 1261.000, total: 3000.000, accuracy: 0.420\n",
            "training epoch: [1145 ] loss: 1.068 correct: 1275.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1146 ] loss: 1.068 correct: 1266.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [1147 ] loss: 1.068 correct: 1273.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [1148 ] loss: 1.068 correct: 1268.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [1149 ] loss: 1.068 correct: 1273.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [1150 ] loss: 1.068 correct: 1271.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [1151 ] loss: 1.068 correct: 1271.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [1152 ] loss: 1.068 correct: 1269.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [1153 ] loss: 1.068 correct: 1273.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [1154 ] loss: 1.068 correct: 1270.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [1155 ] loss: 1.068 correct: 1271.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [1156 ] loss: 1.068 correct: 1271.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [1157 ] loss: 1.068 correct: 1272.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [1158 ] loss: 1.068 correct: 1274.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1159 ] loss: 1.068 correct: 1270.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [1160 ] loss: 1.068 correct: 1272.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [1161 ] loss: 1.068 correct: 1271.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [1162 ] loss: 1.068 correct: 1272.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [1163 ] loss: 1.068 correct: 1271.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [1164 ] loss: 1.067 correct: 1272.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [1165 ] loss: 1.067 correct: 1270.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [1166 ] loss: 1.067 correct: 1273.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [1167 ] loss: 1.067 correct: 1269.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [1168 ] loss: 1.067 correct: 1273.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [1169 ] loss: 1.067 correct: 1269.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [1170 ] loss: 1.067 correct: 1274.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1171 ] loss: 1.067 correct: 1270.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [1172 ] loss: 1.067 correct: 1273.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [1173 ] loss: 1.067 correct: 1270.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [1174 ] loss: 1.067 correct: 1273.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [1175 ] loss: 1.067 correct: 1271.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [1176 ] loss: 1.067 correct: 1273.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [1177 ] loss: 1.067 correct: 1268.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [1178 ] loss: 1.067 correct: 1274.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1179 ] loss: 1.067 correct: 1267.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [1180 ] loss: 1.067 correct: 1272.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [1181 ] loss: 1.067 correct: 1266.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [1182 ] loss: 1.067 correct: 1271.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [1183 ] loss: 1.067 correct: 1265.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [1184 ] loss: 1.067 correct: 1271.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [1185 ] loss: 1.067 correct: 1266.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [1186 ] loss: 1.067 correct: 1271.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [1187 ] loss: 1.067 correct: 1265.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [1188 ] loss: 1.067 correct: 1272.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [1189 ] loss: 1.067 correct: 1270.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [1190 ] loss: 1.067 correct: 1273.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [1191 ] loss: 1.067 correct: 1267.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [1192 ] loss: 1.067 correct: 1277.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [1193 ] loss: 1.067 correct: 1269.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [1194 ] loss: 1.067 correct: 1276.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1195 ] loss: 1.067 correct: 1265.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [1196 ] loss: 1.067 correct: 1282.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [1197 ] loss: 1.067 correct: 1257.000, total: 3000.000, accuracy: 0.419\n",
            "training epoch: [1198 ] loss: 1.067 correct: 1289.000, total: 3000.000, accuracy: 0.430\n",
            "training epoch: [1199 ] loss: 1.067 correct: 1258.000, total: 3000.000, accuracy: 0.419\n",
            "training epoch: [1200 ] loss: 1.067 correct: 1282.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [1201 ] loss: 1.067 correct: 1257.000, total: 3000.000, accuracy: 0.419\n",
            "training epoch: [1202 ] loss: 1.067 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1203 ] loss: 1.067 correct: 1252.000, total: 3000.000, accuracy: 0.417\n",
            "training epoch: [1204 ] loss: 1.067 correct: 1290.000, total: 3000.000, accuracy: 0.430\n",
            "training epoch: [1205 ] loss: 1.067 correct: 1253.000, total: 3000.000, accuracy: 0.418\n",
            "training epoch: [1206 ] loss: 1.067 correct: 1284.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [1207 ] loss: 1.067 correct: 1256.000, total: 3000.000, accuracy: 0.419\n",
            "training epoch: [1208 ] loss: 1.067 correct: 1284.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [1209 ] loss: 1.067 correct: 1262.000, total: 3000.000, accuracy: 0.421\n",
            "training epoch: [1210 ] loss: 1.067 correct: 1278.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [1211 ] loss: 1.067 correct: 1267.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [1212 ] loss: 1.067 correct: 1275.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1213 ] loss: 1.067 correct: 1268.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [1214 ] loss: 1.067 correct: 1273.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [1215 ] loss: 1.067 correct: 1271.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [1216 ] loss: 1.067 correct: 1273.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [1217 ] loss: 1.067 correct: 1274.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1218 ] loss: 1.067 correct: 1270.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [1219 ] loss: 1.067 correct: 1273.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [1220 ] loss: 1.067 correct: 1269.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [1221 ] loss: 1.067 correct: 1274.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1222 ] loss: 1.067 correct: 1269.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [1223 ] loss: 1.067 correct: 1274.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1224 ] loss: 1.067 correct: 1269.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [1225 ] loss: 1.067 correct: 1274.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1226 ] loss: 1.067 correct: 1268.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [1227 ] loss: 1.067 correct: 1274.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1228 ] loss: 1.067 correct: 1270.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [1229 ] loss: 1.067 correct: 1275.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1230 ] loss: 1.067 correct: 1269.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [1231 ] loss: 1.067 correct: 1272.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [1232 ] loss: 1.067 correct: 1270.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [1233 ] loss: 1.067 correct: 1270.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [1234 ] loss: 1.067 correct: 1274.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1235 ] loss: 1.067 correct: 1269.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [1236 ] loss: 1.067 correct: 1275.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1237 ] loss: 1.067 correct: 1268.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [1238 ] loss: 1.067 correct: 1276.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1239 ] loss: 1.067 correct: 1267.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [1240 ] loss: 1.067 correct: 1279.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [1241 ] loss: 1.067 correct: 1267.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [1242 ] loss: 1.067 correct: 1279.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [1243 ] loss: 1.067 correct: 1266.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [1244 ] loss: 1.067 correct: 1279.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [1245 ] loss: 1.067 correct: 1268.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [1246 ] loss: 1.067 correct: 1275.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1247 ] loss: 1.067 correct: 1266.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [1248 ] loss: 1.067 correct: 1277.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [1249 ] loss: 1.067 correct: 1268.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [1250 ] loss: 1.067 correct: 1278.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [1251 ] loss: 1.067 correct: 1266.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [1252 ] loss: 1.067 correct: 1279.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [1253 ] loss: 1.067 correct: 1267.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [1254 ] loss: 1.067 correct: 1279.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [1255 ] loss: 1.067 correct: 1270.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [1256 ] loss: 1.067 correct: 1278.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [1257 ] loss: 1.067 correct: 1266.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [1258 ] loss: 1.067 correct: 1280.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [1259 ] loss: 1.067 correct: 1264.000, total: 3000.000, accuracy: 0.421\n",
            "training epoch: [1260 ] loss: 1.067 correct: 1282.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [1261 ] loss: 1.067 correct: 1258.000, total: 3000.000, accuracy: 0.419\n",
            "training epoch: [1262 ] loss: 1.067 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1263 ] loss: 1.067 correct: 1255.000, total: 3000.000, accuracy: 0.418\n",
            "training epoch: [1264 ] loss: 1.067 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1265 ] loss: 1.067 correct: 1258.000, total: 3000.000, accuracy: 0.419\n",
            "training epoch: [1266 ] loss: 1.067 correct: 1284.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [1267 ] loss: 1.067 correct: 1255.000, total: 3000.000, accuracy: 0.418\n",
            "training epoch: [1268 ] loss: 1.067 correct: 1283.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [1269 ] loss: 1.067 correct: 1255.000, total: 3000.000, accuracy: 0.418\n",
            "training epoch: [1270 ] loss: 1.067 correct: 1282.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [1271 ] loss: 1.067 correct: 1256.000, total: 3000.000, accuracy: 0.419\n",
            "training epoch: [1272 ] loss: 1.067 correct: 1282.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [1273 ] loss: 1.067 correct: 1260.000, total: 3000.000, accuracy: 0.420\n",
            "training epoch: [1274 ] loss: 1.067 correct: 1277.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [1275 ] loss: 1.067 correct: 1264.000, total: 3000.000, accuracy: 0.421\n",
            "training epoch: [1276 ] loss: 1.067 correct: 1276.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1277 ] loss: 1.067 correct: 1264.000, total: 3000.000, accuracy: 0.421\n",
            "training epoch: [1278 ] loss: 1.067 correct: 1280.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [1279 ] loss: 1.067 correct: 1267.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [1280 ] loss: 1.067 correct: 1276.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1281 ] loss: 1.067 correct: 1266.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [1282 ] loss: 1.067 correct: 1274.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1283 ] loss: 1.067 correct: 1267.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [1284 ] loss: 1.067 correct: 1274.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1285 ] loss: 1.067 correct: 1269.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [1286 ] loss: 1.067 correct: 1273.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [1287 ] loss: 1.067 correct: 1269.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [1288 ] loss: 1.067 correct: 1274.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1289 ] loss: 1.067 correct: 1271.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [1290 ] loss: 1.067 correct: 1274.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1291 ] loss: 1.067 correct: 1272.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [1292 ] loss: 1.067 correct: 1275.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1293 ] loss: 1.067 correct: 1270.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [1294 ] loss: 1.067 correct: 1274.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1295 ] loss: 1.067 correct: 1271.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [1296 ] loss: 1.067 correct: 1272.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [1297 ] loss: 1.067 correct: 1271.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [1298 ] loss: 1.067 correct: 1274.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1299 ] loss: 1.067 correct: 1270.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [1300 ] loss: 1.067 correct: 1274.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1301 ] loss: 1.067 correct: 1270.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [1302 ] loss: 1.067 correct: 1272.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [1303 ] loss: 1.067 correct: 1271.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [1304 ] loss: 1.067 correct: 1271.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [1305 ] loss: 1.067 correct: 1273.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [1306 ] loss: 1.067 correct: 1268.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [1307 ] loss: 1.067 correct: 1273.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [1308 ] loss: 1.067 correct: 1266.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [1309 ] loss: 1.067 correct: 1279.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [1310 ] loss: 1.067 correct: 1265.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [1311 ] loss: 1.067 correct: 1279.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [1312 ] loss: 1.067 correct: 1262.000, total: 3000.000, accuracy: 0.421\n",
            "training epoch: [1313 ] loss: 1.067 correct: 1282.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [1314 ] loss: 1.067 correct: 1256.000, total: 3000.000, accuracy: 0.419\n",
            "training epoch: [1315 ] loss: 1.067 correct: 1284.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [1316 ] loss: 1.067 correct: 1251.000, total: 3000.000, accuracy: 0.417\n",
            "training epoch: [1317 ] loss: 1.067 correct: 1289.000, total: 3000.000, accuracy: 0.430\n",
            "training epoch: [1318 ] loss: 1.067 correct: 1252.000, total: 3000.000, accuracy: 0.417\n",
            "training epoch: [1319 ] loss: 1.067 correct: 1289.000, total: 3000.000, accuracy: 0.430\n",
            "training epoch: [1320 ] loss: 1.067 correct: 1251.000, total: 3000.000, accuracy: 0.417\n",
            "training epoch: [1321 ] loss: 1.067 correct: 1289.000, total: 3000.000, accuracy: 0.430\n",
            "training epoch: [1322 ] loss: 1.067 correct: 1252.000, total: 3000.000, accuracy: 0.417\n",
            "training epoch: [1323 ] loss: 1.067 correct: 1290.000, total: 3000.000, accuracy: 0.430\n",
            "training epoch: [1324 ] loss: 1.067 correct: 1256.000, total: 3000.000, accuracy: 0.419\n",
            "training epoch: [1325 ] loss: 1.067 correct: 1290.000, total: 3000.000, accuracy: 0.430\n",
            "training epoch: [1326 ] loss: 1.067 correct: 1256.000, total: 3000.000, accuracy: 0.419\n",
            "training epoch: [1327 ] loss: 1.067 correct: 1285.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [1328 ] loss: 1.067 correct: 1258.000, total: 3000.000, accuracy: 0.419\n",
            "training epoch: [1329 ] loss: 1.067 correct: 1281.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [1330 ] loss: 1.067 correct: 1263.000, total: 3000.000, accuracy: 0.421\n",
            "training epoch: [1331 ] loss: 1.067 correct: 1282.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [1332 ] loss: 1.067 correct: 1265.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [1333 ] loss: 1.066 correct: 1281.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [1334 ] loss: 1.067 correct: 1266.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [1335 ] loss: 1.066 correct: 1281.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [1336 ] loss: 1.066 correct: 1265.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [1337 ] loss: 1.066 correct: 1279.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [1338 ] loss: 1.066 correct: 1268.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [1339 ] loss: 1.066 correct: 1276.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1340 ] loss: 1.066 correct: 1269.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [1341 ] loss: 1.066 correct: 1275.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1342 ] loss: 1.066 correct: 1269.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [1343 ] loss: 1.066 correct: 1275.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1344 ] loss: 1.066 correct: 1270.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [1345 ] loss: 1.066 correct: 1273.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [1346 ] loss: 1.066 correct: 1270.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [1347 ] loss: 1.066 correct: 1276.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1348 ] loss: 1.066 correct: 1270.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [1349 ] loss: 1.066 correct: 1276.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1350 ] loss: 1.066 correct: 1270.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [1351 ] loss: 1.066 correct: 1276.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1352 ] loss: 1.066 correct: 1270.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [1353 ] loss: 1.066 correct: 1276.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1354 ] loss: 1.066 correct: 1272.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [1355 ] loss: 1.066 correct: 1275.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1356 ] loss: 1.066 correct: 1272.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [1357 ] loss: 1.066 correct: 1274.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1358 ] loss: 1.066 correct: 1273.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [1359 ] loss: 1.066 correct: 1274.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1360 ] loss: 1.066 correct: 1273.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [1361 ] loss: 1.066 correct: 1274.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1362 ] loss: 1.066 correct: 1273.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [1363 ] loss: 1.066 correct: 1275.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1364 ] loss: 1.066 correct: 1272.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [1365 ] loss: 1.066 correct: 1274.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1366 ] loss: 1.066 correct: 1274.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1367 ] loss: 1.066 correct: 1277.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [1368 ] loss: 1.066 correct: 1276.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1369 ] loss: 1.066 correct: 1279.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [1370 ] loss: 1.066 correct: 1274.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1371 ] loss: 1.066 correct: 1277.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [1372 ] loss: 1.066 correct: 1275.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1373 ] loss: 1.066 correct: 1278.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [1374 ] loss: 1.066 correct: 1271.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [1375 ] loss: 1.066 correct: 1278.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [1376 ] loss: 1.066 correct: 1270.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [1377 ] loss: 1.066 correct: 1279.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [1378 ] loss: 1.066 correct: 1267.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [1379 ] loss: 1.066 correct: 1278.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [1380 ] loss: 1.066 correct: 1266.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [1381 ] loss: 1.066 correct: 1280.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [1382 ] loss: 1.066 correct: 1266.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [1383 ] loss: 1.066 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1384 ] loss: 1.066 correct: 1257.000, total: 3000.000, accuracy: 0.419\n",
            "training epoch: [1385 ] loss: 1.066 correct: 1289.000, total: 3000.000, accuracy: 0.430\n",
            "training epoch: [1386 ] loss: 1.066 correct: 1250.000, total: 3000.000, accuracy: 0.417\n",
            "training epoch: [1387 ] loss: 1.066 correct: 1290.000, total: 3000.000, accuracy: 0.430\n",
            "training epoch: [1388 ] loss: 1.066 correct: 1250.000, total: 3000.000, accuracy: 0.417\n",
            "training epoch: [1389 ] loss: 1.066 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1390 ] loss: 1.066 correct: 1255.000, total: 3000.000, accuracy: 0.418\n",
            "training epoch: [1391 ] loss: 1.066 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1392 ] loss: 1.066 correct: 1253.000, total: 3000.000, accuracy: 0.418\n",
            "training epoch: [1393 ] loss: 1.066 correct: 1290.000, total: 3000.000, accuracy: 0.430\n",
            "training epoch: [1394 ] loss: 1.066 correct: 1252.000, total: 3000.000, accuracy: 0.417\n",
            "training epoch: [1395 ] loss: 1.066 correct: 1290.000, total: 3000.000, accuracy: 0.430\n",
            "training epoch: [1396 ] loss: 1.066 correct: 1259.000, total: 3000.000, accuracy: 0.420\n",
            "training epoch: [1397 ] loss: 1.066 correct: 1283.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [1398 ] loss: 1.066 correct: 1264.000, total: 3000.000, accuracy: 0.421\n",
            "training epoch: [1399 ] loss: 1.066 correct: 1282.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [1400 ] loss: 1.066 correct: 1269.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [1401 ] loss: 1.066 correct: 1280.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [1402 ] loss: 1.066 correct: 1272.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [1403 ] loss: 1.066 correct: 1280.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [1404 ] loss: 1.066 correct: 1276.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1405 ] loss: 1.066 correct: 1282.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [1406 ] loss: 1.066 correct: 1279.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [1407 ] loss: 1.066 correct: 1279.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [1408 ] loss: 1.066 correct: 1279.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [1409 ] loss: 1.066 correct: 1278.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [1410 ] loss: 1.066 correct: 1279.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [1411 ] loss: 1.066 correct: 1278.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [1412 ] loss: 1.066 correct: 1280.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [1413 ] loss: 1.066 correct: 1279.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [1414 ] loss: 1.066 correct: 1281.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [1415 ] loss: 1.066 correct: 1280.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [1416 ] loss: 1.066 correct: 1280.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [1417 ] loss: 1.066 correct: 1279.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [1418 ] loss: 1.066 correct: 1280.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [1419 ] loss: 1.066 correct: 1281.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [1420 ] loss: 1.066 correct: 1282.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [1421 ] loss: 1.066 correct: 1281.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [1422 ] loss: 1.066 correct: 1281.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [1423 ] loss: 1.066 correct: 1277.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [1424 ] loss: 1.066 correct: 1283.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [1425 ] loss: 1.066 correct: 1277.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [1426 ] loss: 1.066 correct: 1283.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [1427 ] loss: 1.066 correct: 1278.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [1428 ] loss: 1.066 correct: 1276.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1429 ] loss: 1.066 correct: 1279.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [1430 ] loss: 1.066 correct: 1277.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [1431 ] loss: 1.066 correct: 1273.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [1432 ] loss: 1.066 correct: 1277.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [1433 ] loss: 1.066 correct: 1271.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [1434 ] loss: 1.066 correct: 1278.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [1435 ] loss: 1.066 correct: 1269.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [1436 ] loss: 1.066 correct: 1280.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [1437 ] loss: 1.066 correct: 1269.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [1438 ] loss: 1.066 correct: 1283.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [1439 ] loss: 1.066 correct: 1268.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [1440 ] loss: 1.066 correct: 1285.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [1441 ] loss: 1.066 correct: 1264.000, total: 3000.000, accuracy: 0.421\n",
            "training epoch: [1442 ] loss: 1.066 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1443 ] loss: 1.066 correct: 1262.000, total: 3000.000, accuracy: 0.421\n",
            "training epoch: [1444 ] loss: 1.065 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1445 ] loss: 1.065 correct: 1262.000, total: 3000.000, accuracy: 0.421\n",
            "training epoch: [1446 ] loss: 1.065 correct: 1289.000, total: 3000.000, accuracy: 0.430\n",
            "training epoch: [1447 ] loss: 1.065 correct: 1259.000, total: 3000.000, accuracy: 0.420\n",
            "training epoch: [1448 ] loss: 1.065 correct: 1290.000, total: 3000.000, accuracy: 0.430\n",
            "training epoch: [1449 ] loss: 1.065 correct: 1256.000, total: 3000.000, accuracy: 0.419\n",
            "training epoch: [1450 ] loss: 1.065 correct: 1290.000, total: 3000.000, accuracy: 0.430\n",
            "training epoch: [1451 ] loss: 1.065 correct: 1257.000, total: 3000.000, accuracy: 0.419\n",
            "training epoch: [1452 ] loss: 1.065 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1453 ] loss: 1.065 correct: 1257.000, total: 3000.000, accuracy: 0.419\n",
            "training epoch: [1454 ] loss: 1.065 correct: 1289.000, total: 3000.000, accuracy: 0.430\n",
            "training epoch: [1455 ] loss: 1.065 correct: 1256.000, total: 3000.000, accuracy: 0.419\n",
            "training epoch: [1456 ] loss: 1.065 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1457 ] loss: 1.065 correct: 1260.000, total: 3000.000, accuracy: 0.420\n",
            "training epoch: [1458 ] loss: 1.065 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1459 ] loss: 1.065 correct: 1259.000, total: 3000.000, accuracy: 0.420\n",
            "training epoch: [1460 ] loss: 1.065 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1461 ] loss: 1.065 correct: 1265.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [1462 ] loss: 1.065 correct: 1284.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [1463 ] loss: 1.065 correct: 1265.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [1464 ] loss: 1.065 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1465 ] loss: 1.065 correct: 1267.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [1466 ] loss: 1.065 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1467 ] loss: 1.065 correct: 1268.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [1468 ] loss: 1.065 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1469 ] loss: 1.065 correct: 1270.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [1470 ] loss: 1.065 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1471 ] loss: 1.065 correct: 1271.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [1472 ] loss: 1.065 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1473 ] loss: 1.065 correct: 1273.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [1474 ] loss: 1.065 correct: 1285.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [1475 ] loss: 1.065 correct: 1274.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1476 ] loss: 1.065 correct: 1284.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [1477 ] loss: 1.065 correct: 1276.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1478 ] loss: 1.065 correct: 1285.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [1479 ] loss: 1.065 correct: 1276.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1480 ] loss: 1.065 correct: 1285.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [1481 ] loss: 1.065 correct: 1276.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1482 ] loss: 1.065 correct: 1285.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [1483 ] loss: 1.065 correct: 1276.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1484 ] loss: 1.065 correct: 1285.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [1485 ] loss: 1.065 correct: 1276.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1486 ] loss: 1.065 correct: 1285.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [1487 ] loss: 1.065 correct: 1276.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1488 ] loss: 1.065 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1489 ] loss: 1.065 correct: 1276.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1490 ] loss: 1.065 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1491 ] loss: 1.065 correct: 1275.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1492 ] loss: 1.065 correct: 1285.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [1493 ] loss: 1.065 correct: 1275.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1494 ] loss: 1.065 correct: 1285.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [1495 ] loss: 1.065 correct: 1275.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1496 ] loss: 1.065 correct: 1285.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [1497 ] loss: 1.065 correct: 1275.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1498 ] loss: 1.065 correct: 1285.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [1499 ] loss: 1.065 correct: 1274.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1500 ] loss: 1.065 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1501 ] loss: 1.065 correct: 1274.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1502 ] loss: 1.065 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1503 ] loss: 1.065 correct: 1274.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1504 ] loss: 1.065 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1505 ] loss: 1.065 correct: 1272.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [1506 ] loss: 1.065 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1507 ] loss: 1.065 correct: 1270.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [1508 ] loss: 1.065 correct: 1289.000, total: 3000.000, accuracy: 0.430\n",
            "training epoch: [1509 ] loss: 1.065 correct: 1267.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [1510 ] loss: 1.065 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1511 ] loss: 1.065 correct: 1264.000, total: 3000.000, accuracy: 0.421\n",
            "training epoch: [1512 ] loss: 1.065 correct: 1293.000, total: 3000.000, accuracy: 0.431\n",
            "training epoch: [1513 ] loss: 1.065 correct: 1255.000, total: 3000.000, accuracy: 0.418\n",
            "training epoch: [1514 ] loss: 1.065 correct: 1297.000, total: 3000.000, accuracy: 0.432\n",
            "training epoch: [1515 ] loss: 1.065 correct: 1262.000, total: 3000.000, accuracy: 0.421\n",
            "training epoch: [1516 ] loss: 1.065 correct: 1296.000, total: 3000.000, accuracy: 0.432\n",
            "training epoch: [1517 ] loss: 1.065 correct: 1259.000, total: 3000.000, accuracy: 0.420\n",
            "training epoch: [1518 ] loss: 1.065 correct: 1292.000, total: 3000.000, accuracy: 0.431\n",
            "training epoch: [1519 ] loss: 1.065 correct: 1258.000, total: 3000.000, accuracy: 0.419\n",
            "training epoch: [1520 ] loss: 1.065 correct: 1292.000, total: 3000.000, accuracy: 0.431\n",
            "training epoch: [1521 ] loss: 1.065 correct: 1266.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [1522 ] loss: 1.065 correct: 1291.000, total: 3000.000, accuracy: 0.430\n",
            "training epoch: [1523 ] loss: 1.065 correct: 1264.000, total: 3000.000, accuracy: 0.421\n",
            "training epoch: [1524 ] loss: 1.065 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1525 ] loss: 1.065 correct: 1272.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [1526 ] loss: 1.065 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1527 ] loss: 1.065 correct: 1273.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [1528 ] loss: 1.065 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1529 ] loss: 1.065 correct: 1279.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [1530 ] loss: 1.065 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1531 ] loss: 1.065 correct: 1277.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [1532 ] loss: 1.065 correct: 1290.000, total: 3000.000, accuracy: 0.430\n",
            "training epoch: [1533 ] loss: 1.065 correct: 1277.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [1534 ] loss: 1.065 correct: 1290.000, total: 3000.000, accuracy: 0.430\n",
            "training epoch: [1535 ] loss: 1.065 correct: 1279.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [1536 ] loss: 1.065 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1537 ] loss: 1.065 correct: 1281.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [1538 ] loss: 1.065 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1539 ] loss: 1.065 correct: 1281.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [1540 ] loss: 1.065 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1541 ] loss: 1.065 correct: 1281.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [1542 ] loss: 1.065 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1543 ] loss: 1.065 correct: 1280.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [1544 ] loss: 1.065 correct: 1284.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [1545 ] loss: 1.065 correct: 1280.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [1546 ] loss: 1.065 correct: 1280.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [1547 ] loss: 1.065 correct: 1281.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [1548 ] loss: 1.065 correct: 1277.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [1549 ] loss: 1.065 correct: 1283.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [1550 ] loss: 1.065 correct: 1276.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1551 ] loss: 1.065 correct: 1285.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [1552 ] loss: 1.065 correct: 1276.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1553 ] loss: 1.065 correct: 1284.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [1554 ] loss: 1.065 correct: 1274.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1555 ] loss: 1.065 correct: 1282.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [1556 ] loss: 1.065 correct: 1274.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1557 ] loss: 1.065 correct: 1283.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [1558 ] loss: 1.065 correct: 1273.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [1559 ] loss: 1.065 correct: 1284.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [1560 ] loss: 1.065 correct: 1273.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [1561 ] loss: 1.065 correct: 1283.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [1562 ] loss: 1.065 correct: 1274.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1563 ] loss: 1.065 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1564 ] loss: 1.065 correct: 1274.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1565 ] loss: 1.064 correct: 1289.000, total: 3000.000, accuracy: 0.430\n",
            "training epoch: [1566 ] loss: 1.064 correct: 1272.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [1567 ] loss: 1.064 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1568 ] loss: 1.064 correct: 1269.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [1569 ] loss: 1.064 correct: 1289.000, total: 3000.000, accuracy: 0.430\n",
            "training epoch: [1570 ] loss: 1.064 correct: 1272.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [1571 ] loss: 1.064 correct: 1289.000, total: 3000.000, accuracy: 0.430\n",
            "training epoch: [1572 ] loss: 1.064 correct: 1272.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [1573 ] loss: 1.064 correct: 1291.000, total: 3000.000, accuracy: 0.430\n",
            "training epoch: [1574 ] loss: 1.064 correct: 1271.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [1575 ] loss: 1.064 correct: 1294.000, total: 3000.000, accuracy: 0.431\n",
            "training epoch: [1576 ] loss: 1.064 correct: 1268.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [1577 ] loss: 1.064 correct: 1294.000, total: 3000.000, accuracy: 0.431\n",
            "training epoch: [1578 ] loss: 1.064 correct: 1264.000, total: 3000.000, accuracy: 0.421\n",
            "training epoch: [1579 ] loss: 1.064 correct: 1296.000, total: 3000.000, accuracy: 0.432\n",
            "training epoch: [1580 ] loss: 1.064 correct: 1263.000, total: 3000.000, accuracy: 0.421\n",
            "training epoch: [1581 ] loss: 1.064 correct: 1292.000, total: 3000.000, accuracy: 0.431\n",
            "training epoch: [1582 ] loss: 1.064 correct: 1259.000, total: 3000.000, accuracy: 0.420\n",
            "training epoch: [1583 ] loss: 1.064 correct: 1293.000, total: 3000.000, accuracy: 0.431\n",
            "training epoch: [1584 ] loss: 1.064 correct: 1266.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [1585 ] loss: 1.064 correct: 1294.000, total: 3000.000, accuracy: 0.431\n",
            "training epoch: [1586 ] loss: 1.064 correct: 1264.000, total: 3000.000, accuracy: 0.421\n",
            "training epoch: [1587 ] loss: 1.064 correct: 1293.000, total: 3000.000, accuracy: 0.431\n",
            "training epoch: [1588 ] loss: 1.064 correct: 1267.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [1589 ] loss: 1.064 correct: 1291.000, total: 3000.000, accuracy: 0.430\n",
            "training epoch: [1590 ] loss: 1.064 correct: 1266.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [1591 ] loss: 1.064 correct: 1289.000, total: 3000.000, accuracy: 0.430\n",
            "training epoch: [1592 ] loss: 1.064 correct: 1272.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [1593 ] loss: 1.064 correct: 1289.000, total: 3000.000, accuracy: 0.430\n",
            "training epoch: [1594 ] loss: 1.064 correct: 1272.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [1595 ] loss: 1.064 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1596 ] loss: 1.064 correct: 1275.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1597 ] loss: 1.064 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1598 ] loss: 1.064 correct: 1277.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [1599 ] loss: 1.064 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1600 ] loss: 1.064 correct: 1278.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [1601 ] loss: 1.064 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1602 ] loss: 1.064 correct: 1279.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [1603 ] loss: 1.064 correct: 1290.000, total: 3000.000, accuracy: 0.430\n",
            "training epoch: [1604 ] loss: 1.064 correct: 1279.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [1605 ] loss: 1.064 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1606 ] loss: 1.064 correct: 1280.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [1607 ] loss: 1.064 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1608 ] loss: 1.064 correct: 1279.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [1609 ] loss: 1.064 correct: 1289.000, total: 3000.000, accuracy: 0.430\n",
            "training epoch: [1610 ] loss: 1.064 correct: 1277.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [1611 ] loss: 1.064 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1612 ] loss: 1.064 correct: 1277.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [1613 ] loss: 1.064 correct: 1285.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [1614 ] loss: 1.064 correct: 1278.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [1615 ] loss: 1.064 correct: 1289.000, total: 3000.000, accuracy: 0.430\n",
            "training epoch: [1616 ] loss: 1.064 correct: 1280.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [1617 ] loss: 1.064 correct: 1291.000, total: 3000.000, accuracy: 0.430\n",
            "training epoch: [1618 ] loss: 1.064 correct: 1277.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [1619 ] loss: 1.064 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1620 ] loss: 1.064 correct: 1276.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1621 ] loss: 1.064 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1622 ] loss: 1.064 correct: 1276.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1623 ] loss: 1.064 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1624 ] loss: 1.064 correct: 1277.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [1625 ] loss: 1.064 correct: 1284.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [1626 ] loss: 1.064 correct: 1280.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [1627 ] loss: 1.064 correct: 1284.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [1628 ] loss: 1.064 correct: 1278.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [1629 ] loss: 1.064 correct: 1283.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [1630 ] loss: 1.064 correct: 1279.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [1631 ] loss: 1.064 correct: 1281.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [1632 ] loss: 1.064 correct: 1280.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [1633 ] loss: 1.064 correct: 1281.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [1634 ] loss: 1.064 correct: 1282.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [1635 ] loss: 1.064 correct: 1280.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [1636 ] loss: 1.064 correct: 1283.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [1637 ] loss: 1.064 correct: 1280.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [1638 ] loss: 1.064 correct: 1283.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [1639 ] loss: 1.064 correct: 1280.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [1640 ] loss: 1.064 correct: 1285.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [1641 ] loss: 1.064 correct: 1279.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [1642 ] loss: 1.064 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1643 ] loss: 1.064 correct: 1278.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [1644 ] loss: 1.064 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1645 ] loss: 1.064 correct: 1276.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1646 ] loss: 1.064 correct: 1289.000, total: 3000.000, accuracy: 0.430\n",
            "training epoch: [1647 ] loss: 1.064 correct: 1271.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [1648 ] loss: 1.064 correct: 1290.000, total: 3000.000, accuracy: 0.430\n",
            "training epoch: [1649 ] loss: 1.064 correct: 1268.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [1650 ] loss: 1.064 correct: 1297.000, total: 3000.000, accuracy: 0.432\n",
            "training epoch: [1651 ] loss: 1.064 correct: 1263.000, total: 3000.000, accuracy: 0.421\n",
            "training epoch: [1652 ] loss: 1.064 correct: 1296.000, total: 3000.000, accuracy: 0.432\n",
            "training epoch: [1653 ] loss: 1.064 correct: 1266.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [1654 ] loss: 1.064 correct: 1297.000, total: 3000.000, accuracy: 0.432\n",
            "training epoch: [1655 ] loss: 1.064 correct: 1260.000, total: 3000.000, accuracy: 0.420\n",
            "training epoch: [1656 ] loss: 1.064 correct: 1297.000, total: 3000.000, accuracy: 0.432\n",
            "training epoch: [1657 ] loss: 1.064 correct: 1261.000, total: 3000.000, accuracy: 0.420\n",
            "training epoch: [1658 ] loss: 1.064 correct: 1296.000, total: 3000.000, accuracy: 0.432\n",
            "training epoch: [1659 ] loss: 1.064 correct: 1266.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [1660 ] loss: 1.064 correct: 1291.000, total: 3000.000, accuracy: 0.430\n",
            "training epoch: [1661 ] loss: 1.064 correct: 1267.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [1662 ] loss: 1.064 correct: 1290.000, total: 3000.000, accuracy: 0.430\n",
            "training epoch: [1663 ] loss: 1.064 correct: 1273.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [1664 ] loss: 1.064 correct: 1292.000, total: 3000.000, accuracy: 0.431\n",
            "training epoch: [1665 ] loss: 1.064 correct: 1274.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1666 ] loss: 1.064 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1667 ] loss: 1.064 correct: 1279.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [1668 ] loss: 1.064 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1669 ] loss: 1.064 correct: 1281.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [1670 ] loss: 1.064 correct: 1285.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [1671 ] loss: 1.064 correct: 1281.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [1672 ] loss: 1.064 correct: 1284.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [1673 ] loss: 1.064 correct: 1281.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [1674 ] loss: 1.064 correct: 1283.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [1675 ] loss: 1.064 correct: 1281.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [1676 ] loss: 1.064 correct: 1282.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [1677 ] loss: 1.064 correct: 1281.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [1678 ] loss: 1.064 correct: 1282.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [1679 ] loss: 1.064 correct: 1282.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [1680 ] loss: 1.064 correct: 1283.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [1681 ] loss: 1.064 correct: 1282.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [1682 ] loss: 1.064 correct: 1284.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [1683 ] loss: 1.064 correct: 1281.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [1684 ] loss: 1.064 correct: 1285.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [1685 ] loss: 1.064 correct: 1281.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [1686 ] loss: 1.064 correct: 1285.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [1687 ] loss: 1.064 correct: 1280.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [1688 ] loss: 1.064 correct: 1290.000, total: 3000.000, accuracy: 0.430\n",
            "training epoch: [1689 ] loss: 1.064 correct: 1278.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [1690 ] loss: 1.064 correct: 1290.000, total: 3000.000, accuracy: 0.430\n",
            "training epoch: [1691 ] loss: 1.064 correct: 1272.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [1692 ] loss: 1.064 correct: 1290.000, total: 3000.000, accuracy: 0.430\n",
            "training epoch: [1693 ] loss: 1.064 correct: 1275.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1694 ] loss: 1.064 correct: 1289.000, total: 3000.000, accuracy: 0.430\n",
            "training epoch: [1695 ] loss: 1.064 correct: 1271.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [1696 ] loss: 1.064 correct: 1290.000, total: 3000.000, accuracy: 0.430\n",
            "training epoch: [1697 ] loss: 1.064 correct: 1272.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [1698 ] loss: 1.064 correct: 1290.000, total: 3000.000, accuracy: 0.430\n",
            "training epoch: [1699 ] loss: 1.064 correct: 1273.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [1700 ] loss: 1.064 correct: 1293.000, total: 3000.000, accuracy: 0.431\n",
            "training epoch: [1701 ] loss: 1.064 correct: 1273.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [1702 ] loss: 1.064 correct: 1291.000, total: 3000.000, accuracy: 0.430\n",
            "training epoch: [1703 ] loss: 1.064 correct: 1274.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1704 ] loss: 1.064 correct: 1292.000, total: 3000.000, accuracy: 0.431\n",
            "training epoch: [1705 ] loss: 1.064 correct: 1275.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1706 ] loss: 1.064 correct: 1294.000, total: 3000.000, accuracy: 0.431\n",
            "training epoch: [1707 ] loss: 1.064 correct: 1277.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [1708 ] loss: 1.064 correct: 1294.000, total: 3000.000, accuracy: 0.431\n",
            "training epoch: [1709 ] loss: 1.064 correct: 1276.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1710 ] loss: 1.064 correct: 1292.000, total: 3000.000, accuracy: 0.431\n",
            "training epoch: [1711 ] loss: 1.064 correct: 1276.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1712 ] loss: 1.064 correct: 1293.000, total: 3000.000, accuracy: 0.431\n",
            "training epoch: [1713 ] loss: 1.064 correct: 1273.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [1714 ] loss: 1.064 correct: 1291.000, total: 3000.000, accuracy: 0.430\n",
            "training epoch: [1715 ] loss: 1.064 correct: 1270.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [1716 ] loss: 1.064 correct: 1291.000, total: 3000.000, accuracy: 0.430\n",
            "training epoch: [1717 ] loss: 1.064 correct: 1269.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [1718 ] loss: 1.064 correct: 1291.000, total: 3000.000, accuracy: 0.430\n",
            "training epoch: [1719 ] loss: 1.064 correct: 1270.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [1720 ] loss: 1.064 correct: 1294.000, total: 3000.000, accuracy: 0.431\n",
            "training epoch: [1721 ] loss: 1.064 correct: 1265.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [1722 ] loss: 1.064 correct: 1293.000, total: 3000.000, accuracy: 0.431\n",
            "training epoch: [1723 ] loss: 1.064 correct: 1262.000, total: 3000.000, accuracy: 0.421\n",
            "training epoch: [1724 ] loss: 1.064 correct: 1293.000, total: 3000.000, accuracy: 0.431\n",
            "training epoch: [1725 ] loss: 1.064 correct: 1262.000, total: 3000.000, accuracy: 0.421\n",
            "training epoch: [1726 ] loss: 1.064 correct: 1294.000, total: 3000.000, accuracy: 0.431\n",
            "training epoch: [1727 ] loss: 1.064 correct: 1263.000, total: 3000.000, accuracy: 0.421\n",
            "training epoch: [1728 ] loss: 1.064 correct: 1294.000, total: 3000.000, accuracy: 0.431\n",
            "training epoch: [1729 ] loss: 1.064 correct: 1263.000, total: 3000.000, accuracy: 0.421\n",
            "training epoch: [1730 ] loss: 1.064 correct: 1295.000, total: 3000.000, accuracy: 0.432\n",
            "training epoch: [1731 ] loss: 1.064 correct: 1266.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [1732 ] loss: 1.064 correct: 1293.000, total: 3000.000, accuracy: 0.431\n",
            "training epoch: [1733 ] loss: 1.064 correct: 1266.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [1734 ] loss: 1.064 correct: 1292.000, total: 3000.000, accuracy: 0.431\n",
            "training epoch: [1735 ] loss: 1.064 correct: 1270.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [1736 ] loss: 1.064 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1737 ] loss: 1.064 correct: 1271.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [1738 ] loss: 1.064 correct: 1289.000, total: 3000.000, accuracy: 0.430\n",
            "training epoch: [1739 ] loss: 1.064 correct: 1272.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [1740 ] loss: 1.064 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1741 ] loss: 1.064 correct: 1274.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1742 ] loss: 1.064 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1743 ] loss: 1.064 correct: 1274.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1744 ] loss: 1.064 correct: 1285.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [1745 ] loss: 1.064 correct: 1276.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1746 ] loss: 1.064 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1747 ] loss: 1.064 correct: 1276.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1748 ] loss: 1.064 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1749 ] loss: 1.064 correct: 1276.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1750 ] loss: 1.064 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1751 ] loss: 1.064 correct: 1278.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [1752 ] loss: 1.064 correct: 1289.000, total: 3000.000, accuracy: 0.430\n",
            "training epoch: [1753 ] loss: 1.064 correct: 1279.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [1754 ] loss: 1.064 correct: 1289.000, total: 3000.000, accuracy: 0.430\n",
            "training epoch: [1755 ] loss: 1.064 correct: 1280.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [1756 ] loss: 1.064 correct: 1289.000, total: 3000.000, accuracy: 0.430\n",
            "training epoch: [1757 ] loss: 1.064 correct: 1279.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [1758 ] loss: 1.064 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1759 ] loss: 1.064 correct: 1279.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [1760 ] loss: 1.064 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1761 ] loss: 1.064 correct: 1279.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [1762 ] loss: 1.064 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1763 ] loss: 1.064 correct: 1280.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [1764 ] loss: 1.064 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1765 ] loss: 1.064 correct: 1282.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [1766 ] loss: 1.064 correct: 1285.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [1767 ] loss: 1.064 correct: 1282.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [1768 ] loss: 1.064 correct: 1278.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [1769 ] loss: 1.064 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1770 ] loss: 1.064 correct: 1274.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1771 ] loss: 1.064 correct: 1292.000, total: 3000.000, accuracy: 0.431\n",
            "training epoch: [1772 ] loss: 1.064 correct: 1270.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [1773 ] loss: 1.064 correct: 1295.000, total: 3000.000, accuracy: 0.432\n",
            "training epoch: [1774 ] loss: 1.064 correct: 1270.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [1775 ] loss: 1.064 correct: 1298.000, total: 3000.000, accuracy: 0.433\n",
            "training epoch: [1776 ] loss: 1.064 correct: 1269.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [1777 ] loss: 1.064 correct: 1294.000, total: 3000.000, accuracy: 0.431\n",
            "training epoch: [1778 ] loss: 1.064 correct: 1270.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [1779 ] loss: 1.064 correct: 1292.000, total: 3000.000, accuracy: 0.431\n",
            "training epoch: [1780 ] loss: 1.064 correct: 1272.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [1781 ] loss: 1.064 correct: 1298.000, total: 3000.000, accuracy: 0.433\n",
            "training epoch: [1782 ] loss: 1.064 correct: 1270.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [1783 ] loss: 1.063 correct: 1299.000, total: 3000.000, accuracy: 0.433\n",
            "training epoch: [1784 ] loss: 1.064 correct: 1269.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [1785 ] loss: 1.063 correct: 1297.000, total: 3000.000, accuracy: 0.432\n",
            "training epoch: [1786 ] loss: 1.063 correct: 1268.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [1787 ] loss: 1.063 correct: 1302.000, total: 3000.000, accuracy: 0.434\n",
            "training epoch: [1788 ] loss: 1.063 correct: 1264.000, total: 3000.000, accuracy: 0.421\n",
            "training epoch: [1789 ] loss: 1.063 correct: 1296.000, total: 3000.000, accuracy: 0.432\n",
            "training epoch: [1790 ] loss: 1.063 correct: 1270.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [1791 ] loss: 1.063 correct: 1292.000, total: 3000.000, accuracy: 0.431\n",
            "training epoch: [1792 ] loss: 1.063 correct: 1268.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [1793 ] loss: 1.063 correct: 1292.000, total: 3000.000, accuracy: 0.431\n",
            "training epoch: [1794 ] loss: 1.063 correct: 1270.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [1795 ] loss: 1.063 correct: 1289.000, total: 3000.000, accuracy: 0.430\n",
            "training epoch: [1796 ] loss: 1.063 correct: 1273.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [1797 ] loss: 1.063 correct: 1289.000, total: 3000.000, accuracy: 0.430\n",
            "training epoch: [1798 ] loss: 1.063 correct: 1275.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1799 ] loss: 1.063 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1800 ] loss: 1.063 correct: 1276.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1801 ] loss: 1.063 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1802 ] loss: 1.063 correct: 1276.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1803 ] loss: 1.063 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1804 ] loss: 1.063 correct: 1276.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1805 ] loss: 1.063 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1806 ] loss: 1.063 correct: 1275.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1807 ] loss: 1.063 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1808 ] loss: 1.063 correct: 1275.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1809 ] loss: 1.063 correct: 1290.000, total: 3000.000, accuracy: 0.430\n",
            "training epoch: [1810 ] loss: 1.063 correct: 1277.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [1811 ] loss: 1.063 correct: 1292.000, total: 3000.000, accuracy: 0.431\n",
            "training epoch: [1812 ] loss: 1.063 correct: 1277.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [1813 ] loss: 1.063 correct: 1291.000, total: 3000.000, accuracy: 0.430\n",
            "training epoch: [1814 ] loss: 1.063 correct: 1272.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [1815 ] loss: 1.063 correct: 1292.000, total: 3000.000, accuracy: 0.431\n",
            "training epoch: [1816 ] loss: 1.063 correct: 1270.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [1817 ] loss: 1.063 correct: 1292.000, total: 3000.000, accuracy: 0.431\n",
            "training epoch: [1818 ] loss: 1.063 correct: 1271.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [1819 ] loss: 1.063 correct: 1292.000, total: 3000.000, accuracy: 0.431\n",
            "training epoch: [1820 ] loss: 1.063 correct: 1272.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [1821 ] loss: 1.063 correct: 1292.000, total: 3000.000, accuracy: 0.431\n",
            "training epoch: [1822 ] loss: 1.063 correct: 1271.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [1823 ] loss: 1.063 correct: 1294.000, total: 3000.000, accuracy: 0.431\n",
            "training epoch: [1824 ] loss: 1.063 correct: 1269.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [1825 ] loss: 1.063 correct: 1293.000, total: 3000.000, accuracy: 0.431\n",
            "training epoch: [1826 ] loss: 1.063 correct: 1269.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [1827 ] loss: 1.063 correct: 1294.000, total: 3000.000, accuracy: 0.431\n",
            "training epoch: [1828 ] loss: 1.063 correct: 1269.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [1829 ] loss: 1.063 correct: 1293.000, total: 3000.000, accuracy: 0.431\n",
            "training epoch: [1830 ] loss: 1.063 correct: 1270.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [1831 ] loss: 1.063 correct: 1294.000, total: 3000.000, accuracy: 0.431\n",
            "training epoch: [1832 ] loss: 1.063 correct: 1269.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [1833 ] loss: 1.063 correct: 1296.000, total: 3000.000, accuracy: 0.432\n",
            "training epoch: [1834 ] loss: 1.063 correct: 1270.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [1835 ] loss: 1.063 correct: 1299.000, total: 3000.000, accuracy: 0.433\n",
            "training epoch: [1836 ] loss: 1.063 correct: 1269.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [1837 ] loss: 1.063 correct: 1297.000, total: 3000.000, accuracy: 0.432\n",
            "training epoch: [1838 ] loss: 1.063 correct: 1270.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [1839 ] loss: 1.063 correct: 1298.000, total: 3000.000, accuracy: 0.433\n",
            "training epoch: [1840 ] loss: 1.063 correct: 1270.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [1841 ] loss: 1.063 correct: 1300.000, total: 3000.000, accuracy: 0.433\n",
            "training epoch: [1842 ] loss: 1.063 correct: 1271.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [1843 ] loss: 1.063 correct: 1299.000, total: 3000.000, accuracy: 0.433\n",
            "training epoch: [1844 ] loss: 1.063 correct: 1274.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1845 ] loss: 1.063 correct: 1298.000, total: 3000.000, accuracy: 0.433\n",
            "training epoch: [1846 ] loss: 1.063 correct: 1270.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [1847 ] loss: 1.063 correct: 1294.000, total: 3000.000, accuracy: 0.431\n",
            "training epoch: [1848 ] loss: 1.063 correct: 1273.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [1849 ] loss: 1.063 correct: 1299.000, total: 3000.000, accuracy: 0.433\n",
            "training epoch: [1850 ] loss: 1.063 correct: 1274.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1851 ] loss: 1.063 correct: 1301.000, total: 3000.000, accuracy: 0.434\n",
            "training epoch: [1852 ] loss: 1.063 correct: 1273.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [1853 ] loss: 1.063 correct: 1298.000, total: 3000.000, accuracy: 0.433\n",
            "training epoch: [1854 ] loss: 1.063 correct: 1275.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1855 ] loss: 1.063 correct: 1294.000, total: 3000.000, accuracy: 0.431\n",
            "training epoch: [1856 ] loss: 1.063 correct: 1274.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1857 ] loss: 1.063 correct: 1294.000, total: 3000.000, accuracy: 0.431\n",
            "training epoch: [1858 ] loss: 1.063 correct: 1274.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1859 ] loss: 1.063 correct: 1296.000, total: 3000.000, accuracy: 0.432\n",
            "training epoch: [1860 ] loss: 1.063 correct: 1278.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [1861 ] loss: 1.063 correct: 1295.000, total: 3000.000, accuracy: 0.432\n",
            "training epoch: [1862 ] loss: 1.063 correct: 1277.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [1863 ] loss: 1.063 correct: 1289.000, total: 3000.000, accuracy: 0.430\n",
            "training epoch: [1864 ] loss: 1.063 correct: 1277.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [1865 ] loss: 1.063 correct: 1290.000, total: 3000.000, accuracy: 0.430\n",
            "training epoch: [1866 ] loss: 1.063 correct: 1278.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [1867 ] loss: 1.063 correct: 1291.000, total: 3000.000, accuracy: 0.430\n",
            "training epoch: [1868 ] loss: 1.063 correct: 1282.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [1869 ] loss: 1.063 correct: 1291.000, total: 3000.000, accuracy: 0.430\n",
            "training epoch: [1870 ] loss: 1.063 correct: 1281.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [1871 ] loss: 1.063 correct: 1290.000, total: 3000.000, accuracy: 0.430\n",
            "training epoch: [1872 ] loss: 1.063 correct: 1282.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [1873 ] loss: 1.063 correct: 1290.000, total: 3000.000, accuracy: 0.430\n",
            "training epoch: [1874 ] loss: 1.063 correct: 1285.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [1875 ] loss: 1.063 correct: 1289.000, total: 3000.000, accuracy: 0.430\n",
            "training epoch: [1876 ] loss: 1.063 correct: 1285.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [1877 ] loss: 1.063 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1878 ] loss: 1.063 correct: 1285.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [1879 ] loss: 1.063 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1880 ] loss: 1.063 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1881 ] loss: 1.063 correct: 1283.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [1882 ] loss: 1.063 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1883 ] loss: 1.063 correct: 1278.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [1884 ] loss: 1.063 correct: 1289.000, total: 3000.000, accuracy: 0.430\n",
            "training epoch: [1885 ] loss: 1.063 correct: 1274.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1886 ] loss: 1.063 correct: 1294.000, total: 3000.000, accuracy: 0.431\n",
            "training epoch: [1887 ] loss: 1.063 correct: 1275.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1888 ] loss: 1.063 correct: 1292.000, total: 3000.000, accuracy: 0.431\n",
            "training epoch: [1889 ] loss: 1.063 correct: 1273.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [1890 ] loss: 1.063 correct: 1293.000, total: 3000.000, accuracy: 0.431\n",
            "training epoch: [1891 ] loss: 1.063 correct: 1271.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [1892 ] loss: 1.063 correct: 1300.000, total: 3000.000, accuracy: 0.433\n",
            "training epoch: [1893 ] loss: 1.063 correct: 1268.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [1894 ] loss: 1.063 correct: 1298.000, total: 3000.000, accuracy: 0.433\n",
            "training epoch: [1895 ] loss: 1.063 correct: 1271.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [1896 ] loss: 1.063 correct: 1296.000, total: 3000.000, accuracy: 0.432\n",
            "training epoch: [1897 ] loss: 1.063 correct: 1273.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [1898 ] loss: 1.063 correct: 1299.000, total: 3000.000, accuracy: 0.433\n",
            "training epoch: [1899 ] loss: 1.063 correct: 1271.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [1900 ] loss: 1.063 correct: 1298.000, total: 3000.000, accuracy: 0.433\n",
            "training epoch: [1901 ] loss: 1.063 correct: 1268.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [1902 ] loss: 1.063 correct: 1301.000, total: 3000.000, accuracy: 0.434\n",
            "training epoch: [1903 ] loss: 1.063 correct: 1269.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [1904 ] loss: 1.063 correct: 1300.000, total: 3000.000, accuracy: 0.433\n",
            "training epoch: [1905 ] loss: 1.063 correct: 1272.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [1906 ] loss: 1.063 correct: 1302.000, total: 3000.000, accuracy: 0.434\n",
            "training epoch: [1907 ] loss: 1.063 correct: 1271.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [1908 ] loss: 1.063 correct: 1296.000, total: 3000.000, accuracy: 0.432\n",
            "training epoch: [1909 ] loss: 1.063 correct: 1274.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1910 ] loss: 1.063 correct: 1295.000, total: 3000.000, accuracy: 0.432\n",
            "training epoch: [1911 ] loss: 1.063 correct: 1273.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [1912 ] loss: 1.063 correct: 1294.000, total: 3000.000, accuracy: 0.431\n",
            "training epoch: [1913 ] loss: 1.063 correct: 1272.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [1914 ] loss: 1.063 correct: 1294.000, total: 3000.000, accuracy: 0.431\n",
            "training epoch: [1915 ] loss: 1.063 correct: 1272.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [1916 ] loss: 1.063 correct: 1296.000, total: 3000.000, accuracy: 0.432\n",
            "training epoch: [1917 ] loss: 1.063 correct: 1273.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [1918 ] loss: 1.063 correct: 1298.000, total: 3000.000, accuracy: 0.433\n",
            "training epoch: [1919 ] loss: 1.063 correct: 1275.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1920 ] loss: 1.063 correct: 1298.000, total: 3000.000, accuracy: 0.433\n",
            "training epoch: [1921 ] loss: 1.063 correct: 1274.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1922 ] loss: 1.063 correct: 1297.000, total: 3000.000, accuracy: 0.432\n",
            "training epoch: [1923 ] loss: 1.063 correct: 1275.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1924 ] loss: 1.063 correct: 1298.000, total: 3000.000, accuracy: 0.433\n",
            "training epoch: [1925 ] loss: 1.063 correct: 1274.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1926 ] loss: 1.063 correct: 1296.000, total: 3000.000, accuracy: 0.432\n",
            "training epoch: [1927 ] loss: 1.063 correct: 1275.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1928 ] loss: 1.063 correct: 1295.000, total: 3000.000, accuracy: 0.432\n",
            "training epoch: [1929 ] loss: 1.063 correct: 1278.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [1930 ] loss: 1.063 correct: 1295.000, total: 3000.000, accuracy: 0.432\n",
            "training epoch: [1931 ] loss: 1.063 correct: 1277.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [1932 ] loss: 1.063 correct: 1296.000, total: 3000.000, accuracy: 0.432\n",
            "training epoch: [1933 ] loss: 1.063 correct: 1278.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [1934 ] loss: 1.062 correct: 1296.000, total: 3000.000, accuracy: 0.432\n",
            "training epoch: [1935 ] loss: 1.062 correct: 1277.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [1936 ] loss: 1.062 correct: 1296.000, total: 3000.000, accuracy: 0.432\n",
            "training epoch: [1937 ] loss: 1.062 correct: 1277.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [1938 ] loss: 1.062 correct: 1294.000, total: 3000.000, accuracy: 0.431\n",
            "training epoch: [1939 ] loss: 1.062 correct: 1277.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [1940 ] loss: 1.062 correct: 1293.000, total: 3000.000, accuracy: 0.431\n",
            "training epoch: [1941 ] loss: 1.062 correct: 1276.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1942 ] loss: 1.062 correct: 1294.000, total: 3000.000, accuracy: 0.431\n",
            "training epoch: [1943 ] loss: 1.062 correct: 1275.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1944 ] loss: 1.062 correct: 1294.000, total: 3000.000, accuracy: 0.431\n",
            "training epoch: [1945 ] loss: 1.062 correct: 1276.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1946 ] loss: 1.062 correct: 1296.000, total: 3000.000, accuracy: 0.432\n",
            "training epoch: [1947 ] loss: 1.062 correct: 1276.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1948 ] loss: 1.062 correct: 1297.000, total: 3000.000, accuracy: 0.432\n",
            "training epoch: [1949 ] loss: 1.062 correct: 1275.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1950 ] loss: 1.062 correct: 1298.000, total: 3000.000, accuracy: 0.433\n",
            "training epoch: [1951 ] loss: 1.062 correct: 1272.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [1952 ] loss: 1.062 correct: 1298.000, total: 3000.000, accuracy: 0.433\n",
            "training epoch: [1953 ] loss: 1.062 correct: 1274.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1954 ] loss: 1.062 correct: 1299.000, total: 3000.000, accuracy: 0.433\n",
            "training epoch: [1955 ] loss: 1.062 correct: 1275.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1956 ] loss: 1.062 correct: 1306.000, total: 3000.000, accuracy: 0.435\n",
            "training epoch: [1957 ] loss: 1.062 correct: 1276.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1958 ] loss: 1.062 correct: 1303.000, total: 3000.000, accuracy: 0.434\n",
            "training epoch: [1959 ] loss: 1.062 correct: 1272.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [1960 ] loss: 1.062 correct: 1301.000, total: 3000.000, accuracy: 0.434\n",
            "training epoch: [1961 ] loss: 1.062 correct: 1276.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1962 ] loss: 1.062 correct: 1303.000, total: 3000.000, accuracy: 0.434\n",
            "training epoch: [1963 ] loss: 1.062 correct: 1278.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [1964 ] loss: 1.062 correct: 1304.000, total: 3000.000, accuracy: 0.435\n",
            "training epoch: [1965 ] loss: 1.062 correct: 1273.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [1966 ] loss: 1.062 correct: 1302.000, total: 3000.000, accuracy: 0.434\n",
            "training epoch: [1967 ] loss: 1.062 correct: 1273.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [1968 ] loss: 1.062 correct: 1302.000, total: 3000.000, accuracy: 0.434\n",
            "training epoch: [1969 ] loss: 1.062 correct: 1270.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [1970 ] loss: 1.062 correct: 1303.000, total: 3000.000, accuracy: 0.434\n",
            "training epoch: [1971 ] loss: 1.062 correct: 1274.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1972 ] loss: 1.062 correct: 1299.000, total: 3000.000, accuracy: 0.433\n",
            "training epoch: [1973 ] loss: 1.062 correct: 1275.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [1974 ] loss: 1.062 correct: 1299.000, total: 3000.000, accuracy: 0.433\n",
            "training epoch: [1975 ] loss: 1.062 correct: 1277.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [1976 ] loss: 1.062 correct: 1297.000, total: 3000.000, accuracy: 0.432\n",
            "training epoch: [1977 ] loss: 1.062 correct: 1282.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [1978 ] loss: 1.062 correct: 1297.000, total: 3000.000, accuracy: 0.432\n",
            "training epoch: [1979 ] loss: 1.062 correct: 1285.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [1980 ] loss: 1.062 correct: 1297.000, total: 3000.000, accuracy: 0.432\n",
            "training epoch: [1981 ] loss: 1.062 correct: 1290.000, total: 3000.000, accuracy: 0.430\n",
            "training epoch: [1982 ] loss: 1.062 correct: 1297.000, total: 3000.000, accuracy: 0.432\n",
            "training epoch: [1983 ] loss: 1.062 correct: 1293.000, total: 3000.000, accuracy: 0.431\n",
            "training epoch: [1984 ] loss: 1.062 correct: 1296.000, total: 3000.000, accuracy: 0.432\n",
            "training epoch: [1985 ] loss: 1.062 correct: 1292.000, total: 3000.000, accuracy: 0.431\n",
            "training epoch: [1986 ] loss: 1.062 correct: 1296.000, total: 3000.000, accuracy: 0.432\n",
            "training epoch: [1987 ] loss: 1.062 correct: 1291.000, total: 3000.000, accuracy: 0.430\n",
            "training epoch: [1988 ] loss: 1.062 correct: 1295.000, total: 3000.000, accuracy: 0.432\n",
            "training epoch: [1989 ] loss: 1.062 correct: 1289.000, total: 3000.000, accuracy: 0.430\n",
            "training epoch: [1990 ] loss: 1.062 correct: 1293.000, total: 3000.000, accuracy: 0.431\n",
            "training epoch: [1991 ] loss: 1.062 correct: 1290.000, total: 3000.000, accuracy: 0.430\n",
            "training epoch: [1992 ] loss: 1.062 correct: 1292.000, total: 3000.000, accuracy: 0.431\n",
            "training epoch: [1993 ] loss: 1.062 correct: 1294.000, total: 3000.000, accuracy: 0.431\n",
            "training epoch: [1994 ] loss: 1.062 correct: 1291.000, total: 3000.000, accuracy: 0.430\n",
            "training epoch: [1995 ] loss: 1.062 correct: 1295.000, total: 3000.000, accuracy: 0.432\n",
            "training epoch: [1996 ] loss: 1.062 correct: 1290.000, total: 3000.000, accuracy: 0.430\n",
            "training epoch: [1997 ] loss: 1.062 correct: 1296.000, total: 3000.000, accuracy: 0.432\n",
            "training epoch: [1998 ] loss: 1.062 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1999 ] loss: 1.062 correct: 1295.000, total: 3000.000, accuracy: 0.432\n",
            "training epoch: [2000 ] loss: 1.062 correct: 1284.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [2001 ] loss: 1.062 correct: 1296.000, total: 3000.000, accuracy: 0.432\n",
            "training epoch: [2002 ] loss: 1.062 correct: 1281.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [2003 ] loss: 1.062 correct: 1298.000, total: 3000.000, accuracy: 0.433\n",
            "training epoch: [2004 ] loss: 1.062 correct: 1279.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [2005 ] loss: 1.062 correct: 1298.000, total: 3000.000, accuracy: 0.433\n",
            "training epoch: [2006 ] loss: 1.062 correct: 1279.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [2007 ] loss: 1.062 correct: 1301.000, total: 3000.000, accuracy: 0.434\n",
            "training epoch: [2008 ] loss: 1.062 correct: 1275.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [2009 ] loss: 1.062 correct: 1301.000, total: 3000.000, accuracy: 0.434\n",
            "training epoch: [2010 ] loss: 1.062 correct: 1279.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [2011 ] loss: 1.062 correct: 1300.000, total: 3000.000, accuracy: 0.433\n",
            "training epoch: [2012 ] loss: 1.062 correct: 1275.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [2013 ] loss: 1.062 correct: 1304.000, total: 3000.000, accuracy: 0.435\n",
            "training epoch: [2014 ] loss: 1.062 correct: 1271.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [2015 ] loss: 1.062 correct: 1305.000, total: 3000.000, accuracy: 0.435\n",
            "training epoch: [2016 ] loss: 1.062 correct: 1272.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [2017 ] loss: 1.062 correct: 1306.000, total: 3000.000, accuracy: 0.435\n",
            "training epoch: [2018 ] loss: 1.062 correct: 1275.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [2019 ] loss: 1.062 correct: 1303.000, total: 3000.000, accuracy: 0.434\n",
            "training epoch: [2020 ] loss: 1.062 correct: 1275.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [2021 ] loss: 1.062 correct: 1304.000, total: 3000.000, accuracy: 0.435\n",
            "training epoch: [2022 ] loss: 1.062 correct: 1281.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [2023 ] loss: 1.062 correct: 1305.000, total: 3000.000, accuracy: 0.435\n",
            "training epoch: [2024 ] loss: 1.062 correct: 1275.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [2025 ] loss: 1.062 correct: 1305.000, total: 3000.000, accuracy: 0.435\n",
            "training epoch: [2026 ] loss: 1.062 correct: 1274.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [2027 ] loss: 1.062 correct: 1303.000, total: 3000.000, accuracy: 0.434\n",
            "training epoch: [2028 ] loss: 1.062 correct: 1276.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [2029 ] loss: 1.062 correct: 1303.000, total: 3000.000, accuracy: 0.434\n",
            "training epoch: [2030 ] loss: 1.062 correct: 1278.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [2031 ] loss: 1.062 correct: 1299.000, total: 3000.000, accuracy: 0.433\n",
            "training epoch: [2032 ] loss: 1.062 correct: 1275.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [2033 ] loss: 1.062 correct: 1300.000, total: 3000.000, accuracy: 0.433\n",
            "training epoch: [2034 ] loss: 1.062 correct: 1278.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [2035 ] loss: 1.062 correct: 1301.000, total: 3000.000, accuracy: 0.434\n",
            "training epoch: [2036 ] loss: 1.062 correct: 1279.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [2037 ] loss: 1.062 correct: 1299.000, total: 3000.000, accuracy: 0.433\n",
            "training epoch: [2038 ] loss: 1.062 correct: 1282.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [2039 ] loss: 1.062 correct: 1297.000, total: 3000.000, accuracy: 0.432\n",
            "training epoch: [2040 ] loss: 1.062 correct: 1284.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [2041 ] loss: 1.062 correct: 1299.000, total: 3000.000, accuracy: 0.433\n",
            "training epoch: [2042 ] loss: 1.062 correct: 1285.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [2043 ] loss: 1.062 correct: 1299.000, total: 3000.000, accuracy: 0.433\n",
            "training epoch: [2044 ] loss: 1.062 correct: 1285.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [2045 ] loss: 1.062 correct: 1299.000, total: 3000.000, accuracy: 0.433\n",
            "training epoch: [2046 ] loss: 1.062 correct: 1285.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [2047 ] loss: 1.062 correct: 1299.000, total: 3000.000, accuracy: 0.433\n",
            "training epoch: [2048 ] loss: 1.062 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2049 ] loss: 1.062 correct: 1299.000, total: 3000.000, accuracy: 0.433\n",
            "training epoch: [2050 ] loss: 1.062 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2051 ] loss: 1.062 correct: 1299.000, total: 3000.000, accuracy: 0.433\n",
            "training epoch: [2052 ] loss: 1.062 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2053 ] loss: 1.062 correct: 1299.000, total: 3000.000, accuracy: 0.433\n",
            "training epoch: [2054 ] loss: 1.062 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2055 ] loss: 1.062 correct: 1300.000, total: 3000.000, accuracy: 0.433\n",
            "training epoch: [2056 ] loss: 1.062 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2057 ] loss: 1.062 correct: 1300.000, total: 3000.000, accuracy: 0.433\n",
            "training epoch: [2058 ] loss: 1.062 correct: 1285.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [2059 ] loss: 1.062 correct: 1300.000, total: 3000.000, accuracy: 0.433\n",
            "training epoch: [2060 ] loss: 1.062 correct: 1285.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [2061 ] loss: 1.062 correct: 1299.000, total: 3000.000, accuracy: 0.433\n",
            "training epoch: [2062 ] loss: 1.062 correct: 1285.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [2063 ] loss: 1.062 correct: 1299.000, total: 3000.000, accuracy: 0.433\n",
            "training epoch: [2064 ] loss: 1.062 correct: 1284.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [2065 ] loss: 1.062 correct: 1298.000, total: 3000.000, accuracy: 0.433\n",
            "training epoch: [2066 ] loss: 1.062 correct: 1283.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [2067 ] loss: 1.062 correct: 1298.000, total: 3000.000, accuracy: 0.433\n",
            "training epoch: [2068 ] loss: 1.062 correct: 1278.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [2069 ] loss: 1.062 correct: 1300.000, total: 3000.000, accuracy: 0.433\n",
            "training epoch: [2070 ] loss: 1.062 correct: 1281.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [2071 ] loss: 1.062 correct: 1301.000, total: 3000.000, accuracy: 0.434\n",
            "training epoch: [2072 ] loss: 1.062 correct: 1282.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [2073 ] loss: 1.062 correct: 1304.000, total: 3000.000, accuracy: 0.435\n",
            "training epoch: [2074 ] loss: 1.062 correct: 1278.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [2075 ] loss: 1.062 correct: 1307.000, total: 3000.000, accuracy: 0.436\n",
            "training epoch: [2076 ] loss: 1.062 correct: 1277.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [2077 ] loss: 1.062 correct: 1306.000, total: 3000.000, accuracy: 0.435\n",
            "training epoch: [2078 ] loss: 1.062 correct: 1281.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [2079 ] loss: 1.062 correct: 1305.000, total: 3000.000, accuracy: 0.435\n",
            "training epoch: [2080 ] loss: 1.062 correct: 1282.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [2081 ] loss: 1.062 correct: 1307.000, total: 3000.000, accuracy: 0.436\n",
            "training epoch: [2082 ] loss: 1.062 correct: 1281.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [2083 ] loss: 1.062 correct: 1307.000, total: 3000.000, accuracy: 0.436\n",
            "training epoch: [2084 ] loss: 1.062 correct: 1278.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [2085 ] loss: 1.062 correct: 1306.000, total: 3000.000, accuracy: 0.435\n",
            "training epoch: [2086 ] loss: 1.062 correct: 1275.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [2087 ] loss: 1.062 correct: 1305.000, total: 3000.000, accuracy: 0.435\n",
            "training epoch: [2088 ] loss: 1.062 correct: 1275.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [2089 ] loss: 1.061 correct: 1302.000, total: 3000.000, accuracy: 0.434\n",
            "training epoch: [2090 ] loss: 1.062 correct: 1279.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [2091 ] loss: 1.061 correct: 1303.000, total: 3000.000, accuracy: 0.434\n",
            "training epoch: [2092 ] loss: 1.061 correct: 1278.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [2093 ] loss: 1.061 correct: 1300.000, total: 3000.000, accuracy: 0.433\n",
            "training epoch: [2094 ] loss: 1.061 correct: 1283.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [2095 ] loss: 1.061 correct: 1301.000, total: 3000.000, accuracy: 0.434\n",
            "training epoch: [2096 ] loss: 1.061 correct: 1284.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [2097 ] loss: 1.061 correct: 1300.000, total: 3000.000, accuracy: 0.433\n",
            "training epoch: [2098 ] loss: 1.061 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2099 ] loss: 1.061 correct: 1300.000, total: 3000.000, accuracy: 0.433\n",
            "training epoch: [2100 ] loss: 1.061 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2101 ] loss: 1.061 correct: 1299.000, total: 3000.000, accuracy: 0.433\n",
            "training epoch: [2102 ] loss: 1.061 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2103 ] loss: 1.061 correct: 1298.000, total: 3000.000, accuracy: 0.433\n",
            "training epoch: [2104 ] loss: 1.061 correct: 1292.000, total: 3000.000, accuracy: 0.431\n",
            "training epoch: [2105 ] loss: 1.061 correct: 1297.000, total: 3000.000, accuracy: 0.432\n",
            "training epoch: [2106 ] loss: 1.061 correct: 1295.000, total: 3000.000, accuracy: 0.432\n",
            "training epoch: [2107 ] loss: 1.061 correct: 1297.000, total: 3000.000, accuracy: 0.432\n",
            "training epoch: [2108 ] loss: 1.061 correct: 1297.000, total: 3000.000, accuracy: 0.432\n",
            "training epoch: [2109 ] loss: 1.061 correct: 1294.000, total: 3000.000, accuracy: 0.431\n",
            "training epoch: [2110 ] loss: 1.061 correct: 1298.000, total: 3000.000, accuracy: 0.433\n",
            "training epoch: [2111 ] loss: 1.061 correct: 1296.000, total: 3000.000, accuracy: 0.432\n",
            "training epoch: [2112 ] loss: 1.061 correct: 1295.000, total: 3000.000, accuracy: 0.432\n",
            "training epoch: [2113 ] loss: 1.061 correct: 1295.000, total: 3000.000, accuracy: 0.432\n",
            "training epoch: [2114 ] loss: 1.061 correct: 1295.000, total: 3000.000, accuracy: 0.432\n",
            "training epoch: [2115 ] loss: 1.061 correct: 1295.000, total: 3000.000, accuracy: 0.432\n",
            "training epoch: [2116 ] loss: 1.061 correct: 1296.000, total: 3000.000, accuracy: 0.432\n",
            "training epoch: [2117 ] loss: 1.061 correct: 1293.000, total: 3000.000, accuracy: 0.431\n",
            "training epoch: [2118 ] loss: 1.061 correct: 1295.000, total: 3000.000, accuracy: 0.432\n",
            "training epoch: [2119 ] loss: 1.061 correct: 1290.000, total: 3000.000, accuracy: 0.430\n",
            "training epoch: [2120 ] loss: 1.061 correct: 1297.000, total: 3000.000, accuracy: 0.432\n",
            "training epoch: [2121 ] loss: 1.061 correct: 1289.000, total: 3000.000, accuracy: 0.430\n",
            "training epoch: [2122 ] loss: 1.061 correct: 1298.000, total: 3000.000, accuracy: 0.433\n",
            "training epoch: [2123 ] loss: 1.061 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2124 ] loss: 1.061 correct: 1299.000, total: 3000.000, accuracy: 0.433\n",
            "training epoch: [2125 ] loss: 1.061 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2126 ] loss: 1.061 correct: 1301.000, total: 3000.000, accuracy: 0.434\n",
            "training epoch: [2127 ] loss: 1.061 correct: 1283.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [2128 ] loss: 1.061 correct: 1301.000, total: 3000.000, accuracy: 0.434\n",
            "training epoch: [2129 ] loss: 1.061 correct: 1283.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [2130 ] loss: 1.061 correct: 1305.000, total: 3000.000, accuracy: 0.435\n",
            "training epoch: [2131 ] loss: 1.061 correct: 1282.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [2132 ] loss: 1.061 correct: 1305.000, total: 3000.000, accuracy: 0.435\n",
            "training epoch: [2133 ] loss: 1.061 correct: 1275.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [2134 ] loss: 1.061 correct: 1304.000, total: 3000.000, accuracy: 0.435\n",
            "training epoch: [2135 ] loss: 1.061 correct: 1277.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [2136 ] loss: 1.061 correct: 1309.000, total: 3000.000, accuracy: 0.436\n",
            "training epoch: [2137 ] loss: 1.061 correct: 1280.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [2138 ] loss: 1.061 correct: 1309.000, total: 3000.000, accuracy: 0.436\n",
            "training epoch: [2139 ] loss: 1.061 correct: 1280.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [2140 ] loss: 1.061 correct: 1307.000, total: 3000.000, accuracy: 0.436\n",
            "training epoch: [2141 ] loss: 1.061 correct: 1280.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [2142 ] loss: 1.061 correct: 1308.000, total: 3000.000, accuracy: 0.436\n",
            "training epoch: [2143 ] loss: 1.061 correct: 1278.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [2144 ] loss: 1.061 correct: 1307.000, total: 3000.000, accuracy: 0.436\n",
            "training epoch: [2145 ] loss: 1.061 correct: 1282.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [2146 ] loss: 1.061 correct: 1305.000, total: 3000.000, accuracy: 0.435\n",
            "training epoch: [2147 ] loss: 1.061 correct: 1283.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [2148 ] loss: 1.061 correct: 1305.000, total: 3000.000, accuracy: 0.435\n",
            "training epoch: [2149 ] loss: 1.061 correct: 1284.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [2150 ] loss: 1.061 correct: 1301.000, total: 3000.000, accuracy: 0.434\n",
            "training epoch: [2151 ] loss: 1.061 correct: 1282.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [2152 ] loss: 1.061 correct: 1299.000, total: 3000.000, accuracy: 0.433\n",
            "training epoch: [2153 ] loss: 1.061 correct: 1281.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [2154 ] loss: 1.061 correct: 1299.000, total: 3000.000, accuracy: 0.433\n",
            "training epoch: [2155 ] loss: 1.061 correct: 1282.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [2156 ] loss: 1.061 correct: 1301.000, total: 3000.000, accuracy: 0.434\n",
            "training epoch: [2157 ] loss: 1.061 correct: 1282.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [2158 ] loss: 1.061 correct: 1302.000, total: 3000.000, accuracy: 0.434\n",
            "training epoch: [2159 ] loss: 1.061 correct: 1285.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [2160 ] loss: 1.061 correct: 1303.000, total: 3000.000, accuracy: 0.434\n",
            "training epoch: [2161 ] loss: 1.061 correct: 1285.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [2162 ] loss: 1.061 correct: 1306.000, total: 3000.000, accuracy: 0.435\n",
            "training epoch: [2163 ] loss: 1.061 correct: 1285.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [2164 ] loss: 1.061 correct: 1303.000, total: 3000.000, accuracy: 0.434\n",
            "training epoch: [2165 ] loss: 1.061 correct: 1283.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [2166 ] loss: 1.061 correct: 1303.000, total: 3000.000, accuracy: 0.434\n",
            "training epoch: [2167 ] loss: 1.061 correct: 1283.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [2168 ] loss: 1.061 correct: 1304.000, total: 3000.000, accuracy: 0.435\n",
            "training epoch: [2169 ] loss: 1.061 correct: 1285.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [2170 ] loss: 1.061 correct: 1302.000, total: 3000.000, accuracy: 0.434\n",
            "training epoch: [2171 ] loss: 1.061 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2172 ] loss: 1.061 correct: 1302.000, total: 3000.000, accuracy: 0.434\n",
            "training epoch: [2173 ] loss: 1.061 correct: 1289.000, total: 3000.000, accuracy: 0.430\n",
            "training epoch: [2174 ] loss: 1.061 correct: 1301.000, total: 3000.000, accuracy: 0.434\n",
            "training epoch: [2175 ] loss: 1.061 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2176 ] loss: 1.061 correct: 1299.000, total: 3000.000, accuracy: 0.433\n",
            "training epoch: [2177 ] loss: 1.061 correct: 1291.000, total: 3000.000, accuracy: 0.430\n",
            "training epoch: [2178 ] loss: 1.061 correct: 1299.000, total: 3000.000, accuracy: 0.433\n",
            "training epoch: [2179 ] loss: 1.061 correct: 1289.000, total: 3000.000, accuracy: 0.430\n",
            "training epoch: [2180 ] loss: 1.061 correct: 1300.000, total: 3000.000, accuracy: 0.433\n",
            "training epoch: [2181 ] loss: 1.061 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2182 ] loss: 1.061 correct: 1299.000, total: 3000.000, accuracy: 0.433\n",
            "training epoch: [2183 ] loss: 1.061 correct: 1293.000, total: 3000.000, accuracy: 0.431\n",
            "training epoch: [2184 ] loss: 1.061 correct: 1299.000, total: 3000.000, accuracy: 0.433\n",
            "training epoch: [2185 ] loss: 1.061 correct: 1295.000, total: 3000.000, accuracy: 0.432\n",
            "training epoch: [2186 ] loss: 1.061 correct: 1297.000, total: 3000.000, accuracy: 0.432\n",
            "training epoch: [2187 ] loss: 1.061 correct: 1295.000, total: 3000.000, accuracy: 0.432\n",
            "training epoch: [2188 ] loss: 1.061 correct: 1299.000, total: 3000.000, accuracy: 0.433\n",
            "training epoch: [2189 ] loss: 1.061 correct: 1296.000, total: 3000.000, accuracy: 0.432\n",
            "training epoch: [2190 ] loss: 1.061 correct: 1297.000, total: 3000.000, accuracy: 0.432\n",
            "training epoch: [2191 ] loss: 1.061 correct: 1298.000, total: 3000.000, accuracy: 0.433\n",
            "training epoch: [2192 ] loss: 1.061 correct: 1297.000, total: 3000.000, accuracy: 0.432\n",
            "training epoch: [2193 ] loss: 1.061 correct: 1298.000, total: 3000.000, accuracy: 0.433\n",
            "training epoch: [2194 ] loss: 1.061 correct: 1294.000, total: 3000.000, accuracy: 0.431\n",
            "training epoch: [2195 ] loss: 1.061 correct: 1300.000, total: 3000.000, accuracy: 0.433\n",
            "training epoch: [2196 ] loss: 1.061 correct: 1291.000, total: 3000.000, accuracy: 0.430\n",
            "training epoch: [2197 ] loss: 1.061 correct: 1302.000, total: 3000.000, accuracy: 0.434\n",
            "training epoch: [2198 ] loss: 1.061 correct: 1285.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [2199 ] loss: 1.061 correct: 1305.000, total: 3000.000, accuracy: 0.435\n",
            "training epoch: [2200 ] loss: 1.061 correct: 1282.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [2201 ] loss: 1.061 correct: 1309.000, total: 3000.000, accuracy: 0.436\n",
            "training epoch: [2202 ] loss: 1.061 correct: 1280.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [2203 ] loss: 1.061 correct: 1311.000, total: 3000.000, accuracy: 0.437\n",
            "training epoch: [2204 ] loss: 1.061 correct: 1279.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [2205 ] loss: 1.061 correct: 1310.000, total: 3000.000, accuracy: 0.437\n",
            "training epoch: [2206 ] loss: 1.061 correct: 1280.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [2207 ] loss: 1.061 correct: 1310.000, total: 3000.000, accuracy: 0.437\n",
            "training epoch: [2208 ] loss: 1.061 correct: 1279.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [2209 ] loss: 1.061 correct: 1307.000, total: 3000.000, accuracy: 0.436\n",
            "training epoch: [2210 ] loss: 1.061 correct: 1283.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [2211 ] loss: 1.061 correct: 1310.000, total: 3000.000, accuracy: 0.437\n",
            "training epoch: [2212 ] loss: 1.061 correct: 1285.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [2213 ] loss: 1.061 correct: 1308.000, total: 3000.000, accuracy: 0.436\n",
            "training epoch: [2214 ] loss: 1.061 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2215 ] loss: 1.061 correct: 1306.000, total: 3000.000, accuracy: 0.435\n",
            "training epoch: [2216 ] loss: 1.061 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2217 ] loss: 1.061 correct: 1305.000, total: 3000.000, accuracy: 0.435\n",
            "training epoch: [2218 ] loss: 1.061 correct: 1289.000, total: 3000.000, accuracy: 0.430\n",
            "training epoch: [2219 ] loss: 1.061 correct: 1305.000, total: 3000.000, accuracy: 0.435\n",
            "training epoch: [2220 ] loss: 1.061 correct: 1291.000, total: 3000.000, accuracy: 0.430\n",
            "training epoch: [2221 ] loss: 1.061 correct: 1303.000, total: 3000.000, accuracy: 0.434\n",
            "training epoch: [2222 ] loss: 1.061 correct: 1290.000, total: 3000.000, accuracy: 0.430\n",
            "training epoch: [2223 ] loss: 1.061 correct: 1302.000, total: 3000.000, accuracy: 0.434\n",
            "training epoch: [2224 ] loss: 1.061 correct: 1289.000, total: 3000.000, accuracy: 0.430\n",
            "training epoch: [2225 ] loss: 1.061 correct: 1302.000, total: 3000.000, accuracy: 0.434\n",
            "training epoch: [2226 ] loss: 1.061 correct: 1289.000, total: 3000.000, accuracy: 0.430\n",
            "training epoch: [2227 ] loss: 1.061 correct: 1302.000, total: 3000.000, accuracy: 0.434\n",
            "training epoch: [2228 ] loss: 1.061 correct: 1289.000, total: 3000.000, accuracy: 0.430\n",
            "training epoch: [2229 ] loss: 1.061 correct: 1302.000, total: 3000.000, accuracy: 0.434\n",
            "training epoch: [2230 ] loss: 1.061 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2231 ] loss: 1.061 correct: 1304.000, total: 3000.000, accuracy: 0.435\n",
            "training epoch: [2232 ] loss: 1.061 correct: 1290.000, total: 3000.000, accuracy: 0.430\n",
            "training epoch: [2233 ] loss: 1.061 correct: 1302.000, total: 3000.000, accuracy: 0.434\n",
            "training epoch: [2234 ] loss: 1.061 correct: 1290.000, total: 3000.000, accuracy: 0.430\n",
            "training epoch: [2235 ] loss: 1.061 correct: 1305.000, total: 3000.000, accuracy: 0.435\n",
            "training epoch: [2236 ] loss: 1.061 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2237 ] loss: 1.061 correct: 1304.000, total: 3000.000, accuracy: 0.435\n",
            "training epoch: [2238 ] loss: 1.061 correct: 1285.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [2239 ] loss: 1.061 correct: 1304.000, total: 3000.000, accuracy: 0.435\n",
            "training epoch: [2240 ] loss: 1.061 correct: 1285.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [2241 ] loss: 1.061 correct: 1308.000, total: 3000.000, accuracy: 0.436\n",
            "training epoch: [2242 ] loss: 1.061 correct: 1285.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [2243 ] loss: 1.061 correct: 1308.000, total: 3000.000, accuracy: 0.436\n",
            "training epoch: [2244 ] loss: 1.061 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2245 ] loss: 1.061 correct: 1309.000, total: 3000.000, accuracy: 0.436\n",
            "training epoch: [2246 ] loss: 1.061 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2247 ] loss: 1.060 correct: 1309.000, total: 3000.000, accuracy: 0.436\n",
            "training epoch: [2248 ] loss: 1.061 correct: 1285.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [2249 ] loss: 1.060 correct: 1307.000, total: 3000.000, accuracy: 0.436\n",
            "training epoch: [2250 ] loss: 1.060 correct: 1284.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [2251 ] loss: 1.060 correct: 1306.000, total: 3000.000, accuracy: 0.435\n",
            "training epoch: [2252 ] loss: 1.060 correct: 1283.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [2253 ] loss: 1.060 correct: 1308.000, total: 3000.000, accuracy: 0.436\n",
            "training epoch: [2254 ] loss: 1.060 correct: 1284.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [2255 ] loss: 1.060 correct: 1307.000, total: 3000.000, accuracy: 0.436\n",
            "training epoch: [2256 ] loss: 1.060 correct: 1284.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [2257 ] loss: 1.060 correct: 1306.000, total: 3000.000, accuracy: 0.435\n",
            "training epoch: [2258 ] loss: 1.060 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2259 ] loss: 1.060 correct: 1304.000, total: 3000.000, accuracy: 0.435\n",
            "training epoch: [2260 ] loss: 1.060 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2261 ] loss: 1.060 correct: 1304.000, total: 3000.000, accuracy: 0.435\n",
            "training epoch: [2262 ] loss: 1.060 correct: 1289.000, total: 3000.000, accuracy: 0.430\n",
            "training epoch: [2263 ] loss: 1.060 correct: 1308.000, total: 3000.000, accuracy: 0.436\n",
            "training epoch: [2264 ] loss: 1.060 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2265 ] loss: 1.060 correct: 1310.000, total: 3000.000, accuracy: 0.437\n",
            "training epoch: [2266 ] loss: 1.060 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2267 ] loss: 1.060 correct: 1312.000, total: 3000.000, accuracy: 0.437\n",
            "training epoch: [2268 ] loss: 1.060 correct: 1285.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [2269 ] loss: 1.060 correct: 1314.000, total: 3000.000, accuracy: 0.438\n",
            "training epoch: [2270 ] loss: 1.060 correct: 1281.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [2271 ] loss: 1.060 correct: 1311.000, total: 3000.000, accuracy: 0.437\n",
            "training epoch: [2272 ] loss: 1.060 correct: 1280.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [2273 ] loss: 1.060 correct: 1312.000, total: 3000.000, accuracy: 0.437\n",
            "training epoch: [2274 ] loss: 1.060 correct: 1283.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [2275 ] loss: 1.060 correct: 1311.000, total: 3000.000, accuracy: 0.437\n",
            "training epoch: [2276 ] loss: 1.060 correct: 1282.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [2277 ] loss: 1.060 correct: 1312.000, total: 3000.000, accuracy: 0.437\n",
            "training epoch: [2278 ] loss: 1.060 correct: 1285.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [2279 ] loss: 1.060 correct: 1310.000, total: 3000.000, accuracy: 0.437\n",
            "training epoch: [2280 ] loss: 1.060 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2281 ] loss: 1.060 correct: 1306.000, total: 3000.000, accuracy: 0.435\n",
            "training epoch: [2282 ] loss: 1.060 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2283 ] loss: 1.060 correct: 1307.000, total: 3000.000, accuracy: 0.436\n",
            "training epoch: [2284 ] loss: 1.060 correct: 1289.000, total: 3000.000, accuracy: 0.430\n",
            "training epoch: [2285 ] loss: 1.060 correct: 1308.000, total: 3000.000, accuracy: 0.436\n",
            "training epoch: [2286 ] loss: 1.060 correct: 1292.000, total: 3000.000, accuracy: 0.431\n",
            "training epoch: [2287 ] loss: 1.060 correct: 1305.000, total: 3000.000, accuracy: 0.435\n",
            "training epoch: [2288 ] loss: 1.060 correct: 1292.000, total: 3000.000, accuracy: 0.431\n",
            "training epoch: [2289 ] loss: 1.060 correct: 1304.000, total: 3000.000, accuracy: 0.435\n",
            "training epoch: [2290 ] loss: 1.060 correct: 1293.000, total: 3000.000, accuracy: 0.431\n",
            "training epoch: [2291 ] loss: 1.060 correct: 1304.000, total: 3000.000, accuracy: 0.435\n",
            "training epoch: [2292 ] loss: 1.060 correct: 1295.000, total: 3000.000, accuracy: 0.432\n",
            "training epoch: [2293 ] loss: 1.060 correct: 1304.000, total: 3000.000, accuracy: 0.435\n",
            "training epoch: [2294 ] loss: 1.060 correct: 1295.000, total: 3000.000, accuracy: 0.432\n",
            "training epoch: [2295 ] loss: 1.060 correct: 1302.000, total: 3000.000, accuracy: 0.434\n",
            "training epoch: [2296 ] loss: 1.060 correct: 1294.000, total: 3000.000, accuracy: 0.431\n",
            "training epoch: [2297 ] loss: 1.060 correct: 1301.000, total: 3000.000, accuracy: 0.434\n",
            "training epoch: [2298 ] loss: 1.060 correct: 1294.000, total: 3000.000, accuracy: 0.431\n",
            "training epoch: [2299 ] loss: 1.060 correct: 1301.000, total: 3000.000, accuracy: 0.434\n",
            "training epoch: [2300 ] loss: 1.060 correct: 1295.000, total: 3000.000, accuracy: 0.432\n",
            "training epoch: [2301 ] loss: 1.060 correct: 1302.000, total: 3000.000, accuracy: 0.434\n",
            "training epoch: [2302 ] loss: 1.060 correct: 1295.000, total: 3000.000, accuracy: 0.432\n",
            "training epoch: [2303 ] loss: 1.060 correct: 1303.000, total: 3000.000, accuracy: 0.434\n",
            "training epoch: [2304 ] loss: 1.060 correct: 1293.000, total: 3000.000, accuracy: 0.431\n",
            "training epoch: [2305 ] loss: 1.060 correct: 1304.000, total: 3000.000, accuracy: 0.435\n",
            "training epoch: [2306 ] loss: 1.060 correct: 1292.000, total: 3000.000, accuracy: 0.431\n",
            "training epoch: [2307 ] loss: 1.060 correct: 1303.000, total: 3000.000, accuracy: 0.434\n",
            "training epoch: [2308 ] loss: 1.060 correct: 1292.000, total: 3000.000, accuracy: 0.431\n",
            "training epoch: [2309 ] loss: 1.060 correct: 1302.000, total: 3000.000, accuracy: 0.434\n",
            "training epoch: [2310 ] loss: 1.060 correct: 1294.000, total: 3000.000, accuracy: 0.431\n",
            "training epoch: [2311 ] loss: 1.060 correct: 1302.000, total: 3000.000, accuracy: 0.434\n",
            "training epoch: [2312 ] loss: 1.060 correct: 1295.000, total: 3000.000, accuracy: 0.432\n",
            "training epoch: [2313 ] loss: 1.060 correct: 1298.000, total: 3000.000, accuracy: 0.433\n",
            "training epoch: [2314 ] loss: 1.060 correct: 1295.000, total: 3000.000, accuracy: 0.432\n",
            "training epoch: [2315 ] loss: 1.060 correct: 1297.000, total: 3000.000, accuracy: 0.432\n",
            "training epoch: [2316 ] loss: 1.060 correct: 1298.000, total: 3000.000, accuracy: 0.433\n",
            "training epoch: [2317 ] loss: 1.060 correct: 1293.000, total: 3000.000, accuracy: 0.431\n",
            "training epoch: [2318 ] loss: 1.060 correct: 1303.000, total: 3000.000, accuracy: 0.434\n",
            "training epoch: [2319 ] loss: 1.060 correct: 1289.000, total: 3000.000, accuracy: 0.430\n",
            "training epoch: [2320 ] loss: 1.060 correct: 1308.000, total: 3000.000, accuracy: 0.436\n",
            "training epoch: [2321 ] loss: 1.060 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2322 ] loss: 1.060 correct: 1313.000, total: 3000.000, accuracy: 0.438\n",
            "training epoch: [2323 ] loss: 1.060 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2324 ] loss: 1.060 correct: 1313.000, total: 3000.000, accuracy: 0.438\n",
            "training epoch: [2325 ] loss: 1.060 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2326 ] loss: 1.060 correct: 1318.000, total: 3000.000, accuracy: 0.439\n",
            "training epoch: [2327 ] loss: 1.060 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2328 ] loss: 1.060 correct: 1314.000, total: 3000.000, accuracy: 0.438\n",
            "training epoch: [2329 ] loss: 1.060 correct: 1285.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [2330 ] loss: 1.060 correct: 1313.000, total: 3000.000, accuracy: 0.438\n",
            "training epoch: [2331 ] loss: 1.060 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2332 ] loss: 1.060 correct: 1313.000, total: 3000.000, accuracy: 0.438\n",
            "training epoch: [2333 ] loss: 1.060 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2334 ] loss: 1.060 correct: 1315.000, total: 3000.000, accuracy: 0.438\n",
            "training epoch: [2335 ] loss: 1.060 correct: 1289.000, total: 3000.000, accuracy: 0.430\n",
            "training epoch: [2336 ] loss: 1.060 correct: 1314.000, total: 3000.000, accuracy: 0.438\n",
            "training epoch: [2337 ] loss: 1.060 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2338 ] loss: 1.060 correct: 1316.000, total: 3000.000, accuracy: 0.439\n",
            "training epoch: [2339 ] loss: 1.060 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2340 ] loss: 1.060 correct: 1312.000, total: 3000.000, accuracy: 0.437\n",
            "training epoch: [2341 ] loss: 1.060 correct: 1291.000, total: 3000.000, accuracy: 0.430\n",
            "training epoch: [2342 ] loss: 1.060 correct: 1310.000, total: 3000.000, accuracy: 0.437\n",
            "training epoch: [2343 ] loss: 1.060 correct: 1291.000, total: 3000.000, accuracy: 0.430\n",
            "training epoch: [2344 ] loss: 1.060 correct: 1310.000, total: 3000.000, accuracy: 0.437\n",
            "training epoch: [2345 ] loss: 1.060 correct: 1291.000, total: 3000.000, accuracy: 0.430\n",
            "training epoch: [2346 ] loss: 1.060 correct: 1309.000, total: 3000.000, accuracy: 0.436\n",
            "training epoch: [2347 ] loss: 1.060 correct: 1290.000, total: 3000.000, accuracy: 0.430\n",
            "training epoch: [2348 ] loss: 1.060 correct: 1309.000, total: 3000.000, accuracy: 0.436\n",
            "training epoch: [2349 ] loss: 1.060 correct: 1291.000, total: 3000.000, accuracy: 0.430\n",
            "training epoch: [2350 ] loss: 1.060 correct: 1309.000, total: 3000.000, accuracy: 0.436\n",
            "training epoch: [2351 ] loss: 1.060 correct: 1291.000, total: 3000.000, accuracy: 0.430\n",
            "training epoch: [2352 ] loss: 1.060 correct: 1309.000, total: 3000.000, accuracy: 0.436\n",
            "training epoch: [2353 ] loss: 1.060 correct: 1291.000, total: 3000.000, accuracy: 0.430\n",
            "training epoch: [2354 ] loss: 1.060 correct: 1309.000, total: 3000.000, accuracy: 0.436\n",
            "training epoch: [2355 ] loss: 1.060 correct: 1292.000, total: 3000.000, accuracy: 0.431\n",
            "training epoch: [2356 ] loss: 1.060 correct: 1309.000, total: 3000.000, accuracy: 0.436\n",
            "training epoch: [2357 ] loss: 1.060 correct: 1292.000, total: 3000.000, accuracy: 0.431\n",
            "training epoch: [2358 ] loss: 1.060 correct: 1309.000, total: 3000.000, accuracy: 0.436\n",
            "training epoch: [2359 ] loss: 1.060 correct: 1293.000, total: 3000.000, accuracy: 0.431\n",
            "training epoch: [2360 ] loss: 1.060 correct: 1309.000, total: 3000.000, accuracy: 0.436\n",
            "training epoch: [2361 ] loss: 1.060 correct: 1291.000, total: 3000.000, accuracy: 0.430\n",
            "training epoch: [2362 ] loss: 1.060 correct: 1308.000, total: 3000.000, accuracy: 0.436\n",
            "training epoch: [2363 ] loss: 1.060 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2364 ] loss: 1.060 correct: 1308.000, total: 3000.000, accuracy: 0.436\n",
            "training epoch: [2365 ] loss: 1.060 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2366 ] loss: 1.060 correct: 1310.000, total: 3000.000, accuracy: 0.437\n",
            "training epoch: [2367 ] loss: 1.060 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2368 ] loss: 1.060 correct: 1310.000, total: 3000.000, accuracy: 0.437\n",
            "training epoch: [2369 ] loss: 1.060 correct: 1285.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [2370 ] loss: 1.060 correct: 1310.000, total: 3000.000, accuracy: 0.437\n",
            "training epoch: [2371 ] loss: 1.060 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2372 ] loss: 1.060 correct: 1310.000, total: 3000.000, accuracy: 0.437\n",
            "training epoch: [2373 ] loss: 1.060 correct: 1285.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [2374 ] loss: 1.060 correct: 1310.000, total: 3000.000, accuracy: 0.437\n",
            "training epoch: [2375 ] loss: 1.060 correct: 1284.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [2376 ] loss: 1.060 correct: 1310.000, total: 3000.000, accuracy: 0.437\n",
            "training epoch: [2377 ] loss: 1.060 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2378 ] loss: 1.060 correct: 1310.000, total: 3000.000, accuracy: 0.437\n",
            "training epoch: [2379 ] loss: 1.060 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2380 ] loss: 1.060 correct: 1313.000, total: 3000.000, accuracy: 0.438\n",
            "training epoch: [2381 ] loss: 1.060 correct: 1289.000, total: 3000.000, accuracy: 0.430\n",
            "training epoch: [2382 ] loss: 1.060 correct: 1311.000, total: 3000.000, accuracy: 0.437\n",
            "training epoch: [2383 ] loss: 1.060 correct: 1289.000, total: 3000.000, accuracy: 0.430\n",
            "training epoch: [2384 ] loss: 1.060 correct: 1316.000, total: 3000.000, accuracy: 0.439\n",
            "training epoch: [2385 ] loss: 1.060 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2386 ] loss: 1.060 correct: 1316.000, total: 3000.000, accuracy: 0.439\n",
            "training epoch: [2387 ] loss: 1.060 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2388 ] loss: 1.059 correct: 1313.000, total: 3000.000, accuracy: 0.438\n",
            "training epoch: [2389 ] loss: 1.060 correct: 1291.000, total: 3000.000, accuracy: 0.430\n",
            "training epoch: [2390 ] loss: 1.059 correct: 1314.000, total: 3000.000, accuracy: 0.438\n",
            "training epoch: [2391 ] loss: 1.060 correct: 1290.000, total: 3000.000, accuracy: 0.430\n",
            "training epoch: [2392 ] loss: 1.059 correct: 1315.000, total: 3000.000, accuracy: 0.438\n",
            "training epoch: [2393 ] loss: 1.060 correct: 1292.000, total: 3000.000, accuracy: 0.431\n",
            "training epoch: [2394 ] loss: 1.059 correct: 1317.000, total: 3000.000, accuracy: 0.439\n",
            "training epoch: [2395 ] loss: 1.060 correct: 1291.000, total: 3000.000, accuracy: 0.430\n",
            "training epoch: [2396 ] loss: 1.059 correct: 1315.000, total: 3000.000, accuracy: 0.438\n",
            "training epoch: [2397 ] loss: 1.059 correct: 1289.000, total: 3000.000, accuracy: 0.430\n",
            "training epoch: [2398 ] loss: 1.059 correct: 1315.000, total: 3000.000, accuracy: 0.438\n",
            "training epoch: [2399 ] loss: 1.059 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2400 ] loss: 1.059 correct: 1315.000, total: 3000.000, accuracy: 0.438\n",
            "training epoch: [2401 ] loss: 1.059 correct: 1285.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [2402 ] loss: 1.059 correct: 1313.000, total: 3000.000, accuracy: 0.438\n",
            "training epoch: [2403 ] loss: 1.059 correct: 1290.000, total: 3000.000, accuracy: 0.430\n",
            "training epoch: [2404 ] loss: 1.059 correct: 1308.000, total: 3000.000, accuracy: 0.436\n",
            "training epoch: [2405 ] loss: 1.059 correct: 1291.000, total: 3000.000, accuracy: 0.430\n",
            "training epoch: [2406 ] loss: 1.059 correct: 1309.000, total: 3000.000, accuracy: 0.436\n",
            "training epoch: [2407 ] loss: 1.059 correct: 1292.000, total: 3000.000, accuracy: 0.431\n",
            "training epoch: [2408 ] loss: 1.059 correct: 1308.000, total: 3000.000, accuracy: 0.436\n",
            "training epoch: [2409 ] loss: 1.059 correct: 1295.000, total: 3000.000, accuracy: 0.432\n",
            "training epoch: [2410 ] loss: 1.059 correct: 1307.000, total: 3000.000, accuracy: 0.436\n",
            "training epoch: [2411 ] loss: 1.059 correct: 1297.000, total: 3000.000, accuracy: 0.432\n",
            "training epoch: [2412 ] loss: 1.059 correct: 1307.000, total: 3000.000, accuracy: 0.436\n",
            "training epoch: [2413 ] loss: 1.059 correct: 1298.000, total: 3000.000, accuracy: 0.433\n",
            "training epoch: [2414 ] loss: 1.059 correct: 1307.000, total: 3000.000, accuracy: 0.436\n",
            "training epoch: [2415 ] loss: 1.059 correct: 1300.000, total: 3000.000, accuracy: 0.433\n",
            "training epoch: [2416 ] loss: 1.059 correct: 1302.000, total: 3000.000, accuracy: 0.434\n",
            "training epoch: [2417 ] loss: 1.059 correct: 1301.000, total: 3000.000, accuracy: 0.434\n",
            "training epoch: [2418 ] loss: 1.059 correct: 1302.000, total: 3000.000, accuracy: 0.434\n",
            "training epoch: [2419 ] loss: 1.059 correct: 1303.000, total: 3000.000, accuracy: 0.434\n",
            "training epoch: [2420 ] loss: 1.059 correct: 1302.000, total: 3000.000, accuracy: 0.434\n",
            "training epoch: [2421 ] loss: 1.059 correct: 1305.000, total: 3000.000, accuracy: 0.435\n",
            "training epoch: [2422 ] loss: 1.059 correct: 1303.000, total: 3000.000, accuracy: 0.434\n",
            "training epoch: [2423 ] loss: 1.059 correct: 1303.000, total: 3000.000, accuracy: 0.434\n",
            "training epoch: [2424 ] loss: 1.059 correct: 1303.000, total: 3000.000, accuracy: 0.434\n",
            "training epoch: [2425 ] loss: 1.059 correct: 1301.000, total: 3000.000, accuracy: 0.434\n",
            "training epoch: [2426 ] loss: 1.059 correct: 1303.000, total: 3000.000, accuracy: 0.434\n",
            "training epoch: [2427 ] loss: 1.059 correct: 1302.000, total: 3000.000, accuracy: 0.434\n",
            "training epoch: [2428 ] loss: 1.059 correct: 1301.000, total: 3000.000, accuracy: 0.434\n",
            "training epoch: [2429 ] loss: 1.059 correct: 1303.000, total: 3000.000, accuracy: 0.434\n",
            "training epoch: [2430 ] loss: 1.059 correct: 1301.000, total: 3000.000, accuracy: 0.434\n",
            "training epoch: [2431 ] loss: 1.059 correct: 1305.000, total: 3000.000, accuracy: 0.435\n",
            "training epoch: [2432 ] loss: 1.059 correct: 1298.000, total: 3000.000, accuracy: 0.433\n",
            "training epoch: [2433 ] loss: 1.059 correct: 1306.000, total: 3000.000, accuracy: 0.435\n",
            "training epoch: [2434 ] loss: 1.059 correct: 1295.000, total: 3000.000, accuracy: 0.432\n",
            "training epoch: [2435 ] loss: 1.059 correct: 1309.000, total: 3000.000, accuracy: 0.436\n",
            "training epoch: [2436 ] loss: 1.059 correct: 1293.000, total: 3000.000, accuracy: 0.431\n",
            "training epoch: [2437 ] loss: 1.059 correct: 1312.000, total: 3000.000, accuracy: 0.437\n",
            "training epoch: [2438 ] loss: 1.059 correct: 1291.000, total: 3000.000, accuracy: 0.430\n",
            "training epoch: [2439 ] loss: 1.059 correct: 1313.000, total: 3000.000, accuracy: 0.438\n",
            "training epoch: [2440 ] loss: 1.059 correct: 1284.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [2441 ] loss: 1.059 correct: 1312.000, total: 3000.000, accuracy: 0.437\n",
            "training epoch: [2442 ] loss: 1.059 correct: 1283.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [2443 ] loss: 1.059 correct: 1318.000, total: 3000.000, accuracy: 0.439\n",
            "training epoch: [2444 ] loss: 1.059 correct: 1290.000, total: 3000.000, accuracy: 0.430\n",
            "training epoch: [2445 ] loss: 1.059 correct: 1319.000, total: 3000.000, accuracy: 0.440\n",
            "training epoch: [2446 ] loss: 1.059 correct: 1289.000, total: 3000.000, accuracy: 0.430\n",
            "training epoch: [2447 ] loss: 1.059 correct: 1318.000, total: 3000.000, accuracy: 0.439\n",
            "training epoch: [2448 ] loss: 1.059 correct: 1290.000, total: 3000.000, accuracy: 0.430\n",
            "training epoch: [2449 ] loss: 1.059 correct: 1321.000, total: 3000.000, accuracy: 0.440\n",
            "training epoch: [2450 ] loss: 1.059 correct: 1289.000, total: 3000.000, accuracy: 0.430\n",
            "training epoch: [2451 ] loss: 1.059 correct: 1320.000, total: 3000.000, accuracy: 0.440\n",
            "training epoch: [2452 ] loss: 1.059 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2453 ] loss: 1.059 correct: 1315.000, total: 3000.000, accuracy: 0.438\n",
            "training epoch: [2454 ] loss: 1.059 correct: 1290.000, total: 3000.000, accuracy: 0.430\n",
            "training epoch: [2455 ] loss: 1.059 correct: 1316.000, total: 3000.000, accuracy: 0.439\n",
            "training epoch: [2456 ] loss: 1.059 correct: 1290.000, total: 3000.000, accuracy: 0.430\n",
            "training epoch: [2457 ] loss: 1.059 correct: 1317.000, total: 3000.000, accuracy: 0.439\n",
            "training epoch: [2458 ] loss: 1.059 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2459 ] loss: 1.059 correct: 1314.000, total: 3000.000, accuracy: 0.438\n",
            "training epoch: [2460 ] loss: 1.059 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2461 ] loss: 1.059 correct: 1311.000, total: 3000.000, accuracy: 0.437\n",
            "training epoch: [2462 ] loss: 1.059 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2463 ] loss: 1.059 correct: 1309.000, total: 3000.000, accuracy: 0.436\n",
            "training epoch: [2464 ] loss: 1.059 correct: 1291.000, total: 3000.000, accuracy: 0.430\n",
            "training epoch: [2465 ] loss: 1.059 correct: 1310.000, total: 3000.000, accuracy: 0.437\n",
            "training epoch: [2466 ] loss: 1.059 correct: 1294.000, total: 3000.000, accuracy: 0.431\n",
            "training epoch: [2467 ] loss: 1.059 correct: 1309.000, total: 3000.000, accuracy: 0.436\n",
            "training epoch: [2468 ] loss: 1.059 correct: 1296.000, total: 3000.000, accuracy: 0.432\n",
            "training epoch: [2469 ] loss: 1.059 correct: 1309.000, total: 3000.000, accuracy: 0.436\n",
            "training epoch: [2470 ] loss: 1.059 correct: 1299.000, total: 3000.000, accuracy: 0.433\n",
            "training epoch: [2471 ] loss: 1.059 correct: 1308.000, total: 3000.000, accuracy: 0.436\n",
            "training epoch: [2472 ] loss: 1.059 correct: 1298.000, total: 3000.000, accuracy: 0.433\n",
            "training epoch: [2473 ] loss: 1.059 correct: 1308.000, total: 3000.000, accuracy: 0.436\n",
            "training epoch: [2474 ] loss: 1.059 correct: 1299.000, total: 3000.000, accuracy: 0.433\n",
            "training epoch: [2475 ] loss: 1.059 correct: 1307.000, total: 3000.000, accuracy: 0.436\n",
            "training epoch: [2476 ] loss: 1.059 correct: 1300.000, total: 3000.000, accuracy: 0.433\n",
            "training epoch: [2477 ] loss: 1.059 correct: 1309.000, total: 3000.000, accuracy: 0.436\n",
            "training epoch: [2478 ] loss: 1.059 correct: 1299.000, total: 3000.000, accuracy: 0.433\n",
            "training epoch: [2479 ] loss: 1.059 correct: 1309.000, total: 3000.000, accuracy: 0.436\n",
            "training epoch: [2480 ] loss: 1.059 correct: 1301.000, total: 3000.000, accuracy: 0.434\n",
            "training epoch: [2481 ] loss: 1.059 correct: 1310.000, total: 3000.000, accuracy: 0.437\n",
            "training epoch: [2482 ] loss: 1.059 correct: 1296.000, total: 3000.000, accuracy: 0.432\n",
            "training epoch: [2483 ] loss: 1.059 correct: 1310.000, total: 3000.000, accuracy: 0.437\n",
            "training epoch: [2484 ] loss: 1.059 correct: 1297.000, total: 3000.000, accuracy: 0.432\n",
            "training epoch: [2485 ] loss: 1.058 correct: 1312.000, total: 3000.000, accuracy: 0.437\n",
            "training epoch: [2486 ] loss: 1.059 correct: 1295.000, total: 3000.000, accuracy: 0.432\n",
            "training epoch: [2487 ] loss: 1.058 correct: 1315.000, total: 3000.000, accuracy: 0.438\n",
            "training epoch: [2488 ] loss: 1.058 correct: 1295.000, total: 3000.000, accuracy: 0.432\n",
            "training epoch: [2489 ] loss: 1.058 correct: 1314.000, total: 3000.000, accuracy: 0.438\n",
            "training epoch: [2490 ] loss: 1.058 correct: 1294.000, total: 3000.000, accuracy: 0.431\n",
            "training epoch: [2491 ] loss: 1.058 correct: 1314.000, total: 3000.000, accuracy: 0.438\n",
            "training epoch: [2492 ] loss: 1.058 correct: 1295.000, total: 3000.000, accuracy: 0.432\n",
            "training epoch: [2493 ] loss: 1.058 correct: 1312.000, total: 3000.000, accuracy: 0.437\n",
            "training epoch: [2494 ] loss: 1.058 correct: 1299.000, total: 3000.000, accuracy: 0.433\n",
            "training epoch: [2495 ] loss: 1.058 correct: 1310.000, total: 3000.000, accuracy: 0.437\n",
            "training epoch: [2496 ] loss: 1.058 correct: 1302.000, total: 3000.000, accuracy: 0.434\n",
            "training epoch: [2497 ] loss: 1.058 correct: 1310.000, total: 3000.000, accuracy: 0.437\n",
            "training epoch: [2498 ] loss: 1.058 correct: 1305.000, total: 3000.000, accuracy: 0.435\n",
            "training epoch: [2499 ] loss: 1.058 correct: 1308.000, total: 3000.000, accuracy: 0.436\n",
            "training epoch: [2500 ] loss: 1.058 correct: 1304.000, total: 3000.000, accuracy: 0.435\n",
            "Finished Training run \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AciJnAh5nfug"
      },
      "source": [
        "columns = [\"epochs\", \"argmax > 0.5\" ,\"argmax < 0.5\", \"focus_true_pred_true\", \"focus_false_pred_true\", \"focus_true_pred_false\", \"focus_false_pred_false\" ]\n",
        "df_train = pd.DataFrame()\n",
        "df_test = pd.DataFrame()\n",
        "df_train[columns[0]] = np.arange(0,epoch+2)\n",
        "df_train[columns[1]] = analysis_data_tr[:,-2]/30\n",
        "df_train[columns[2]] = analysis_data_tr[:,-1]/30\n",
        "df_train[columns[3]] = analysis_data_tr[:,0]/30\n",
        "df_train[columns[4]] = analysis_data_tr[:,1]/30\n",
        "df_train[columns[5]] = analysis_data_tr[:,2]/30\n",
        "df_train[columns[6]] = analysis_data_tr[:,3]/30"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NoQpS_6scRsC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "a77941ee-aaf7-4391-efe3-efef1b7b7085"
      },
      "source": [
        "df_train"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>epochs</th>\n",
              "      <th>argmax &gt; 0.5</th>\n",
              "      <th>argmax &lt; 0.5</th>\n",
              "      <th>focus_true_pred_true</th>\n",
              "      <th>focus_false_pred_true</th>\n",
              "      <th>focus_true_pred_false</th>\n",
              "      <th>focus_false_pred_false</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>9.800000</td>\n",
              "      <td>90.200000</td>\n",
              "      <td>3.800000</td>\n",
              "      <td>28.566667</td>\n",
              "      <td>7.966667</td>\n",
              "      <td>59.666667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>9.800000</td>\n",
              "      <td>90.200000</td>\n",
              "      <td>3.800000</td>\n",
              "      <td>28.500000</td>\n",
              "      <td>7.966667</td>\n",
              "      <td>59.733333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>9.800000</td>\n",
              "      <td>90.200000</td>\n",
              "      <td>3.800000</td>\n",
              "      <td>28.533333</td>\n",
              "      <td>7.966667</td>\n",
              "      <td>59.700000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>9.800000</td>\n",
              "      <td>90.200000</td>\n",
              "      <td>3.800000</td>\n",
              "      <td>28.633333</td>\n",
              "      <td>7.966667</td>\n",
              "      <td>59.600000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>9.833333</td>\n",
              "      <td>90.166667</td>\n",
              "      <td>3.800000</td>\n",
              "      <td>28.566667</td>\n",
              "      <td>7.966667</td>\n",
              "      <td>59.666667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2496</th>\n",
              "      <td>2496</td>\n",
              "      <td>16.200000</td>\n",
              "      <td>83.800000</td>\n",
              "      <td>13.200000</td>\n",
              "      <td>30.200000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>49.600000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2497</th>\n",
              "      <td>2497</td>\n",
              "      <td>16.233333</td>\n",
              "      <td>83.766667</td>\n",
              "      <td>13.333333</td>\n",
              "      <td>30.333333</td>\n",
              "      <td>6.866667</td>\n",
              "      <td>49.466667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2498</th>\n",
              "      <td>2498</td>\n",
              "      <td>16.200000</td>\n",
              "      <td>83.800000</td>\n",
              "      <td>13.233333</td>\n",
              "      <td>30.266667</td>\n",
              "      <td>6.966667</td>\n",
              "      <td>49.533333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2499</th>\n",
              "      <td>2499</td>\n",
              "      <td>16.233333</td>\n",
              "      <td>83.766667</td>\n",
              "      <td>13.333333</td>\n",
              "      <td>30.266667</td>\n",
              "      <td>6.866667</td>\n",
              "      <td>49.533333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2500</th>\n",
              "      <td>2500</td>\n",
              "      <td>16.200000</td>\n",
              "      <td>83.800000</td>\n",
              "      <td>13.233333</td>\n",
              "      <td>30.233333</td>\n",
              "      <td>6.966667</td>\n",
              "      <td>49.566667</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2501 rows  7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      epochs  argmax > 0.5  ...  focus_true_pred_false  focus_false_pred_false\n",
              "0          0      9.800000  ...               7.966667               59.666667\n",
              "1          1      9.800000  ...               7.966667               59.733333\n",
              "2          2      9.800000  ...               7.966667               59.700000\n",
              "3          3      9.800000  ...               7.966667               59.600000\n",
              "4          4      9.833333  ...               7.966667               59.666667\n",
              "...      ...           ...  ...                    ...                     ...\n",
              "2496    2496     16.200000  ...               7.000000               49.600000\n",
              "2497    2497     16.233333  ...               6.866667               49.466667\n",
              "2498    2498     16.200000  ...               6.966667               49.533333\n",
              "2499    2499     16.233333  ...               6.866667               49.533333\n",
              "2500    2500     16.200000  ...               6.966667               49.566667\n",
              "\n",
              "[2501 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EY_j8B274vuH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "outputId": "73e83ae9-0757-4102-aa5f-f81b88cad07d"
      },
      "source": [
        "%cd /content/\n",
        "plot_analysis(df_train,columns,[0,500,1000,1500,2000,2500])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCnS6r2_3WdU"
      },
      "source": [
        "aph = []\n",
        "for i in bg:\n",
        "  aph.append(F.softmax(i,dim=1).detach().numpy())\n",
        "  \n",
        "aph = np.concatenate(aph,axis=0)\n",
        "# torch.save({\n",
        "#             'epoch': 500,\n",
        "#             'model_state_dict': what_net.state_dict(),\n",
        "#             #'optimizer_state_dict': optimizer_what.state_dict(),\n",
        "#             \"optimizer_alpha\":optim1,\n",
        "#             \"FTPT_analysis\":analysis_data_tr,\n",
        "#             \"alpha\":aph\n",
        "\n",
        "#             }, \"type4_what_net_500.pt\")"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVzrDOGS4UxU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7e6e2e5-25a2-4d99-90f2-3b8fc78c3d6b"
      },
      "source": [
        "aph[0]"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.03183711, 0.05601705, 0.1275119 , 0.06249511, 0.3937274 ,\n",
              "       0.22241463, 0.05506597, 0.01307493, 0.03785592], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Ut6ZTAXbvqx"
      },
      "source": [
        "avrg = []\n",
        "avrg_lbls = []\n",
        "with torch.no_grad():\n",
        "  for i, data1 in  enumerate(train_loader):\n",
        "          inputs , labels , fore_idx = data1\n",
        "          inputs = inputs.double()\n",
        "          inputs = inputs.to(\"cuda\")\n",
        "          beta  = bg[i]\n",
        "          beta = beta.to(\"cuda\")\n",
        "          avg,alpha = attn_avg(inputs,beta)\n",
        "          \n",
        "          avrg.append(avg.detach().cpu().numpy())\n",
        "          avrg_lbls.append(labels.numpy())\n",
        "avrg= np.concatenate(avrg,axis=0)\n",
        "avrg_lbls = np.concatenate(avrg_lbls,axis=0)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KQFYlmTLG0N",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "a0a5dbf2-b483-4ad6-cfa2-02e39c00a07b"
      },
      "source": [
        "%cd /content/drive/MyDrive/Neural_Tangent_Kernel/\n",
        "data = np.load(\"type_4_data.npy\",allow_pickle=True)\n",
        "%cd /content/\n",
        "plot_decision_boundary(what_net,[1,8,2,9],data,bg,avrg,avrg_lbls)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Neural_Tangent_Kernel\n",
            "/content\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqS70WNvOcIw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GmyEWKD92_T"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ZQMzXXJa8lB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6WUeyvNO3iP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQRNesx7O3Xr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLPSDVK_QWId"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hw2rtHfzFyFA"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3lfGywVHL6S"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
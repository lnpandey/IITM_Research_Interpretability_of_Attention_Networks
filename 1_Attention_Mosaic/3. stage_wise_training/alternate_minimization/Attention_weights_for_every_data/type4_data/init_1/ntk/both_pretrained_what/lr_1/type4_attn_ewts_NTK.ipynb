{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "type4_attn_ewts_NTK.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7pI7PJ8XATdT",
        "outputId": "423517a6-4d35-49c8-c02e-18bde66f6c32"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9qVDQd_BBqS",
        "outputId": "1b3bd01b-9a5b-4d80-f2d4-36fc7d46a26b"
      },
      "source": [
        "%cd /content/drive/MyDrive/Neural_Tangent_Kernel/"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Neural_Tangent_Kernel\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWIyC9Ip_bcq"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "\n",
        "from myrmsprop import MyRmsprop\n",
        "from utils import plot_decision_boundary,attn_avg,plot_analysis\n",
        "from synthetic_dataset import MosaicDataset1\n",
        "from eval_model import calculate_attn_loss,analyse_data\n",
        "\n",
        "\n",
        "\n",
        "%matplotlib inline\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGVy-1EllAc_"
      },
      "source": [
        "train_data = np.load(\"train_type4_data.npy\",allow_pickle=True)\n",
        "\n",
        "test_data = np.load(\"test_type4_data.npy\",allow_pickle=True)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uL771xuGZC5Q"
      },
      "source": [
        "mosaic_list_of_images = train_data[0][\"mosaic_list\"]\n",
        "mosaic_label = train_data[0][\"mosaic_label\"]\n",
        "fore_idx = train_data[0][\"fore_idx\"]\n",
        "\n",
        "\n",
        "test_mosaic_list_of_images = test_data[0][\"mosaic_list\"]\n",
        "test_mosaic_label = test_data[0][\"mosaic_label\"]\n",
        "test_fore_idx = test_data[0][\"fore_idx\"]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uf76JwkxZCT0"
      },
      "source": [
        "batch = 3000\n",
        "train_dataset = MosaicDataset1(mosaic_list_of_images, mosaic_label, fore_idx)\n",
        "train_loader = DataLoader( train_dataset,batch_size= batch ,shuffle=False)\n",
        "#batch = 2000\n",
        "#test_dataset = MosaicDataset1(test_mosaic_list_of_images, test_mosaic_label, test_fore_idx)\n",
        "#test_loader = DataLoader(test_dataset,batch_size= batch ,shuffle=False)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-Lv8nHoB8z-"
      },
      "source": [
        "# NTK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmGjlMfTBp3F"
      },
      "source": [
        "data = np.load(\"NTK_1.npy\",allow_pickle=True)\n",
        "# H = data[0]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beyk_-qYB_Ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02e3a961-8825-461a-a4a8-d25eef8121e8"
      },
      "source": [
        "print(data[0].keys())\n",
        "H = torch.tensor(data[0][\"NTK\"])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['NTK'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ULTAsyF6G6a"
      },
      "source": [
        "lr_1 = 1/1470559.2\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnqbuxdO2U4j"
      },
      "source": [
        "# p_vec = nn.utils.parameters_to_vector(where_func.parameters())\n",
        "# p, = p_vec.shape\n",
        "# n_m, n_obj,_ = inputs.shape  # number of mosaic images x number of objects in each mosaic  x d\n",
        "# # this is the transpose jacobian (grad y(w))^T)\n",
        "# features = torch.zeros(n_m*n_obj, p, requires_grad=False)\n",
        " \n",
        "# k = 0 \n",
        "\n",
        "\n",
        "# for i in range(27000):\n",
        "#     out = where_func(inpp[i])\n",
        "#     where_func.zero_grad()\n",
        "#     out.backward(retain_graph=False)\n",
        "#     p_grad = torch.tensor([], requires_grad=False)\n",
        "#     for p in where_func.parameters():\n",
        "#       p_grad = torch.cat((p_grad, p.grad.reshape(-1)))\n",
        "#     features[k,:] = p_grad\n",
        "#     k = k+1\n",
        "# tangent_kernel =  features@features.T"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SInPc5gk9XDH"
      },
      "source": [
        "# class Module1(nn.Module):\n",
        "#   def __init__(self):\n",
        "#     super(Module1, self).__init__()\n",
        "#     self.linear1 = nn.Linear(2,100)\n",
        "#     self.linear2 = nn.Linear(100,1)\n",
        "\n",
        "#   def forward(self,x):\n",
        "#     x = F.relu(self.linear1(x))\n",
        "#     x = self.linear2(x)\n",
        "#     return x"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VW0lzy6i9wk0"
      },
      "source": [
        "# from tqdm import tqdm as tqdm"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cti_LAbE8-dn"
      },
      "source": [
        "# inputs,_,_ = iter(train_loader).next()\n",
        "# inputs = torch.reshape(inputs,(27000,2))\n",
        "# inputs = (inputs - torch.mean(inputs,dim=0,keepdims=True) )/torch.std(inputs,dim=0,keepdims=True)\n",
        "# where_net = Module1()\n",
        "# outputs = where_net(inputs)\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-03FnsNP5bk"
      },
      "source": [
        "# feature1 = torch.zeros((27000,200))\n",
        "# feature2  = torch.zeros((27000,100))\n",
        "# for i in tqdm(range(27000)):\n",
        "#   where_net.zero_grad()\n",
        "#   outputs[i].backward(retain_graph=True)\n",
        "#   par = []\n",
        "#   j = 0\n",
        "#   for p in where_net.parameters():\n",
        "#     if j%2 == 0:\n",
        "#       vec = torch.nn.utils.parameters_to_vector(p)\n",
        "#       p_grad = p.grad.reshape(-1)\n",
        "#       par.append(p_grad)\n",
        "#     j = j+1\n",
        "#   feature1[i,:] = par[0]\n",
        "#   feature2[i,:] = par[1]"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eI20WxiR-zCi"
      },
      "source": [
        "# H = feature1@feature1.T + feature2@feature2.T"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWIBQfQly25h"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HbrMidFCla6h"
      },
      "source": [
        "class Module2(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Module2, self).__init__()\n",
        "    self.linear1 = nn.Linear(2,100)\n",
        "    self.linear2 = nn.Linear(100,3)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = F.relu(self.linear1(x))\n",
        "    x = self.linear2(x)\n",
        "    return x"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXpnLkMoCocj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd805add-9c41-4e7f-c4f9-8bfe15161213"
      },
      "source": [
        "print(H)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[75.1031, 35.6146, 79.1224,  ..., 63.2880, 38.2985, 72.4251],\n",
            "        [35.6146, 21.1840, 42.9770,  ..., 33.6579, 25.0675, 44.2191],\n",
            "        [79.1224, 42.9770, 92.8314,  ..., 73.0225, 48.9726, 88.7758],\n",
            "        ...,\n",
            "        [63.2880, 33.6579, 73.0225,  ..., 58.0862, 38.0167, 69.3583],\n",
            "        [38.2985, 25.0675, 48.9726,  ..., 38.0167, 34.9229, 54.7437],\n",
            "        [72.4251, 44.2191, 88.7758,  ..., 69.3583, 54.7437, 95.1200]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRDhoG3rEp_w"
      },
      "source": [
        ""
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRqj2VELllkX"
      },
      "source": [
        "torch.manual_seed(1234)\n",
        "what_net = Module2().double()\n",
        "\n",
        "what_net.load_state_dict(torch.load(\"type4_what_net.pt\"))\n",
        "what_net = what_net.to(\"cuda\")"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oc1pKMEVfhat"
      },
      "source": [
        ""
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOpZfj1bq7wN"
      },
      "source": [
        "n_batches = 3000//batch\n",
        "bg = []\n",
        "for i in range(n_batches):\n",
        "  torch.manual_seed(i)\n",
        "  betag = torch.randn(3000,9)#torch.ones((250,9))/9\n",
        "  bg.append( betag.requires_grad_() )"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76PwzSMACDDj"
      },
      "source": [
        "# training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S633XgMToeN3"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lrDkUUaDFCR"
      },
      "source": [
        "optim1 = []\n",
        "H= H.to(\"cpu\")\n",
        "for i in range(n_batches):\n",
        "  optim1.append(MyRmsprop([bg[i]],H=H,lr=1))\n",
        "# instantiate what net optimizer\n",
        "optimizer_what = optim.RMSprop(what_net.parameters(), lr=0.0001)#, momentum=0.9)#,nesterov=True)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPaYaojinMTA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e19f7a3a-34af-4cc8-a251-afe12a32a0a2"
      },
      "source": [
        "\n",
        "acti = []\n",
        "analysis_data_tr = []\n",
        "analysis_data_tst = []\n",
        "loss_curi_tr = []\n",
        "loss_curi_tst = []\n",
        "epochs = 2500\n",
        "\n",
        "\n",
        "# calculate zeroth epoch loss and FTPT values\n",
        "running_loss,anlys_data,correct,total,accuracy = calculate_attn_loss(train_loader,bg,what_net,criterion)\n",
        "print('training epoch: [%d ] loss: %.3f correct: %.3f, total: %.3f, accuracy: %.3f' %(0,running_loss,correct,total,accuracy)) \n",
        "loss_curi_tr.append(running_loss)\n",
        "analysis_data_tr.append(anlys_data)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# training starts \n",
        "for epoch in range(epochs): # loop over the dataset multiple times\n",
        "  ep_lossi = []\n",
        "  running_loss = 0.0\n",
        "  what_net.train()\n",
        "  for i, data in enumerate(train_loader, 0):\n",
        "    # get the inputs\n",
        "    inputs, labels,_  = data\n",
        "    inputs = inputs.double()\n",
        "    beta = bg[i] # alpha for ith batch\n",
        "    #print(labels)\n",
        "    inputs, labels,beta = inputs.to(\"cuda\"),labels.to(\"cuda\"),beta.to(\"cuda\")\n",
        "        \n",
        "    # zero the parameter gradients\n",
        "    optimizer_what.zero_grad()\n",
        "    optim1[i].zero_grad()\n",
        "      \n",
        "    # forward + backward + optimize\n",
        "    avg,alpha = attn_avg(inputs,beta)\n",
        "    outputs = what_net(avg)     \n",
        "    loss = criterion(outputs, labels)\n",
        "\n",
        "    # print statistics\n",
        "    running_loss += loss.item()\n",
        "    #alpha.retain_grad()\n",
        "    loss.backward(retain_graph=False)\n",
        "    optimizer_what.step()\n",
        "    optim1[i].step()\n",
        "\n",
        "\n",
        "  running_loss_tr,anls_data,correct,total,accuracy = calculate_attn_loss(train_loader,bg,what_net,criterion)\n",
        "  analysis_data_tr.append(anls_data)\n",
        "  loss_curi_tr.append(running_loss_tr)   #loss per epoch\n",
        "  print('training epoch: [%d ] loss: %.3f correct: %.3f, total: %.3f, accuracy: %.3f' %(epoch+1,running_loss_tr,correct,total,accuracy)) \n",
        "\n",
        "\n",
        "  \n",
        "  if running_loss_tr<=0.08:\n",
        "    break\n",
        "print('Finished Training run ')\n",
        "analysis_data_tr = np.array(analysis_data_tr)\n",
        "\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training epoch: [0 ] loss: 10.201 correct: 1133.000, total: 3000.000, accuracy: 0.378\n",
            "training epoch: [1 ] loss: 21.519 correct: 982.000, total: 3000.000, accuracy: 0.327\n",
            "training epoch: [2 ] loss: 19.156 correct: 981.000, total: 3000.000, accuracy: 0.327\n",
            "training epoch: [3 ] loss: 18.476 correct: 980.000, total: 3000.000, accuracy: 0.327\n",
            "training epoch: [4 ] loss: 18.127 correct: 981.000, total: 3000.000, accuracy: 0.327\n",
            "training epoch: [5 ] loss: 17.889 correct: 985.000, total: 3000.000, accuracy: 0.328\n",
            "training epoch: [6 ] loss: 17.703 correct: 988.000, total: 3000.000, accuracy: 0.329\n",
            "training epoch: [7 ] loss: 17.549 correct: 992.000, total: 3000.000, accuracy: 0.331\n",
            "training epoch: [8 ] loss: 17.411 correct: 997.000, total: 3000.000, accuracy: 0.332\n",
            "training epoch: [9 ] loss: 17.288 correct: 1001.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [10 ] loss: 17.176 correct: 1002.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [11 ] loss: 17.072 correct: 1002.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [12 ] loss: 16.976 correct: 1002.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [13 ] loss: 16.886 correct: 1006.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [14 ] loss: 16.802 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [15 ] loss: 16.724 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [16 ] loss: 16.649 correct: 1018.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [17 ] loss: 16.576 correct: 1028.000, total: 3000.000, accuracy: 0.343\n",
            "training epoch: [18 ] loss: 16.506 correct: 1029.000, total: 3000.000, accuracy: 0.343\n",
            "training epoch: [19 ] loss: 16.439 correct: 1031.000, total: 3000.000, accuracy: 0.344\n",
            "training epoch: [20 ] loss: 16.375 correct: 1035.000, total: 3000.000, accuracy: 0.345\n",
            "training epoch: [21 ] loss: 16.312 correct: 1036.000, total: 3000.000, accuracy: 0.345\n",
            "training epoch: [22 ] loss: 16.252 correct: 1038.000, total: 3000.000, accuracy: 0.346\n",
            "training epoch: [23 ] loss: 16.195 correct: 1042.000, total: 3000.000, accuracy: 0.347\n",
            "training epoch: [24 ] loss: 16.139 correct: 1043.000, total: 3000.000, accuracy: 0.348\n",
            "training epoch: [25 ] loss: 16.085 correct: 1044.000, total: 3000.000, accuracy: 0.348\n",
            "training epoch: [26 ] loss: 16.032 correct: 1045.000, total: 3000.000, accuracy: 0.348\n",
            "training epoch: [27 ] loss: 15.980 correct: 1047.000, total: 3000.000, accuracy: 0.349\n",
            "training epoch: [28 ] loss: 15.931 correct: 1049.000, total: 3000.000, accuracy: 0.350\n",
            "training epoch: [29 ] loss: 15.883 correct: 1053.000, total: 3000.000, accuracy: 0.351\n",
            "training epoch: [30 ] loss: 15.837 correct: 1054.000, total: 3000.000, accuracy: 0.351\n",
            "training epoch: [31 ] loss: 15.791 correct: 1057.000, total: 3000.000, accuracy: 0.352\n",
            "training epoch: [32 ] loss: 15.747 correct: 1056.000, total: 3000.000, accuracy: 0.352\n",
            "training epoch: [33 ] loss: 15.703 correct: 1057.000, total: 3000.000, accuracy: 0.352\n",
            "training epoch: [34 ] loss: 15.660 correct: 1057.000, total: 3000.000, accuracy: 0.352\n",
            "training epoch: [35 ] loss: 15.617 correct: 1057.000, total: 3000.000, accuracy: 0.352\n",
            "training epoch: [36 ] loss: 15.576 correct: 1060.000, total: 3000.000, accuracy: 0.353\n",
            "training epoch: [37 ] loss: 15.535 correct: 1062.000, total: 3000.000, accuracy: 0.354\n",
            "training epoch: [38 ] loss: 15.495 correct: 1064.000, total: 3000.000, accuracy: 0.355\n",
            "training epoch: [39 ] loss: 15.455 correct: 1066.000, total: 3000.000, accuracy: 0.355\n",
            "training epoch: [40 ] loss: 15.416 correct: 1067.000, total: 3000.000, accuracy: 0.356\n",
            "training epoch: [41 ] loss: 15.376 correct: 1062.000, total: 3000.000, accuracy: 0.354\n",
            "training epoch: [42 ] loss: 15.338 correct: 1064.000, total: 3000.000, accuracy: 0.355\n",
            "training epoch: [43 ] loss: 15.300 correct: 1063.000, total: 3000.000, accuracy: 0.354\n",
            "training epoch: [44 ] loss: 15.264 correct: 1063.000, total: 3000.000, accuracy: 0.354\n",
            "training epoch: [45 ] loss: 15.227 correct: 1062.000, total: 3000.000, accuracy: 0.354\n",
            "training epoch: [46 ] loss: 15.191 correct: 1061.000, total: 3000.000, accuracy: 0.354\n",
            "training epoch: [47 ] loss: 15.156 correct: 1056.000, total: 3000.000, accuracy: 0.352\n",
            "training epoch: [48 ] loss: 15.122 correct: 1058.000, total: 3000.000, accuracy: 0.353\n",
            "training epoch: [49 ] loss: 15.088 correct: 1057.000, total: 3000.000, accuracy: 0.352\n",
            "training epoch: [50 ] loss: 15.056 correct: 1059.000, total: 3000.000, accuracy: 0.353\n",
            "training epoch: [51 ] loss: 15.023 correct: 1058.000, total: 3000.000, accuracy: 0.353\n",
            "training epoch: [52 ] loss: 14.992 correct: 1061.000, total: 3000.000, accuracy: 0.354\n",
            "training epoch: [53 ] loss: 14.961 correct: 1058.000, total: 3000.000, accuracy: 0.353\n",
            "training epoch: [54 ] loss: 14.931 correct: 1053.000, total: 3000.000, accuracy: 0.351\n",
            "training epoch: [55 ] loss: 14.901 correct: 1052.000, total: 3000.000, accuracy: 0.351\n",
            "training epoch: [56 ] loss: 14.872 correct: 1053.000, total: 3000.000, accuracy: 0.351\n",
            "training epoch: [57 ] loss: 14.843 correct: 1054.000, total: 3000.000, accuracy: 0.351\n",
            "training epoch: [58 ] loss: 14.814 correct: 1051.000, total: 3000.000, accuracy: 0.350\n",
            "training epoch: [59 ] loss: 14.786 correct: 1052.000, total: 3000.000, accuracy: 0.351\n",
            "training epoch: [60 ] loss: 14.758 correct: 1051.000, total: 3000.000, accuracy: 0.350\n",
            "training epoch: [61 ] loss: 14.730 correct: 1052.000, total: 3000.000, accuracy: 0.351\n",
            "training epoch: [62 ] loss: 14.702 correct: 1052.000, total: 3000.000, accuracy: 0.351\n",
            "training epoch: [63 ] loss: 14.674 correct: 1055.000, total: 3000.000, accuracy: 0.352\n",
            "training epoch: [64 ] loss: 14.647 correct: 1058.000, total: 3000.000, accuracy: 0.353\n",
            "training epoch: [65 ] loss: 14.620 correct: 1056.000, total: 3000.000, accuracy: 0.352\n",
            "training epoch: [66 ] loss: 14.594 correct: 1054.000, total: 3000.000, accuracy: 0.351\n",
            "training epoch: [67 ] loss: 14.568 correct: 1054.000, total: 3000.000, accuracy: 0.351\n",
            "training epoch: [68 ] loss: 14.543 correct: 1054.000, total: 3000.000, accuracy: 0.351\n",
            "training epoch: [69 ] loss: 14.517 correct: 1054.000, total: 3000.000, accuracy: 0.351\n",
            "training epoch: [70 ] loss: 14.492 correct: 1055.000, total: 3000.000, accuracy: 0.352\n",
            "training epoch: [71 ] loss: 14.467 correct: 1059.000, total: 3000.000, accuracy: 0.353\n",
            "training epoch: [72 ] loss: 14.442 correct: 1060.000, total: 3000.000, accuracy: 0.353\n",
            "training epoch: [73 ] loss: 14.416 correct: 1064.000, total: 3000.000, accuracy: 0.355\n",
            "training epoch: [74 ] loss: 14.391 correct: 1066.000, total: 3000.000, accuracy: 0.355\n",
            "training epoch: [75 ] loss: 14.365 correct: 1069.000, total: 3000.000, accuracy: 0.356\n",
            "training epoch: [76 ] loss: 14.341 correct: 1069.000, total: 3000.000, accuracy: 0.356\n",
            "training epoch: [77 ] loss: 14.316 correct: 1068.000, total: 3000.000, accuracy: 0.356\n",
            "training epoch: [78 ] loss: 14.292 correct: 1070.000, total: 3000.000, accuracy: 0.357\n",
            "training epoch: [79 ] loss: 14.268 correct: 1067.000, total: 3000.000, accuracy: 0.356\n",
            "training epoch: [80 ] loss: 14.245 correct: 1066.000, total: 3000.000, accuracy: 0.355\n",
            "training epoch: [81 ] loss: 14.221 correct: 1066.000, total: 3000.000, accuracy: 0.355\n",
            "training epoch: [82 ] loss: 14.198 correct: 1066.000, total: 3000.000, accuracy: 0.355\n",
            "training epoch: [83 ] loss: 14.175 correct: 1060.000, total: 3000.000, accuracy: 0.353\n",
            "training epoch: [84 ] loss: 14.152 correct: 1056.000, total: 3000.000, accuracy: 0.352\n",
            "training epoch: [85 ] loss: 14.131 correct: 1056.000, total: 3000.000, accuracy: 0.352\n",
            "training epoch: [86 ] loss: 14.110 correct: 1055.000, total: 3000.000, accuracy: 0.352\n",
            "training epoch: [87 ] loss: 14.089 correct: 1054.000, total: 3000.000, accuracy: 0.351\n",
            "training epoch: [88 ] loss: 14.069 correct: 1057.000, total: 3000.000, accuracy: 0.352\n",
            "training epoch: [89 ] loss: 14.049 correct: 1057.000, total: 3000.000, accuracy: 0.352\n",
            "training epoch: [90 ] loss: 14.029 correct: 1058.000, total: 3000.000, accuracy: 0.353\n",
            "training epoch: [91 ] loss: 14.009 correct: 1058.000, total: 3000.000, accuracy: 0.353\n",
            "training epoch: [92 ] loss: 13.989 correct: 1058.000, total: 3000.000, accuracy: 0.353\n",
            "training epoch: [93 ] loss: 13.969 correct: 1058.000, total: 3000.000, accuracy: 0.353\n",
            "training epoch: [94 ] loss: 13.950 correct: 1058.000, total: 3000.000, accuracy: 0.353\n",
            "training epoch: [95 ] loss: 13.930 correct: 1058.000, total: 3000.000, accuracy: 0.353\n",
            "training epoch: [96 ] loss: 13.911 correct: 1058.000, total: 3000.000, accuracy: 0.353\n",
            "training epoch: [97 ] loss: 13.891 correct: 1059.000, total: 3000.000, accuracy: 0.353\n",
            "training epoch: [98 ] loss: 13.871 correct: 1059.000, total: 3000.000, accuracy: 0.353\n",
            "training epoch: [99 ] loss: 13.851 correct: 1059.000, total: 3000.000, accuracy: 0.353\n",
            "training epoch: [100 ] loss: 13.831 correct: 1058.000, total: 3000.000, accuracy: 0.353\n",
            "training epoch: [101 ] loss: 13.812 correct: 1058.000, total: 3000.000, accuracy: 0.353\n",
            "training epoch: [102 ] loss: 13.792 correct: 1059.000, total: 3000.000, accuracy: 0.353\n",
            "training epoch: [103 ] loss: 13.772 correct: 1059.000, total: 3000.000, accuracy: 0.353\n",
            "training epoch: [104 ] loss: 13.753 correct: 1059.000, total: 3000.000, accuracy: 0.353\n",
            "training epoch: [105 ] loss: 13.734 correct: 1059.000, total: 3000.000, accuracy: 0.353\n",
            "training epoch: [106 ] loss: 13.715 correct: 1059.000, total: 3000.000, accuracy: 0.353\n",
            "training epoch: [107 ] loss: 13.695 correct: 1059.000, total: 3000.000, accuracy: 0.353\n",
            "training epoch: [108 ] loss: 13.676 correct: 1060.000, total: 3000.000, accuracy: 0.353\n",
            "training epoch: [109 ] loss: 13.657 correct: 1062.000, total: 3000.000, accuracy: 0.354\n",
            "training epoch: [110 ] loss: 13.638 correct: 1061.000, total: 3000.000, accuracy: 0.354\n",
            "training epoch: [111 ] loss: 13.619 correct: 1060.000, total: 3000.000, accuracy: 0.353\n",
            "training epoch: [112 ] loss: 13.600 correct: 1058.000, total: 3000.000, accuracy: 0.353\n",
            "training epoch: [113 ] loss: 13.582 correct: 1058.000, total: 3000.000, accuracy: 0.353\n",
            "training epoch: [114 ] loss: 13.564 correct: 1059.000, total: 3000.000, accuracy: 0.353\n",
            "training epoch: [115 ] loss: 13.546 correct: 1059.000, total: 3000.000, accuracy: 0.353\n",
            "training epoch: [116 ] loss: 13.528 correct: 1058.000, total: 3000.000, accuracy: 0.353\n",
            "training epoch: [117 ] loss: 13.510 correct: 1057.000, total: 3000.000, accuracy: 0.352\n",
            "training epoch: [118 ] loss: 13.492 correct: 1057.000, total: 3000.000, accuracy: 0.352\n",
            "training epoch: [119 ] loss: 13.474 correct: 1056.000, total: 3000.000, accuracy: 0.352\n",
            "training epoch: [120 ] loss: 13.456 correct: 1056.000, total: 3000.000, accuracy: 0.352\n",
            "training epoch: [121 ] loss: 13.437 correct: 1056.000, total: 3000.000, accuracy: 0.352\n",
            "training epoch: [122 ] loss: 13.420 correct: 1055.000, total: 3000.000, accuracy: 0.352\n",
            "training epoch: [123 ] loss: 13.401 correct: 1055.000, total: 3000.000, accuracy: 0.352\n",
            "training epoch: [124 ] loss: 13.384 correct: 1054.000, total: 3000.000, accuracy: 0.351\n",
            "training epoch: [125 ] loss: 13.365 correct: 1055.000, total: 3000.000, accuracy: 0.352\n",
            "training epoch: [126 ] loss: 13.347 correct: 1055.000, total: 3000.000, accuracy: 0.352\n",
            "training epoch: [127 ] loss: 13.329 correct: 1055.000, total: 3000.000, accuracy: 0.352\n",
            "training epoch: [128 ] loss: 13.311 correct: 1055.000, total: 3000.000, accuracy: 0.352\n",
            "training epoch: [129 ] loss: 13.294 correct: 1053.000, total: 3000.000, accuracy: 0.351\n",
            "training epoch: [130 ] loss: 13.276 correct: 1051.000, total: 3000.000, accuracy: 0.350\n",
            "training epoch: [131 ] loss: 13.258 correct: 1050.000, total: 3000.000, accuracy: 0.350\n",
            "training epoch: [132 ] loss: 13.240 correct: 1051.000, total: 3000.000, accuracy: 0.350\n",
            "training epoch: [133 ] loss: 13.222 correct: 1050.000, total: 3000.000, accuracy: 0.350\n",
            "training epoch: [134 ] loss: 13.205 correct: 1051.000, total: 3000.000, accuracy: 0.350\n",
            "training epoch: [135 ] loss: 13.187 correct: 1050.000, total: 3000.000, accuracy: 0.350\n",
            "training epoch: [136 ] loss: 13.169 correct: 1049.000, total: 3000.000, accuracy: 0.350\n",
            "training epoch: [137 ] loss: 13.151 correct: 1050.000, total: 3000.000, accuracy: 0.350\n",
            "training epoch: [138 ] loss: 13.134 correct: 1049.000, total: 3000.000, accuracy: 0.350\n",
            "training epoch: [139 ] loss: 13.117 correct: 1049.000, total: 3000.000, accuracy: 0.350\n",
            "training epoch: [140 ] loss: 13.100 correct: 1048.000, total: 3000.000, accuracy: 0.349\n",
            "training epoch: [141 ] loss: 13.083 correct: 1047.000, total: 3000.000, accuracy: 0.349\n",
            "training epoch: [142 ] loss: 13.066 correct: 1047.000, total: 3000.000, accuracy: 0.349\n",
            "training epoch: [143 ] loss: 13.050 correct: 1045.000, total: 3000.000, accuracy: 0.348\n",
            "training epoch: [144 ] loss: 13.033 correct: 1045.000, total: 3000.000, accuracy: 0.348\n",
            "training epoch: [145 ] loss: 13.017 correct: 1044.000, total: 3000.000, accuracy: 0.348\n",
            "training epoch: [146 ] loss: 13.000 correct: 1043.000, total: 3000.000, accuracy: 0.348\n",
            "training epoch: [147 ] loss: 12.984 correct: 1043.000, total: 3000.000, accuracy: 0.348\n",
            "training epoch: [148 ] loss: 12.968 correct: 1043.000, total: 3000.000, accuracy: 0.348\n",
            "training epoch: [149 ] loss: 12.952 correct: 1043.000, total: 3000.000, accuracy: 0.348\n",
            "training epoch: [150 ] loss: 12.936 correct: 1042.000, total: 3000.000, accuracy: 0.347\n",
            "training epoch: [151 ] loss: 12.920 correct: 1042.000, total: 3000.000, accuracy: 0.347\n",
            "training epoch: [152 ] loss: 12.904 correct: 1042.000, total: 3000.000, accuracy: 0.347\n",
            "training epoch: [153 ] loss: 12.889 correct: 1041.000, total: 3000.000, accuracy: 0.347\n",
            "training epoch: [154 ] loss: 12.873 correct: 1040.000, total: 3000.000, accuracy: 0.347\n",
            "training epoch: [155 ] loss: 12.858 correct: 1039.000, total: 3000.000, accuracy: 0.346\n",
            "training epoch: [156 ] loss: 12.842 correct: 1040.000, total: 3000.000, accuracy: 0.347\n",
            "training epoch: [157 ] loss: 12.827 correct: 1040.000, total: 3000.000, accuracy: 0.347\n",
            "training epoch: [158 ] loss: 12.811 correct: 1041.000, total: 3000.000, accuracy: 0.347\n",
            "training epoch: [159 ] loss: 12.795 correct: 1039.000, total: 3000.000, accuracy: 0.346\n",
            "training epoch: [160 ] loss: 12.780 correct: 1038.000, total: 3000.000, accuracy: 0.346\n",
            "training epoch: [161 ] loss: 12.765 correct: 1038.000, total: 3000.000, accuracy: 0.346\n",
            "training epoch: [162 ] loss: 12.749 correct: 1035.000, total: 3000.000, accuracy: 0.345\n",
            "training epoch: [163 ] loss: 12.734 correct: 1034.000, total: 3000.000, accuracy: 0.345\n",
            "training epoch: [164 ] loss: 12.718 correct: 1034.000, total: 3000.000, accuracy: 0.345\n",
            "training epoch: [165 ] loss: 12.703 correct: 1034.000, total: 3000.000, accuracy: 0.345\n",
            "training epoch: [166 ] loss: 12.689 correct: 1034.000, total: 3000.000, accuracy: 0.345\n",
            "training epoch: [167 ] loss: 12.675 correct: 1034.000, total: 3000.000, accuracy: 0.345\n",
            "training epoch: [168 ] loss: 12.661 correct: 1033.000, total: 3000.000, accuracy: 0.344\n",
            "training epoch: [169 ] loss: 12.647 correct: 1033.000, total: 3000.000, accuracy: 0.344\n",
            "training epoch: [170 ] loss: 12.633 correct: 1033.000, total: 3000.000, accuracy: 0.344\n",
            "training epoch: [171 ] loss: 12.619 correct: 1034.000, total: 3000.000, accuracy: 0.345\n",
            "training epoch: [172 ] loss: 12.606 correct: 1035.000, total: 3000.000, accuracy: 0.345\n",
            "training epoch: [173 ] loss: 12.592 correct: 1034.000, total: 3000.000, accuracy: 0.345\n",
            "training epoch: [174 ] loss: 12.578 correct: 1034.000, total: 3000.000, accuracy: 0.345\n",
            "training epoch: [175 ] loss: 12.565 correct: 1034.000, total: 3000.000, accuracy: 0.345\n",
            "training epoch: [176 ] loss: 12.552 correct: 1034.000, total: 3000.000, accuracy: 0.345\n",
            "training epoch: [177 ] loss: 12.539 correct: 1033.000, total: 3000.000, accuracy: 0.344\n",
            "training epoch: [178 ] loss: 12.526 correct: 1033.000, total: 3000.000, accuracy: 0.344\n",
            "training epoch: [179 ] loss: 12.513 correct: 1033.000, total: 3000.000, accuracy: 0.344\n",
            "training epoch: [180 ] loss: 12.500 correct: 1032.000, total: 3000.000, accuracy: 0.344\n",
            "training epoch: [181 ] loss: 12.488 correct: 1032.000, total: 3000.000, accuracy: 0.344\n",
            "training epoch: [182 ] loss: 12.475 correct: 1032.000, total: 3000.000, accuracy: 0.344\n",
            "training epoch: [183 ] loss: 12.463 correct: 1030.000, total: 3000.000, accuracy: 0.343\n",
            "training epoch: [184 ] loss: 12.450 correct: 1031.000, total: 3000.000, accuracy: 0.344\n",
            "training epoch: [185 ] loss: 12.437 correct: 1029.000, total: 3000.000, accuracy: 0.343\n",
            "training epoch: [186 ] loss: 12.425 correct: 1031.000, total: 3000.000, accuracy: 0.344\n",
            "training epoch: [187 ] loss: 12.412 correct: 1033.000, total: 3000.000, accuracy: 0.344\n",
            "training epoch: [188 ] loss: 12.400 correct: 1033.000, total: 3000.000, accuracy: 0.344\n",
            "training epoch: [189 ] loss: 12.387 correct: 1032.000, total: 3000.000, accuracy: 0.344\n",
            "training epoch: [190 ] loss: 12.374 correct: 1032.000, total: 3000.000, accuracy: 0.344\n",
            "training epoch: [191 ] loss: 12.361 correct: 1034.000, total: 3000.000, accuracy: 0.345\n",
            "training epoch: [192 ] loss: 12.349 correct: 1035.000, total: 3000.000, accuracy: 0.345\n",
            "training epoch: [193 ] loss: 12.336 correct: 1035.000, total: 3000.000, accuracy: 0.345\n",
            "training epoch: [194 ] loss: 12.324 correct: 1035.000, total: 3000.000, accuracy: 0.345\n",
            "training epoch: [195 ] loss: 12.312 correct: 1035.000, total: 3000.000, accuracy: 0.345\n",
            "training epoch: [196 ] loss: 12.300 correct: 1034.000, total: 3000.000, accuracy: 0.345\n",
            "training epoch: [197 ] loss: 12.288 correct: 1033.000, total: 3000.000, accuracy: 0.344\n",
            "training epoch: [198 ] loss: 12.276 correct: 1031.000, total: 3000.000, accuracy: 0.344\n",
            "training epoch: [199 ] loss: 12.264 correct: 1030.000, total: 3000.000, accuracy: 0.343\n",
            "training epoch: [200 ] loss: 12.252 correct: 1030.000, total: 3000.000, accuracy: 0.343\n",
            "training epoch: [201 ] loss: 12.240 correct: 1030.000, total: 3000.000, accuracy: 0.343\n",
            "training epoch: [202 ] loss: 12.229 correct: 1028.000, total: 3000.000, accuracy: 0.343\n",
            "training epoch: [203 ] loss: 12.217 correct: 1028.000, total: 3000.000, accuracy: 0.343\n",
            "training epoch: [204 ] loss: 12.205 correct: 1028.000, total: 3000.000, accuracy: 0.343\n",
            "training epoch: [205 ] loss: 12.193 correct: 1029.000, total: 3000.000, accuracy: 0.343\n",
            "training epoch: [206 ] loss: 12.181 correct: 1028.000, total: 3000.000, accuracy: 0.343\n",
            "training epoch: [207 ] loss: 12.169 correct: 1028.000, total: 3000.000, accuracy: 0.343\n",
            "training epoch: [208 ] loss: 12.158 correct: 1029.000, total: 3000.000, accuracy: 0.343\n",
            "training epoch: [209 ] loss: 12.146 correct: 1031.000, total: 3000.000, accuracy: 0.344\n",
            "training epoch: [210 ] loss: 12.134 correct: 1031.000, total: 3000.000, accuracy: 0.344\n",
            "training epoch: [211 ] loss: 12.122 correct: 1029.000, total: 3000.000, accuracy: 0.343\n",
            "training epoch: [212 ] loss: 12.111 correct: 1030.000, total: 3000.000, accuracy: 0.343\n",
            "training epoch: [213 ] loss: 12.099 correct: 1028.000, total: 3000.000, accuracy: 0.343\n",
            "training epoch: [214 ] loss: 12.088 correct: 1028.000, total: 3000.000, accuracy: 0.343\n",
            "training epoch: [215 ] loss: 12.077 correct: 1028.000, total: 3000.000, accuracy: 0.343\n",
            "training epoch: [216 ] loss: 12.066 correct: 1028.000, total: 3000.000, accuracy: 0.343\n",
            "training epoch: [217 ] loss: 12.055 correct: 1026.000, total: 3000.000, accuracy: 0.342\n",
            "training epoch: [218 ] loss: 12.044 correct: 1026.000, total: 3000.000, accuracy: 0.342\n",
            "training epoch: [219 ] loss: 12.033 correct: 1025.000, total: 3000.000, accuracy: 0.342\n",
            "training epoch: [220 ] loss: 12.022 correct: 1026.000, total: 3000.000, accuracy: 0.342\n",
            "training epoch: [221 ] loss: 12.011 correct: 1027.000, total: 3000.000, accuracy: 0.342\n",
            "training epoch: [222 ] loss: 12.000 correct: 1026.000, total: 3000.000, accuracy: 0.342\n",
            "training epoch: [223 ] loss: 11.989 correct: 1026.000, total: 3000.000, accuracy: 0.342\n",
            "training epoch: [224 ] loss: 11.978 correct: 1024.000, total: 3000.000, accuracy: 0.341\n",
            "training epoch: [225 ] loss: 11.967 correct: 1024.000, total: 3000.000, accuracy: 0.341\n",
            "training epoch: [226 ] loss: 11.957 correct: 1024.000, total: 3000.000, accuracy: 0.341\n",
            "training epoch: [227 ] loss: 11.946 correct: 1023.000, total: 3000.000, accuracy: 0.341\n",
            "training epoch: [228 ] loss: 11.936 correct: 1022.000, total: 3000.000, accuracy: 0.341\n",
            "training epoch: [229 ] loss: 11.926 correct: 1020.000, total: 3000.000, accuracy: 0.340\n",
            "training epoch: [230 ] loss: 11.916 correct: 1019.000, total: 3000.000, accuracy: 0.340\n",
            "training epoch: [231 ] loss: 11.906 correct: 1019.000, total: 3000.000, accuracy: 0.340\n",
            "training epoch: [232 ] loss: 11.896 correct: 1019.000, total: 3000.000, accuracy: 0.340\n",
            "training epoch: [233 ] loss: 11.886 correct: 1019.000, total: 3000.000, accuracy: 0.340\n",
            "training epoch: [234 ] loss: 11.876 correct: 1019.000, total: 3000.000, accuracy: 0.340\n",
            "training epoch: [235 ] loss: 11.865 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [236 ] loss: 11.855 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [237 ] loss: 11.845 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [238 ] loss: 11.835 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [239 ] loss: 11.825 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [240 ] loss: 11.815 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [241 ] loss: 11.805 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [242 ] loss: 11.795 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [243 ] loss: 11.785 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [244 ] loss: 11.776 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [245 ] loss: 11.765 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [246 ] loss: 11.755 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [247 ] loss: 11.745 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [248 ] loss: 11.735 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [249 ] loss: 11.726 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [250 ] loss: 11.716 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [251 ] loss: 11.706 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [252 ] loss: 11.697 correct: 1011.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [253 ] loss: 11.687 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [254 ] loss: 11.678 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [255 ] loss: 11.668 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [256 ] loss: 11.658 correct: 1011.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [257 ] loss: 11.648 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [258 ] loss: 11.638 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [259 ] loss: 11.628 correct: 1011.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [260 ] loss: 11.618 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [261 ] loss: 11.609 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [262 ] loss: 11.599 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [263 ] loss: 11.589 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [264 ] loss: 11.579 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [265 ] loss: 11.570 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [266 ] loss: 11.560 correct: 1007.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [267 ] loss: 11.550 correct: 1005.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [268 ] loss: 11.539 correct: 1005.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [269 ] loss: 11.529 correct: 1006.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [270 ] loss: 11.519 correct: 1005.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [271 ] loss: 11.509 correct: 1003.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [272 ] loss: 11.498 correct: 1002.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [273 ] loss: 11.488 correct: 1002.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [274 ] loss: 11.477 correct: 1002.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [275 ] loss: 11.466 correct: 1003.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [276 ] loss: 11.455 correct: 1004.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [277 ] loss: 11.444 correct: 1003.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [278 ] loss: 11.433 correct: 999.000, total: 3000.000, accuracy: 0.333\n",
            "training epoch: [279 ] loss: 11.422 correct: 998.000, total: 3000.000, accuracy: 0.333\n",
            "training epoch: [280 ] loss: 11.411 correct: 999.000, total: 3000.000, accuracy: 0.333\n",
            "training epoch: [281 ] loss: 11.399 correct: 999.000, total: 3000.000, accuracy: 0.333\n",
            "training epoch: [282 ] loss: 11.388 correct: 997.000, total: 3000.000, accuracy: 0.332\n",
            "training epoch: [283 ] loss: 11.376 correct: 997.000, total: 3000.000, accuracy: 0.332\n",
            "training epoch: [284 ] loss: 11.365 correct: 998.000, total: 3000.000, accuracy: 0.333\n",
            "training epoch: [285 ] loss: 11.353 correct: 999.000, total: 3000.000, accuracy: 0.333\n",
            "training epoch: [286 ] loss: 11.341 correct: 999.000, total: 3000.000, accuracy: 0.333\n",
            "training epoch: [287 ] loss: 11.329 correct: 997.000, total: 3000.000, accuracy: 0.332\n",
            "training epoch: [288 ] loss: 11.317 correct: 995.000, total: 3000.000, accuracy: 0.332\n",
            "training epoch: [289 ] loss: 11.305 correct: 995.000, total: 3000.000, accuracy: 0.332\n",
            "training epoch: [290 ] loss: 11.293 correct: 993.000, total: 3000.000, accuracy: 0.331\n",
            "training epoch: [291 ] loss: 11.280 correct: 994.000, total: 3000.000, accuracy: 0.331\n",
            "training epoch: [292 ] loss: 11.268 correct: 992.000, total: 3000.000, accuracy: 0.331\n",
            "training epoch: [293 ] loss: 11.255 correct: 991.000, total: 3000.000, accuracy: 0.330\n",
            "training epoch: [294 ] loss: 11.242 correct: 992.000, total: 3000.000, accuracy: 0.331\n",
            "training epoch: [295 ] loss: 11.230 correct: 992.000, total: 3000.000, accuracy: 0.331\n",
            "training epoch: [296 ] loss: 11.217 correct: 993.000, total: 3000.000, accuracy: 0.331\n",
            "training epoch: [297 ] loss: 11.204 correct: 993.000, total: 3000.000, accuracy: 0.331\n",
            "training epoch: [298 ] loss: 11.190 correct: 994.000, total: 3000.000, accuracy: 0.331\n",
            "training epoch: [299 ] loss: 11.177 correct: 994.000, total: 3000.000, accuracy: 0.331\n",
            "training epoch: [300 ] loss: 11.163 correct: 993.000, total: 3000.000, accuracy: 0.331\n",
            "training epoch: [301 ] loss: 11.150 correct: 993.000, total: 3000.000, accuracy: 0.331\n",
            "training epoch: [302 ] loss: 11.136 correct: 993.000, total: 3000.000, accuracy: 0.331\n",
            "training epoch: [303 ] loss: 11.123 correct: 995.000, total: 3000.000, accuracy: 0.332\n",
            "training epoch: [304 ] loss: 11.109 correct: 994.000, total: 3000.000, accuracy: 0.331\n",
            "training epoch: [305 ] loss: 11.095 correct: 994.000, total: 3000.000, accuracy: 0.331\n",
            "training epoch: [306 ] loss: 11.081 correct: 995.000, total: 3000.000, accuracy: 0.332\n",
            "training epoch: [307 ] loss: 11.067 correct: 995.000, total: 3000.000, accuracy: 0.332\n",
            "training epoch: [308 ] loss: 11.053 correct: 994.000, total: 3000.000, accuracy: 0.331\n",
            "training epoch: [309 ] loss: 11.038 correct: 994.000, total: 3000.000, accuracy: 0.331\n",
            "training epoch: [310 ] loss: 11.024 correct: 993.000, total: 3000.000, accuracy: 0.331\n",
            "training epoch: [311 ] loss: 11.009 correct: 992.000, total: 3000.000, accuracy: 0.331\n",
            "training epoch: [312 ] loss: 10.994 correct: 993.000, total: 3000.000, accuracy: 0.331\n",
            "training epoch: [313 ] loss: 10.979 correct: 994.000, total: 3000.000, accuracy: 0.331\n",
            "training epoch: [314 ] loss: 10.964 correct: 994.000, total: 3000.000, accuracy: 0.331\n",
            "training epoch: [315 ] loss: 10.948 correct: 993.000, total: 3000.000, accuracy: 0.331\n",
            "training epoch: [316 ] loss: 10.932 correct: 993.000, total: 3000.000, accuracy: 0.331\n",
            "training epoch: [317 ] loss: 10.916 correct: 993.000, total: 3000.000, accuracy: 0.331\n",
            "training epoch: [318 ] loss: 10.900 correct: 992.000, total: 3000.000, accuracy: 0.331\n",
            "training epoch: [319 ] loss: 10.884 correct: 992.000, total: 3000.000, accuracy: 0.331\n",
            "training epoch: [320 ] loss: 10.867 correct: 990.000, total: 3000.000, accuracy: 0.330\n",
            "training epoch: [321 ] loss: 10.851 correct: 989.000, total: 3000.000, accuracy: 0.330\n",
            "training epoch: [322 ] loss: 10.834 correct: 990.000, total: 3000.000, accuracy: 0.330\n",
            "training epoch: [323 ] loss: 10.817 correct: 991.000, total: 3000.000, accuracy: 0.330\n",
            "training epoch: [324 ] loss: 10.799 correct: 990.000, total: 3000.000, accuracy: 0.330\n",
            "training epoch: [325 ] loss: 10.782 correct: 990.000, total: 3000.000, accuracy: 0.330\n",
            "training epoch: [326 ] loss: 10.764 correct: 990.000, total: 3000.000, accuracy: 0.330\n",
            "training epoch: [327 ] loss: 10.746 correct: 990.000, total: 3000.000, accuracy: 0.330\n",
            "training epoch: [328 ] loss: 10.728 correct: 991.000, total: 3000.000, accuracy: 0.330\n",
            "training epoch: [329 ] loss: 10.709 correct: 992.000, total: 3000.000, accuracy: 0.331\n",
            "training epoch: [330 ] loss: 10.690 correct: 992.000, total: 3000.000, accuracy: 0.331\n",
            "training epoch: [331 ] loss: 10.671 correct: 992.000, total: 3000.000, accuracy: 0.331\n",
            "training epoch: [332 ] loss: 10.652 correct: 992.000, total: 3000.000, accuracy: 0.331\n",
            "training epoch: [333 ] loss: 10.632 correct: 994.000, total: 3000.000, accuracy: 0.331\n",
            "training epoch: [334 ] loss: 10.613 correct: 994.000, total: 3000.000, accuracy: 0.331\n",
            "training epoch: [335 ] loss: 10.593 correct: 994.000, total: 3000.000, accuracy: 0.331\n",
            "training epoch: [336 ] loss: 10.573 correct: 994.000, total: 3000.000, accuracy: 0.331\n",
            "training epoch: [337 ] loss: 10.552 correct: 994.000, total: 3000.000, accuracy: 0.331\n",
            "training epoch: [338 ] loss: 10.531 correct: 995.000, total: 3000.000, accuracy: 0.332\n",
            "training epoch: [339 ] loss: 10.510 correct: 994.000, total: 3000.000, accuracy: 0.331\n",
            "training epoch: [340 ] loss: 10.489 correct: 994.000, total: 3000.000, accuracy: 0.331\n",
            "training epoch: [341 ] loss: 10.468 correct: 994.000, total: 3000.000, accuracy: 0.331\n",
            "training epoch: [342 ] loss: 10.446 correct: 994.000, total: 3000.000, accuracy: 0.331\n",
            "training epoch: [343 ] loss: 10.424 correct: 993.000, total: 3000.000, accuracy: 0.331\n",
            "training epoch: [344 ] loss: 10.401 correct: 993.000, total: 3000.000, accuracy: 0.331\n",
            "training epoch: [345 ] loss: 10.378 correct: 993.000, total: 3000.000, accuracy: 0.331\n",
            "training epoch: [346 ] loss: 10.355 correct: 993.000, total: 3000.000, accuracy: 0.331\n",
            "training epoch: [347 ] loss: 10.332 correct: 994.000, total: 3000.000, accuracy: 0.331\n",
            "training epoch: [348 ] loss: 10.308 correct: 994.000, total: 3000.000, accuracy: 0.331\n",
            "training epoch: [349 ] loss: 10.284 correct: 994.000, total: 3000.000, accuracy: 0.331\n",
            "training epoch: [350 ] loss: 10.260 correct: 994.000, total: 3000.000, accuracy: 0.331\n",
            "training epoch: [351 ] loss: 10.236 correct: 994.000, total: 3000.000, accuracy: 0.331\n",
            "training epoch: [352 ] loss: 10.211 correct: 994.000, total: 3000.000, accuracy: 0.331\n",
            "training epoch: [353 ] loss: 10.186 correct: 995.000, total: 3000.000, accuracy: 0.332\n",
            "training epoch: [354 ] loss: 10.161 correct: 995.000, total: 3000.000, accuracy: 0.332\n",
            "training epoch: [355 ] loss: 10.135 correct: 995.000, total: 3000.000, accuracy: 0.332\n",
            "training epoch: [356 ] loss: 10.109 correct: 995.000, total: 3000.000, accuracy: 0.332\n",
            "training epoch: [357 ] loss: 10.083 correct: 995.000, total: 3000.000, accuracy: 0.332\n",
            "training epoch: [358 ] loss: 10.057 correct: 996.000, total: 3000.000, accuracy: 0.332\n",
            "training epoch: [359 ] loss: 10.030 correct: 996.000, total: 3000.000, accuracy: 0.332\n",
            "training epoch: [360 ] loss: 10.003 correct: 996.000, total: 3000.000, accuracy: 0.332\n",
            "training epoch: [361 ] loss: 9.976 correct: 996.000, total: 3000.000, accuracy: 0.332\n",
            "training epoch: [362 ] loss: 9.949 correct: 996.000, total: 3000.000, accuracy: 0.332\n",
            "training epoch: [363 ] loss: 9.921 correct: 996.000, total: 3000.000, accuracy: 0.332\n",
            "training epoch: [364 ] loss: 9.893 correct: 996.000, total: 3000.000, accuracy: 0.332\n",
            "training epoch: [365 ] loss: 9.865 correct: 996.000, total: 3000.000, accuracy: 0.332\n",
            "training epoch: [366 ] loss: 9.837 correct: 997.000, total: 3000.000, accuracy: 0.332\n",
            "training epoch: [367 ] loss: 9.809 correct: 996.000, total: 3000.000, accuracy: 0.332\n",
            "training epoch: [368 ] loss: 9.780 correct: 996.000, total: 3000.000, accuracy: 0.332\n",
            "training epoch: [369 ] loss: 9.751 correct: 996.000, total: 3000.000, accuracy: 0.332\n",
            "training epoch: [370 ] loss: 9.722 correct: 997.000, total: 3000.000, accuracy: 0.332\n",
            "training epoch: [371 ] loss: 9.693 correct: 998.000, total: 3000.000, accuracy: 0.333\n",
            "training epoch: [372 ] loss: 9.663 correct: 999.000, total: 3000.000, accuracy: 0.333\n",
            "training epoch: [373 ] loss: 9.634 correct: 999.000, total: 3000.000, accuracy: 0.333\n",
            "training epoch: [374 ] loss: 9.604 correct: 999.000, total: 3000.000, accuracy: 0.333\n",
            "training epoch: [375 ] loss: 9.574 correct: 999.000, total: 3000.000, accuracy: 0.333\n",
            "training epoch: [376 ] loss: 9.544 correct: 999.000, total: 3000.000, accuracy: 0.333\n",
            "training epoch: [377 ] loss: 9.514 correct: 1000.000, total: 3000.000, accuracy: 0.333\n",
            "training epoch: [378 ] loss: 9.484 correct: 1000.000, total: 3000.000, accuracy: 0.333\n",
            "training epoch: [379 ] loss: 9.453 correct: 1000.000, total: 3000.000, accuracy: 0.333\n",
            "training epoch: [380 ] loss: 9.423 correct: 1000.000, total: 3000.000, accuracy: 0.333\n",
            "training epoch: [381 ] loss: 9.392 correct: 1000.000, total: 3000.000, accuracy: 0.333\n",
            "training epoch: [382 ] loss: 9.361 correct: 1001.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [383 ] loss: 9.330 correct: 1001.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [384 ] loss: 9.299 correct: 1001.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [385 ] loss: 9.268 correct: 1001.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [386 ] loss: 9.237 correct: 1001.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [387 ] loss: 9.206 correct: 1000.000, total: 3000.000, accuracy: 0.333\n",
            "training epoch: [388 ] loss: 9.175 correct: 1000.000, total: 3000.000, accuracy: 0.333\n",
            "training epoch: [389 ] loss: 9.144 correct: 1000.000, total: 3000.000, accuracy: 0.333\n",
            "training epoch: [390 ] loss: 9.113 correct: 1000.000, total: 3000.000, accuracy: 0.333\n",
            "training epoch: [391 ] loss: 9.082 correct: 1000.000, total: 3000.000, accuracy: 0.333\n",
            "training epoch: [392 ] loss: 9.050 correct: 1000.000, total: 3000.000, accuracy: 0.333\n",
            "training epoch: [393 ] loss: 9.019 correct: 1000.000, total: 3000.000, accuracy: 0.333\n",
            "training epoch: [394 ] loss: 8.988 correct: 1000.000, total: 3000.000, accuracy: 0.333\n",
            "training epoch: [395 ] loss: 8.957 correct: 1000.000, total: 3000.000, accuracy: 0.333\n",
            "training epoch: [396 ] loss: 8.926 correct: 1000.000, total: 3000.000, accuracy: 0.333\n",
            "training epoch: [397 ] loss: 8.894 correct: 1000.000, total: 3000.000, accuracy: 0.333\n",
            "training epoch: [398 ] loss: 8.863 correct: 1000.000, total: 3000.000, accuracy: 0.333\n",
            "training epoch: [399 ] loss: 8.832 correct: 1000.000, total: 3000.000, accuracy: 0.333\n",
            "training epoch: [400 ] loss: 8.801 correct: 1000.000, total: 3000.000, accuracy: 0.333\n",
            "training epoch: [401 ] loss: 8.770 correct: 1000.000, total: 3000.000, accuracy: 0.333\n",
            "training epoch: [402 ] loss: 8.739 correct: 1000.000, total: 3000.000, accuracy: 0.333\n",
            "training epoch: [403 ] loss: 8.708 correct: 1000.000, total: 3000.000, accuracy: 0.333\n",
            "training epoch: [404 ] loss: 8.677 correct: 1000.000, total: 3000.000, accuracy: 0.333\n",
            "training epoch: [405 ] loss: 8.646 correct: 1000.000, total: 3000.000, accuracy: 0.333\n",
            "training epoch: [406 ] loss: 8.616 correct: 1000.000, total: 3000.000, accuracy: 0.333\n",
            "training epoch: [407 ] loss: 8.585 correct: 1000.000, total: 3000.000, accuracy: 0.333\n",
            "training epoch: [408 ] loss: 8.554 correct: 1000.000, total: 3000.000, accuracy: 0.333\n",
            "training epoch: [409 ] loss: 8.524 correct: 1000.000, total: 3000.000, accuracy: 0.333\n",
            "training epoch: [410 ] loss: 8.494 correct: 1000.000, total: 3000.000, accuracy: 0.333\n",
            "training epoch: [411 ] loss: 8.463 correct: 1000.000, total: 3000.000, accuracy: 0.333\n",
            "training epoch: [412 ] loss: 8.433 correct: 1000.000, total: 3000.000, accuracy: 0.333\n",
            "training epoch: [413 ] loss: 8.403 correct: 1000.000, total: 3000.000, accuracy: 0.333\n",
            "training epoch: [414 ] loss: 8.373 correct: 1000.000, total: 3000.000, accuracy: 0.333\n",
            "training epoch: [415 ] loss: 8.343 correct: 1000.000, total: 3000.000, accuracy: 0.333\n",
            "training epoch: [416 ] loss: 8.314 correct: 1000.000, total: 3000.000, accuracy: 0.333\n",
            "training epoch: [417 ] loss: 8.284 correct: 1000.000, total: 3000.000, accuracy: 0.333\n",
            "training epoch: [418 ] loss: 8.255 correct: 1000.000, total: 3000.000, accuracy: 0.333\n",
            "training epoch: [419 ] loss: 8.226 correct: 1000.000, total: 3000.000, accuracy: 0.333\n",
            "training epoch: [420 ] loss: 8.196 correct: 1000.000, total: 3000.000, accuracy: 0.333\n",
            "training epoch: [421 ] loss: 8.167 correct: 1000.000, total: 3000.000, accuracy: 0.333\n",
            "training epoch: [422 ] loss: 8.139 correct: 1000.000, total: 3000.000, accuracy: 0.333\n",
            "training epoch: [423 ] loss: 8.110 correct: 1000.000, total: 3000.000, accuracy: 0.333\n",
            "training epoch: [424 ] loss: 8.082 correct: 1000.000, total: 3000.000, accuracy: 0.333\n",
            "training epoch: [425 ] loss: 8.053 correct: 1000.000, total: 3000.000, accuracy: 0.333\n",
            "training epoch: [426 ] loss: 8.025 correct: 1000.000, total: 3000.000, accuracy: 0.333\n",
            "training epoch: [427 ] loss: 7.997 correct: 1000.000, total: 3000.000, accuracy: 0.333\n",
            "training epoch: [428 ] loss: 7.969 correct: 1000.000, total: 3000.000, accuracy: 0.333\n",
            "training epoch: [429 ] loss: 7.941 correct: 1002.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [430 ] loss: 7.914 correct: 1002.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [431 ] loss: 7.887 correct: 1002.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [432 ] loss: 7.859 correct: 1002.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [433 ] loss: 7.833 correct: 1002.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [434 ] loss: 7.806 correct: 1001.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [435 ] loss: 7.779 correct: 1003.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [436 ] loss: 7.753 correct: 1000.000, total: 3000.000, accuracy: 0.333\n",
            "training epoch: [437 ] loss: 7.726 correct: 1003.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [438 ] loss: 7.700 correct: 1005.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [439 ] loss: 7.674 correct: 1002.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [440 ] loss: 7.649 correct: 1003.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [441 ] loss: 7.623 correct: 1001.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [442 ] loss: 7.598 correct: 1001.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [443 ] loss: 7.573 correct: 1001.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [444 ] loss: 7.548 correct: 1000.000, total: 3000.000, accuracy: 0.333\n",
            "training epoch: [445 ] loss: 7.523 correct: 1000.000, total: 3000.000, accuracy: 0.333\n",
            "training epoch: [446 ] loss: 7.498 correct: 998.000, total: 3000.000, accuracy: 0.333\n",
            "training epoch: [447 ] loss: 7.474 correct: 994.000, total: 3000.000, accuracy: 0.331\n",
            "training epoch: [448 ] loss: 7.450 correct: 996.000, total: 3000.000, accuracy: 0.332\n",
            "training epoch: [449 ] loss: 7.425 correct: 993.000, total: 3000.000, accuracy: 0.331\n",
            "training epoch: [450 ] loss: 7.402 correct: 992.000, total: 3000.000, accuracy: 0.331\n",
            "training epoch: [451 ] loss: 7.378 correct: 994.000, total: 3000.000, accuracy: 0.331\n",
            "training epoch: [452 ] loss: 7.354 correct: 996.000, total: 3000.000, accuracy: 0.332\n",
            "training epoch: [453 ] loss: 7.331 correct: 997.000, total: 3000.000, accuracy: 0.332\n",
            "training epoch: [454 ] loss: 7.308 correct: 997.000, total: 3000.000, accuracy: 0.332\n",
            "training epoch: [455 ] loss: 7.284 correct: 998.000, total: 3000.000, accuracy: 0.333\n",
            "training epoch: [456 ] loss: 7.262 correct: 999.000, total: 3000.000, accuracy: 0.333\n",
            "training epoch: [457 ] loss: 7.239 correct: 998.000, total: 3000.000, accuracy: 0.333\n",
            "training epoch: [458 ] loss: 7.216 correct: 997.000, total: 3000.000, accuracy: 0.332\n",
            "training epoch: [459 ] loss: 7.194 correct: 1002.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [460 ] loss: 7.172 correct: 1002.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [461 ] loss: 7.149 correct: 1007.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [462 ] loss: 7.127 correct: 1006.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [463 ] loss: 7.106 correct: 1006.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [464 ] loss: 7.084 correct: 1007.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [465 ] loss: 7.062 correct: 1003.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [466 ] loss: 7.041 correct: 1006.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [467 ] loss: 7.020 correct: 1004.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [468 ] loss: 6.999 correct: 998.000, total: 3000.000, accuracy: 0.333\n",
            "training epoch: [469 ] loss: 6.978 correct: 994.000, total: 3000.000, accuracy: 0.331\n",
            "training epoch: [470 ] loss: 6.957 correct: 996.000, total: 3000.000, accuracy: 0.332\n",
            "training epoch: [471 ] loss: 6.936 correct: 995.000, total: 3000.000, accuracy: 0.332\n",
            "training epoch: [472 ] loss: 6.916 correct: 993.000, total: 3000.000, accuracy: 0.331\n",
            "training epoch: [473 ] loss: 6.895 correct: 990.000, total: 3000.000, accuracy: 0.330\n",
            "training epoch: [474 ] loss: 6.875 correct: 993.000, total: 3000.000, accuracy: 0.331\n",
            "training epoch: [475 ] loss: 6.855 correct: 998.000, total: 3000.000, accuracy: 0.333\n",
            "training epoch: [476 ] loss: 6.834 correct: 994.000, total: 3000.000, accuracy: 0.331\n",
            "training epoch: [477 ] loss: 6.814 correct: 992.000, total: 3000.000, accuracy: 0.331\n",
            "training epoch: [478 ] loss: 6.795 correct: 990.000, total: 3000.000, accuracy: 0.330\n",
            "training epoch: [479 ] loss: 6.775 correct: 987.000, total: 3000.000, accuracy: 0.329\n",
            "training epoch: [480 ] loss: 6.755 correct: 987.000, total: 3000.000, accuracy: 0.329\n",
            "training epoch: [481 ] loss: 6.736 correct: 985.000, total: 3000.000, accuracy: 0.328\n",
            "training epoch: [482 ] loss: 6.716 correct: 986.000, total: 3000.000, accuracy: 0.329\n",
            "training epoch: [483 ] loss: 6.697 correct: 991.000, total: 3000.000, accuracy: 0.330\n",
            "training epoch: [484 ] loss: 6.677 correct: 990.000, total: 3000.000, accuracy: 0.330\n",
            "training epoch: [485 ] loss: 6.658 correct: 986.000, total: 3000.000, accuracy: 0.329\n",
            "training epoch: [486 ] loss: 6.639 correct: 984.000, total: 3000.000, accuracy: 0.328\n",
            "training epoch: [487 ] loss: 6.620 correct: 987.000, total: 3000.000, accuracy: 0.329\n",
            "training epoch: [488 ] loss: 6.601 correct: 987.000, total: 3000.000, accuracy: 0.329\n",
            "training epoch: [489 ] loss: 6.582 correct: 990.000, total: 3000.000, accuracy: 0.330\n",
            "training epoch: [490 ] loss: 6.564 correct: 992.000, total: 3000.000, accuracy: 0.331\n",
            "training epoch: [491 ] loss: 6.545 correct: 993.000, total: 3000.000, accuracy: 0.331\n",
            "training epoch: [492 ] loss: 6.526 correct: 990.000, total: 3000.000, accuracy: 0.330\n",
            "training epoch: [493 ] loss: 6.508 correct: 995.000, total: 3000.000, accuracy: 0.332\n",
            "training epoch: [494 ] loss: 6.489 correct: 996.000, total: 3000.000, accuracy: 0.332\n",
            "training epoch: [495 ] loss: 6.471 correct: 996.000, total: 3000.000, accuracy: 0.332\n",
            "training epoch: [496 ] loss: 6.453 correct: 1000.000, total: 3000.000, accuracy: 0.333\n",
            "training epoch: [497 ] loss: 6.434 correct: 1005.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [498 ] loss: 6.416 correct: 1003.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [499 ] loss: 6.398 correct: 1006.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [500 ] loss: 6.380 correct: 1008.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [501 ] loss: 6.362 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [502 ] loss: 6.344 correct: 1007.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [503 ] loss: 6.326 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [504 ] loss: 6.308 correct: 1008.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [505 ] loss: 6.290 correct: 1006.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [506 ] loss: 6.273 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [507 ] loss: 6.255 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [508 ] loss: 6.237 correct: 1008.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [509 ] loss: 6.220 correct: 1011.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [510 ] loss: 6.202 correct: 1008.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [511 ] loss: 6.184 correct: 1007.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [512 ] loss: 6.167 correct: 1007.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [513 ] loss: 6.149 correct: 1005.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [514 ] loss: 6.132 correct: 999.000, total: 3000.000, accuracy: 0.333\n",
            "training epoch: [515 ] loss: 6.114 correct: 994.000, total: 3000.000, accuracy: 0.331\n",
            "training epoch: [516 ] loss: 6.097 correct: 994.000, total: 3000.000, accuracy: 0.331\n",
            "training epoch: [517 ] loss: 6.080 correct: 990.000, total: 3000.000, accuracy: 0.330\n",
            "training epoch: [518 ] loss: 6.062 correct: 986.000, total: 3000.000, accuracy: 0.329\n",
            "training epoch: [519 ] loss: 6.045 correct: 985.000, total: 3000.000, accuracy: 0.328\n",
            "training epoch: [520 ] loss: 6.028 correct: 985.000, total: 3000.000, accuracy: 0.328\n",
            "training epoch: [521 ] loss: 6.010 correct: 984.000, total: 3000.000, accuracy: 0.328\n",
            "training epoch: [522 ] loss: 5.993 correct: 986.000, total: 3000.000, accuracy: 0.329\n",
            "training epoch: [523 ] loss: 5.976 correct: 982.000, total: 3000.000, accuracy: 0.327\n",
            "training epoch: [524 ] loss: 5.959 correct: 978.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [525 ] loss: 5.941 correct: 980.000, total: 3000.000, accuracy: 0.327\n",
            "training epoch: [526 ] loss: 5.924 correct: 981.000, total: 3000.000, accuracy: 0.327\n",
            "training epoch: [527 ] loss: 5.907 correct: 981.000, total: 3000.000, accuracy: 0.327\n",
            "training epoch: [528 ] loss: 5.890 correct: 980.000, total: 3000.000, accuracy: 0.327\n",
            "training epoch: [529 ] loss: 5.873 correct: 978.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [530 ] loss: 5.856 correct: 978.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [531 ] loss: 5.839 correct: 978.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [532 ] loss: 5.821 correct: 976.000, total: 3000.000, accuracy: 0.325\n",
            "training epoch: [533 ] loss: 5.804 correct: 974.000, total: 3000.000, accuracy: 0.325\n",
            "training epoch: [534 ] loss: 5.787 correct: 973.000, total: 3000.000, accuracy: 0.324\n",
            "training epoch: [535 ] loss: 5.770 correct: 974.000, total: 3000.000, accuracy: 0.325\n",
            "training epoch: [536 ] loss: 5.753 correct: 978.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [537 ] loss: 5.736 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [538 ] loss: 5.719 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [539 ] loss: 5.702 correct: 975.000, total: 3000.000, accuracy: 0.325\n",
            "training epoch: [540 ] loss: 5.685 correct: 975.000, total: 3000.000, accuracy: 0.325\n",
            "training epoch: [541 ] loss: 5.668 correct: 976.000, total: 3000.000, accuracy: 0.325\n",
            "training epoch: [542 ] loss: 5.651 correct: 977.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [543 ] loss: 5.633 correct: 981.000, total: 3000.000, accuracy: 0.327\n",
            "training epoch: [544 ] loss: 5.616 correct: 980.000, total: 3000.000, accuracy: 0.327\n",
            "training epoch: [545 ] loss: 5.599 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [546 ] loss: 5.582 correct: 978.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [547 ] loss: 5.565 correct: 982.000, total: 3000.000, accuracy: 0.327\n",
            "training epoch: [548 ] loss: 5.548 correct: 984.000, total: 3000.000, accuracy: 0.328\n",
            "training epoch: [549 ] loss: 5.531 correct: 985.000, total: 3000.000, accuracy: 0.328\n",
            "training epoch: [550 ] loss: 5.514 correct: 984.000, total: 3000.000, accuracy: 0.328\n",
            "training epoch: [551 ] loss: 5.497 correct: 982.000, total: 3000.000, accuracy: 0.327\n",
            "training epoch: [552 ] loss: 5.480 correct: 985.000, total: 3000.000, accuracy: 0.328\n",
            "training epoch: [553 ] loss: 5.463 correct: 983.000, total: 3000.000, accuracy: 0.328\n",
            "training epoch: [554 ] loss: 5.446 correct: 985.000, total: 3000.000, accuracy: 0.328\n",
            "training epoch: [555 ] loss: 5.429 correct: 986.000, total: 3000.000, accuracy: 0.329\n",
            "training epoch: [556 ] loss: 5.412 correct: 985.000, total: 3000.000, accuracy: 0.328\n",
            "training epoch: [557 ] loss: 5.395 correct: 986.000, total: 3000.000, accuracy: 0.329\n",
            "training epoch: [558 ] loss: 5.378 correct: 984.000, total: 3000.000, accuracy: 0.328\n",
            "training epoch: [559 ] loss: 5.361 correct: 988.000, total: 3000.000, accuracy: 0.329\n",
            "training epoch: [560 ] loss: 5.344 correct: 990.000, total: 3000.000, accuracy: 0.330\n",
            "training epoch: [561 ] loss: 5.327 correct: 989.000, total: 3000.000, accuracy: 0.330\n",
            "training epoch: [562 ] loss: 5.310 correct: 988.000, total: 3000.000, accuracy: 0.329\n",
            "training epoch: [563 ] loss: 5.292 correct: 990.000, total: 3000.000, accuracy: 0.330\n",
            "training epoch: [564 ] loss: 5.275 correct: 988.000, total: 3000.000, accuracy: 0.329\n",
            "training epoch: [565 ] loss: 5.258 correct: 987.000, total: 3000.000, accuracy: 0.329\n",
            "training epoch: [566 ] loss: 5.241 correct: 986.000, total: 3000.000, accuracy: 0.329\n",
            "training epoch: [567 ] loss: 5.224 correct: 987.000, total: 3000.000, accuracy: 0.329\n",
            "training epoch: [568 ] loss: 5.207 correct: 989.000, total: 3000.000, accuracy: 0.330\n",
            "training epoch: [569 ] loss: 5.190 correct: 991.000, total: 3000.000, accuracy: 0.330\n",
            "training epoch: [570 ] loss: 5.173 correct: 993.000, total: 3000.000, accuracy: 0.331\n",
            "training epoch: [571 ] loss: 5.156 correct: 993.000, total: 3000.000, accuracy: 0.331\n",
            "training epoch: [572 ] loss: 5.139 correct: 994.000, total: 3000.000, accuracy: 0.331\n",
            "training epoch: [573 ] loss: 5.122 correct: 994.000, total: 3000.000, accuracy: 0.331\n",
            "training epoch: [574 ] loss: 5.105 correct: 995.000, total: 3000.000, accuracy: 0.332\n",
            "training epoch: [575 ] loss: 5.088 correct: 994.000, total: 3000.000, accuracy: 0.331\n",
            "training epoch: [576 ] loss: 5.071 correct: 995.000, total: 3000.000, accuracy: 0.332\n",
            "training epoch: [577 ] loss: 5.054 correct: 996.000, total: 3000.000, accuracy: 0.332\n",
            "training epoch: [578 ] loss: 5.037 correct: 997.000, total: 3000.000, accuracy: 0.332\n",
            "training epoch: [579 ] loss: 5.020 correct: 997.000, total: 3000.000, accuracy: 0.332\n",
            "training epoch: [580 ] loss: 5.003 correct: 997.000, total: 3000.000, accuracy: 0.332\n",
            "training epoch: [581 ] loss: 4.986 correct: 998.000, total: 3000.000, accuracy: 0.333\n",
            "training epoch: [582 ] loss: 4.969 correct: 1001.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [583 ] loss: 4.952 correct: 1003.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [584 ] loss: 4.935 correct: 1000.000, total: 3000.000, accuracy: 0.333\n",
            "training epoch: [585 ] loss: 4.919 correct: 999.000, total: 3000.000, accuracy: 0.333\n",
            "training epoch: [586 ] loss: 4.902 correct: 997.000, total: 3000.000, accuracy: 0.332\n",
            "training epoch: [587 ] loss: 4.885 correct: 997.000, total: 3000.000, accuracy: 0.332\n",
            "training epoch: [588 ] loss: 4.868 correct: 998.000, total: 3000.000, accuracy: 0.333\n",
            "training epoch: [589 ] loss: 4.851 correct: 998.000, total: 3000.000, accuracy: 0.333\n",
            "training epoch: [590 ] loss: 4.834 correct: 1000.000, total: 3000.000, accuracy: 0.333\n",
            "training epoch: [591 ] loss: 4.817 correct: 1001.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [592 ] loss: 4.800 correct: 1002.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [593 ] loss: 4.783 correct: 1002.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [594 ] loss: 4.767 correct: 1002.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [595 ] loss: 4.750 correct: 1002.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [596 ] loss: 4.733 correct: 1003.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [597 ] loss: 4.716 correct: 1003.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [598 ] loss: 4.699 correct: 1004.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [599 ] loss: 4.683 correct: 1004.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [600 ] loss: 4.666 correct: 1004.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [601 ] loss: 4.649 correct: 1006.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [602 ] loss: 4.632 correct: 1006.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [603 ] loss: 4.616 correct: 1005.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [604 ] loss: 4.599 correct: 1006.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [605 ] loss: 4.583 correct: 1006.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [606 ] loss: 4.566 correct: 1005.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [607 ] loss: 4.549 correct: 1005.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [608 ] loss: 4.533 correct: 1005.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [609 ] loss: 4.517 correct: 1004.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [610 ] loss: 4.500 correct: 1003.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [611 ] loss: 4.484 correct: 1004.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [612 ] loss: 4.467 correct: 1005.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [613 ] loss: 4.451 correct: 1003.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [614 ] loss: 4.435 correct: 1002.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [615 ] loss: 4.419 correct: 1002.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [616 ] loss: 4.403 correct: 1002.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [617 ] loss: 4.387 correct: 1003.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [618 ] loss: 4.371 correct: 1002.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [619 ] loss: 4.355 correct: 1003.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [620 ] loss: 4.339 correct: 1004.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [621 ] loss: 4.323 correct: 1002.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [622 ] loss: 4.307 correct: 1003.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [623 ] loss: 4.292 correct: 1004.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [624 ] loss: 4.276 correct: 1004.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [625 ] loss: 4.261 correct: 1003.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [626 ] loss: 4.246 correct: 1001.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [627 ] loss: 4.231 correct: 1003.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [628 ] loss: 4.216 correct: 1004.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [629 ] loss: 4.201 correct: 1003.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [630 ] loss: 4.186 correct: 1002.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [631 ] loss: 4.172 correct: 1004.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [632 ] loss: 4.157 correct: 1006.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [633 ] loss: 4.143 correct: 1005.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [634 ] loss: 4.129 correct: 1000.000, total: 3000.000, accuracy: 0.333\n",
            "training epoch: [635 ] loss: 4.114 correct: 1001.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [636 ] loss: 4.101 correct: 1001.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [637 ] loss: 4.087 correct: 999.000, total: 3000.000, accuracy: 0.333\n",
            "training epoch: [638 ] loss: 4.073 correct: 998.000, total: 3000.000, accuracy: 0.333\n",
            "training epoch: [639 ] loss: 4.060 correct: 1000.000, total: 3000.000, accuracy: 0.333\n",
            "training epoch: [640 ] loss: 4.047 correct: 1004.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [641 ] loss: 4.034 correct: 1006.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [642 ] loss: 4.021 correct: 1003.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [643 ] loss: 4.008 correct: 1002.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [644 ] loss: 3.996 correct: 1000.000, total: 3000.000, accuracy: 0.333\n",
            "training epoch: [645 ] loss: 3.984 correct: 1001.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [646 ] loss: 3.972 correct: 1002.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [647 ] loss: 3.960 correct: 1002.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [648 ] loss: 3.948 correct: 1005.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [649 ] loss: 3.937 correct: 1006.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [650 ] loss: 3.926 correct: 1004.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [651 ] loss: 3.915 correct: 1007.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [652 ] loss: 3.904 correct: 1005.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [653 ] loss: 3.894 correct: 1004.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [654 ] loss: 3.884 correct: 1003.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [655 ] loss: 3.874 correct: 1000.000, total: 3000.000, accuracy: 0.333\n",
            "training epoch: [656 ] loss: 3.864 correct: 997.000, total: 3000.000, accuracy: 0.332\n",
            "training epoch: [657 ] loss: 3.854 correct: 997.000, total: 3000.000, accuracy: 0.332\n",
            "training epoch: [658 ] loss: 3.845 correct: 1000.000, total: 3000.000, accuracy: 0.333\n",
            "training epoch: [659 ] loss: 3.836 correct: 998.000, total: 3000.000, accuracy: 0.333\n",
            "training epoch: [660 ] loss: 3.827 correct: 999.000, total: 3000.000, accuracy: 0.333\n",
            "training epoch: [661 ] loss: 3.818 correct: 1004.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [662 ] loss: 3.810 correct: 1004.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [663 ] loss: 3.802 correct: 1006.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [664 ] loss: 3.794 correct: 1008.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [665 ] loss: 3.786 correct: 1006.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [666 ] loss: 3.779 correct: 1008.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [667 ] loss: 3.772 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [668 ] loss: 3.765 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [669 ] loss: 3.758 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [670 ] loss: 3.751 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [671 ] loss: 3.745 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [672 ] loss: 3.739 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [673 ] loss: 3.733 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [674 ] loss: 3.727 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [675 ] loss: 3.722 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [676 ] loss: 3.717 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [677 ] loss: 3.711 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [678 ] loss: 3.707 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [679 ] loss: 3.702 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [680 ] loss: 3.697 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [681 ] loss: 3.693 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [682 ] loss: 3.689 correct: 1005.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [683 ] loss: 3.685 correct: 1007.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [684 ] loss: 3.681 correct: 1005.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [685 ] loss: 3.677 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [686 ] loss: 3.674 correct: 1011.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [687 ] loss: 3.670 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [688 ] loss: 3.667 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [689 ] loss: 3.664 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [690 ] loss: 3.661 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [691 ] loss: 3.658 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [692 ] loss: 3.655 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [693 ] loss: 3.653 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [694 ] loss: 3.650 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [695 ] loss: 3.648 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [696 ] loss: 3.646 correct: 1011.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [697 ] loss: 3.644 correct: 1011.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [698 ] loss: 3.641 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [699 ] loss: 3.639 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [700 ] loss: 3.638 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [701 ] loss: 3.636 correct: 1011.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [702 ] loss: 3.634 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [703 ] loss: 3.632 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [704 ] loss: 3.631 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [705 ] loss: 3.629 correct: 1004.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [706 ] loss: 3.628 correct: 1006.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [707 ] loss: 3.627 correct: 1006.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [708 ] loss: 3.625 correct: 1004.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [709 ] loss: 3.624 correct: 1002.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [710 ] loss: 3.623 correct: 1003.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [711 ] loss: 3.622 correct: 1004.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [712 ] loss: 3.621 correct: 1002.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [713 ] loss: 3.620 correct: 1001.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [714 ] loss: 3.619 correct: 1002.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [715 ] loss: 3.618 correct: 1001.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [716 ] loss: 3.617 correct: 1001.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [717 ] loss: 3.616 correct: 1001.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [718 ] loss: 3.615 correct: 1001.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [719 ] loss: 3.615 correct: 1003.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [720 ] loss: 3.614 correct: 1003.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [721 ] loss: 3.613 correct: 1001.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [722 ] loss: 3.612 correct: 1001.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [723 ] loss: 3.612 correct: 1004.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [724 ] loss: 3.611 correct: 1000.000, total: 3000.000, accuracy: 0.333\n",
            "training epoch: [725 ] loss: 3.611 correct: 1001.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [726 ] loss: 3.610 correct: 1004.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [727 ] loss: 3.609 correct: 1002.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [728 ] loss: 3.609 correct: 1003.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [729 ] loss: 3.608 correct: 1002.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [730 ] loss: 3.608 correct: 1007.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [731 ] loss: 3.607 correct: 1006.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [732 ] loss: 3.607 correct: 1007.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [733 ] loss: 3.606 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [734 ] loss: 3.606 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [735 ] loss: 3.605 correct: 1011.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [736 ] loss: 3.605 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [737 ] loss: 3.605 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [738 ] loss: 3.604 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [739 ] loss: 3.604 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [740 ] loss: 3.603 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [741 ] loss: 3.603 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [742 ] loss: 3.603 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [743 ] loss: 3.602 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [744 ] loss: 3.602 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [745 ] loss: 3.602 correct: 1011.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [746 ] loss: 3.601 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [747 ] loss: 3.601 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [748 ] loss: 3.601 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [749 ] loss: 3.600 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [750 ] loss: 3.600 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [751 ] loss: 3.600 correct: 1007.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [752 ] loss: 3.599 correct: 1007.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [753 ] loss: 3.599 correct: 1008.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [754 ] loss: 3.599 correct: 1008.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [755 ] loss: 3.598 correct: 1008.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [756 ] loss: 3.598 correct: 1007.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [757 ] loss: 3.598 correct: 1008.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [758 ] loss: 3.598 correct: 1008.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [759 ] loss: 3.597 correct: 1008.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [760 ] loss: 3.597 correct: 1007.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [761 ] loss: 3.597 correct: 1007.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [762 ] loss: 3.596 correct: 1007.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [763 ] loss: 3.596 correct: 1008.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [764 ] loss: 3.596 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [765 ] loss: 3.595 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [766 ] loss: 3.595 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [767 ] loss: 3.595 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [768 ] loss: 3.595 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [769 ] loss: 3.594 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [770 ] loss: 3.594 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [771 ] loss: 3.594 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [772 ] loss: 3.593 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [773 ] loss: 3.593 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [774 ] loss: 3.593 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [775 ] loss: 3.593 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [776 ] loss: 3.592 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [777 ] loss: 3.592 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [778 ] loss: 3.592 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [779 ] loss: 3.591 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [780 ] loss: 3.591 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [781 ] loss: 3.591 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [782 ] loss: 3.590 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [783 ] loss: 3.590 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [784 ] loss: 3.590 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [785 ] loss: 3.590 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [786 ] loss: 3.589 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [787 ] loss: 3.589 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [788 ] loss: 3.589 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [789 ] loss: 3.588 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [790 ] loss: 3.588 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [791 ] loss: 3.588 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [792 ] loss: 3.588 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [793 ] loss: 3.587 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [794 ] loss: 3.587 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [795 ] loss: 3.587 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [796 ] loss: 3.586 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [797 ] loss: 3.586 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [798 ] loss: 3.586 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [799 ] loss: 3.585 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [800 ] loss: 3.585 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [801 ] loss: 3.585 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [802 ] loss: 3.584 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [803 ] loss: 3.584 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [804 ] loss: 3.584 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [805 ] loss: 3.583 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [806 ] loss: 3.583 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [807 ] loss: 3.583 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [808 ] loss: 3.582 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [809 ] loss: 3.582 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [810 ] loss: 3.582 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [811 ] loss: 3.581 correct: 1011.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [812 ] loss: 3.581 correct: 1011.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [813 ] loss: 3.581 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [814 ] loss: 3.580 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [815 ] loss: 3.580 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [816 ] loss: 3.580 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [817 ] loss: 3.579 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [818 ] loss: 3.579 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [819 ] loss: 3.579 correct: 1011.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [820 ] loss: 3.578 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [821 ] loss: 3.578 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [822 ] loss: 3.577 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [823 ] loss: 3.577 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [824 ] loss: 3.577 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [825 ] loss: 3.576 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [826 ] loss: 3.576 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [827 ] loss: 3.575 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [828 ] loss: 3.575 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [829 ] loss: 3.575 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [830 ] loss: 3.574 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [831 ] loss: 3.574 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [832 ] loss: 3.573 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [833 ] loss: 3.573 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [834 ] loss: 3.572 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [835 ] loss: 3.572 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [836 ] loss: 3.571 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [837 ] loss: 3.571 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [838 ] loss: 3.570 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [839 ] loss: 3.570 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [840 ] loss: 3.569 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [841 ] loss: 3.569 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [842 ] loss: 3.569 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [843 ] loss: 3.568 correct: 1011.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [844 ] loss: 3.568 correct: 1011.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [845 ] loss: 3.567 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [846 ] loss: 3.567 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [847 ] loss: 3.566 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [848 ] loss: 3.566 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [849 ] loss: 3.565 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [850 ] loss: 3.564 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [851 ] loss: 3.564 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [852 ] loss: 3.563 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [853 ] loss: 3.563 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [854 ] loss: 3.562 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [855 ] loss: 3.562 correct: 1018.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [856 ] loss: 3.561 correct: 1018.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [857 ] loss: 3.561 correct: 1018.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [858 ] loss: 3.560 correct: 1018.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [859 ] loss: 3.560 correct: 1018.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [860 ] loss: 3.559 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [861 ] loss: 3.559 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [862 ] loss: 3.558 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [863 ] loss: 3.558 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [864 ] loss: 3.558 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [865 ] loss: 3.557 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [866 ] loss: 3.557 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [867 ] loss: 3.556 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [868 ] loss: 3.556 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [869 ] loss: 3.555 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [870 ] loss: 3.555 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [871 ] loss: 3.554 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [872 ] loss: 3.554 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [873 ] loss: 3.553 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [874 ] loss: 3.553 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [875 ] loss: 3.552 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [876 ] loss: 3.552 correct: 1011.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [877 ] loss: 3.551 correct: 1011.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [878 ] loss: 3.551 correct: 1011.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [879 ] loss: 3.550 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [880 ] loss: 3.550 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [881 ] loss: 3.549 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [882 ] loss: 3.549 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [883 ] loss: 3.548 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [884 ] loss: 3.548 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [885 ] loss: 3.548 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [886 ] loss: 3.547 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [887 ] loss: 3.547 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [888 ] loss: 3.546 correct: 1011.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [889 ] loss: 3.546 correct: 1011.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [890 ] loss: 3.545 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [891 ] loss: 3.545 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [892 ] loss: 3.544 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [893 ] loss: 3.544 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [894 ] loss: 3.543 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [895 ] loss: 3.543 correct: 1011.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [896 ] loss: 3.542 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [897 ] loss: 3.542 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [898 ] loss: 3.541 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [899 ] loss: 3.541 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [900 ] loss: 3.540 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [901 ] loss: 3.540 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [902 ] loss: 3.539 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [903 ] loss: 3.539 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [904 ] loss: 3.538 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [905 ] loss: 3.538 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [906 ] loss: 3.537 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [907 ] loss: 3.537 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [908 ] loss: 3.536 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [909 ] loss: 3.536 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [910 ] loss: 3.535 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [911 ] loss: 3.534 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [912 ] loss: 3.534 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [913 ] loss: 3.533 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [914 ] loss: 3.533 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [915 ] loss: 3.532 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [916 ] loss: 3.532 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [917 ] loss: 3.531 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [918 ] loss: 3.531 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [919 ] loss: 3.530 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [920 ] loss: 3.530 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [921 ] loss: 3.529 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [922 ] loss: 3.529 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [923 ] loss: 3.528 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [924 ] loss: 3.528 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [925 ] loss: 3.527 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [926 ] loss: 3.526 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [927 ] loss: 3.526 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [928 ] loss: 3.525 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [929 ] loss: 3.525 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [930 ] loss: 3.524 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [931 ] loss: 3.524 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [932 ] loss: 3.523 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [933 ] loss: 3.523 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [934 ] loss: 3.522 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [935 ] loss: 3.521 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [936 ] loss: 3.521 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [937 ] loss: 3.520 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [938 ] loss: 3.520 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [939 ] loss: 3.519 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [940 ] loss: 3.518 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [941 ] loss: 3.518 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [942 ] loss: 3.517 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [943 ] loss: 3.517 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [944 ] loss: 3.516 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [945 ] loss: 3.516 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [946 ] loss: 3.515 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [947 ] loss: 3.514 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [948 ] loss: 3.514 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [949 ] loss: 3.513 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [950 ] loss: 3.513 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [951 ] loss: 3.512 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [952 ] loss: 3.511 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [953 ] loss: 3.511 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [954 ] loss: 3.510 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [955 ] loss: 3.509 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [956 ] loss: 3.509 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [957 ] loss: 3.508 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [958 ] loss: 3.508 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [959 ] loss: 3.507 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [960 ] loss: 3.506 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [961 ] loss: 3.506 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [962 ] loss: 3.505 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [963 ] loss: 3.504 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [964 ] loss: 3.504 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [965 ] loss: 3.503 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [966 ] loss: 3.503 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [967 ] loss: 3.502 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [968 ] loss: 3.501 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [969 ] loss: 3.501 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [970 ] loss: 3.500 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [971 ] loss: 3.499 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [972 ] loss: 3.499 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [973 ] loss: 3.498 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [974 ] loss: 3.497 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [975 ] loss: 3.497 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [976 ] loss: 3.496 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [977 ] loss: 3.495 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [978 ] loss: 3.495 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [979 ] loss: 3.494 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [980 ] loss: 3.493 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [981 ] loss: 3.493 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [982 ] loss: 3.492 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [983 ] loss: 3.491 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [984 ] loss: 3.491 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [985 ] loss: 3.490 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [986 ] loss: 3.489 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [987 ] loss: 3.489 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [988 ] loss: 3.488 correct: 1008.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [989 ] loss: 3.487 correct: 1008.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [990 ] loss: 3.487 correct: 1008.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [991 ] loss: 3.486 correct: 1008.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [992 ] loss: 3.485 correct: 1008.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [993 ] loss: 3.484 correct: 1008.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [994 ] loss: 3.484 correct: 1008.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [995 ] loss: 3.483 correct: 1008.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [996 ] loss: 3.482 correct: 1008.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [997 ] loss: 3.482 correct: 1008.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [998 ] loss: 3.481 correct: 1008.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [999 ] loss: 3.480 correct: 1008.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1000 ] loss: 3.480 correct: 1008.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1001 ] loss: 3.479 correct: 1008.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1002 ] loss: 3.478 correct: 1008.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1003 ] loss: 3.477 correct: 1008.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1004 ] loss: 3.477 correct: 1008.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1005 ] loss: 3.476 correct: 1007.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1006 ] loss: 3.475 correct: 1007.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1007 ] loss: 3.475 correct: 1007.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1008 ] loss: 3.474 correct: 1007.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1009 ] loss: 3.473 correct: 1007.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1010 ] loss: 3.472 correct: 1007.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1011 ] loss: 3.472 correct: 1007.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1012 ] loss: 3.471 correct: 1007.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1013 ] loss: 3.470 correct: 1007.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1014 ] loss: 3.470 correct: 1007.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1015 ] loss: 3.469 correct: 1007.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1016 ] loss: 3.468 correct: 1008.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1017 ] loss: 3.467 correct: 1008.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1018 ] loss: 3.467 correct: 1007.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1019 ] loss: 3.466 correct: 1007.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1020 ] loss: 3.465 correct: 1007.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1021 ] loss: 3.464 correct: 1007.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1022 ] loss: 3.464 correct: 1007.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1023 ] loss: 3.463 correct: 1007.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1024 ] loss: 3.462 correct: 1007.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1025 ] loss: 3.461 correct: 1007.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1026 ] loss: 3.461 correct: 1007.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1027 ] loss: 3.460 correct: 1007.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1028 ] loss: 3.459 correct: 1007.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1029 ] loss: 3.458 correct: 1007.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1030 ] loss: 3.458 correct: 1007.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1031 ] loss: 3.457 correct: 1007.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1032 ] loss: 3.456 correct: 1007.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1033 ] loss: 3.455 correct: 1007.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1034 ] loss: 3.455 correct: 1007.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1035 ] loss: 3.454 correct: 1007.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1036 ] loss: 3.453 correct: 1007.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1037 ] loss: 3.452 correct: 1007.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1038 ] loss: 3.452 correct: 1007.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1039 ] loss: 3.451 correct: 1008.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1040 ] loss: 3.450 correct: 1008.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1041 ] loss: 3.449 correct: 1008.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1042 ] loss: 3.448 correct: 1008.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1043 ] loss: 3.448 correct: 1008.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1044 ] loss: 3.447 correct: 1008.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1045 ] loss: 3.446 correct: 1008.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1046 ] loss: 3.445 correct: 1008.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1047 ] loss: 3.445 correct: 1007.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1048 ] loss: 3.444 correct: 1007.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1049 ] loss: 3.443 correct: 1007.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1050 ] loss: 3.442 correct: 1007.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1051 ] loss: 3.441 correct: 1008.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1052 ] loss: 3.441 correct: 1008.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1053 ] loss: 3.440 correct: 1008.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1054 ] loss: 3.439 correct: 1008.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1055 ] loss: 3.438 correct: 1008.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1056 ] loss: 3.437 correct: 1008.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1057 ] loss: 3.436 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1058 ] loss: 3.435 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1059 ] loss: 3.435 correct: 1008.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1060 ] loss: 3.434 correct: 1007.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1061 ] loss: 3.433 correct: 1006.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [1062 ] loss: 3.432 correct: 1006.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [1063 ] loss: 3.431 correct: 1006.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [1064 ] loss: 3.430 correct: 1006.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [1065 ] loss: 3.429 correct: 1006.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [1066 ] loss: 3.428 correct: 1006.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [1067 ] loss: 3.427 correct: 1006.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [1068 ] loss: 3.426 correct: 1006.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [1069 ] loss: 3.426 correct: 1007.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1070 ] loss: 3.425 correct: 1007.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1071 ] loss: 3.424 correct: 1006.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [1072 ] loss: 3.423 correct: 1005.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [1073 ] loss: 3.422 correct: 1005.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [1074 ] loss: 3.421 correct: 1005.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [1075 ] loss: 3.420 correct: 1005.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [1076 ] loss: 3.419 correct: 1005.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [1077 ] loss: 3.418 correct: 1005.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [1078 ] loss: 3.416 correct: 1006.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [1079 ] loss: 3.415 correct: 1006.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [1080 ] loss: 3.414 correct: 1006.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [1081 ] loss: 3.413 correct: 1006.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [1082 ] loss: 3.412 correct: 1006.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [1083 ] loss: 3.411 correct: 1006.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [1084 ] loss: 3.409 correct: 1006.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [1085 ] loss: 3.408 correct: 1005.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [1086 ] loss: 3.407 correct: 1005.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [1087 ] loss: 3.406 correct: 1005.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [1088 ] loss: 3.404 correct: 1005.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [1089 ] loss: 3.403 correct: 1005.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [1090 ] loss: 3.402 correct: 1004.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [1091 ] loss: 3.400 correct: 1004.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [1092 ] loss: 3.399 correct: 1005.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [1093 ] loss: 3.397 correct: 1004.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [1094 ] loss: 3.395 correct: 1004.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [1095 ] loss: 3.394 correct: 1004.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [1096 ] loss: 3.392 correct: 1004.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [1097 ] loss: 3.390 correct: 1004.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [1098 ] loss: 3.388 correct: 1004.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [1099 ] loss: 3.387 correct: 1004.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [1100 ] loss: 3.385 correct: 1006.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [1101 ] loss: 3.383 correct: 1007.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1102 ] loss: 3.381 correct: 1006.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [1103 ] loss: 3.379 correct: 1006.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [1104 ] loss: 3.377 correct: 1006.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [1105 ] loss: 3.375 correct: 1004.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [1106 ] loss: 3.373 correct: 1004.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [1107 ] loss: 3.371 correct: 1004.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [1108 ] loss: 3.369 correct: 1005.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [1109 ] loss: 3.367 correct: 1006.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [1110 ] loss: 3.365 correct: 1006.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [1111 ] loss: 3.364 correct: 1006.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [1112 ] loss: 3.362 correct: 1006.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [1113 ] loss: 3.360 correct: 1007.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1114 ] loss: 3.358 correct: 1007.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1115 ] loss: 3.357 correct: 1007.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1116 ] loss: 3.355 correct: 1008.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1117 ] loss: 3.353 correct: 1007.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1118 ] loss: 3.352 correct: 1007.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1119 ] loss: 3.350 correct: 1007.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1120 ] loss: 3.348 correct: 1006.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [1121 ] loss: 3.347 correct: 1006.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [1122 ] loss: 3.345 correct: 1006.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [1123 ] loss: 3.344 correct: 1006.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [1124 ] loss: 3.342 correct: 1006.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [1125 ] loss: 3.341 correct: 1006.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [1126 ] loss: 3.339 correct: 1006.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [1127 ] loss: 3.338 correct: 1006.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [1128 ] loss: 3.336 correct: 1006.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [1129 ] loss: 3.335 correct: 1011.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1130 ] loss: 3.333 correct: 1011.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1131 ] loss: 3.332 correct: 1011.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1132 ] loss: 3.330 correct: 1011.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1133 ] loss: 3.329 correct: 1011.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1134 ] loss: 3.328 correct: 1011.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1135 ] loss: 3.326 correct: 1011.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1136 ] loss: 3.325 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1137 ] loss: 3.324 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1138 ] loss: 3.322 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1139 ] loss: 3.321 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1140 ] loss: 3.320 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1141 ] loss: 3.318 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1142 ] loss: 3.317 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1143 ] loss: 3.316 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1144 ] loss: 3.314 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1145 ] loss: 3.313 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1146 ] loss: 3.312 correct: 1011.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1147 ] loss: 3.310 correct: 1011.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1148 ] loss: 3.309 correct: 1011.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1149 ] loss: 3.308 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1150 ] loss: 3.306 correct: 1011.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1151 ] loss: 3.305 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1152 ] loss: 3.304 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1153 ] loss: 3.303 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1154 ] loss: 3.301 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1155 ] loss: 3.300 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1156 ] loss: 3.299 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1157 ] loss: 3.297 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1158 ] loss: 3.296 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1159 ] loss: 3.295 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1160 ] loss: 3.294 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1161 ] loss: 3.292 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1162 ] loss: 3.291 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1163 ] loss: 3.290 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1164 ] loss: 3.289 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1165 ] loss: 3.287 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1166 ] loss: 3.286 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1167 ] loss: 3.285 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1168 ] loss: 3.284 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1169 ] loss: 3.282 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1170 ] loss: 3.281 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1171 ] loss: 3.280 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1172 ] loss: 3.279 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1173 ] loss: 3.277 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1174 ] loss: 3.276 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1175 ] loss: 3.275 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1176 ] loss: 3.274 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1177 ] loss: 3.272 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1178 ] loss: 3.271 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1179 ] loss: 3.270 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1180 ] loss: 3.269 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1181 ] loss: 3.267 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1182 ] loss: 3.266 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1183 ] loss: 3.265 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1184 ] loss: 3.264 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1185 ] loss: 3.262 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1186 ] loss: 3.261 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1187 ] loss: 3.260 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1188 ] loss: 3.259 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1189 ] loss: 3.257 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1190 ] loss: 3.256 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1191 ] loss: 3.255 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1192 ] loss: 3.254 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1193 ] loss: 3.252 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1194 ] loss: 3.251 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1195 ] loss: 3.250 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1196 ] loss: 3.249 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1197 ] loss: 3.247 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1198 ] loss: 3.246 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1199 ] loss: 3.245 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1200 ] loss: 3.243 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1201 ] loss: 3.242 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1202 ] loss: 3.241 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1203 ] loss: 3.240 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1204 ] loss: 3.238 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1205 ] loss: 3.237 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1206 ] loss: 3.236 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1207 ] loss: 3.235 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1208 ] loss: 3.233 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1209 ] loss: 3.232 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1210 ] loss: 3.231 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1211 ] loss: 3.230 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1212 ] loss: 3.228 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1213 ] loss: 3.227 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1214 ] loss: 3.226 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1215 ] loss: 3.225 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1216 ] loss: 3.223 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1217 ] loss: 3.222 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1218 ] loss: 3.221 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1219 ] loss: 3.219 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1220 ] loss: 3.218 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1221 ] loss: 3.217 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1222 ] loss: 3.216 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1223 ] loss: 3.214 correct: 1008.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1224 ] loss: 3.213 correct: 1008.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1225 ] loss: 3.212 correct: 1008.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1226 ] loss: 3.211 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1227 ] loss: 3.209 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1228 ] loss: 3.208 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1229 ] loss: 3.207 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1230 ] loss: 3.206 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1231 ] loss: 3.204 correct: 1008.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1232 ] loss: 3.203 correct: 1008.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1233 ] loss: 3.202 correct: 1008.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1234 ] loss: 3.201 correct: 1008.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1235 ] loss: 3.199 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1236 ] loss: 3.198 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1237 ] loss: 3.197 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1238 ] loss: 3.196 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1239 ] loss: 3.194 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1240 ] loss: 3.193 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1241 ] loss: 3.192 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1242 ] loss: 3.191 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1243 ] loss: 3.189 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1244 ] loss: 3.188 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1245 ] loss: 3.187 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1246 ] loss: 3.186 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1247 ] loss: 3.184 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1248 ] loss: 3.183 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1249 ] loss: 3.182 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1250 ] loss: 3.181 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1251 ] loss: 3.179 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1252 ] loss: 3.178 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1253 ] loss: 3.177 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1254 ] loss: 3.176 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1255 ] loss: 3.174 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1256 ] loss: 3.173 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1257 ] loss: 3.172 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1258 ] loss: 3.171 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1259 ] loss: 3.169 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1260 ] loss: 3.168 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1261 ] loss: 3.167 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1262 ] loss: 3.166 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1263 ] loss: 3.164 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1264 ] loss: 3.163 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1265 ] loss: 3.162 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1266 ] loss: 3.161 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1267 ] loss: 3.159 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1268 ] loss: 3.158 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1269 ] loss: 3.157 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1270 ] loss: 3.156 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1271 ] loss: 3.154 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1272 ] loss: 3.153 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1273 ] loss: 3.152 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1274 ] loss: 3.151 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1275 ] loss: 3.149 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1276 ] loss: 3.148 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1277 ] loss: 3.147 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1278 ] loss: 3.146 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1279 ] loss: 3.144 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1280 ] loss: 3.143 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1281 ] loss: 3.142 correct: 1011.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1282 ] loss: 3.141 correct: 1011.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1283 ] loss: 3.140 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1284 ] loss: 3.138 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1285 ] loss: 3.137 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1286 ] loss: 3.136 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1287 ] loss: 3.135 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1288 ] loss: 3.133 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1289 ] loss: 3.132 correct: 1011.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1290 ] loss: 3.131 correct: 1011.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1291 ] loss: 3.130 correct: 1011.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1292 ] loss: 3.128 correct: 1011.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1293 ] loss: 3.127 correct: 1011.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1294 ] loss: 3.126 correct: 1011.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1295 ] loss: 3.125 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1296 ] loss: 3.123 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1297 ] loss: 3.122 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1298 ] loss: 3.121 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1299 ] loss: 3.120 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1300 ] loss: 3.119 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1301 ] loss: 3.117 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1302 ] loss: 3.116 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1303 ] loss: 3.115 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1304 ] loss: 3.114 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1305 ] loss: 3.112 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1306 ] loss: 3.111 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1307 ] loss: 3.110 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1308 ] loss: 3.109 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1309 ] loss: 3.107 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1310 ] loss: 3.106 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1311 ] loss: 3.105 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1312 ] loss: 3.104 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1313 ] loss: 3.103 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1314 ] loss: 3.101 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1315 ] loss: 3.100 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1316 ] loss: 3.099 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1317 ] loss: 3.098 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1318 ] loss: 3.097 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1319 ] loss: 3.095 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1320 ] loss: 3.094 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1321 ] loss: 3.093 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1322 ] loss: 3.092 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1323 ] loss: 3.091 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1324 ] loss: 3.089 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1325 ] loss: 3.088 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1326 ] loss: 3.087 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1327 ] loss: 3.086 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1328 ] loss: 3.085 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1329 ] loss: 3.083 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1330 ] loss: 3.082 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1331 ] loss: 3.081 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1332 ] loss: 3.080 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1333 ] loss: 3.079 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1334 ] loss: 3.078 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1335 ] loss: 3.076 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1336 ] loss: 3.075 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1337 ] loss: 3.074 correct: 1011.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1338 ] loss: 3.073 correct: 1011.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1339 ] loss: 3.072 correct: 1011.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1340 ] loss: 3.071 correct: 1011.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1341 ] loss: 3.069 correct: 1011.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1342 ] loss: 3.068 correct: 1011.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1343 ] loss: 3.067 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1344 ] loss: 3.066 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1345 ] loss: 3.065 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1346 ] loss: 3.064 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1347 ] loss: 3.063 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1348 ] loss: 3.061 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1349 ] loss: 3.060 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1350 ] loss: 3.059 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1351 ] loss: 3.058 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1352 ] loss: 3.057 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1353 ] loss: 3.056 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1354 ] loss: 3.055 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1355 ] loss: 3.054 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1356 ] loss: 3.053 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1357 ] loss: 3.051 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1358 ] loss: 3.050 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1359 ] loss: 3.049 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1360 ] loss: 3.048 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1361 ] loss: 3.047 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1362 ] loss: 3.046 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1363 ] loss: 3.045 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1364 ] loss: 3.044 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1365 ] loss: 3.043 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1366 ] loss: 3.042 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1367 ] loss: 3.041 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1368 ] loss: 3.040 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1369 ] loss: 3.039 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1370 ] loss: 3.038 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1371 ] loss: 3.037 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1372 ] loss: 3.035 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1373 ] loss: 3.034 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1374 ] loss: 3.033 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1375 ] loss: 3.032 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1376 ] loss: 3.031 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1377 ] loss: 3.030 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1378 ] loss: 3.029 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1379 ] loss: 3.028 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1380 ] loss: 3.027 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1381 ] loss: 3.026 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1382 ] loss: 3.025 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1383 ] loss: 3.024 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1384 ] loss: 3.023 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1385 ] loss: 3.022 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1386 ] loss: 3.021 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1387 ] loss: 3.020 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1388 ] loss: 3.019 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1389 ] loss: 3.018 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1390 ] loss: 3.017 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1391 ] loss: 3.016 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1392 ] loss: 3.015 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1393 ] loss: 3.014 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1394 ] loss: 3.013 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1395 ] loss: 3.012 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1396 ] loss: 3.011 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1397 ] loss: 3.010 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1398 ] loss: 3.009 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1399 ] loss: 3.008 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1400 ] loss: 3.007 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1401 ] loss: 3.006 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1402 ] loss: 3.006 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1403 ] loss: 3.005 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1404 ] loss: 3.004 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1405 ] loss: 3.003 correct: 1018.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1406 ] loss: 3.002 correct: 1019.000, total: 3000.000, accuracy: 0.340\n",
            "training epoch: [1407 ] loss: 3.001 correct: 1019.000, total: 3000.000, accuracy: 0.340\n",
            "training epoch: [1408 ] loss: 3.000 correct: 1019.000, total: 3000.000, accuracy: 0.340\n",
            "training epoch: [1409 ] loss: 2.999 correct: 1018.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1410 ] loss: 2.998 correct: 1018.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1411 ] loss: 2.997 correct: 1018.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1412 ] loss: 2.996 correct: 1018.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1413 ] loss: 2.995 correct: 1018.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1414 ] loss: 2.994 correct: 1018.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1415 ] loss: 2.993 correct: 1018.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1416 ] loss: 2.992 correct: 1018.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1417 ] loss: 2.991 correct: 1018.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1418 ] loss: 2.990 correct: 1019.000, total: 3000.000, accuracy: 0.340\n",
            "training epoch: [1419 ] loss: 2.989 correct: 1018.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1420 ] loss: 2.988 correct: 1018.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1421 ] loss: 2.987 correct: 1018.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1422 ] loss: 2.986 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1423 ] loss: 2.985 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1424 ] loss: 2.984 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1425 ] loss: 2.983 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1426 ] loss: 2.982 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1427 ] loss: 2.981 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1428 ] loss: 2.980 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1429 ] loss: 2.979 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1430 ] loss: 2.978 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1431 ] loss: 2.977 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1432 ] loss: 2.975 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1433 ] loss: 2.974 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1434 ] loss: 2.973 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1435 ] loss: 2.972 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1436 ] loss: 2.971 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1437 ] loss: 2.970 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1438 ] loss: 2.969 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1439 ] loss: 2.968 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1440 ] loss: 2.966 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1441 ] loss: 2.965 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1442 ] loss: 2.964 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1443 ] loss: 2.963 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1444 ] loss: 2.961 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1445 ] loss: 2.960 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1446 ] loss: 2.959 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1447 ] loss: 2.957 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1448 ] loss: 2.956 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1449 ] loss: 2.955 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1450 ] loss: 2.953 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1451 ] loss: 2.952 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1452 ] loss: 2.950 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1453 ] loss: 2.949 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1454 ] loss: 2.947 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1455 ] loss: 2.946 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1456 ] loss: 2.945 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1457 ] loss: 2.943 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1458 ] loss: 2.942 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1459 ] loss: 2.940 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1460 ] loss: 2.939 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1461 ] loss: 2.937 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1462 ] loss: 2.936 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1463 ] loss: 2.935 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1464 ] loss: 2.933 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1465 ] loss: 2.932 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1466 ] loss: 2.931 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1467 ] loss: 2.929 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1468 ] loss: 2.928 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1469 ] loss: 2.927 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1470 ] loss: 2.925 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1471 ] loss: 2.924 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1472 ] loss: 2.923 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1473 ] loss: 2.922 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1474 ] loss: 2.920 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1475 ] loss: 2.919 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1476 ] loss: 2.918 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1477 ] loss: 2.917 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1478 ] loss: 2.916 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1479 ] loss: 2.915 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1480 ] loss: 2.913 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1481 ] loss: 2.912 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1482 ] loss: 2.911 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1483 ] loss: 2.910 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1484 ] loss: 2.909 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1485 ] loss: 2.908 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1486 ] loss: 2.907 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1487 ] loss: 2.905 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1488 ] loss: 2.904 correct: 1011.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1489 ] loss: 2.903 correct: 1011.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1490 ] loss: 2.902 correct: 1011.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1491 ] loss: 2.901 correct: 1011.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1492 ] loss: 2.900 correct: 1011.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1493 ] loss: 2.899 correct: 1011.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1494 ] loss: 2.898 correct: 1011.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1495 ] loss: 2.897 correct: 1011.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1496 ] loss: 2.896 correct: 1011.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1497 ] loss: 2.895 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1498 ] loss: 2.894 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1499 ] loss: 2.893 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1500 ] loss: 2.892 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1501 ] loss: 2.891 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1502 ] loss: 2.890 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1503 ] loss: 2.889 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1504 ] loss: 2.888 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1505 ] loss: 2.887 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1506 ] loss: 2.886 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1507 ] loss: 2.885 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1508 ] loss: 2.884 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1509 ] loss: 2.883 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1510 ] loss: 2.882 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1511 ] loss: 2.881 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1512 ] loss: 2.880 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1513 ] loss: 2.879 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1514 ] loss: 2.878 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1515 ] loss: 2.877 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1516 ] loss: 2.876 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1517 ] loss: 2.875 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1518 ] loss: 2.874 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1519 ] loss: 2.873 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1520 ] loss: 2.872 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1521 ] loss: 2.871 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1522 ] loss: 2.870 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1523 ] loss: 2.869 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1524 ] loss: 2.868 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1525 ] loss: 2.867 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1526 ] loss: 2.866 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1527 ] loss: 2.865 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1528 ] loss: 2.864 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1529 ] loss: 2.863 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1530 ] loss: 2.862 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1531 ] loss: 2.861 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1532 ] loss: 2.860 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1533 ] loss: 2.859 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1534 ] loss: 2.859 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1535 ] loss: 2.858 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1536 ] loss: 2.857 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1537 ] loss: 2.856 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1538 ] loss: 2.855 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1539 ] loss: 2.854 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1540 ] loss: 2.853 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1541 ] loss: 2.852 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1542 ] loss: 2.851 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1543 ] loss: 2.850 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1544 ] loss: 2.849 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1545 ] loss: 2.848 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1546 ] loss: 2.848 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1547 ] loss: 2.847 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1548 ] loss: 2.846 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1549 ] loss: 2.845 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1550 ] loss: 2.844 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1551 ] loss: 2.843 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1552 ] loss: 2.842 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1553 ] loss: 2.841 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1554 ] loss: 2.840 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1555 ] loss: 2.839 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1556 ] loss: 2.839 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1557 ] loss: 2.838 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1558 ] loss: 2.837 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1559 ] loss: 2.836 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1560 ] loss: 2.835 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1561 ] loss: 2.834 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1562 ] loss: 2.833 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1563 ] loss: 2.832 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1564 ] loss: 2.831 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1565 ] loss: 2.831 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1566 ] loss: 2.830 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1567 ] loss: 2.829 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1568 ] loss: 2.828 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1569 ] loss: 2.827 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1570 ] loss: 2.826 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1571 ] loss: 2.825 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1572 ] loss: 2.824 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1573 ] loss: 2.824 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1574 ] loss: 2.823 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1575 ] loss: 2.822 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1576 ] loss: 2.821 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1577 ] loss: 2.820 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1578 ] loss: 2.819 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1579 ] loss: 2.818 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1580 ] loss: 2.817 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1581 ] loss: 2.817 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1582 ] loss: 2.816 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1583 ] loss: 2.815 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1584 ] loss: 2.814 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1585 ] loss: 2.813 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1586 ] loss: 2.812 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1587 ] loss: 2.811 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1588 ] loss: 2.810 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1589 ] loss: 2.810 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1590 ] loss: 2.809 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1591 ] loss: 2.808 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1592 ] loss: 2.807 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1593 ] loss: 2.806 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1594 ] loss: 2.805 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1595 ] loss: 2.804 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1596 ] loss: 2.803 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1597 ] loss: 2.803 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1598 ] loss: 2.802 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1599 ] loss: 2.801 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1600 ] loss: 2.800 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1601 ] loss: 2.799 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1602 ] loss: 2.798 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1603 ] loss: 2.797 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1604 ] loss: 2.797 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1605 ] loss: 2.796 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1606 ] loss: 2.795 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1607 ] loss: 2.794 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1608 ] loss: 2.793 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1609 ] loss: 2.792 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1610 ] loss: 2.791 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1611 ] loss: 2.790 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1612 ] loss: 2.790 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1613 ] loss: 2.789 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1614 ] loss: 2.788 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1615 ] loss: 2.787 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1616 ] loss: 2.786 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1617 ] loss: 2.785 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1618 ] loss: 2.784 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1619 ] loss: 2.783 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1620 ] loss: 2.783 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1621 ] loss: 2.782 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1622 ] loss: 2.781 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1623 ] loss: 2.780 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1624 ] loss: 2.779 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1625 ] loss: 2.778 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1626 ] loss: 2.777 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1627 ] loss: 2.776 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1628 ] loss: 2.776 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1629 ] loss: 2.775 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1630 ] loss: 2.774 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1631 ] loss: 2.773 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1632 ] loss: 2.772 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1633 ] loss: 2.771 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1634 ] loss: 2.770 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1635 ] loss: 2.769 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1636 ] loss: 2.769 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1637 ] loss: 2.768 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1638 ] loss: 2.767 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1639 ] loss: 2.766 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1640 ] loss: 2.765 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1641 ] loss: 2.764 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1642 ] loss: 2.763 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1643 ] loss: 2.762 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1644 ] loss: 2.761 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1645 ] loss: 2.761 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1646 ] loss: 2.760 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1647 ] loss: 2.759 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1648 ] loss: 2.758 correct: 1018.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1649 ] loss: 2.757 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1650 ] loss: 2.756 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1651 ] loss: 2.755 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1652 ] loss: 2.754 correct: 1018.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1653 ] loss: 2.753 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1654 ] loss: 2.752 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1655 ] loss: 2.752 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1656 ] loss: 2.751 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1657 ] loss: 2.750 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1658 ] loss: 2.749 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1659 ] loss: 2.748 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1660 ] loss: 2.747 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1661 ] loss: 2.746 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1662 ] loss: 2.745 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1663 ] loss: 2.744 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1664 ] loss: 2.743 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1665 ] loss: 2.742 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1666 ] loss: 2.742 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1667 ] loss: 2.741 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1668 ] loss: 2.740 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1669 ] loss: 2.739 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1670 ] loss: 2.738 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1671 ] loss: 2.737 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1672 ] loss: 2.736 correct: 1018.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1673 ] loss: 2.735 correct: 1018.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1674 ] loss: 2.734 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1675 ] loss: 2.733 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1676 ] loss: 2.732 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1677 ] loss: 2.732 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1678 ] loss: 2.731 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1679 ] loss: 2.730 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1680 ] loss: 2.729 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1681 ] loss: 2.728 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1682 ] loss: 2.727 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1683 ] loss: 2.726 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1684 ] loss: 2.725 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1685 ] loss: 2.724 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1686 ] loss: 2.723 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1687 ] loss: 2.722 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1688 ] loss: 2.722 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1689 ] loss: 2.721 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1690 ] loss: 2.720 correct: 1018.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1691 ] loss: 2.719 correct: 1018.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1692 ] loss: 2.718 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1693 ] loss: 2.717 correct: 1018.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1694 ] loss: 2.716 correct: 1018.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1695 ] loss: 2.715 correct: 1018.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1696 ] loss: 2.714 correct: 1018.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1697 ] loss: 2.713 correct: 1018.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1698 ] loss: 2.712 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1699 ] loss: 2.711 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1700 ] loss: 2.711 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1701 ] loss: 2.710 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1702 ] loss: 2.709 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1703 ] loss: 2.708 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1704 ] loss: 2.707 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1705 ] loss: 2.706 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1706 ] loss: 2.705 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1707 ] loss: 2.704 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1708 ] loss: 2.703 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1709 ] loss: 2.702 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1710 ] loss: 2.701 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1711 ] loss: 2.700 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1712 ] loss: 2.700 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1713 ] loss: 2.699 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1714 ] loss: 2.698 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1715 ] loss: 2.697 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1716 ] loss: 2.696 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1717 ] loss: 2.695 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1718 ] loss: 2.694 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1719 ] loss: 2.693 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1720 ] loss: 2.692 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1721 ] loss: 2.691 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1722 ] loss: 2.690 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1723 ] loss: 2.689 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1724 ] loss: 2.689 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1725 ] loss: 2.688 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1726 ] loss: 2.687 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1727 ] loss: 2.686 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1728 ] loss: 2.685 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1729 ] loss: 2.684 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1730 ] loss: 2.683 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1731 ] loss: 2.682 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1732 ] loss: 2.681 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1733 ] loss: 2.680 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1734 ] loss: 2.680 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1735 ] loss: 2.679 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1736 ] loss: 2.678 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1737 ] loss: 2.677 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1738 ] loss: 2.676 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1739 ] loss: 2.675 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1740 ] loss: 2.674 correct: 1018.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1741 ] loss: 2.673 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1742 ] loss: 2.672 correct: 1018.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1743 ] loss: 2.671 correct: 1019.000, total: 3000.000, accuracy: 0.340\n",
            "training epoch: [1744 ] loss: 2.670 correct: 1020.000, total: 3000.000, accuracy: 0.340\n",
            "training epoch: [1745 ] loss: 2.670 correct: 1019.000, total: 3000.000, accuracy: 0.340\n",
            "training epoch: [1746 ] loss: 2.669 correct: 1020.000, total: 3000.000, accuracy: 0.340\n",
            "training epoch: [1747 ] loss: 2.668 correct: 1019.000, total: 3000.000, accuracy: 0.340\n",
            "training epoch: [1748 ] loss: 2.667 correct: 1020.000, total: 3000.000, accuracy: 0.340\n",
            "training epoch: [1749 ] loss: 2.666 correct: 1019.000, total: 3000.000, accuracy: 0.340\n",
            "training epoch: [1750 ] loss: 2.665 correct: 1020.000, total: 3000.000, accuracy: 0.340\n",
            "training epoch: [1751 ] loss: 2.664 correct: 1019.000, total: 3000.000, accuracy: 0.340\n",
            "training epoch: [1752 ] loss: 2.663 correct: 1020.000, total: 3000.000, accuracy: 0.340\n",
            "training epoch: [1753 ] loss: 2.662 correct: 1018.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1754 ] loss: 2.662 correct: 1021.000, total: 3000.000, accuracy: 0.340\n",
            "training epoch: [1755 ] loss: 2.661 correct: 1019.000, total: 3000.000, accuracy: 0.340\n",
            "training epoch: [1756 ] loss: 2.660 correct: 1020.000, total: 3000.000, accuracy: 0.340\n",
            "training epoch: [1757 ] loss: 2.659 correct: 1021.000, total: 3000.000, accuracy: 0.340\n",
            "training epoch: [1758 ] loss: 2.658 correct: 1021.000, total: 3000.000, accuracy: 0.340\n",
            "training epoch: [1759 ] loss: 2.657 correct: 1023.000, total: 3000.000, accuracy: 0.341\n",
            "training epoch: [1760 ] loss: 2.656 correct: 1020.000, total: 3000.000, accuracy: 0.340\n",
            "training epoch: [1761 ] loss: 2.655 correct: 1026.000, total: 3000.000, accuracy: 0.342\n",
            "training epoch: [1762 ] loss: 2.654 correct: 1020.000, total: 3000.000, accuracy: 0.340\n",
            "training epoch: [1763 ] loss: 2.654 correct: 1023.000, total: 3000.000, accuracy: 0.341\n",
            "training epoch: [1764 ] loss: 2.653 correct: 1021.000, total: 3000.000, accuracy: 0.340\n",
            "training epoch: [1765 ] loss: 2.652 correct: 1022.000, total: 3000.000, accuracy: 0.341\n",
            "training epoch: [1766 ] loss: 2.651 correct: 1020.000, total: 3000.000, accuracy: 0.340\n",
            "training epoch: [1767 ] loss: 2.650 correct: 1020.000, total: 3000.000, accuracy: 0.340\n",
            "training epoch: [1768 ] loss: 2.649 correct: 1021.000, total: 3000.000, accuracy: 0.340\n",
            "training epoch: [1769 ] loss: 2.648 correct: 1018.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1770 ] loss: 2.647 correct: 1021.000, total: 3000.000, accuracy: 0.340\n",
            "training epoch: [1771 ] loss: 2.646 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1772 ] loss: 2.646 correct: 1019.000, total: 3000.000, accuracy: 0.340\n",
            "training epoch: [1773 ] loss: 2.645 correct: 1018.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1774 ] loss: 2.644 correct: 1020.000, total: 3000.000, accuracy: 0.340\n",
            "training epoch: [1775 ] loss: 2.643 correct: 1018.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1776 ] loss: 2.642 correct: 1021.000, total: 3000.000, accuracy: 0.340\n",
            "training epoch: [1777 ] loss: 2.641 correct: 1018.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1778 ] loss: 2.640 correct: 1021.000, total: 3000.000, accuracy: 0.340\n",
            "training epoch: [1779 ] loss: 2.639 correct: 1018.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1780 ] loss: 2.638 correct: 1021.000, total: 3000.000, accuracy: 0.340\n",
            "training epoch: [1781 ] loss: 2.638 correct: 1018.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1782 ] loss: 2.637 correct: 1020.000, total: 3000.000, accuracy: 0.340\n",
            "training epoch: [1783 ] loss: 2.636 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1784 ] loss: 2.635 correct: 1020.000, total: 3000.000, accuracy: 0.340\n",
            "training epoch: [1785 ] loss: 2.634 correct: 1019.000, total: 3000.000, accuracy: 0.340\n",
            "training epoch: [1786 ] loss: 2.633 correct: 1020.000, total: 3000.000, accuracy: 0.340\n",
            "training epoch: [1787 ] loss: 2.632 correct: 1019.000, total: 3000.000, accuracy: 0.340\n",
            "training epoch: [1788 ] loss: 2.631 correct: 1020.000, total: 3000.000, accuracy: 0.340\n",
            "training epoch: [1789 ] loss: 2.631 correct: 1019.000, total: 3000.000, accuracy: 0.340\n",
            "training epoch: [1790 ] loss: 2.630 correct: 1019.000, total: 3000.000, accuracy: 0.340\n",
            "training epoch: [1791 ] loss: 2.629 correct: 1019.000, total: 3000.000, accuracy: 0.340\n",
            "training epoch: [1792 ] loss: 2.628 correct: 1019.000, total: 3000.000, accuracy: 0.340\n",
            "training epoch: [1793 ] loss: 2.627 correct: 1019.000, total: 3000.000, accuracy: 0.340\n",
            "training epoch: [1794 ] loss: 2.626 correct: 1019.000, total: 3000.000, accuracy: 0.340\n",
            "training epoch: [1795 ] loss: 2.625 correct: 1019.000, total: 3000.000, accuracy: 0.340\n",
            "training epoch: [1796 ] loss: 2.624 correct: 1019.000, total: 3000.000, accuracy: 0.340\n",
            "training epoch: [1797 ] loss: 2.624 correct: 1019.000, total: 3000.000, accuracy: 0.340\n",
            "training epoch: [1798 ] loss: 2.623 correct: 1019.000, total: 3000.000, accuracy: 0.340\n",
            "training epoch: [1799 ] loss: 2.622 correct: 1019.000, total: 3000.000, accuracy: 0.340\n",
            "training epoch: [1800 ] loss: 2.621 correct: 1019.000, total: 3000.000, accuracy: 0.340\n",
            "training epoch: [1801 ] loss: 2.620 correct: 1019.000, total: 3000.000, accuracy: 0.340\n",
            "training epoch: [1802 ] loss: 2.619 correct: 1019.000, total: 3000.000, accuracy: 0.340\n",
            "training epoch: [1803 ] loss: 2.618 correct: 1019.000, total: 3000.000, accuracy: 0.340\n",
            "training epoch: [1804 ] loss: 2.617 correct: 1019.000, total: 3000.000, accuracy: 0.340\n",
            "training epoch: [1805 ] loss: 2.617 correct: 1018.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1806 ] loss: 2.616 correct: 1018.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1807 ] loss: 2.615 correct: 1018.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1808 ] loss: 2.614 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1809 ] loss: 2.613 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1810 ] loss: 2.612 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1811 ] loss: 2.611 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1812 ] loss: 2.610 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1813 ] loss: 2.609 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1814 ] loss: 2.608 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1815 ] loss: 2.608 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1816 ] loss: 2.607 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1817 ] loss: 2.606 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1818 ] loss: 2.605 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1819 ] loss: 2.604 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1820 ] loss: 2.603 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1821 ] loss: 2.602 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1822 ] loss: 2.601 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1823 ] loss: 2.600 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1824 ] loss: 2.599 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1825 ] loss: 2.598 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1826 ] loss: 2.597 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1827 ] loss: 2.596 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1828 ] loss: 2.595 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1829 ] loss: 2.594 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1830 ] loss: 2.593 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1831 ] loss: 2.591 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1832 ] loss: 2.590 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1833 ] loss: 2.589 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1834 ] loss: 2.588 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1835 ] loss: 2.587 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1836 ] loss: 2.586 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1837 ] loss: 2.585 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1838 ] loss: 2.584 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1839 ] loss: 2.583 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1840 ] loss: 2.581 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1841 ] loss: 2.580 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1842 ] loss: 2.579 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1843 ] loss: 2.578 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1844 ] loss: 2.577 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1845 ] loss: 2.576 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1846 ] loss: 2.574 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1847 ] loss: 2.573 correct: 1019.000, total: 3000.000, accuracy: 0.340\n",
            "training epoch: [1848 ] loss: 2.572 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1849 ] loss: 2.571 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1850 ] loss: 2.570 correct: 1019.000, total: 3000.000, accuracy: 0.340\n",
            "training epoch: [1851 ] loss: 2.568 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1852 ] loss: 2.567 correct: 1018.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1853 ] loss: 2.566 correct: 1018.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1854 ] loss: 2.564 correct: 1020.000, total: 3000.000, accuracy: 0.340\n",
            "training epoch: [1855 ] loss: 2.563 correct: 1022.000, total: 3000.000, accuracy: 0.341\n",
            "training epoch: [1856 ] loss: 2.562 correct: 1022.000, total: 3000.000, accuracy: 0.341\n",
            "training epoch: [1857 ] loss: 2.560 correct: 1023.000, total: 3000.000, accuracy: 0.341\n",
            "training epoch: [1858 ] loss: 2.559 correct: 1023.000, total: 3000.000, accuracy: 0.341\n",
            "training epoch: [1859 ] loss: 2.558 correct: 1023.000, total: 3000.000, accuracy: 0.341\n",
            "training epoch: [1860 ] loss: 2.557 correct: 1022.000, total: 3000.000, accuracy: 0.341\n",
            "training epoch: [1861 ] loss: 2.555 correct: 1022.000, total: 3000.000, accuracy: 0.341\n",
            "training epoch: [1862 ] loss: 2.554 correct: 1022.000, total: 3000.000, accuracy: 0.341\n",
            "training epoch: [1863 ] loss: 2.553 correct: 1023.000, total: 3000.000, accuracy: 0.341\n",
            "training epoch: [1864 ] loss: 2.551 correct: 1022.000, total: 3000.000, accuracy: 0.341\n",
            "training epoch: [1865 ] loss: 2.550 correct: 1023.000, total: 3000.000, accuracy: 0.341\n",
            "training epoch: [1866 ] loss: 2.548 correct: 1022.000, total: 3000.000, accuracy: 0.341\n",
            "training epoch: [1867 ] loss: 2.547 correct: 1022.000, total: 3000.000, accuracy: 0.341\n",
            "training epoch: [1868 ] loss: 2.546 correct: 1023.000, total: 3000.000, accuracy: 0.341\n",
            "training epoch: [1869 ] loss: 2.544 correct: 1023.000, total: 3000.000, accuracy: 0.341\n",
            "training epoch: [1870 ] loss: 2.543 correct: 1024.000, total: 3000.000, accuracy: 0.341\n",
            "training epoch: [1871 ] loss: 2.542 correct: 1024.000, total: 3000.000, accuracy: 0.341\n",
            "training epoch: [1872 ] loss: 2.541 correct: 1023.000, total: 3000.000, accuracy: 0.341\n",
            "training epoch: [1873 ] loss: 2.539 correct: 1023.000, total: 3000.000, accuracy: 0.341\n",
            "training epoch: [1874 ] loss: 2.538 correct: 1023.000, total: 3000.000, accuracy: 0.341\n",
            "training epoch: [1875 ] loss: 2.537 correct: 1022.000, total: 3000.000, accuracy: 0.341\n",
            "training epoch: [1876 ] loss: 2.535 correct: 1022.000, total: 3000.000, accuracy: 0.341\n",
            "training epoch: [1877 ] loss: 2.534 correct: 1021.000, total: 3000.000, accuracy: 0.340\n",
            "training epoch: [1878 ] loss: 2.533 correct: 1022.000, total: 3000.000, accuracy: 0.341\n",
            "training epoch: [1879 ] loss: 2.531 correct: 1022.000, total: 3000.000, accuracy: 0.341\n",
            "training epoch: [1880 ] loss: 2.530 correct: 1022.000, total: 3000.000, accuracy: 0.341\n",
            "training epoch: [1881 ] loss: 2.529 correct: 1023.000, total: 3000.000, accuracy: 0.341\n",
            "training epoch: [1882 ] loss: 2.527 correct: 1020.000, total: 3000.000, accuracy: 0.340\n",
            "training epoch: [1883 ] loss: 2.526 correct: 1020.000, total: 3000.000, accuracy: 0.340\n",
            "training epoch: [1884 ] loss: 2.525 correct: 1019.000, total: 3000.000, accuracy: 0.340\n",
            "training epoch: [1885 ] loss: 2.524 correct: 1020.000, total: 3000.000, accuracy: 0.340\n",
            "training epoch: [1886 ] loss: 2.522 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1887 ] loss: 2.521 correct: 1018.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1888 ] loss: 2.520 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1889 ] loss: 2.518 correct: 1018.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1890 ] loss: 2.517 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1891 ] loss: 2.516 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1892 ] loss: 2.515 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1893 ] loss: 2.513 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1894 ] loss: 2.512 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1895 ] loss: 2.511 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1896 ] loss: 2.510 correct: 1018.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1897 ] loss: 2.508 correct: 1018.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1898 ] loss: 2.507 correct: 1018.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1899 ] loss: 2.506 correct: 1018.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1900 ] loss: 2.505 correct: 1018.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1901 ] loss: 2.503 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1902 ] loss: 2.502 correct: 1018.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1903 ] loss: 2.501 correct: 1018.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1904 ] loss: 2.500 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1905 ] loss: 2.498 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1906 ] loss: 2.497 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1907 ] loss: 2.496 correct: 1018.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1908 ] loss: 2.495 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1909 ] loss: 2.494 correct: 1018.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1910 ] loss: 2.492 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1911 ] loss: 2.491 correct: 1018.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1912 ] loss: 2.490 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1913 ] loss: 2.489 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1914 ] loss: 2.487 correct: 1018.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1915 ] loss: 2.486 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1916 ] loss: 2.485 correct: 1020.000, total: 3000.000, accuracy: 0.340\n",
            "training epoch: [1917 ] loss: 2.484 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1918 ] loss: 2.483 correct: 1020.000, total: 3000.000, accuracy: 0.340\n",
            "training epoch: [1919 ] loss: 2.481 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1920 ] loss: 2.480 correct: 1020.000, total: 3000.000, accuracy: 0.340\n",
            "training epoch: [1921 ] loss: 2.479 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1922 ] loss: 2.478 correct: 1023.000, total: 3000.000, accuracy: 0.341\n",
            "training epoch: [1923 ] loss: 2.477 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1924 ] loss: 2.475 correct: 1023.000, total: 3000.000, accuracy: 0.341\n",
            "training epoch: [1925 ] loss: 2.474 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1926 ] loss: 2.473 correct: 1023.000, total: 3000.000, accuracy: 0.341\n",
            "training epoch: [1927 ] loss: 2.472 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1928 ] loss: 2.471 correct: 1019.000, total: 3000.000, accuracy: 0.340\n",
            "training epoch: [1929 ] loss: 2.469 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1930 ] loss: 2.468 correct: 1019.000, total: 3000.000, accuracy: 0.340\n",
            "training epoch: [1931 ] loss: 2.467 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1932 ] loss: 2.466 correct: 1019.000, total: 3000.000, accuracy: 0.340\n",
            "training epoch: [1933 ] loss: 2.465 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1934 ] loss: 2.464 correct: 1018.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1935 ] loss: 2.462 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1936 ] loss: 2.461 correct: 1018.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1937 ] loss: 2.460 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1938 ] loss: 2.459 correct: 1018.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1939 ] loss: 2.458 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1940 ] loss: 2.456 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1941 ] loss: 2.455 correct: 1011.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1942 ] loss: 2.454 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1943 ] loss: 2.453 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1944 ] loss: 2.452 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1945 ] loss: 2.451 correct: 1011.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1946 ] loss: 2.449 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1947 ] loss: 2.448 correct: 1011.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1948 ] loss: 2.447 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1949 ] loss: 2.446 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1950 ] loss: 2.445 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1951 ] loss: 2.444 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1952 ] loss: 2.442 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1953 ] loss: 2.441 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1954 ] loss: 2.440 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1955 ] loss: 2.439 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1956 ] loss: 2.438 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1957 ] loss: 2.437 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1958 ] loss: 2.435 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1959 ] loss: 2.434 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1960 ] loss: 2.433 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1961 ] loss: 2.432 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1962 ] loss: 2.431 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1963 ] loss: 2.430 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1964 ] loss: 2.429 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1965 ] loss: 2.427 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1966 ] loss: 2.426 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1967 ] loss: 2.425 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1968 ] loss: 2.424 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1969 ] loss: 2.423 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1970 ] loss: 2.422 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1971 ] loss: 2.421 correct: 1011.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1972 ] loss: 2.419 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1973 ] loss: 2.418 correct: 1011.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1974 ] loss: 2.417 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [1975 ] loss: 2.416 correct: 1011.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1976 ] loss: 2.415 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1977 ] loss: 2.414 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1978 ] loss: 2.413 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1979 ] loss: 2.411 correct: 1011.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1980 ] loss: 2.410 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1981 ] loss: 2.409 correct: 1011.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1982 ] loss: 2.408 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [1983 ] loss: 2.407 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1984 ] loss: 2.406 correct: 1011.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1985 ] loss: 2.405 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1986 ] loss: 2.403 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1987 ] loss: 2.402 correct: 1011.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1988 ] loss: 2.401 correct: 1008.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1989 ] loss: 2.400 correct: 1011.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1990 ] loss: 2.399 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1991 ] loss: 2.398 correct: 1007.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1992 ] loss: 2.397 correct: 1008.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1993 ] loss: 2.395 correct: 1006.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [1994 ] loss: 2.394 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1995 ] loss: 2.393 correct: 1006.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [1996 ] loss: 2.392 correct: 1008.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1997 ] loss: 2.391 correct: 1008.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [1998 ] loss: 2.390 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [1999 ] loss: 2.389 correct: 1008.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [2000 ] loss: 2.388 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [2001 ] loss: 2.386 correct: 1007.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [2002 ] loss: 2.385 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [2003 ] loss: 2.384 correct: 1008.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [2004 ] loss: 2.383 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [2005 ] loss: 2.382 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [2006 ] loss: 2.381 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [2007 ] loss: 2.380 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [2008 ] loss: 2.378 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [2009 ] loss: 2.377 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [2010 ] loss: 2.376 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [2011 ] loss: 2.375 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [2012 ] loss: 2.374 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [2013 ] loss: 2.373 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [2014 ] loss: 2.372 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [2015 ] loss: 2.371 correct: 1011.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [2016 ] loss: 2.369 correct: 1011.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [2017 ] loss: 2.368 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [2018 ] loss: 2.367 correct: 1011.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [2019 ] loss: 2.366 correct: 1011.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [2020 ] loss: 2.365 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [2021 ] loss: 2.364 correct: 1011.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [2022 ] loss: 2.363 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [2023 ] loss: 2.362 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [2024 ] loss: 2.360 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [2025 ] loss: 2.359 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [2026 ] loss: 2.358 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [2027 ] loss: 2.357 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [2028 ] loss: 2.356 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [2029 ] loss: 2.355 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [2030 ] loss: 2.354 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [2031 ] loss: 2.353 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [2032 ] loss: 2.351 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [2033 ] loss: 2.350 correct: 1008.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [2034 ] loss: 2.349 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [2035 ] loss: 2.348 correct: 1006.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [2036 ] loss: 2.347 correct: 1011.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [2037 ] loss: 2.346 correct: 1006.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [2038 ] loss: 2.345 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [2039 ] loss: 2.344 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [2040 ] loss: 2.342 correct: 1011.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [2041 ] loss: 2.341 correct: 1008.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [2042 ] loss: 2.340 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [2043 ] loss: 2.339 correct: 1006.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [2044 ] loss: 2.338 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [2045 ] loss: 2.337 correct: 1006.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [2046 ] loss: 2.336 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [2047 ] loss: 2.335 correct: 1007.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [2048 ] loss: 2.333 correct: 1011.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [2049 ] loss: 2.332 correct: 1006.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [2050 ] loss: 2.331 correct: 1011.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [2051 ] loss: 2.330 correct: 1006.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [2052 ] loss: 2.329 correct: 1011.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [2053 ] loss: 2.328 correct: 1006.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [2054 ] loss: 2.327 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [2055 ] loss: 2.326 correct: 1006.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [2056 ] loss: 2.324 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [2057 ] loss: 2.323 correct: 1005.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [2058 ] loss: 2.322 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [2059 ] loss: 2.321 correct: 1004.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [2060 ] loss: 2.320 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [2061 ] loss: 2.319 correct: 1003.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [2062 ] loss: 2.318 correct: 1011.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [2063 ] loss: 2.316 correct: 1006.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [2064 ] loss: 2.315 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [2065 ] loss: 2.314 correct: 1006.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [2066 ] loss: 2.313 correct: 1007.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [2067 ] loss: 2.312 correct: 1003.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [2068 ] loss: 2.311 correct: 1007.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [2069 ] loss: 2.310 correct: 1001.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [2070 ] loss: 2.308 correct: 1007.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [2071 ] loss: 2.307 correct: 1000.000, total: 3000.000, accuracy: 0.333\n",
            "training epoch: [2072 ] loss: 2.306 correct: 1008.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [2073 ] loss: 2.305 correct: 999.000, total: 3000.000, accuracy: 0.333\n",
            "training epoch: [2074 ] loss: 2.304 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [2075 ] loss: 2.303 correct: 1000.000, total: 3000.000, accuracy: 0.333\n",
            "training epoch: [2076 ] loss: 2.301 correct: 1008.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [2077 ] loss: 2.300 correct: 1001.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [2078 ] loss: 2.299 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [2079 ] loss: 2.298 correct: 1005.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [2080 ] loss: 2.297 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [2081 ] loss: 2.296 correct: 1003.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [2082 ] loss: 2.294 correct: 1007.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [2083 ] loss: 2.293 correct: 1004.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [2084 ] loss: 2.292 correct: 1008.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [2085 ] loss: 2.291 correct: 1005.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [2086 ] loss: 2.290 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [2087 ] loss: 2.288 correct: 1003.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [2088 ] loss: 2.287 correct: 1008.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [2089 ] loss: 2.286 correct: 1005.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [2090 ] loss: 2.285 correct: 1007.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [2091 ] loss: 2.284 correct: 1006.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [2092 ] loss: 2.282 correct: 1007.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [2093 ] loss: 2.281 correct: 1004.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [2094 ] loss: 2.280 correct: 1008.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [2095 ] loss: 2.279 correct: 1003.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [2096 ] loss: 2.277 correct: 1007.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [2097 ] loss: 2.276 correct: 1004.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [2098 ] loss: 2.275 correct: 1005.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [2099 ] loss: 2.274 correct: 1004.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [2100 ] loss: 2.272 correct: 1004.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [2101 ] loss: 2.271 correct: 1004.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [2102 ] loss: 2.270 correct: 1005.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [2103 ] loss: 2.268 correct: 1004.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [2104 ] loss: 2.267 correct: 1005.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [2105 ] loss: 2.266 correct: 1001.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [2106 ] loss: 2.265 correct: 1006.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [2107 ] loss: 2.263 correct: 1002.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [2108 ] loss: 2.262 correct: 1003.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [2109 ] loss: 2.261 correct: 1001.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [2110 ] loss: 2.260 correct: 1003.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [2111 ] loss: 2.258 correct: 1000.000, total: 3000.000, accuracy: 0.333\n",
            "training epoch: [2112 ] loss: 2.257 correct: 1003.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [2113 ] loss: 2.256 correct: 1000.000, total: 3000.000, accuracy: 0.333\n",
            "training epoch: [2114 ] loss: 2.254 correct: 1003.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [2115 ] loss: 2.253 correct: 1001.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [2116 ] loss: 2.252 correct: 1002.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [2117 ] loss: 2.251 correct: 1003.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [2118 ] loss: 2.249 correct: 1002.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [2119 ] loss: 2.248 correct: 1003.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [2120 ] loss: 2.247 correct: 1002.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [2121 ] loss: 2.246 correct: 1001.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [2122 ] loss: 2.244 correct: 1002.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [2123 ] loss: 2.243 correct: 1002.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [2124 ] loss: 2.242 correct: 1003.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [2125 ] loss: 2.241 correct: 1000.000, total: 3000.000, accuracy: 0.333\n",
            "training epoch: [2126 ] loss: 2.240 correct: 1004.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [2127 ] loss: 2.238 correct: 1003.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [2128 ] loss: 2.237 correct: 1004.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [2129 ] loss: 2.236 correct: 1005.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [2130 ] loss: 2.235 correct: 1004.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [2131 ] loss: 2.234 correct: 1005.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [2132 ] loss: 2.233 correct: 1005.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [2133 ] loss: 2.231 correct: 1006.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [2134 ] loss: 2.230 correct: 1004.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [2135 ] loss: 2.229 correct: 1007.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [2136 ] loss: 2.228 correct: 1004.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [2137 ] loss: 2.227 correct: 1008.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [2138 ] loss: 2.226 correct: 1007.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [2139 ] loss: 2.225 correct: 1007.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [2140 ] loss: 2.224 correct: 1007.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [2141 ] loss: 2.223 correct: 1007.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [2142 ] loss: 2.222 correct: 1007.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [2143 ] loss: 2.221 correct: 1005.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [2144 ] loss: 2.220 correct: 1008.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [2145 ] loss: 2.219 correct: 1005.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [2146 ] loss: 2.218 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [2147 ] loss: 2.217 correct: 1006.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [2148 ] loss: 2.216 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [2149 ] loss: 2.215 correct: 1006.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [2150 ] loss: 2.214 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [2151 ] loss: 2.213 correct: 1006.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [2152 ] loss: 2.212 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [2153 ] loss: 2.211 correct: 1004.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [2154 ] loss: 2.210 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [2155 ] loss: 2.209 correct: 1004.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [2156 ] loss: 2.208 correct: 1008.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [2157 ] loss: 2.207 correct: 1005.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [2158 ] loss: 2.206 correct: 1008.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [2159 ] loss: 2.205 correct: 1006.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [2160 ] loss: 2.204 correct: 1008.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [2161 ] loss: 2.203 correct: 1006.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [2162 ] loss: 2.202 correct: 1007.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [2163 ] loss: 2.201 correct: 1007.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [2164 ] loss: 2.201 correct: 1007.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [2165 ] loss: 2.200 correct: 1006.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [2166 ] loss: 2.199 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [2167 ] loss: 2.198 correct: 1006.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [2168 ] loss: 2.197 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [2169 ] loss: 2.196 correct: 1006.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [2170 ] loss: 2.195 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [2171 ] loss: 2.194 correct: 1006.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [2172 ] loss: 2.193 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [2173 ] loss: 2.192 correct: 1006.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [2174 ] loss: 2.192 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [2175 ] loss: 2.191 correct: 1007.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [2176 ] loss: 2.190 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [2177 ] loss: 2.189 correct: 1008.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [2178 ] loss: 2.188 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [2179 ] loss: 2.187 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [2180 ] loss: 2.186 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [2181 ] loss: 2.185 correct: 1008.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [2182 ] loss: 2.185 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [2183 ] loss: 2.184 correct: 1008.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [2184 ] loss: 2.183 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [2185 ] loss: 2.182 correct: 1008.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [2186 ] loss: 2.181 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2187 ] loss: 2.180 correct: 1008.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [2188 ] loss: 2.179 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2189 ] loss: 2.179 correct: 1008.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [2190 ] loss: 2.178 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2191 ] loss: 2.177 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [2192 ] loss: 2.176 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2193 ] loss: 2.175 correct: 1011.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [2194 ] loss: 2.174 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2195 ] loss: 2.174 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [2196 ] loss: 2.173 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2197 ] loss: 2.172 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [2198 ] loss: 2.171 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2199 ] loss: 2.170 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [2200 ] loss: 2.169 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2201 ] loss: 2.169 correct: 1011.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [2202 ] loss: 2.168 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2203 ] loss: 2.167 correct: 1011.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [2204 ] loss: 2.166 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2205 ] loss: 2.165 correct: 1011.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [2206 ] loss: 2.164 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2207 ] loss: 2.163 correct: 1011.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [2208 ] loss: 2.163 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2209 ] loss: 2.162 correct: 1011.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [2210 ] loss: 2.161 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2211 ] loss: 2.160 correct: 1011.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [2212 ] loss: 2.159 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [2213 ] loss: 2.159 correct: 1011.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [2214 ] loss: 2.158 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [2215 ] loss: 2.157 correct: 1011.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [2216 ] loss: 2.156 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2217 ] loss: 2.155 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [2218 ] loss: 2.154 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2219 ] loss: 2.154 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [2220 ] loss: 2.153 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2221 ] loss: 2.152 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [2222 ] loss: 2.151 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2223 ] loss: 2.150 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [2224 ] loss: 2.149 correct: 1011.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [2225 ] loss: 2.149 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [2226 ] loss: 2.148 correct: 1011.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [2227 ] loss: 2.147 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [2228 ] loss: 2.146 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [2229 ] loss: 2.145 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [2230 ] loss: 2.145 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [2231 ] loss: 2.144 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [2232 ] loss: 2.143 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2233 ] loss: 2.142 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [2234 ] loss: 2.141 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2235 ] loss: 2.141 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [2236 ] loss: 2.140 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2237 ] loss: 2.139 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [2238 ] loss: 2.138 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2239 ] loss: 2.137 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [2240 ] loss: 2.137 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2241 ] loss: 2.136 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [2242 ] loss: 2.135 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [2243 ] loss: 2.134 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [2244 ] loss: 2.133 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [2245 ] loss: 2.133 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [2246 ] loss: 2.132 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [2247 ] loss: 2.131 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2248 ] loss: 2.130 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2249 ] loss: 2.129 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2250 ] loss: 2.128 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2251 ] loss: 2.128 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2252 ] loss: 2.127 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2253 ] loss: 2.126 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2254 ] loss: 2.125 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2255 ] loss: 2.124 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2256 ] loss: 2.124 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2257 ] loss: 2.123 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2258 ] loss: 2.122 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2259 ] loss: 2.121 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2260 ] loss: 2.120 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2261 ] loss: 2.120 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2262 ] loss: 2.119 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2263 ] loss: 2.118 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2264 ] loss: 2.117 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2265 ] loss: 2.116 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2266 ] loss: 2.116 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2267 ] loss: 2.115 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2268 ] loss: 2.114 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2269 ] loss: 2.113 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2270 ] loss: 2.112 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2271 ] loss: 2.112 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [2272 ] loss: 2.111 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2273 ] loss: 2.110 correct: 1018.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [2274 ] loss: 2.109 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2275 ] loss: 2.108 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [2276 ] loss: 2.108 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2277 ] loss: 2.107 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [2278 ] loss: 2.106 correct: 1011.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [2279 ] loss: 2.105 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [2280 ] loss: 2.104 correct: 1011.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [2281 ] loss: 2.104 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [2282 ] loss: 2.103 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [2283 ] loss: 2.102 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [2284 ] loss: 2.101 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2285 ] loss: 2.100 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [2286 ] loss: 2.100 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2287 ] loss: 2.099 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [2288 ] loss: 2.098 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2289 ] loss: 2.097 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [2290 ] loss: 2.096 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2291 ] loss: 2.095 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [2292 ] loss: 2.095 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2293 ] loss: 2.094 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [2294 ] loss: 2.093 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2295 ] loss: 2.092 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [2296 ] loss: 2.091 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [2297 ] loss: 2.091 correct: 1018.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [2298 ] loss: 2.090 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [2299 ] loss: 2.089 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2300 ] loss: 2.088 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [2301 ] loss: 2.087 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [2302 ] loss: 2.087 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2303 ] loss: 2.086 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [2304 ] loss: 2.085 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2305 ] loss: 2.084 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [2306 ] loss: 2.083 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2307 ] loss: 2.083 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2308 ] loss: 2.082 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2309 ] loss: 2.081 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2310 ] loss: 2.080 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2311 ] loss: 2.079 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2312 ] loss: 2.079 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2313 ] loss: 2.078 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2314 ] loss: 2.077 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2315 ] loss: 2.076 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2316 ] loss: 2.075 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2317 ] loss: 2.075 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2318 ] loss: 2.074 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2319 ] loss: 2.073 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2320 ] loss: 2.072 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2321 ] loss: 2.071 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2322 ] loss: 2.071 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2323 ] loss: 2.070 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2324 ] loss: 2.069 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [2325 ] loss: 2.068 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2326 ] loss: 2.067 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [2327 ] loss: 2.067 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2328 ] loss: 2.066 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [2329 ] loss: 2.065 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2330 ] loss: 2.064 correct: 1018.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [2331 ] loss: 2.063 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2332 ] loss: 2.063 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [2333 ] loss: 2.062 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2334 ] loss: 2.061 correct: 1017.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [2335 ] loss: 2.060 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2336 ] loss: 2.059 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [2337 ] loss: 2.059 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2338 ] loss: 2.058 correct: 1016.000, total: 3000.000, accuracy: 0.339\n",
            "training epoch: [2339 ] loss: 2.057 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2340 ] loss: 2.056 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2341 ] loss: 2.055 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2342 ] loss: 2.055 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2343 ] loss: 2.054 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2344 ] loss: 2.053 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2345 ] loss: 2.052 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2346 ] loss: 2.051 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2347 ] loss: 2.051 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2348 ] loss: 2.050 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2349 ] loss: 2.049 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2350 ] loss: 2.048 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2351 ] loss: 2.047 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2352 ] loss: 2.047 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2353 ] loss: 2.046 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2354 ] loss: 2.045 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2355 ] loss: 2.044 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [2356 ] loss: 2.043 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2357 ] loss: 2.043 correct: 1011.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [2358 ] loss: 2.042 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2359 ] loss: 2.041 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [2360 ] loss: 2.040 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2361 ] loss: 2.039 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2362 ] loss: 2.039 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2363 ] loss: 2.038 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2364 ] loss: 2.037 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2365 ] loss: 2.036 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2366 ] loss: 2.035 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2367 ] loss: 2.035 correct: 1012.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [2368 ] loss: 2.034 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2369 ] loss: 2.033 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [2370 ] loss: 2.032 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2371 ] loss: 2.031 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [2372 ] loss: 2.030 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2373 ] loss: 2.030 correct: 1008.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [2374 ] loss: 2.029 correct: 1015.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2375 ] loss: 2.028 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [2376 ] loss: 2.027 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2377 ] loss: 2.026 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [2378 ] loss: 2.026 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2379 ] loss: 2.025 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [2380 ] loss: 2.024 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2381 ] loss: 2.023 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [2382 ] loss: 2.022 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2383 ] loss: 2.022 correct: 1011.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [2384 ] loss: 2.021 correct: 1014.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2385 ] loss: 2.020 correct: 1011.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [2386 ] loss: 2.019 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2387 ] loss: 2.018 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [2388 ] loss: 2.017 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2389 ] loss: 2.017 correct: 1010.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [2390 ] loss: 2.016 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2391 ] loss: 2.015 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [2392 ] loss: 2.014 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2393 ] loss: 2.013 correct: 1008.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [2394 ] loss: 2.012 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2395 ] loss: 2.012 correct: 1008.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [2396 ] loss: 2.011 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2397 ] loss: 2.010 correct: 1008.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [2398 ] loss: 2.009 correct: 1013.000, total: 3000.000, accuracy: 0.338\n",
            "training epoch: [2399 ] loss: 2.008 correct: 1008.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [2400 ] loss: 2.007 correct: 1011.000, total: 3000.000, accuracy: 0.337\n",
            "training epoch: [2401 ] loss: 2.006 correct: 1007.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [2402 ] loss: 2.006 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [2403 ] loss: 2.005 correct: 1007.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [2404 ] loss: 2.004 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [2405 ] loss: 2.003 correct: 1008.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [2406 ] loss: 2.002 correct: 1008.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [2407 ] loss: 2.001 correct: 1008.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [2408 ] loss: 2.000 correct: 1007.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [2409 ] loss: 1.999 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [2410 ] loss: 1.999 correct: 1006.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [2411 ] loss: 1.998 correct: 1008.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [2412 ] loss: 1.997 correct: 1006.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [2413 ] loss: 1.996 correct: 1008.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [2414 ] loss: 1.995 correct: 1006.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [2415 ] loss: 1.994 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [2416 ] loss: 1.993 correct: 1006.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [2417 ] loss: 1.992 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [2418 ] loss: 1.992 correct: 1005.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [2419 ] loss: 1.991 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [2420 ] loss: 1.990 correct: 1004.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [2421 ] loss: 1.989 correct: 1008.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [2422 ] loss: 1.988 correct: 1004.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [2423 ] loss: 1.987 correct: 1007.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [2424 ] loss: 1.986 correct: 1003.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [2425 ] loss: 1.985 correct: 1008.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [2426 ] loss: 1.984 correct: 1002.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [2427 ] loss: 1.984 correct: 1008.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [2428 ] loss: 1.983 correct: 1002.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [2429 ] loss: 1.982 correct: 1006.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [2430 ] loss: 1.981 correct: 1001.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [2431 ] loss: 1.980 correct: 1005.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [2432 ] loss: 1.979 correct: 1001.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [2433 ] loss: 1.978 correct: 1005.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [2434 ] loss: 1.977 correct: 1001.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [2435 ] loss: 1.976 correct: 1005.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [2436 ] loss: 1.975 correct: 1001.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [2437 ] loss: 1.974 correct: 1005.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [2438 ] loss: 1.973 correct: 1000.000, total: 3000.000, accuracy: 0.333\n",
            "training epoch: [2439 ] loss: 1.972 correct: 1005.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [2440 ] loss: 1.971 correct: 1001.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [2441 ] loss: 1.970 correct: 1003.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [2442 ] loss: 1.969 correct: 1001.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [2443 ] loss: 1.968 correct: 1003.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [2444 ] loss: 1.967 correct: 1000.000, total: 3000.000, accuracy: 0.333\n",
            "training epoch: [2445 ] loss: 1.966 correct: 1003.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [2446 ] loss: 1.965 correct: 1000.000, total: 3000.000, accuracy: 0.333\n",
            "training epoch: [2447 ] loss: 1.964 correct: 1002.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [2448 ] loss: 1.963 correct: 1001.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [2449 ] loss: 1.962 correct: 1001.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [2450 ] loss: 1.961 correct: 1003.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [2451 ] loss: 1.960 correct: 1000.000, total: 3000.000, accuracy: 0.333\n",
            "training epoch: [2452 ] loss: 1.959 correct: 1006.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [2453 ] loss: 1.958 correct: 1003.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [2454 ] loss: 1.957 correct: 1005.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [2455 ] loss: 1.956 correct: 1003.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [2456 ] loss: 1.955 correct: 1004.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [2457 ] loss: 1.954 correct: 1003.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [2458 ] loss: 1.953 correct: 1004.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [2459 ] loss: 1.952 correct: 1004.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [2460 ] loss: 1.951 correct: 1003.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [2461 ] loss: 1.950 correct: 1003.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [2462 ] loss: 1.949 correct: 1002.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [2463 ] loss: 1.948 correct: 1002.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [2464 ] loss: 1.947 correct: 1002.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [2465 ] loss: 1.946 correct: 1004.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [2466 ] loss: 1.945 correct: 1002.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [2467 ] loss: 1.944 correct: 1007.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [2468 ] loss: 1.943 correct: 1002.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [2469 ] loss: 1.942 correct: 1008.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [2470 ] loss: 1.941 correct: 1003.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [2471 ] loss: 1.940 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [2472 ] loss: 1.939 correct: 1004.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [2473 ] loss: 1.937 correct: 1007.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [2474 ] loss: 1.936 correct: 1004.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [2475 ] loss: 1.935 correct: 1006.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [2476 ] loss: 1.934 correct: 1005.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [2477 ] loss: 1.933 correct: 1004.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [2478 ] loss: 1.932 correct: 1005.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [2479 ] loss: 1.931 correct: 1004.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [2480 ] loss: 1.930 correct: 1007.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [2481 ] loss: 1.928 correct: 1005.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [2482 ] loss: 1.927 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [2483 ] loss: 1.926 correct: 1005.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [2484 ] loss: 1.925 correct: 1005.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [2485 ] loss: 1.924 correct: 1006.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [2486 ] loss: 1.923 correct: 1007.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [2487 ] loss: 1.921 correct: 1008.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [2488 ] loss: 1.920 correct: 1004.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [2489 ] loss: 1.919 correct: 1008.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [2490 ] loss: 1.918 correct: 1002.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [2491 ] loss: 1.917 correct: 1009.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [2492 ] loss: 1.916 correct: 1001.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [2493 ] loss: 1.914 correct: 1006.000, total: 3000.000, accuracy: 0.335\n",
            "training epoch: [2494 ] loss: 1.913 correct: 1000.000, total: 3000.000, accuracy: 0.333\n",
            "training epoch: [2495 ] loss: 1.912 correct: 1007.000, total: 3000.000, accuracy: 0.336\n",
            "training epoch: [2496 ] loss: 1.911 correct: 998.000, total: 3000.000, accuracy: 0.333\n",
            "training epoch: [2497 ] loss: 1.910 correct: 1003.000, total: 3000.000, accuracy: 0.334\n",
            "training epoch: [2498 ] loss: 1.908 correct: 1000.000, total: 3000.000, accuracy: 0.333\n",
            "training epoch: [2499 ] loss: 1.907 correct: 1000.000, total: 3000.000, accuracy: 0.333\n",
            "training epoch: [2500 ] loss: 1.906 correct: 998.000, total: 3000.000, accuracy: 0.333\n",
            "Finished Training run \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AciJnAh5nfug"
      },
      "source": [
        "columns = [\"epochs\", \"argmax > 0.5\" ,\"argmax < 0.5\", \"focus_true_pred_true\", \"focus_false_pred_true\", \"focus_true_pred_false\", \"focus_false_pred_false\" ]\n",
        "df_train = pd.DataFrame()\n",
        "df_test = pd.DataFrame()\n",
        "df_train[columns[0]] = np.arange(0,epoch+2)\n",
        "df_train[columns[1]] = analysis_data_tr[:,-2]/30\n",
        "df_train[columns[2]] = analysis_data_tr[:,-1]/30\n",
        "df_train[columns[3]] = analysis_data_tr[:,0]/30\n",
        "df_train[columns[4]] = analysis_data_tr[:,1]/30\n",
        "df_train[columns[5]] = analysis_data_tr[:,2]/30\n",
        "df_train[columns[6]] = analysis_data_tr[:,3]/30"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NoQpS_6scRsC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "e44b904f-8e86-4fc2-d980-24b72a5085a6"
      },
      "source": [
        "df_train"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>epochs</th>\n",
              "      <th>argmax &gt; 0.5</th>\n",
              "      <th>argmax &lt; 0.5</th>\n",
              "      <th>focus_true_pred_true</th>\n",
              "      <th>focus_false_pred_true</th>\n",
              "      <th>focus_true_pred_false</th>\n",
              "      <th>focus_false_pred_false</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>9.800000</td>\n",
              "      <td>90.200000</td>\n",
              "      <td>5.233333</td>\n",
              "      <td>32.533333</td>\n",
              "      <td>6.533333</td>\n",
              "      <td>55.700000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>59.966667</td>\n",
              "      <td>40.033333</td>\n",
              "      <td>8.966667</td>\n",
              "      <td>23.766667</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>67.166667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>63.166667</td>\n",
              "      <td>36.833333</td>\n",
              "      <td>12.866667</td>\n",
              "      <td>19.833333</td>\n",
              "      <td>0.166667</td>\n",
              "      <td>67.133333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>64.500000</td>\n",
              "      <td>35.500000</td>\n",
              "      <td>13.700000</td>\n",
              "      <td>18.966667</td>\n",
              "      <td>0.133333</td>\n",
              "      <td>67.200000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>64.800000</td>\n",
              "      <td>35.200000</td>\n",
              "      <td>13.800000</td>\n",
              "      <td>18.900000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>67.200000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2496</th>\n",
              "      <td>2496</td>\n",
              "      <td>98.666667</td>\n",
              "      <td>1.333333</td>\n",
              "      <td>0.066667</td>\n",
              "      <td>33.200000</td>\n",
              "      <td>1.766667</td>\n",
              "      <td>64.966667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2497</th>\n",
              "      <td>2497</td>\n",
              "      <td>98.666667</td>\n",
              "      <td>1.333333</td>\n",
              "      <td>0.066667</td>\n",
              "      <td>33.366667</td>\n",
              "      <td>1.766667</td>\n",
              "      <td>64.800000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2498</th>\n",
              "      <td>2498</td>\n",
              "      <td>98.666667</td>\n",
              "      <td>1.333333</td>\n",
              "      <td>0.066667</td>\n",
              "      <td>33.266667</td>\n",
              "      <td>1.766667</td>\n",
              "      <td>64.900000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2499</th>\n",
              "      <td>2499</td>\n",
              "      <td>98.666667</td>\n",
              "      <td>1.333333</td>\n",
              "      <td>0.066667</td>\n",
              "      <td>33.266667</td>\n",
              "      <td>1.766667</td>\n",
              "      <td>64.900000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2500</th>\n",
              "      <td>2500</td>\n",
              "      <td>98.666667</td>\n",
              "      <td>1.333333</td>\n",
              "      <td>0.066667</td>\n",
              "      <td>33.200000</td>\n",
              "      <td>1.766667</td>\n",
              "      <td>64.966667</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2501 rows  7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      epochs  argmax > 0.5  ...  focus_true_pred_false  focus_false_pred_false\n",
              "0          0      9.800000  ...               6.533333               55.700000\n",
              "1          1     59.966667  ...               0.100000               67.166667\n",
              "2          2     63.166667  ...               0.166667               67.133333\n",
              "3          3     64.500000  ...               0.133333               67.200000\n",
              "4          4     64.800000  ...               0.100000               67.200000\n",
              "...      ...           ...  ...                    ...                     ...\n",
              "2496    2496     98.666667  ...               1.766667               64.966667\n",
              "2497    2497     98.666667  ...               1.766667               64.800000\n",
              "2498    2498     98.666667  ...               1.766667               64.900000\n",
              "2499    2499     98.666667  ...               1.766667               64.900000\n",
              "2500    2500     98.666667  ...               1.766667               64.966667\n",
              "\n",
              "[2501 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EY_j8B274vuH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "outputId": "dfd9dd63-ae6d-4d43-b5e0-18e04dca10a3"
      },
      "source": [
        "%cd /content/\n",
        "plot_analysis(df_train,columns,[0,500,1000,1500,2000,2500])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCnS6r2_3WdU"
      },
      "source": [
        "aph = []\n",
        "for i in bg:\n",
        "  aph.append(F.softmax(i,dim=1).detach().numpy())\n",
        "  \n",
        "aph = np.concatenate(aph,axis=0)\n",
        "# torch.save({\n",
        "#             'epoch': 500,\n",
        "#             'model_state_dict': what_net.state_dict(),\n",
        "#             #'optimizer_state_dict': optimizer_what.state_dict(),\n",
        "#             \"optimizer_alpha\":optim1,\n",
        "#             \"FTPT_analysis\":analysis_data_tr,\n",
        "#             \"alpha\":aph\n",
        "\n",
        "#             }, \"type4_what_net_500.pt\")"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVzrDOGS4UxU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3c3494c-5263-4ece-8fdc-543e098c171f"
      },
      "source": [
        "aph[0]"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([9.9999940e-01, 0.0000000e+00, 9.0554377e-36, 0.0000000e+00,\n",
              "       0.0000000e+00, 3.9413095e-07, 0.0000000e+00, 2.9103995e-07,\n",
              "       0.0000000e+00], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Ut6ZTAXbvqx"
      },
      "source": [
        "avrg = []\n",
        "avrg_lbls = []\n",
        "with torch.no_grad():\n",
        "  for i, data1 in  enumerate(train_loader):\n",
        "          inputs , labels , fore_idx = data1\n",
        "          inputs = inputs.double()\n",
        "          inputs = inputs.to(\"cuda\")\n",
        "          beta  = bg[i]\n",
        "          beta = beta.to(\"cuda\")\n",
        "          avg,alpha = attn_avg(inputs,beta)\n",
        "          \n",
        "          avrg.append(avg.detach().cpu().numpy())\n",
        "          avrg_lbls.append(labels.numpy())\n",
        "avrg= np.concatenate(avrg,axis=0)\n",
        "avrg_lbls = np.concatenate(avrg_lbls,axis=0)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KQFYlmTLG0N",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "9aa45b00-feb5-4eac-b950-41c954239a5e"
      },
      "source": [
        "%cd /content/drive/MyDrive/Neural_Tangent_Kernel/\n",
        "data = np.load(\"type_4_data.npy\",allow_pickle=True)\n",
        "%cd /content/\n",
        "plot_decision_boundary(what_net,[1,8,2,9],data,bg,avrg,avrg_lbls)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Neural_Tangent_Kernel\n",
            "/content\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqS70WNvOcIw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "56dc4970-40c0-44cd-91ba-ee5435af8e62"
      },
      "source": [
        "plt.plot(loss_curi_tr)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fceda344610>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GmyEWKD92_T"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ZQMzXXJa8lB"
      },
      "source": [
        ""
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6WUeyvNO3iP"
      },
      "source": [
        ""
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQRNesx7O3Xr"
      },
      "source": [
        ""
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLPSDVK_QWId"
      },
      "source": [
        ""
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hw2rtHfzFyFA"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3lfGywVHL6S"
      },
      "source": [
        ""
      ],
      "execution_count": 30,
      "outputs": []
    }
  ]
}
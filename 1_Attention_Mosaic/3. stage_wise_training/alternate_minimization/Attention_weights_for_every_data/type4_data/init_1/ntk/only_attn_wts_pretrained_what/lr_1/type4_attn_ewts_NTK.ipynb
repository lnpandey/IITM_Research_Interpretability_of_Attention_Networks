{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "type4_attn_ewts_NTK.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "id": "7pI7PJ8XATdT",
        "outputId": "bc120537-39df-4b14-d339-0d1004856e67"
      },
      "source": [
        "/from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-5fbca8c9cf17>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    from(google.colab, import, drive)\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9qVDQd_BBqS",
        "outputId": "f1fcaf48-6586-42ac-abba-c78e32768fc1"
      },
      "source": [
        "%cd /content/drive/MyDrive/Neural_Tangent_Kernel/"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Neural_Tangent_Kernel\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWIyC9Ip_bcq"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "\n",
        "from myrmsprop import MyRmsprop\n",
        "from utils import plot_decision_boundary,attn_avg,plot_analysis\n",
        "from synthetic_dataset import MosaicDataset1\n",
        "from eval_model import calculate_attn_loss,analyse_data\n",
        "\n",
        "\n",
        "\n",
        "%matplotlib inline\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGVy-1EllAc_"
      },
      "source": [
        "train_data = np.load(\"train_type4_data.npy\",allow_pickle=True)\n",
        "\n",
        "test_data = np.load(\"test_type4_data.npy\",allow_pickle=True)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uL771xuGZC5Q"
      },
      "source": [
        "mosaic_list_of_images = train_data[0][\"mosaic_list\"]\n",
        "mosaic_label = train_data[0][\"mosaic_label\"]\n",
        "fore_idx = train_data[0][\"fore_idx\"]\n",
        "\n",
        "\n",
        "test_mosaic_list_of_images = test_data[0][\"mosaic_list\"]\n",
        "test_mosaic_label = test_data[0][\"mosaic_label\"]\n",
        "test_fore_idx = test_data[0][\"fore_idx\"]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uf76JwkxZCT0"
      },
      "source": [
        "batch = 3000\n",
        "train_dataset = MosaicDataset1(mosaic_list_of_images, mosaic_label, fore_idx)\n",
        "train_loader = DataLoader( train_dataset,batch_size= batch ,shuffle=False)\n",
        "#batch = 2000\n",
        "#test_dataset = MosaicDataset1(test_mosaic_list_of_images, test_mosaic_label, test_fore_idx)\n",
        "#test_loader = DataLoader(test_dataset,batch_size= batch ,shuffle=False)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-Lv8nHoB8z-"
      },
      "source": [
        "# NTK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmGjlMfTBp3F"
      },
      "source": [
        "data = np.load(\"NTK_1.npy\",allow_pickle=True)\n",
        "# H = data[0]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beyk_-qYB_Ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44b5d05c-18bc-4659-9ad7-9a8784fc6f3c"
      },
      "source": [
        "print(data[0].keys())\n",
        "H = torch.tensor(data[0][\"NTK\"])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['NTK'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ULTAsyF6G6a"
      },
      "source": [
        "lr_1 = 1/1470559.2\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnqbuxdO2U4j"
      },
      "source": [
        "# p_vec = nn.utils.parameters_to_vector(where_func.parameters())\n",
        "# p, = p_vec.shape\n",
        "# n_m, n_obj,_ = inputs.shape  # number of mosaic images x number of objects in each mosaic  x d\n",
        "# # this is the transpose jacobian (grad y(w))^T)\n",
        "# features = torch.zeros(n_m*n_obj, p, requires_grad=False)\n",
        " \n",
        "# k = 0 \n",
        "\n",
        "\n",
        "# for i in range(27000):\n",
        "#     out = where_func(inpp[i])\n",
        "#     where_func.zero_grad()\n",
        "#     out.backward(retain_graph=False)\n",
        "#     p_grad = torch.tensor([], requires_grad=False)\n",
        "#     for p in where_func.parameters():\n",
        "#       p_grad = torch.cat((p_grad, p.grad.reshape(-1)))\n",
        "#     features[k,:] = p_grad\n",
        "#     k = k+1\n",
        "# tangent_kernel =  features@features.T"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SInPc5gk9XDH"
      },
      "source": [
        "# class Module1(nn.Module):\n",
        "#   def __init__(self):\n",
        "#     super(Module1, self).__init__()\n",
        "#     self.linear1 = nn.Linear(2,100)\n",
        "#     self.linear2 = nn.Linear(100,1)\n",
        "\n",
        "#   def forward(self,x):\n",
        "#     x = F.relu(self.linear1(x))\n",
        "#     x = self.linear2(x)\n",
        "#     return x"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VW0lzy6i9wk0"
      },
      "source": [
        "# from tqdm import tqdm as tqdm"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cti_LAbE8-dn"
      },
      "source": [
        "# inputs,_,_ = iter(train_loader).next()\n",
        "# inputs = torch.reshape(inputs,(27000,2))\n",
        "# inputs = (inputs - torch.mean(inputs,dim=0,keepdims=True) )/torch.std(inputs,dim=0,keepdims=True)\n",
        "# where_net = Module1()\n",
        "# outputs = where_net(inputs)\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-03FnsNP5bk"
      },
      "source": [
        "# feature1 = torch.zeros((27000,200))\n",
        "# feature2  = torch.zeros((27000,100))\n",
        "# for i in tqdm(range(27000)):\n",
        "#   where_net.zero_grad()\n",
        "#   outputs[i].backward(retain_graph=True)\n",
        "#   par = []\n",
        "#   j = 0\n",
        "#   for p in where_net.parameters():\n",
        "#     if j%2 == 0:\n",
        "#       vec = torch.nn.utils.parameters_to_vector(p)\n",
        "#       p_grad = p.grad.reshape(-1)\n",
        "#       par.append(p_grad)\n",
        "#     j = j+1\n",
        "#   feature1[i,:] = par[0]\n",
        "#   feature2[i,:] = par[1]"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eI20WxiR-zCi"
      },
      "source": [
        "# H = feature1@feature1.T + feature2@feature2.T"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWIBQfQly25h"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HbrMidFCla6h"
      },
      "source": [
        "class Module2(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Module2, self).__init__()\n",
        "    self.linear1 = nn.Linear(2,100)\n",
        "    self.linear2 = nn.Linear(100,3)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = F.relu(self.linear1(x))\n",
        "    x = self.linear2(x)\n",
        "    return x"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXpnLkMoCocj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a364005-3fec-4c0d-acb8-5bc3a3c96e19"
      },
      "source": [
        "print(H)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[75.1031, 35.6146, 79.1224,  ..., 63.2880, 38.2985, 72.4251],\n",
            "        [35.6146, 21.1840, 42.9770,  ..., 33.6579, 25.0675, 44.2191],\n",
            "        [79.1224, 42.9770, 92.8314,  ..., 73.0225, 48.9726, 88.7758],\n",
            "        ...,\n",
            "        [63.2880, 33.6579, 73.0225,  ..., 58.0862, 38.0167, 69.3583],\n",
            "        [38.2985, 25.0675, 48.9726,  ..., 38.0167, 34.9229, 54.7437],\n",
            "        [72.4251, 44.2191, 88.7758,  ..., 69.3583, 54.7437, 95.1200]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRDhoG3rEp_w"
      },
      "source": [
        ""
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRqj2VELllkX"
      },
      "source": [
        "torch.manual_seed(1234)\n",
        "what_net = Module2().double()\n",
        "\n",
        "what_net.load_state_dict(torch.load(\"type4_what_net.pt\"))\n",
        "what_net = what_net.to(\"cuda\")"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oc1pKMEVfhat"
      },
      "source": [
        "for param in what_net.parameters():\n",
        "    param.requires_grad = False"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOpZfj1bq7wN"
      },
      "source": [
        "n_batches = 3000//batch\n",
        "bg = []\n",
        "for i in range(n_batches):\n",
        "  torch.manual_seed(i)\n",
        "  betag = torch.randn(3000,9)#torch.ones((250,9))/9\n",
        "  bg.append( betag.requires_grad_() )"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76PwzSMACDDj"
      },
      "source": [
        "# training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S633XgMToeN3"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lrDkUUaDFCR"
      },
      "source": [
        "optim1 = []\n",
        "H= H.to(\"cpu\")\n",
        "for i in range(n_batches):\n",
        "  optim1.append(MyRmsprop([bg[i]],H=H,lr=1))\n",
        "# instantiate what net optimizer\n",
        "#optimizer_what = optim.RMSprop(what_net.parameters(), lr=0.0001)#, momentum=0.9)#,nesterov=True)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPaYaojinMTA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab92b567-d5e6-4c29-acce-aba6f007360d"
      },
      "source": [
        "\n",
        "acti = []\n",
        "analysis_data_tr = []\n",
        "analysis_data_tst = []\n",
        "loss_curi_tr = []\n",
        "loss_curi_tst = []\n",
        "epochs = 2500\n",
        "\n",
        "\n",
        "# calculate zeroth epoch loss and FTPT values\n",
        "running_loss,anlys_data,correct,total,accuracy = calculate_attn_loss(train_loader,bg,what_net,criterion)\n",
        "print('training epoch: [%d ] loss: %.3f correct: %.3f, total: %.3f, accuracy: %.3f' %(0,running_loss,correct,total,accuracy)) \n",
        "loss_curi_tr.append(running_loss)\n",
        "analysis_data_tr.append(anlys_data)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# training starts \n",
        "for epoch in range(epochs): # loop over the dataset multiple times\n",
        "  ep_lossi = []\n",
        "  running_loss = 0.0\n",
        "  #what_net.train()\n",
        "  for i, data in enumerate(train_loader, 0):\n",
        "    # get the inputs\n",
        "    inputs, labels,_  = data\n",
        "    inputs = inputs.double()\n",
        "    beta = bg[i] # alpha for ith batch\n",
        "    #print(labels)\n",
        "    inputs, labels,beta = inputs.to(\"cuda\"),labels.to(\"cuda\"),beta.to(\"cuda\")\n",
        "        \n",
        "    # zero the parameter gradients\n",
        "    #optimizer_what.zero_grad()\n",
        "    optim1[i].zero_grad()\n",
        "      \n",
        "    # forward + backward + optimize\n",
        "    avg,alpha = attn_avg(inputs,beta)\n",
        "    outputs = what_net(avg)     \n",
        "    loss = criterion(outputs, labels)\n",
        "\n",
        "    # print statistics\n",
        "    running_loss += loss.item()\n",
        "    #alpha.retain_grad()\n",
        "    loss.backward(retain_graph=False)\n",
        "    #optimizer_what.step()\n",
        "    optim1[i].step()\n",
        "\n",
        "\n",
        "  running_loss_tr,anls_data,correct,total,accuracy = calculate_attn_loss(train_loader,bg,what_net,criterion)\n",
        "  analysis_data_tr.append(anls_data)\n",
        "  loss_curi_tr.append(running_loss_tr)   #loss per epoch\n",
        "  print('training epoch: [%d ] loss: %.3f correct: %.3f, total: %.3f, accuracy: %.3f' %(epoch+1,running_loss_tr,correct,total,accuracy)) \n",
        "\n",
        "\n",
        "  \n",
        "  if running_loss_tr<=0.08:\n",
        "    break\n",
        "print('Finished Training run ')\n",
        "analysis_data_tr = np.array(analysis_data_tr)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training epoch: [0 ] loss: 10.201 correct: 1133.000, total: 3000.000, accuracy: 0.378\n",
            "training epoch: [1 ] loss: 21.288 correct: 982.000, total: 3000.000, accuracy: 0.327\n",
            "training epoch: [2 ] loss: 19.404 correct: 980.000, total: 3000.000, accuracy: 0.327\n",
            "training epoch: [3 ] loss: 19.021 correct: 980.000, total: 3000.000, accuracy: 0.327\n",
            "training epoch: [4 ] loss: 18.903 correct: 978.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [5 ] loss: 18.851 correct: 977.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [6 ] loss: 18.831 correct: 977.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [7 ] loss: 18.821 correct: 977.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [8 ] loss: 18.816 correct: 976.000, total: 3000.000, accuracy: 0.325\n",
            "training epoch: [9 ] loss: 18.813 correct: 976.000, total: 3000.000, accuracy: 0.325\n",
            "training epoch: [10 ] loss: 18.811 correct: 976.000, total: 3000.000, accuracy: 0.325\n",
            "training epoch: [11 ] loss: 18.811 correct: 976.000, total: 3000.000, accuracy: 0.325\n",
            "training epoch: [12 ] loss: 18.811 correct: 976.000, total: 3000.000, accuracy: 0.325\n",
            "training epoch: [13 ] loss: 18.811 correct: 976.000, total: 3000.000, accuracy: 0.325\n",
            "training epoch: [14 ] loss: 18.811 correct: 976.000, total: 3000.000, accuracy: 0.325\n",
            "training epoch: [15 ] loss: 18.812 correct: 976.000, total: 3000.000, accuracy: 0.325\n",
            "training epoch: [16 ] loss: 18.813 correct: 976.000, total: 3000.000, accuracy: 0.325\n",
            "training epoch: [17 ] loss: 18.814 correct: 976.000, total: 3000.000, accuracy: 0.325\n",
            "training epoch: [18 ] loss: 18.815 correct: 976.000, total: 3000.000, accuracy: 0.325\n",
            "training epoch: [19 ] loss: 18.816 correct: 976.000, total: 3000.000, accuracy: 0.325\n",
            "training epoch: [20 ] loss: 18.817 correct: 976.000, total: 3000.000, accuracy: 0.325\n",
            "training epoch: [21 ] loss: 18.819 correct: 976.000, total: 3000.000, accuracy: 0.325\n",
            "training epoch: [22 ] loss: 18.820 correct: 976.000, total: 3000.000, accuracy: 0.325\n",
            "training epoch: [23 ] loss: 18.821 correct: 976.000, total: 3000.000, accuracy: 0.325\n",
            "training epoch: [24 ] loss: 18.822 correct: 976.000, total: 3000.000, accuracy: 0.325\n",
            "training epoch: [25 ] loss: 18.823 correct: 976.000, total: 3000.000, accuracy: 0.325\n",
            "training epoch: [26 ] loss: 18.824 correct: 976.000, total: 3000.000, accuracy: 0.325\n",
            "training epoch: [27 ] loss: 18.824 correct: 977.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [28 ] loss: 18.825 correct: 977.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [29 ] loss: 18.826 correct: 977.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [30 ] loss: 18.827 correct: 977.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [31 ] loss: 18.827 correct: 977.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [32 ] loss: 18.828 correct: 978.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [33 ] loss: 18.829 correct: 978.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [34 ] loss: 18.829 correct: 978.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [35 ] loss: 18.829 correct: 978.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [36 ] loss: 18.830 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [37 ] loss: 18.830 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [38 ] loss: 18.831 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [39 ] loss: 18.831 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [40 ] loss: 18.831 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [41 ] loss: 18.832 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [42 ] loss: 18.832 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [43 ] loss: 18.832 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [44 ] loss: 18.832 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [45 ] loss: 18.832 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [46 ] loss: 18.832 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [47 ] loss: 18.833 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [48 ] loss: 18.833 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [49 ] loss: 18.833 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [50 ] loss: 18.833 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [51 ] loss: 18.834 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [52 ] loss: 18.834 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [53 ] loss: 18.834 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [54 ] loss: 18.835 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [55 ] loss: 18.835 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [56 ] loss: 18.835 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [57 ] loss: 18.836 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [58 ] loss: 18.836 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [59 ] loss: 18.836 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [60 ] loss: 18.837 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [61 ] loss: 18.837 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [62 ] loss: 18.837 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [63 ] loss: 18.837 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [64 ] loss: 18.837 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [65 ] loss: 18.837 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [66 ] loss: 18.837 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [67 ] loss: 18.837 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [68 ] loss: 18.838 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [69 ] loss: 18.838 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [70 ] loss: 18.838 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [71 ] loss: 18.838 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [72 ] loss: 18.838 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [73 ] loss: 18.838 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [74 ] loss: 18.838 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [75 ] loss: 18.838 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [76 ] loss: 18.838 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [77 ] loss: 18.838 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [78 ] loss: 18.838 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [79 ] loss: 18.838 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [80 ] loss: 18.838 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [81 ] loss: 18.838 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [82 ] loss: 18.839 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [83 ] loss: 18.839 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [84 ] loss: 18.839 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [85 ] loss: 18.839 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [86 ] loss: 18.839 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [87 ] loss: 18.839 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [88 ] loss: 18.839 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [89 ] loss: 18.839 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [90 ] loss: 18.839 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [91 ] loss: 18.839 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [92 ] loss: 18.839 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [93 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [94 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [95 ] loss: 18.839 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [96 ] loss: 18.839 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [97 ] loss: 18.839 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [98 ] loss: 18.839 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [99 ] loss: 18.839 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [100 ] loss: 18.839 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [101 ] loss: 18.839 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [102 ] loss: 18.839 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [103 ] loss: 18.839 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [104 ] loss: 18.839 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [105 ] loss: 18.839 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [106 ] loss: 18.839 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [107 ] loss: 18.839 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [108 ] loss: 18.839 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [109 ] loss: 18.839 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [110 ] loss: 18.839 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [111 ] loss: 18.839 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [112 ] loss: 18.839 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [113 ] loss: 18.839 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [114 ] loss: 18.839 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [115 ] loss: 18.839 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [116 ] loss: 18.839 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [117 ] loss: 18.839 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [118 ] loss: 18.839 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [119 ] loss: 18.839 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [120 ] loss: 18.839 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [121 ] loss: 18.839 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [122 ] loss: 18.839 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [123 ] loss: 18.839 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [124 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [125 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [126 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [127 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [128 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [129 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [130 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [131 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [132 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [133 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [134 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [135 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [136 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [137 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [138 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [139 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [140 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [141 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [142 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [143 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [144 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [145 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [146 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [147 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [148 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [149 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [150 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [151 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [152 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [153 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [154 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [155 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [156 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [157 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [158 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [159 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [160 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [161 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [162 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [163 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [164 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [165 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [166 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [167 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [168 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [169 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [170 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [171 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [172 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [173 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [174 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [175 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [176 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [177 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [178 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [179 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [180 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [181 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [182 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [183 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [184 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [185 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [186 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [187 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [188 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [189 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [190 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [191 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [192 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [193 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [194 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [195 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [196 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [197 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [198 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [199 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [200 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [201 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [202 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [203 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [204 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [205 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [206 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [207 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [208 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [209 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [210 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [211 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [212 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [213 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [214 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [215 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [216 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [217 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [218 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [219 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [220 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [221 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [222 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [223 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [224 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [225 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [226 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [227 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [228 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [229 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [230 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [231 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [232 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [233 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [234 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [235 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [236 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [237 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [238 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [239 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [240 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [241 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [242 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [243 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [244 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [245 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [246 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [247 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [248 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [249 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [250 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [251 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [252 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [253 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [254 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [255 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [256 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [257 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [258 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [259 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [260 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [261 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [262 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [263 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [264 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [265 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [266 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [267 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [268 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [269 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [270 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [271 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [272 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [273 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [274 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [275 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [276 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [277 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [278 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [279 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [280 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [281 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [282 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [283 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [284 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [285 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [286 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [287 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [288 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [289 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [290 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [291 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [292 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [293 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [294 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [295 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [296 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [297 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [298 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [299 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [300 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [301 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [302 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [303 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [304 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [305 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [306 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [307 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [308 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [309 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [310 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [311 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [312 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [313 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [314 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [315 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [316 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [317 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [318 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [319 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [320 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [321 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [322 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [323 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [324 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [325 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [326 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [327 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [328 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [329 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [330 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [331 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [332 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [333 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [334 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [335 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [336 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [337 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [338 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [339 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [340 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [341 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [342 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [343 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [344 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [345 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [346 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [347 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [348 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [349 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [350 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [351 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [352 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [353 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [354 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [355 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [356 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [357 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [358 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [359 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [360 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [361 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [362 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [363 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [364 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [365 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [366 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [367 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [368 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [369 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [370 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [371 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [372 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [373 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [374 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [375 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [376 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [377 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [378 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [379 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [380 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [381 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [382 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [383 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [384 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [385 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [386 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [387 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [388 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [389 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [390 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [391 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [392 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [393 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [394 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [395 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [396 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [397 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [398 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [399 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [400 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [401 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [402 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [403 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [404 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [405 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [406 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [407 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [408 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [409 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [410 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [411 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [412 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [413 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [414 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [415 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [416 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [417 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [418 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [419 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [420 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [421 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [422 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [423 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [424 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [425 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [426 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [427 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [428 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [429 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [430 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [431 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [432 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [433 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [434 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [435 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [436 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [437 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [438 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [439 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [440 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [441 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [442 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [443 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [444 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [445 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [446 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [447 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [448 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [449 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [450 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [451 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [452 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [453 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [454 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [455 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [456 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [457 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [458 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [459 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [460 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [461 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [462 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [463 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [464 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [465 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [466 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [467 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [468 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [469 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [470 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [471 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [472 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [473 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [474 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [475 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [476 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [477 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [478 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [479 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [480 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [481 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [482 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [483 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [484 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [485 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [486 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [487 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [488 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [489 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [490 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [491 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [492 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [493 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [494 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [495 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [496 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [497 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [498 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [499 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [500 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [501 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [502 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [503 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [504 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [505 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [506 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [507 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [508 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [509 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [510 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [511 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [512 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [513 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [514 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [515 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [516 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [517 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [518 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [519 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [520 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [521 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [522 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [523 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [524 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [525 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [526 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [527 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [528 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [529 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [530 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [531 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [532 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [533 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [534 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [535 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [536 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [537 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [538 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [539 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [540 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [541 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [542 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [543 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [544 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [545 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [546 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [547 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [548 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [549 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [550 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [551 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [552 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [553 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [554 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [555 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [556 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [557 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [558 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [559 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [560 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [561 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [562 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [563 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [564 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [565 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [566 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [567 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [568 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [569 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [570 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [571 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [572 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [573 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [574 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [575 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [576 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [577 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [578 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [579 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [580 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [581 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [582 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [583 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [584 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [585 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [586 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [587 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [588 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [589 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [590 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [591 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [592 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [593 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [594 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [595 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [596 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [597 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [598 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [599 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [600 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [601 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [602 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [603 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [604 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [605 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [606 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [607 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [608 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [609 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [610 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [611 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [612 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [613 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [614 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [615 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [616 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [617 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [618 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [619 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [620 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [621 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [622 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [623 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [624 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [625 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [626 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [627 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [628 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [629 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [630 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [631 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [632 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [633 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [634 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [635 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [636 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [637 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [638 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [639 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [640 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [641 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [642 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [643 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [644 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [645 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [646 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [647 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [648 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [649 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [650 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [651 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [652 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [653 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [654 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [655 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [656 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [657 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [658 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [659 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [660 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [661 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [662 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [663 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [664 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [665 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [666 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [667 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [668 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [669 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [670 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [671 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [672 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [673 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [674 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [675 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [676 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [677 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [678 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [679 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [680 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [681 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [682 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [683 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [684 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [685 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [686 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [687 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [688 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [689 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [690 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [691 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [692 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [693 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [694 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [695 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [696 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [697 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [698 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [699 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [700 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [701 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [702 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [703 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [704 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [705 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [706 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [707 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [708 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [709 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [710 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [711 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [712 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [713 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [714 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [715 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [716 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [717 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [718 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [719 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [720 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [721 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [722 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [723 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [724 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [725 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [726 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [727 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [728 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [729 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [730 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [731 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [732 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [733 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [734 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [735 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [736 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [737 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [738 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [739 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [740 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [741 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [742 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [743 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [744 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [745 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [746 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [747 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [748 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [749 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [750 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [751 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [752 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [753 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [754 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [755 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [756 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [757 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [758 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [759 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [760 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [761 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [762 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [763 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [764 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [765 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [766 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [767 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [768 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [769 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [770 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [771 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [772 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [773 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [774 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [775 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [776 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [777 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [778 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [779 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [780 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [781 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [782 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [783 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [784 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [785 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [786 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [787 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [788 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [789 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [790 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [791 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [792 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [793 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [794 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [795 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [796 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [797 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [798 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [799 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [800 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [801 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [802 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [803 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [804 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [805 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [806 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [807 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [808 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [809 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [810 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [811 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [812 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [813 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [814 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [815 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [816 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [817 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [818 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [819 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [820 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [821 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [822 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [823 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [824 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [825 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [826 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [827 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [828 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [829 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [830 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [831 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [832 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [833 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [834 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [835 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [836 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [837 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [838 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [839 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [840 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [841 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [842 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [843 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [844 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [845 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [846 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [847 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [848 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [849 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [850 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [851 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [852 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [853 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [854 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [855 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [856 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [857 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [858 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [859 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [860 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [861 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [862 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [863 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [864 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [865 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [866 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [867 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [868 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [869 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [870 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [871 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [872 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [873 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [874 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [875 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [876 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [877 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [878 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [879 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [880 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [881 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [882 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [883 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [884 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [885 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [886 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [887 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [888 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [889 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [890 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [891 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [892 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [893 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [894 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [895 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [896 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [897 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [898 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [899 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [900 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [901 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [902 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [903 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [904 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [905 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [906 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [907 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [908 ] loss: 18.840 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [909 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [910 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [911 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [912 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [913 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [914 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [915 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [916 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [917 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [918 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [919 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [920 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [921 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [922 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [923 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [924 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [925 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [926 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [927 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [928 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [929 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [930 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [931 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [932 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [933 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [934 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [935 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [936 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [937 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [938 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [939 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [940 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [941 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [942 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [943 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [944 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [945 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [946 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [947 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [948 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [949 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [950 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [951 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [952 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [953 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [954 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [955 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [956 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [957 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [958 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [959 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [960 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [961 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [962 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [963 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [964 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [965 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [966 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [967 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [968 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [969 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [970 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [971 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [972 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [973 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [974 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [975 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [976 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [977 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [978 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [979 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [980 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [981 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [982 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [983 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [984 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [985 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [986 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [987 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [988 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [989 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [990 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [991 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [992 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [993 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [994 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [995 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [996 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [997 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [998 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [999 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1000 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1001 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1002 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1003 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1004 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1005 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1006 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1007 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1008 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1009 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1010 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1011 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1012 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1013 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1014 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1015 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1016 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1017 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1018 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1019 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1020 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1021 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1022 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1023 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1024 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1025 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1026 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1027 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1028 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1029 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1030 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1031 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1032 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1033 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1034 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1035 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1036 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1037 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1038 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1039 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1040 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1041 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1042 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1043 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1044 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1045 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1046 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1047 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1048 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1049 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1050 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1051 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1052 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1053 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1054 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1055 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1056 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1057 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1058 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1059 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1060 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1061 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1062 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1063 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1064 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1065 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1066 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1067 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1068 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1069 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1070 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1071 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1072 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1073 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1074 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1075 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1076 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1077 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1078 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1079 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1080 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1081 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1082 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1083 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1084 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1085 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1086 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1087 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1088 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1089 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1090 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1091 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1092 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1093 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1094 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1095 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1096 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1097 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1098 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1099 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1100 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1101 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1102 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1103 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1104 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1105 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1106 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1107 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1108 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1109 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1110 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1111 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1112 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1113 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1114 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1115 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1116 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1117 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1118 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1119 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1120 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1121 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1122 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1123 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1124 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1125 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1126 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1127 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1128 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1129 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1130 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1131 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1132 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1133 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1134 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1135 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1136 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1137 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1138 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1139 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1140 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1141 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1142 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1143 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1144 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1145 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1146 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1147 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1148 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1149 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1150 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1151 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1152 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1153 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1154 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1155 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1156 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1157 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1158 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1159 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1160 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1161 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1162 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1163 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1164 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1165 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1166 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1167 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1168 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1169 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1170 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1171 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1172 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1173 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1174 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1175 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1176 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1177 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1178 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1179 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1180 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1181 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1182 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1183 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1184 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1185 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1186 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1187 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1188 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1189 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1190 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1191 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1192 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1193 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1194 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1195 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1196 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1197 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1198 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1199 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1200 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1201 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1202 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1203 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1204 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1205 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1206 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1207 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1208 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1209 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1210 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1211 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1212 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1213 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1214 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1215 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1216 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1217 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1218 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1219 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1220 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1221 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1222 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1223 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1224 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1225 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1226 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1227 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1228 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1229 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1230 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1231 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1232 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1233 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1234 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1235 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1236 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1237 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1238 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1239 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1240 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1241 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1242 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1243 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1244 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1245 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1246 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1247 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1248 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1249 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1250 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1251 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1252 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1253 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1254 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1255 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1256 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1257 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1258 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1259 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1260 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1261 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1262 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1263 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1264 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1265 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1266 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1267 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1268 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1269 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1270 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1271 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1272 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1273 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1274 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1275 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1276 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1277 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1278 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1279 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1280 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1281 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1282 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1283 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1284 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1285 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1286 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1287 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1288 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1289 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1290 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1291 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1292 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1293 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1294 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1295 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1296 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1297 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1298 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1299 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1300 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1301 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1302 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1303 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1304 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1305 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1306 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1307 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1308 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1309 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1310 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1311 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1312 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1313 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1314 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1315 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1316 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1317 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1318 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1319 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1320 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1321 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1322 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1323 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1324 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1325 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1326 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1327 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1328 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1329 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1330 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1331 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1332 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1333 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1334 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1335 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1336 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1337 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1338 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1339 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1340 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1341 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1342 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1343 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1344 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1345 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1346 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1347 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1348 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1349 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1350 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1351 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1352 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1353 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1354 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1355 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1356 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1357 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1358 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1359 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1360 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1361 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1362 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1363 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1364 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1365 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1366 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1367 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1368 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1369 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1370 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1371 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1372 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1373 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1374 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1375 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1376 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1377 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1378 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1379 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1380 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1381 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1382 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1383 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1384 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1385 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1386 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1387 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1388 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1389 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1390 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1391 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1392 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1393 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1394 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1395 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1396 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1397 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1398 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1399 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1400 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1401 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1402 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1403 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1404 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1405 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1406 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1407 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1408 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1409 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1410 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1411 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1412 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1413 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1414 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1415 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1416 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1417 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1418 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1419 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1420 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1421 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1422 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1423 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1424 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1425 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1426 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1427 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1428 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1429 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1430 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1431 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1432 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1433 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1434 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1435 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1436 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1437 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1438 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1439 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1440 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1441 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1442 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1443 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1444 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1445 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1446 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1447 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1448 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1449 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1450 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1451 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1452 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1453 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1454 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1455 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1456 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1457 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1458 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1459 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1460 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1461 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1462 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1463 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1464 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1465 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1466 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1467 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1468 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1469 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1470 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1471 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1472 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1473 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1474 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1475 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1476 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1477 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1478 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1479 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1480 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1481 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1482 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1483 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1484 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1485 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1486 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1487 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1488 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1489 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1490 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1491 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1492 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1493 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1494 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1495 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1496 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1497 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1498 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1499 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1500 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1501 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1502 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1503 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1504 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1505 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1506 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1507 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1508 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1509 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1510 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1511 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1512 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1513 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1514 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1515 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1516 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1517 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1518 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1519 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1520 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1521 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1522 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1523 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1524 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1525 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1526 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1527 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1528 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1529 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1530 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1531 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1532 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1533 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1534 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1535 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1536 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1537 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1538 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1539 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1540 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1541 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1542 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1543 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1544 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1545 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1546 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1547 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1548 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1549 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1550 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1551 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1552 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1553 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1554 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1555 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1556 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1557 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1558 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1559 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1560 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1561 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1562 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1563 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1564 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1565 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1566 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1567 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1568 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1569 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1570 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1571 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1572 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1573 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1574 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1575 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1576 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1577 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1578 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1579 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1580 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1581 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1582 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1583 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1584 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1585 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1586 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1587 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1588 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1589 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1590 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1591 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1592 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1593 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1594 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1595 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1596 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1597 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1598 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1599 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1600 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1601 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1602 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1603 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1604 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1605 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1606 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1607 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1608 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1609 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1610 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1611 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1612 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1613 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1614 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1615 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1616 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1617 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1618 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1619 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1620 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1621 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1622 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1623 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1624 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1625 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1626 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1627 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1628 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1629 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1630 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1631 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1632 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1633 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1634 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1635 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1636 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1637 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1638 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1639 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1640 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1641 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1642 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1643 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1644 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1645 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1646 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1647 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1648 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1649 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1650 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1651 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1652 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1653 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1654 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1655 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1656 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1657 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1658 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1659 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1660 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1661 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1662 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1663 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1664 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1665 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1666 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1667 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1668 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1669 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1670 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1671 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1672 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1673 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1674 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1675 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1676 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1677 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1678 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1679 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1680 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1681 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1682 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1683 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1684 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1685 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1686 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1687 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1688 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1689 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1690 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1691 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1692 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1693 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1694 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1695 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1696 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1697 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1698 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1699 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1700 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1701 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1702 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1703 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1704 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1705 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1706 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1707 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1708 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1709 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1710 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1711 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1712 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1713 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1714 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1715 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1716 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1717 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1718 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1719 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1720 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1721 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1722 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1723 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1724 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1725 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1726 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1727 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1728 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1729 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1730 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1731 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1732 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1733 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1734 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1735 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1736 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1737 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1738 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1739 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1740 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1741 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1742 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1743 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1744 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1745 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1746 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1747 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1748 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1749 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1750 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1751 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1752 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1753 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1754 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1755 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1756 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1757 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1758 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1759 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1760 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1761 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1762 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1763 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1764 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1765 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1766 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1767 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1768 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1769 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1770 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1771 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1772 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1773 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1774 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1775 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1776 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1777 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1778 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1779 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1780 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1781 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1782 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1783 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1784 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1785 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1786 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1787 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1788 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1789 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1790 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1791 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1792 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1793 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1794 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1795 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1796 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1797 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1798 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1799 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1800 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1801 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1802 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1803 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1804 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1805 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1806 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1807 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1808 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1809 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1810 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1811 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1812 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1813 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1814 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1815 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1816 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1817 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1818 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1819 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1820 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1821 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1822 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1823 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1824 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1825 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1826 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1827 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1828 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1829 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1830 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1831 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1832 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1833 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1834 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1835 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1836 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1837 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1838 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1839 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1840 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1841 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1842 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1843 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1844 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1845 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1846 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1847 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1848 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1849 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1850 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1851 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1852 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1853 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1854 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1855 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1856 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1857 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1858 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1859 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1860 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1861 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1862 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1863 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1864 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1865 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1866 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1867 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1868 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1869 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1870 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1871 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1872 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1873 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1874 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1875 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1876 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1877 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1878 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1879 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1880 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1881 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1882 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1883 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1884 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1885 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1886 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1887 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1888 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1889 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1890 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1891 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1892 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1893 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1894 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1895 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1896 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1897 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1898 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1899 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1900 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1901 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1902 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1903 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1904 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1905 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1906 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1907 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1908 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1909 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1910 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1911 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1912 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1913 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1914 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1915 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1916 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1917 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1918 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1919 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1920 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1921 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1922 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1923 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1924 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1925 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1926 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1927 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1928 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1929 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1930 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1931 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1932 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1933 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1934 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1935 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1936 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1937 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1938 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1939 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1940 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1941 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1942 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1943 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1944 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1945 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1946 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1947 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1948 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1949 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1950 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1951 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1952 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1953 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1954 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1955 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1956 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1957 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1958 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1959 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1960 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1961 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1962 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1963 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1964 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1965 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1966 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1967 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1968 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1969 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1970 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1971 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1972 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1973 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1974 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1975 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1976 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1977 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1978 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1979 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1980 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1981 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1982 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1983 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1984 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1985 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1986 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1987 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1988 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1989 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1990 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1991 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1992 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1993 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1994 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1995 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1996 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1997 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1998 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [1999 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2000 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2001 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2002 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2003 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2004 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2005 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2006 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2007 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2008 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2009 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2010 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2011 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2012 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2013 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2014 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2015 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2016 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2017 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2018 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2019 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2020 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2021 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2022 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2023 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2024 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2025 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2026 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2027 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2028 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2029 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2030 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2031 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2032 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2033 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2034 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2035 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2036 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2037 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2038 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2039 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2040 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2041 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2042 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2043 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2044 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2045 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2046 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2047 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2048 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2049 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2050 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2051 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2052 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2053 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2054 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2055 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2056 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2057 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2058 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2059 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2060 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2061 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2062 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2063 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2064 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2065 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2066 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2067 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2068 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2069 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2070 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2071 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2072 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2073 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2074 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2075 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2076 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2077 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2078 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2079 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2080 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2081 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2082 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2083 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2084 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2085 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2086 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2087 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2088 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2089 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2090 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2091 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2092 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2093 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2094 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2095 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2096 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2097 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2098 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2099 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2100 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2101 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2102 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2103 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2104 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2105 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2106 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2107 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2108 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2109 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2110 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2111 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2112 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2113 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2114 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2115 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2116 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2117 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2118 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2119 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2120 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2121 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2122 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2123 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2124 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2125 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2126 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2127 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2128 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2129 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2130 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2131 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2132 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2133 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2134 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2135 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2136 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2137 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2138 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2139 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2140 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2141 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2142 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2143 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2144 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2145 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2146 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2147 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2148 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2149 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2150 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2151 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2152 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2153 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2154 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2155 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2156 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2157 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2158 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2159 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2160 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2161 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2162 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2163 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2164 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2165 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2166 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2167 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2168 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2169 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2170 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2171 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2172 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2173 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2174 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2175 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2176 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2177 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2178 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2179 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2180 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2181 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2182 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2183 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2184 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2185 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2186 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2187 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2188 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2189 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2190 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2191 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2192 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2193 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2194 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2195 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2196 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2197 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2198 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2199 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2200 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2201 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2202 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2203 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2204 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2205 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2206 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2207 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2208 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2209 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2210 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2211 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2212 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2213 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2214 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2215 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2216 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2217 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2218 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2219 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2220 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2221 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2222 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2223 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2224 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2225 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2226 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2227 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2228 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2229 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2230 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2231 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2232 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2233 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2234 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2235 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2236 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2237 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2238 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2239 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2240 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2241 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2242 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2243 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2244 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2245 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2246 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2247 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2248 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2249 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2250 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2251 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2252 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2253 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2254 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2255 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2256 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2257 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2258 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2259 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2260 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2261 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2262 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2263 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2264 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2265 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2266 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2267 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2268 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2269 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2270 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2271 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2272 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2273 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2274 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2275 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2276 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2277 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2278 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2279 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2280 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2281 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2282 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2283 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2284 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2285 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2286 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2287 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2288 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2289 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2290 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2291 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2292 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2293 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2294 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2295 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2296 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2297 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2298 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2299 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2300 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2301 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2302 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2303 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2304 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2305 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2306 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2307 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2308 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2309 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2310 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2311 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2312 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2313 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2314 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2315 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2316 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2317 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2318 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2319 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2320 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2321 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2322 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2323 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2324 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2325 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2326 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2327 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2328 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2329 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2330 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2331 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2332 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2333 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2334 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2335 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2336 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2337 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2338 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2339 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2340 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2341 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2342 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2343 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2344 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2345 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2346 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2347 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2348 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2349 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2350 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2351 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2352 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2353 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2354 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2355 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2356 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2357 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2358 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2359 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2360 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2361 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2362 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2363 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2364 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2365 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2366 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2367 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2368 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2369 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2370 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2371 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2372 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2373 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2374 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2375 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2376 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2377 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2378 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2379 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2380 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2381 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2382 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2383 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2384 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2385 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2386 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2387 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2388 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2389 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2390 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2391 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2392 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2393 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2394 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2395 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2396 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2397 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2398 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2399 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2400 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2401 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2402 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2403 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2404 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2405 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2406 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2407 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2408 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2409 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2410 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2411 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2412 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2413 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2414 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2415 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2416 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2417 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2418 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2419 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2420 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2421 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2422 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2423 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2424 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2425 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2426 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2427 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2428 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2429 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2430 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2431 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2432 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2433 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2434 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2435 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2436 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2437 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2438 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2439 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2440 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2441 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2442 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2443 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2444 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2445 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2446 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2447 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2448 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2449 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2450 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2451 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2452 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2453 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2454 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2455 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2456 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2457 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2458 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2459 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2460 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2461 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2462 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2463 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2464 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2465 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2466 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2467 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2468 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2469 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2470 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2471 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2472 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2473 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2474 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2475 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2476 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2477 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2478 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2479 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2480 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2481 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2482 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2483 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2484 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2485 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2486 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2487 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2488 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2489 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2490 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2491 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2492 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2493 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2494 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2495 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2496 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2497 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2498 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2499 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "training epoch: [2500 ] loss: 18.841 correct: 979.000, total: 3000.000, accuracy: 0.326\n",
            "Finished Training run \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AciJnAh5nfug"
      },
      "source": [
        "columns = [\"epochs\", \"argmax > 0.5\" ,\"argmax < 0.5\", \"focus_true_pred_true\", \"focus_false_pred_true\", \"focus_true_pred_false\", \"focus_false_pred_false\" ]\n",
        "df_train = pd.DataFrame()\n",
        "df_test = pd.DataFrame()\n",
        "df_train[columns[0]] = np.arange(0,epoch+2)\n",
        "df_train[columns[1]] = analysis_data_tr[:,-2]/30\n",
        "df_train[columns[2]] = analysis_data_tr[:,-1]/30\n",
        "df_train[columns[3]] = analysis_data_tr[:,0]/30\n",
        "df_train[columns[4]] = analysis_data_tr[:,1]/30\n",
        "df_train[columns[5]] = analysis_data_tr[:,2]/30\n",
        "df_train[columns[6]] = analysis_data_tr[:,3]/30"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NoQpS_6scRsC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        },
        "outputId": "f1ae14d6-a442-4fa1-d9c1-143c33e3f804"
      },
      "source": [
        "df_train"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>epochs</th>\n",
              "      <th>argmax &gt; 0.5</th>\n",
              "      <th>argmax &lt; 0.5</th>\n",
              "      <th>focus_true_pred_true</th>\n",
              "      <th>focus_false_pred_true</th>\n",
              "      <th>focus_true_pred_false</th>\n",
              "      <th>focus_false_pred_false</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>9.800000</td>\n",
              "      <td>90.200000</td>\n",
              "      <td>5.233333</td>\n",
              "      <td>32.533333</td>\n",
              "      <td>6.533333</td>\n",
              "      <td>55.700000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>59.966667</td>\n",
              "      <td>40.033333</td>\n",
              "      <td>8.966667</td>\n",
              "      <td>23.766667</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>67.166667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>62.900000</td>\n",
              "      <td>37.100000</td>\n",
              "      <td>12.700000</td>\n",
              "      <td>19.966667</td>\n",
              "      <td>0.166667</td>\n",
              "      <td>67.166667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>64.300000</td>\n",
              "      <td>35.700000</td>\n",
              "      <td>13.633333</td>\n",
              "      <td>19.033333</td>\n",
              "      <td>0.133333</td>\n",
              "      <td>67.200000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>64.700000</td>\n",
              "      <td>35.300000</td>\n",
              "      <td>13.733333</td>\n",
              "      <td>18.866667</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>67.300000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2496</th>\n",
              "      <td>2496</td>\n",
              "      <td>65.233333</td>\n",
              "      <td>34.766667</td>\n",
              "      <td>14.466667</td>\n",
              "      <td>18.166667</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>67.266667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2497</th>\n",
              "      <td>2497</td>\n",
              "      <td>65.233333</td>\n",
              "      <td>34.766667</td>\n",
              "      <td>14.466667</td>\n",
              "      <td>18.166667</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>67.266667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2498</th>\n",
              "      <td>2498</td>\n",
              "      <td>65.233333</td>\n",
              "      <td>34.766667</td>\n",
              "      <td>14.466667</td>\n",
              "      <td>18.166667</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>67.266667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2499</th>\n",
              "      <td>2499</td>\n",
              "      <td>65.233333</td>\n",
              "      <td>34.766667</td>\n",
              "      <td>14.466667</td>\n",
              "      <td>18.166667</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>67.266667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2500</th>\n",
              "      <td>2500</td>\n",
              "      <td>65.233333</td>\n",
              "      <td>34.766667</td>\n",
              "      <td>14.466667</td>\n",
              "      <td>18.166667</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>67.266667</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2501 rows  7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      epochs  argmax > 0.5  ...  focus_true_pred_false  focus_false_pred_false\n",
              "0          0      9.800000  ...               6.533333               55.700000\n",
              "1          1     59.966667  ...               0.100000               67.166667\n",
              "2          2     62.900000  ...               0.166667               67.166667\n",
              "3          3     64.300000  ...               0.133333               67.200000\n",
              "4          4     64.700000  ...               0.100000               67.300000\n",
              "...      ...           ...  ...                    ...                     ...\n",
              "2496    2496     65.233333  ...               0.100000               67.266667\n",
              "2497    2497     65.233333  ...               0.100000               67.266667\n",
              "2498    2498     65.233333  ...               0.100000               67.266667\n",
              "2499    2499     65.233333  ...               0.100000               67.266667\n",
              "2500    2500     65.233333  ...               0.100000               67.266667\n",
              "\n",
              "[2501 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EY_j8B274vuH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "outputId": "87466a0a-fba0-4d33-870a-eb905b6d29a3"
      },
      "source": [
        "%cd /content/\n",
        "plot_analysis(df_train,columns,[0,500,1000,1500,2000,2500])"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCnS6r2_3WdU"
      },
      "source": [
        "aph = []\n",
        "for i in bg:\n",
        "  aph.append(F.softmax(i,dim=1).detach().numpy())\n",
        "  \n",
        "aph = np.concatenate(aph,axis=0)\n",
        "# torch.save({\n",
        "#             'epoch': 500,\n",
        "#             'model_state_dict': what_net.state_dict(),\n",
        "#             #'optimizer_state_dict': optimizer_what.state_dict(),\n",
        "#             \"optimizer_alpha\":optim1,\n",
        "#             \"FTPT_analysis\":analysis_data_tr,\n",
        "#             \"alpha\":aph\n",
        "\n",
        "#             }, \"type4_what_net_500.pt\")"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVzrDOGS4UxU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a5722a9-4779-4e8b-ed2c-ad8a2b2d57ca"
      },
      "source": [
        "aph[0]"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([3.9826255e-02, 1.1709866e-10, 4.1227254e-01, 6.3836644e-11,\n",
              "       1.8977366e-09, 5.1874942e-01, 3.7600995e-11, 2.9151743e-02,\n",
              "       5.5202560e-13], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Ut6ZTAXbvqx"
      },
      "source": [
        "avrg = []\n",
        "avrg_lbls = []\n",
        "with torch.no_grad():\n",
        "  for i, data1 in  enumerate(train_loader):\n",
        "          inputs , labels , fore_idx = data1\n",
        "          inputs = inputs.double()\n",
        "          inputs = inputs.to(\"cuda\")\n",
        "          beta  = bg[i]\n",
        "          beta = beta.to(\"cuda\")\n",
        "          avg,alpha = attn_avg(inputs,beta)\n",
        "          \n",
        "          avrg.append(avg.detach().cpu().numpy())\n",
        "          avrg_lbls.append(labels.numpy())\n",
        "avrg= np.concatenate(avrg,axis=0)\n",
        "avrg_lbls = np.concatenate(avrg_lbls,axis=0)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KQFYlmTLG0N",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "6348e4b1-8658-4a3d-9706-1b79266d80df"
      },
      "source": [
        "%cd /content/drive/MyDrive/Neural_Tangent_Kernel/\n",
        "data = np.load(\"type_4_data.npy\",allow_pickle=True)\n",
        "%cd /content/\n",
        "plot_decision_boundary(what_net,[1,8,2,9],data,bg,avrg,avrg_lbls)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Neural_Tangent_Kernel\n",
            "/content\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqS70WNvOcIw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "9984db31-4f0f-4005-e1a2-9059781ccae5"
      },
      "source": [
        "plt.plot(loss_curi_tr)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f9a81394dd0>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GmyEWKD92_T"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ZQMzXXJa8lB"
      },
      "source": [
        ""
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6WUeyvNO3iP"
      },
      "source": [
        ""
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQRNesx7O3Xr"
      },
      "source": [
        ""
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLPSDVK_QWId"
      },
      "source": [
        ""
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hw2rtHfzFyFA"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3lfGywVHL6S"
      },
      "source": [
        ""
      ],
      "execution_count": 31,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "type4_attn_ewts_NTK.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7pI7PJ8XATdT",
        "outputId": "2a4552b8-03f6-4b8e-e1dc-83670bd2eca4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9qVDQd_BBqS",
        "outputId": "782cddfc-8929-4d9c-e77f-260291846cab"
      },
      "source": [
        "%cd /content/drive/MyDrive/Neural_Tangent_Kernel/"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Neural_Tangent_Kernel\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWIyC9Ip_bcq"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "\n",
        "from myrmsprop import MyRmsprop\n",
        "from utils import plot_decision_boundary,attn_avg,plot_analysis\n",
        "from synthetic_dataset import MosaicDataset1\n",
        "from eval_model import calculate_attn_loss,analyse_data\n",
        "\n",
        "\n",
        "\n",
        "%matplotlib inline\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGVy-1EllAc_"
      },
      "source": [
        "train_data = np.load(\"train_type4_data.npy\",allow_pickle=True)\n",
        "\n",
        "test_data = np.load(\"test_type4_data.npy\",allow_pickle=True)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uL771xuGZC5Q"
      },
      "source": [
        "mosaic_list_of_images = train_data[0][\"mosaic_list\"]\n",
        "mosaic_label = train_data[0][\"mosaic_label\"]\n",
        "fore_idx = train_data[0][\"fore_idx\"]\n",
        "\n",
        "\n",
        "test_mosaic_list_of_images = test_data[0][\"mosaic_list\"]\n",
        "test_mosaic_label = test_data[0][\"mosaic_label\"]\n",
        "test_fore_idx = test_data[0][\"fore_idx\"]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uf76JwkxZCT0"
      },
      "source": [
        "batch = 3000\n",
        "train_dataset = MosaicDataset1(mosaic_list_of_images, mosaic_label, fore_idx)\n",
        "train_loader = DataLoader( train_dataset,batch_size= batch ,shuffle=False)\n",
        "#batch = 2000\n",
        "#test_dataset = MosaicDataset1(test_mosaic_list_of_images, test_mosaic_label, test_fore_idx)\n",
        "#test_loader = DataLoader(test_dataset,batch_size= batch ,shuffle=False)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-Lv8nHoB8z-"
      },
      "source": [
        "# NTK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmGjlMfTBp3F"
      },
      "source": [
        "data = np.load(\"NTK_1.npy\",allow_pickle=True)\n",
        "# H = data[0]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beyk_-qYB_Ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d2e2b78-ddbd-4920-b59a-220732f86492"
      },
      "source": [
        "print(data[0].keys())\n",
        "H = torch.tensor(data[0][\"NTK\"])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['NTK'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ULTAsyF6G6a"
      },
      "source": [
        "lr_1 = 1/1470559.2\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnqbuxdO2U4j"
      },
      "source": [
        "# p_vec = nn.utils.parameters_to_vector(where_func.parameters())\n",
        "# p, = p_vec.shape\n",
        "# n_m, n_obj,_ = inputs.shape  # number of mosaic images x number of objects in each mosaic  x d\n",
        "# # this is the transpose jacobian (grad y(w))^T)\n",
        "# features = torch.zeros(n_m*n_obj, p, requires_grad=False)\n",
        " \n",
        "# k = 0 \n",
        "\n",
        "\n",
        "# for i in range(27000):\n",
        "#     out = where_func(inpp[i])\n",
        "#     where_func.zero_grad()\n",
        "#     out.backward(retain_graph=False)\n",
        "#     p_grad = torch.tensor([], requires_grad=False)\n",
        "#     for p in where_func.parameters():\n",
        "#       p_grad = torch.cat((p_grad, p.grad.reshape(-1)))\n",
        "#     features[k,:] = p_grad\n",
        "#     k = k+1\n",
        "# tangent_kernel =  features@features.T"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SInPc5gk9XDH"
      },
      "source": [
        "# class Module1(nn.Module):\n",
        "#   def __init__(self):\n",
        "#     super(Module1, self).__init__()\n",
        "#     self.linear1 = nn.Linear(2,100)\n",
        "#     self.linear2 = nn.Linear(100,1)\n",
        "\n",
        "#   def forward(self,x):\n",
        "#     x = F.relu(self.linear1(x))\n",
        "#     x = self.linear2(x)\n",
        "#     return x"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VW0lzy6i9wk0"
      },
      "source": [
        "# from tqdm import tqdm as tqdm"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cti_LAbE8-dn"
      },
      "source": [
        "# inputs,_,_ = iter(train_loader).next()\n",
        "# inputs = torch.reshape(inputs,(27000,2))\n",
        "# inputs = (inputs - torch.mean(inputs,dim=0,keepdims=True) )/torch.std(inputs,dim=0,keepdims=True)\n",
        "# where_net = Module1()\n",
        "# outputs = where_net(inputs)\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-03FnsNP5bk"
      },
      "source": [
        "# feature1 = torch.zeros((27000,200))\n",
        "# feature2  = torch.zeros((27000,100))\n",
        "# for i in tqdm(range(27000)):\n",
        "#   where_net.zero_grad()\n",
        "#   outputs[i].backward(retain_graph=True)\n",
        "#   par = []\n",
        "#   j = 0\n",
        "#   for p in where_net.parameters():\n",
        "#     if j%2 == 0:\n",
        "#       vec = torch.nn.utils.parameters_to_vector(p)\n",
        "#       p_grad = p.grad.reshape(-1)\n",
        "#       par.append(p_grad)\n",
        "#     j = j+1\n",
        "#   feature1[i,:] = par[0]\n",
        "#   feature2[i,:] = par[1]"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eI20WxiR-zCi"
      },
      "source": [
        "# H = feature1@feature1.T + feature2@feature2.T"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWIBQfQly25h"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HbrMidFCla6h"
      },
      "source": [
        "class Module2(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Module2, self).__init__()\n",
        "    self.linear1 = nn.Linear(2,100)\n",
        "    self.linear2 = nn.Linear(100,3)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = F.relu(self.linear1(x))\n",
        "    x = self.linear2(x)\n",
        "    return x"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXpnLkMoCocj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "488d478c-c664-4f48-f605-a71c53cdb750"
      },
      "source": [
        "print(H)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[75.1031, 35.6146, 79.1224,  ..., 63.2880, 38.2985, 72.4251],\n",
            "        [35.6146, 21.1840, 42.9770,  ..., 33.6579, 25.0675, 44.2191],\n",
            "        [79.1224, 42.9770, 92.8314,  ..., 73.0225, 48.9726, 88.7758],\n",
            "        ...,\n",
            "        [63.2880, 33.6579, 73.0225,  ..., 58.0862, 38.0167, 69.3583],\n",
            "        [38.2985, 25.0675, 48.9726,  ..., 38.0167, 34.9229, 54.7437],\n",
            "        [72.4251, 44.2191, 88.7758,  ..., 69.3583, 54.7437, 95.1200]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRDhoG3rEp_w"
      },
      "source": [
        ""
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRqj2VELllkX"
      },
      "source": [
        "torch.manual_seed(1234)\n",
        "what_net = Module2().double()\n",
        "\n",
        "what_net.load_state_dict(torch.load(\"type4_what_net.pt\"))\n",
        "what_net = what_net.to(\"cuda\")"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oc1pKMEVfhat"
      },
      "source": [
        "for param in what_net.parameters():\n",
        "    param.requires_grad = False"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOpZfj1bq7wN"
      },
      "source": [
        "n_batches = 3000//batch\n",
        "bg = []\n",
        "for i in range(n_batches):\n",
        "  torch.manual_seed(i)\n",
        "  betag = torch.randn(3000,9)#torch.ones((250,9))/9\n",
        "  bg.append( betag.requires_grad_() )"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76PwzSMACDDj"
      },
      "source": [
        "# training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S633XgMToeN3"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lrDkUUaDFCR"
      },
      "source": [
        "optim1 = []\n",
        "H= H.to(\"cpu\")\n",
        "for i in range(n_batches):\n",
        "  optim1.append(MyRmsprop([bg[i]],H=H,lr=0.01))\n",
        "# instantiate what net optimizer\n",
        "#optimizer_what = optim.RMSprop(what_net.parameters(), lr=0.0001)#, momentum=0.9)#,nesterov=True)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPaYaojinMTA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df692d1d-50ba-495e-fb0b-2cb4d401d829"
      },
      "source": [
        "\n",
        "acti = []\n",
        "analysis_data_tr = []\n",
        "analysis_data_tst = []\n",
        "loss_curi_tr = []\n",
        "loss_curi_tst = []\n",
        "epochs = 2500\n",
        "\n",
        "\n",
        "# calculate zeroth epoch loss and FTPT values\n",
        "running_loss,anlys_data,correct,total,accuracy = calculate_attn_loss(train_loader,bg,what_net,criterion)\n",
        "print('training epoch: [%d ] loss: %.3f correct: %.3f, total: %.3f, accuracy: %.3f' %(0,running_loss,correct,total,accuracy)) \n",
        "loss_curi_tr.append(running_loss)\n",
        "analysis_data_tr.append(anlys_data)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# training starts \n",
        "for epoch in range(epochs): # loop over the dataset multiple times\n",
        "  ep_lossi = []\n",
        "  running_loss = 0.0\n",
        "  #what_net.train()\n",
        "  for i, data in enumerate(train_loader, 0):\n",
        "    # get the inputs\n",
        "    inputs, labels,_  = data\n",
        "    inputs = inputs.double()\n",
        "    beta = bg[i] # alpha for ith batch\n",
        "    #print(labels)\n",
        "    inputs, labels,beta = inputs.to(\"cuda\"),labels.to(\"cuda\"),beta.to(\"cuda\")\n",
        "        \n",
        "    # zero the parameter gradients\n",
        "    #optimizer_what.zero_grad()\n",
        "    optim1[i].zero_grad()\n",
        "      \n",
        "    # forward + backward + optimize\n",
        "    avg,alpha = attn_avg(inputs,beta)\n",
        "    outputs = what_net(avg)     \n",
        "    loss = criterion(outputs, labels)\n",
        "\n",
        "    # print statistics\n",
        "    running_loss += loss.item()\n",
        "    #alpha.retain_grad()\n",
        "    loss.backward(retain_graph=False)\n",
        "    #optimizer_what.step()\n",
        "    optim1[i].step()\n",
        "\n",
        "\n",
        "  running_loss_tr,anls_data,correct,total,accuracy = calculate_attn_loss(train_loader,bg,what_net,criterion)\n",
        "  analysis_data_tr.append(anls_data)\n",
        "  loss_curi_tr.append(running_loss_tr)   #loss per epoch\n",
        "  print('training epoch: [%d ] loss: %.3f correct: %.3f, total: %.3f, accuracy: %.3f' %(epoch+1,running_loss_tr,correct,total,accuracy)) \n",
        "\n",
        "\n",
        "  \n",
        "  if running_loss_tr<=0.08:\n",
        "    break\n",
        "print('Finished Training run ')\n",
        "analysis_data_tr = np.array(analysis_data_tr)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training epoch: [0 ] loss: 10.201 correct: 1133.000, total: 3000.000, accuracy: 0.378\n",
            "training epoch: [1 ] loss: 9.980 correct: 1132.000, total: 3000.000, accuracy: 0.377\n",
            "training epoch: [2 ] loss: 9.827 correct: 1123.000, total: 3000.000, accuracy: 0.374\n",
            "training epoch: [3 ] loss: 9.693 correct: 1117.000, total: 3000.000, accuracy: 0.372\n",
            "training epoch: [4 ] loss: 9.566 correct: 1123.000, total: 3000.000, accuracy: 0.374\n",
            "training epoch: [5 ] loss: 9.444 correct: 1129.000, total: 3000.000, accuracy: 0.376\n",
            "training epoch: [6 ] loss: 9.329 correct: 1128.000, total: 3000.000, accuracy: 0.376\n",
            "training epoch: [7 ] loss: 9.222 correct: 1130.000, total: 3000.000, accuracy: 0.377\n",
            "training epoch: [8 ] loss: 9.121 correct: 1135.000, total: 3000.000, accuracy: 0.378\n",
            "training epoch: [9 ] loss: 9.026 correct: 1136.000, total: 3000.000, accuracy: 0.379\n",
            "training epoch: [10 ] loss: 8.939 correct: 1136.000, total: 3000.000, accuracy: 0.379\n",
            "training epoch: [11 ] loss: 8.856 correct: 1141.000, total: 3000.000, accuracy: 0.380\n",
            "training epoch: [12 ] loss: 8.778 correct: 1143.000, total: 3000.000, accuracy: 0.381\n",
            "training epoch: [13 ] loss: 8.704 correct: 1154.000, total: 3000.000, accuracy: 0.385\n",
            "training epoch: [14 ] loss: 8.634 correct: 1160.000, total: 3000.000, accuracy: 0.387\n",
            "training epoch: [15 ] loss: 8.567 correct: 1164.000, total: 3000.000, accuracy: 0.388\n",
            "training epoch: [16 ] loss: 8.504 correct: 1164.000, total: 3000.000, accuracy: 0.388\n",
            "training epoch: [17 ] loss: 8.444 correct: 1167.000, total: 3000.000, accuracy: 0.389\n",
            "training epoch: [18 ] loss: 8.388 correct: 1171.000, total: 3000.000, accuracy: 0.390\n",
            "training epoch: [19 ] loss: 8.334 correct: 1173.000, total: 3000.000, accuracy: 0.391\n",
            "training epoch: [20 ] loss: 8.283 correct: 1174.000, total: 3000.000, accuracy: 0.391\n",
            "training epoch: [21 ] loss: 8.234 correct: 1182.000, total: 3000.000, accuracy: 0.394\n",
            "training epoch: [22 ] loss: 8.186 correct: 1187.000, total: 3000.000, accuracy: 0.396\n",
            "training epoch: [23 ] loss: 8.140 correct: 1194.000, total: 3000.000, accuracy: 0.398\n",
            "training epoch: [24 ] loss: 8.095 correct: 1194.000, total: 3000.000, accuracy: 0.398\n",
            "training epoch: [25 ] loss: 8.052 correct: 1200.000, total: 3000.000, accuracy: 0.400\n",
            "training epoch: [26 ] loss: 8.011 correct: 1208.000, total: 3000.000, accuracy: 0.403\n",
            "training epoch: [27 ] loss: 7.970 correct: 1211.000, total: 3000.000, accuracy: 0.404\n",
            "training epoch: [28 ] loss: 7.931 correct: 1212.000, total: 3000.000, accuracy: 0.404\n",
            "training epoch: [29 ] loss: 7.892 correct: 1216.000, total: 3000.000, accuracy: 0.405\n",
            "training epoch: [30 ] loss: 7.854 correct: 1220.000, total: 3000.000, accuracy: 0.407\n",
            "training epoch: [31 ] loss: 7.817 correct: 1224.000, total: 3000.000, accuracy: 0.408\n",
            "training epoch: [32 ] loss: 7.781 correct: 1223.000, total: 3000.000, accuracy: 0.408\n",
            "training epoch: [33 ] loss: 7.747 correct: 1224.000, total: 3000.000, accuracy: 0.408\n",
            "training epoch: [34 ] loss: 7.712 correct: 1225.000, total: 3000.000, accuracy: 0.408\n",
            "training epoch: [35 ] loss: 7.677 correct: 1228.000, total: 3000.000, accuracy: 0.409\n",
            "training epoch: [36 ] loss: 7.644 correct: 1232.000, total: 3000.000, accuracy: 0.411\n",
            "training epoch: [37 ] loss: 7.612 correct: 1229.000, total: 3000.000, accuracy: 0.410\n",
            "training epoch: [38 ] loss: 7.580 correct: 1229.000, total: 3000.000, accuracy: 0.410\n",
            "training epoch: [39 ] loss: 7.549 correct: 1229.000, total: 3000.000, accuracy: 0.410\n",
            "training epoch: [40 ] loss: 7.518 correct: 1228.000, total: 3000.000, accuracy: 0.409\n",
            "training epoch: [41 ] loss: 7.488 correct: 1230.000, total: 3000.000, accuracy: 0.410\n",
            "training epoch: [42 ] loss: 7.458 correct: 1230.000, total: 3000.000, accuracy: 0.410\n",
            "training epoch: [43 ] loss: 7.428 correct: 1228.000, total: 3000.000, accuracy: 0.409\n",
            "training epoch: [44 ] loss: 7.400 correct: 1231.000, total: 3000.000, accuracy: 0.410\n",
            "training epoch: [45 ] loss: 7.371 correct: 1236.000, total: 3000.000, accuracy: 0.412\n",
            "training epoch: [46 ] loss: 7.344 correct: 1238.000, total: 3000.000, accuracy: 0.413\n",
            "training epoch: [47 ] loss: 7.317 correct: 1241.000, total: 3000.000, accuracy: 0.414\n",
            "training epoch: [48 ] loss: 7.290 correct: 1242.000, total: 3000.000, accuracy: 0.414\n",
            "training epoch: [49 ] loss: 7.263 correct: 1245.000, total: 3000.000, accuracy: 0.415\n",
            "training epoch: [50 ] loss: 7.237 correct: 1249.000, total: 3000.000, accuracy: 0.416\n",
            "training epoch: [51 ] loss: 7.213 correct: 1247.000, total: 3000.000, accuracy: 0.416\n",
            "training epoch: [52 ] loss: 7.188 correct: 1249.000, total: 3000.000, accuracy: 0.416\n",
            "training epoch: [53 ] loss: 7.165 correct: 1251.000, total: 3000.000, accuracy: 0.417\n",
            "training epoch: [54 ] loss: 7.142 correct: 1253.000, total: 3000.000, accuracy: 0.418\n",
            "training epoch: [55 ] loss: 7.119 correct: 1252.000, total: 3000.000, accuracy: 0.417\n",
            "training epoch: [56 ] loss: 7.097 correct: 1254.000, total: 3000.000, accuracy: 0.418\n",
            "training epoch: [57 ] loss: 7.074 correct: 1254.000, total: 3000.000, accuracy: 0.418\n",
            "training epoch: [58 ] loss: 7.053 correct: 1254.000, total: 3000.000, accuracy: 0.418\n",
            "training epoch: [59 ] loss: 7.031 correct: 1254.000, total: 3000.000, accuracy: 0.418\n",
            "training epoch: [60 ] loss: 7.011 correct: 1258.000, total: 3000.000, accuracy: 0.419\n",
            "training epoch: [61 ] loss: 6.991 correct: 1259.000, total: 3000.000, accuracy: 0.420\n",
            "training epoch: [62 ] loss: 6.971 correct: 1261.000, total: 3000.000, accuracy: 0.420\n",
            "training epoch: [63 ] loss: 6.952 correct: 1262.000, total: 3000.000, accuracy: 0.421\n",
            "training epoch: [64 ] loss: 6.933 correct: 1265.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [65 ] loss: 6.914 correct: 1266.000, total: 3000.000, accuracy: 0.422\n",
            "training epoch: [66 ] loss: 6.896 correct: 1268.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [67 ] loss: 6.879 correct: 1268.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [68 ] loss: 6.862 correct: 1271.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [69 ] loss: 6.845 correct: 1273.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [70 ] loss: 6.829 correct: 1274.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [71 ] loss: 6.814 correct: 1271.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [72 ] loss: 6.798 correct: 1272.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [73 ] loss: 6.783 correct: 1273.000, total: 3000.000, accuracy: 0.424\n",
            "training epoch: [74 ] loss: 6.769 correct: 1270.000, total: 3000.000, accuracy: 0.423\n",
            "training epoch: [75 ] loss: 6.756 correct: 1274.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [76 ] loss: 6.742 correct: 1277.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [77 ] loss: 6.730 correct: 1277.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [78 ] loss: 6.718 correct: 1277.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [79 ] loss: 6.706 correct: 1280.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [80 ] loss: 6.695 correct: 1281.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [81 ] loss: 6.685 correct: 1280.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [82 ] loss: 6.675 correct: 1283.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [83 ] loss: 6.665 correct: 1282.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [84 ] loss: 6.655 correct: 1281.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [85 ] loss: 6.646 correct: 1282.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [86 ] loss: 6.638 correct: 1282.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [87 ] loss: 6.629 correct: 1281.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [88 ] loss: 6.621 correct: 1280.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [89 ] loss: 6.612 correct: 1280.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [90 ] loss: 6.605 correct: 1283.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [91 ] loss: 6.598 correct: 1283.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [92 ] loss: 6.592 correct: 1279.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [93 ] loss: 6.586 correct: 1279.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [94 ] loss: 6.580 correct: 1277.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [95 ] loss: 6.575 correct: 1276.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [96 ] loss: 6.570 correct: 1276.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [97 ] loss: 6.565 correct: 1277.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [98 ] loss: 6.561 correct: 1277.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [99 ] loss: 6.556 correct: 1275.000, total: 3000.000, accuracy: 0.425\n",
            "training epoch: [100 ] loss: 6.552 correct: 1278.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [101 ] loss: 6.548 correct: 1279.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [102 ] loss: 6.544 correct: 1282.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [103 ] loss: 6.541 correct: 1284.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [104 ] loss: 6.538 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [105 ] loss: 6.534 correct: 1285.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [106 ] loss: 6.531 correct: 1284.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [107 ] loss: 6.528 correct: 1284.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [108 ] loss: 6.524 correct: 1285.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [109 ] loss: 6.521 correct: 1285.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [110 ] loss: 6.518 correct: 1285.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [111 ] loss: 6.516 correct: 1285.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [112 ] loss: 6.513 correct: 1285.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [113 ] loss: 6.511 correct: 1284.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [114 ] loss: 6.508 correct: 1284.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [115 ] loss: 6.506 correct: 1282.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [116 ] loss: 6.505 correct: 1281.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [117 ] loss: 6.503 correct: 1281.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [118 ] loss: 6.501 correct: 1281.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [119 ] loss: 6.499 correct: 1279.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [120 ] loss: 6.497 correct: 1279.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [121 ] loss: 6.495 correct: 1280.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [122 ] loss: 6.494 correct: 1279.000, total: 3000.000, accuracy: 0.426\n",
            "training epoch: [123 ] loss: 6.492 correct: 1280.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [124 ] loss: 6.490 correct: 1280.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [125 ] loss: 6.489 correct: 1280.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [126 ] loss: 6.487 correct: 1281.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [127 ] loss: 6.486 correct: 1281.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [128 ] loss: 6.484 correct: 1281.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [129 ] loss: 6.483 correct: 1281.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [130 ] loss: 6.482 correct: 1281.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [131 ] loss: 6.481 correct: 1281.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [132 ] loss: 6.481 correct: 1282.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [133 ] loss: 6.480 correct: 1282.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [134 ] loss: 6.479 correct: 1282.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [135 ] loss: 6.478 correct: 1283.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [136 ] loss: 6.478 correct: 1283.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [137 ] loss: 6.477 correct: 1282.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [138 ] loss: 6.476 correct: 1280.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [139 ] loss: 6.475 correct: 1280.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [140 ] loss: 6.475 correct: 1281.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [141 ] loss: 6.473 correct: 1281.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [142 ] loss: 6.473 correct: 1282.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [143 ] loss: 6.472 correct: 1282.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [144 ] loss: 6.471 correct: 1282.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [145 ] loss: 6.470 correct: 1282.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [146 ] loss: 6.470 correct: 1282.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [147 ] loss: 6.469 correct: 1282.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [148 ] loss: 6.468 correct: 1282.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [149 ] loss: 6.468 correct: 1283.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [150 ] loss: 6.467 correct: 1283.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [151 ] loss: 6.467 correct: 1283.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [152 ] loss: 6.466 correct: 1281.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [153 ] loss: 6.466 correct: 1282.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [154 ] loss: 6.465 correct: 1281.000, total: 3000.000, accuracy: 0.427\n",
            "training epoch: [155 ] loss: 6.465 correct: 1283.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [156 ] loss: 6.464 correct: 1284.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [157 ] loss: 6.464 correct: 1284.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [158 ] loss: 6.463 correct: 1284.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [159 ] loss: 6.463 correct: 1284.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [160 ] loss: 6.463 correct: 1285.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [161 ] loss: 6.462 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [162 ] loss: 6.462 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [163 ] loss: 6.462 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [164 ] loss: 6.462 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [165 ] loss: 6.461 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [166 ] loss: 6.461 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [167 ] loss: 6.461 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [168 ] loss: 6.460 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [169 ] loss: 6.460 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [170 ] loss: 6.460 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [171 ] loss: 6.460 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [172 ] loss: 6.460 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [173 ] loss: 6.460 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [174 ] loss: 6.460 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [175 ] loss: 6.459 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [176 ] loss: 6.459 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [177 ] loss: 6.459 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [178 ] loss: 6.459 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [179 ] loss: 6.460 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [180 ] loss: 6.460 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [181 ] loss: 6.460 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [182 ] loss: 6.460 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [183 ] loss: 6.460 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [184 ] loss: 6.460 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [185 ] loss: 6.460 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [186 ] loss: 6.460 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [187 ] loss: 6.461 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [188 ] loss: 6.461 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [189 ] loss: 6.461 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [190 ] loss: 6.461 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [191 ] loss: 6.461 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [192 ] loss: 6.461 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [193 ] loss: 6.461 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [194 ] loss: 6.462 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [195 ] loss: 6.462 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [196 ] loss: 6.462 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [197 ] loss: 6.462 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [198 ] loss: 6.462 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [199 ] loss: 6.462 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [200 ] loss: 6.462 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [201 ] loss: 6.462 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [202 ] loss: 6.462 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [203 ] loss: 6.462 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [204 ] loss: 6.462 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [205 ] loss: 6.462 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [206 ] loss: 6.462 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [207 ] loss: 6.462 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [208 ] loss: 6.462 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [209 ] loss: 6.462 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [210 ] loss: 6.462 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [211 ] loss: 6.463 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [212 ] loss: 6.463 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [213 ] loss: 6.463 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [214 ] loss: 6.463 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [215 ] loss: 6.463 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [216 ] loss: 6.463 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [217 ] loss: 6.463 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [218 ] loss: 6.463 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [219 ] loss: 6.464 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [220 ] loss: 6.464 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [221 ] loss: 6.464 correct: 1285.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [222 ] loss: 6.464 correct: 1285.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [223 ] loss: 6.464 correct: 1285.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [224 ] loss: 6.464 correct: 1285.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [225 ] loss: 6.465 correct: 1285.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [226 ] loss: 6.465 correct: 1285.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [227 ] loss: 6.465 correct: 1285.000, total: 3000.000, accuracy: 0.428\n",
            "training epoch: [228 ] loss: 6.465 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [229 ] loss: 6.465 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [230 ] loss: 6.465 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [231 ] loss: 6.465 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [232 ] loss: 6.465 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [233 ] loss: 6.465 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [234 ] loss: 6.465 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [235 ] loss: 6.466 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [236 ] loss: 6.466 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [237 ] loss: 6.466 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [238 ] loss: 6.466 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [239 ] loss: 6.466 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [240 ] loss: 6.466 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [241 ] loss: 6.467 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [242 ] loss: 6.467 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [243 ] loss: 6.467 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [244 ] loss: 6.467 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [245 ] loss: 6.467 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [246 ] loss: 6.467 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [247 ] loss: 6.468 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [248 ] loss: 6.468 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [249 ] loss: 6.468 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [250 ] loss: 6.468 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [251 ] loss: 6.468 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [252 ] loss: 6.468 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [253 ] loss: 6.468 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [254 ] loss: 6.468 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [255 ] loss: 6.469 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [256 ] loss: 6.469 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [257 ] loss: 6.469 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [258 ] loss: 6.469 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [259 ] loss: 6.469 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [260 ] loss: 6.469 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [261 ] loss: 6.469 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [262 ] loss: 6.469 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [263 ] loss: 6.469 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [264 ] loss: 6.470 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [265 ] loss: 6.470 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [266 ] loss: 6.470 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [267 ] loss: 6.470 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [268 ] loss: 6.470 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [269 ] loss: 6.471 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [270 ] loss: 6.471 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [271 ] loss: 6.471 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [272 ] loss: 6.471 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [273 ] loss: 6.471 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [274 ] loss: 6.472 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [275 ] loss: 6.472 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [276 ] loss: 6.472 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [277 ] loss: 6.472 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [278 ] loss: 6.472 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [279 ] loss: 6.473 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [280 ] loss: 6.473 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [281 ] loss: 6.473 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [282 ] loss: 6.473 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [283 ] loss: 6.473 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [284 ] loss: 6.473 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [285 ] loss: 6.474 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [286 ] loss: 6.474 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [287 ] loss: 6.474 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [288 ] loss: 6.474 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [289 ] loss: 6.474 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [290 ] loss: 6.474 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [291 ] loss: 6.474 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [292 ] loss: 6.474 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [293 ] loss: 6.475 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [294 ] loss: 6.475 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [295 ] loss: 6.475 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [296 ] loss: 6.475 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [297 ] loss: 6.475 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [298 ] loss: 6.475 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [299 ] loss: 6.475 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [300 ] loss: 6.475 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [301 ] loss: 6.475 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [302 ] loss: 6.475 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [303 ] loss: 6.475 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [304 ] loss: 6.475 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [305 ] loss: 6.476 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [306 ] loss: 6.476 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [307 ] loss: 6.476 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [308 ] loss: 6.476 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [309 ] loss: 6.476 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [310 ] loss: 6.476 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [311 ] loss: 6.476 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [312 ] loss: 6.476 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [313 ] loss: 6.476 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [314 ] loss: 6.476 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [315 ] loss: 6.476 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [316 ] loss: 6.476 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [317 ] loss: 6.476 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [318 ] loss: 6.476 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [319 ] loss: 6.476 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [320 ] loss: 6.476 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [321 ] loss: 6.476 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [322 ] loss: 6.476 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [323 ] loss: 6.476 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [324 ] loss: 6.476 correct: 1286.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [325 ] loss: 6.476 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [326 ] loss: 6.476 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [327 ] loss: 6.476 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [328 ] loss: 6.476 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [329 ] loss: 6.476 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [330 ] loss: 6.476 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [331 ] loss: 6.476 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [332 ] loss: 6.476 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [333 ] loss: 6.476 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [334 ] loss: 6.476 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [335 ] loss: 6.476 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [336 ] loss: 6.476 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [337 ] loss: 6.476 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [338 ] loss: 6.476 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [339 ] loss: 6.476 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [340 ] loss: 6.476 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [341 ] loss: 6.476 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [342 ] loss: 6.476 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [343 ] loss: 6.476 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [344 ] loss: 6.476 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [345 ] loss: 6.476 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [346 ] loss: 6.476 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [347 ] loss: 6.476 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [348 ] loss: 6.476 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [349 ] loss: 6.476 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [350 ] loss: 6.476 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [351 ] loss: 6.476 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [352 ] loss: 6.476 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [353 ] loss: 6.476 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [354 ] loss: 6.476 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [355 ] loss: 6.476 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [356 ] loss: 6.476 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [357 ] loss: 6.476 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [358 ] loss: 6.476 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [359 ] loss: 6.476 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [360 ] loss: 6.476 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [361 ] loss: 6.476 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [362 ] loss: 6.476 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [363 ] loss: 6.476 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [364 ] loss: 6.476 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [365 ] loss: 6.476 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [366 ] loss: 6.476 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [367 ] loss: 6.476 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [368 ] loss: 6.476 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [369 ] loss: 6.476 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [370 ] loss: 6.476 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [371 ] loss: 6.476 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [372 ] loss: 6.476 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [373 ] loss: 6.476 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [374 ] loss: 6.476 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [375 ] loss: 6.476 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [376 ] loss: 6.476 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [377 ] loss: 6.476 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [378 ] loss: 6.476 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [379 ] loss: 6.476 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [380 ] loss: 6.476 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [381 ] loss: 6.476 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [382 ] loss: 6.476 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [383 ] loss: 6.476 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [384 ] loss: 6.476 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [385 ] loss: 6.476 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [386 ] loss: 6.475 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [387 ] loss: 6.475 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [388 ] loss: 6.475 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [389 ] loss: 6.475 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [390 ] loss: 6.475 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [391 ] loss: 6.475 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [392 ] loss: 6.475 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [393 ] loss: 6.475 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [394 ] loss: 6.475 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [395 ] loss: 6.475 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [396 ] loss: 6.475 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [397 ] loss: 6.476 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [398 ] loss: 6.476 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [399 ] loss: 6.476 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [400 ] loss: 6.476 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [401 ] loss: 6.476 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [402 ] loss: 6.476 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [403 ] loss: 6.476 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [404 ] loss: 6.476 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [405 ] loss: 6.476 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [406 ] loss: 6.476 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [407 ] loss: 6.476 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [408 ] loss: 6.476 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [409 ] loss: 6.476 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [410 ] loss: 6.476 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [411 ] loss: 6.476 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [412 ] loss: 6.476 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [413 ] loss: 6.476 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [414 ] loss: 6.476 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [415 ] loss: 6.476 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [416 ] loss: 6.476 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [417 ] loss: 6.476 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [418 ] loss: 6.476 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [419 ] loss: 6.476 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [420 ] loss: 6.476 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [421 ] loss: 6.476 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [422 ] loss: 6.476 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [423 ] loss: 6.476 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [424 ] loss: 6.476 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [425 ] loss: 6.476 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [426 ] loss: 6.476 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [427 ] loss: 6.476 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [428 ] loss: 6.476 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [429 ] loss: 6.476 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [430 ] loss: 6.476 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [431 ] loss: 6.476 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [432 ] loss: 6.476 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [433 ] loss: 6.476 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [434 ] loss: 6.476 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [435 ] loss: 6.476 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [436 ] loss: 6.476 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [437 ] loss: 6.476 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [438 ] loss: 6.476 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [439 ] loss: 6.477 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [440 ] loss: 6.477 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [441 ] loss: 6.477 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [442 ] loss: 6.477 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [443 ] loss: 6.477 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [444 ] loss: 6.477 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [445 ] loss: 6.477 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [446 ] loss: 6.477 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [447 ] loss: 6.477 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [448 ] loss: 6.477 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [449 ] loss: 6.477 correct: 1288.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [450 ] loss: 6.477 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [451 ] loss: 6.477 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [452 ] loss: 6.477 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [453 ] loss: 6.477 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [454 ] loss: 6.477 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [455 ] loss: 6.477 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [456 ] loss: 6.477 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [457 ] loss: 6.477 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [458 ] loss: 6.477 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [459 ] loss: 6.477 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [460 ] loss: 6.477 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [461 ] loss: 6.477 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [462 ] loss: 6.477 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [463 ] loss: 6.477 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [464 ] loss: 6.477 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [465 ] loss: 6.477 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [466 ] loss: 6.477 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [467 ] loss: 6.477 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [468 ] loss: 6.477 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [469 ] loss: 6.477 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [470 ] loss: 6.477 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [471 ] loss: 6.477 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [472 ] loss: 6.477 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [473 ] loss: 6.477 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [474 ] loss: 6.477 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [475 ] loss: 6.477 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [476 ] loss: 6.477 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [477 ] loss: 6.477 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [478 ] loss: 6.477 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [479 ] loss: 6.477 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [480 ] loss: 6.477 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [481 ] loss: 6.477 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [482 ] loss: 6.477 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [483 ] loss: 6.477 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [484 ] loss: 6.477 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [485 ] loss: 6.477 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [486 ] loss: 6.477 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [487 ] loss: 6.477 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [488 ] loss: 6.477 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [489 ] loss: 6.477 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [490 ] loss: 6.477 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [491 ] loss: 6.477 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [492 ] loss: 6.477 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [493 ] loss: 6.477 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [494 ] loss: 6.477 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [495 ] loss: 6.477 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [496 ] loss: 6.477 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [497 ] loss: 6.477 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [498 ] loss: 6.477 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [499 ] loss: 6.477 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [500 ] loss: 6.477 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [501 ] loss: 6.477 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [502 ] loss: 6.477 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [503 ] loss: 6.477 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [504 ] loss: 6.477 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [505 ] loss: 6.477 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [506 ] loss: 6.477 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [507 ] loss: 6.477 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [508 ] loss: 6.477 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [509 ] loss: 6.477 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [510 ] loss: 6.477 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [511 ] loss: 6.477 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [512 ] loss: 6.477 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [513 ] loss: 6.477 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [514 ] loss: 6.477 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [515 ] loss: 6.477 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [516 ] loss: 6.477 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [517 ] loss: 6.477 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [518 ] loss: 6.477 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [519 ] loss: 6.477 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [520 ] loss: 6.477 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [521 ] loss: 6.477 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [522 ] loss: 6.477 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [523 ] loss: 6.477 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [524 ] loss: 6.477 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [525 ] loss: 6.477 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [526 ] loss: 6.477 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [527 ] loss: 6.477 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [528 ] loss: 6.477 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [529 ] loss: 6.477 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [530 ] loss: 6.477 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [531 ] loss: 6.477 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [532 ] loss: 6.477 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [533 ] loss: 6.477 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [534 ] loss: 6.477 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [535 ] loss: 6.477 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [536 ] loss: 6.477 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [537 ] loss: 6.477 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [538 ] loss: 6.477 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [539 ] loss: 6.477 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [540 ] loss: 6.477 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [541 ] loss: 6.477 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [542 ] loss: 6.477 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [543 ] loss: 6.477 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [544 ] loss: 6.477 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [545 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [546 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [547 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [548 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [549 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [550 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [551 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [552 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [553 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [554 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [555 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [556 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [557 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [558 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [559 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [560 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [561 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [562 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [563 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [564 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [565 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [566 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [567 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [568 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [569 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [570 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [571 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [572 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [573 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [574 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [575 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [576 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [577 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [578 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [579 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [580 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [581 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [582 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [583 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [584 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [585 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [586 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [587 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [588 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [589 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [590 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [591 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [592 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [593 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [594 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [595 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [596 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [597 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [598 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [599 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [600 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [601 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [602 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [603 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [604 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [605 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [606 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [607 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [608 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [609 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [610 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [611 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [612 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [613 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [614 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [615 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [616 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [617 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [618 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [619 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [620 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [621 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [622 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [623 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [624 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [625 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [626 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [627 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [628 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [629 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [630 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [631 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [632 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [633 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [634 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [635 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [636 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [637 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [638 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [639 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [640 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [641 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [642 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [643 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [644 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [645 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [646 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [647 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [648 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [649 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [650 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [651 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [652 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [653 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [654 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [655 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [656 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [657 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [658 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [659 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [660 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [661 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [662 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [663 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [664 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [665 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [666 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [667 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [668 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [669 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [670 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [671 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [672 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [673 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [674 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [675 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [676 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [677 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [678 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [679 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [680 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [681 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [682 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [683 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [684 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [685 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [686 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [687 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [688 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [689 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [690 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [691 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [692 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [693 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [694 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [695 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [696 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [697 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [698 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [699 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [700 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [701 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [702 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [703 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [704 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [705 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [706 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [707 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [708 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [709 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [710 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [711 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [712 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [713 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [714 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [715 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [716 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [717 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [718 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [719 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [720 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [721 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [722 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [723 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [724 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [725 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [726 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [727 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [728 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [729 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [730 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [731 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [732 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [733 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [734 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [735 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [736 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [737 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [738 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [739 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [740 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [741 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [742 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [743 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [744 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [745 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [746 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [747 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [748 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [749 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [750 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [751 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [752 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [753 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [754 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [755 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [756 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [757 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [758 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [759 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [760 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [761 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [762 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [763 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [764 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [765 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [766 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [767 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [768 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [769 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [770 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [771 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [772 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [773 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [774 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [775 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [776 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [777 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [778 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [779 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [780 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [781 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [782 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [783 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [784 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [785 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [786 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [787 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [788 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [789 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [790 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [791 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [792 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [793 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [794 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [795 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [796 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [797 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [798 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [799 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [800 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [801 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [802 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [803 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [804 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [805 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [806 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [807 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [808 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [809 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [810 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [811 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [812 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [813 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [814 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [815 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [816 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [817 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [818 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [819 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [820 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [821 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [822 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [823 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [824 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [825 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [826 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [827 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [828 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [829 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [830 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [831 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [832 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [833 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [834 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [835 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [836 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [837 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [838 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [839 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [840 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [841 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [842 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [843 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [844 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [845 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [846 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [847 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [848 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [849 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [850 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [851 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [852 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [853 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [854 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [855 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [856 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [857 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [858 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [859 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [860 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [861 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [862 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [863 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [864 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [865 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [866 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [867 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [868 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [869 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [870 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [871 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [872 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [873 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [874 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [875 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [876 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [877 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [878 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [879 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [880 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [881 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [882 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [883 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [884 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [885 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [886 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [887 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [888 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [889 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [890 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [891 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [892 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [893 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [894 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [895 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [896 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [897 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [898 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [899 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [900 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [901 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [902 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [903 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [904 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [905 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [906 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [907 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [908 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [909 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [910 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [911 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [912 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [913 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [914 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [915 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [916 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [917 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [918 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [919 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [920 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [921 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [922 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [923 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [924 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [925 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [926 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [927 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [928 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [929 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [930 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [931 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [932 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [933 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [934 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [935 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [936 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [937 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [938 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [939 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [940 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [941 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [942 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [943 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [944 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [945 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [946 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [947 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [948 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [949 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [950 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [951 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [952 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [953 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [954 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [955 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [956 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [957 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [958 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [959 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [960 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [961 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [962 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [963 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [964 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [965 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [966 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [967 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [968 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [969 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [970 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [971 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [972 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [973 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [974 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [975 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [976 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [977 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [978 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [979 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [980 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [981 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [982 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [983 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [984 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [985 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [986 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [987 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [988 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [989 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [990 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [991 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [992 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [993 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [994 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [995 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [996 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [997 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [998 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [999 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1000 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1001 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1002 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1003 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1004 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1005 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1006 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1007 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1008 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1009 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1010 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1011 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1012 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1013 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1014 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1015 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1016 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1017 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1018 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1019 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1020 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1021 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1022 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1023 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1024 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1025 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1026 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1027 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1028 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1029 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1030 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1031 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1032 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1033 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1034 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1035 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1036 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1037 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1038 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1039 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1040 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1041 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1042 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1043 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1044 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1045 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1046 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1047 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1048 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1049 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1050 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1051 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1052 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1053 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1054 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1055 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1056 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1057 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1058 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1059 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1060 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1061 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1062 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1063 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1064 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1065 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1066 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1067 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1068 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1069 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1070 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1071 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1072 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1073 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1074 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1075 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1076 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1077 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1078 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1079 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1080 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1081 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1082 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1083 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1084 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1085 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1086 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1087 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1088 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1089 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1090 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1091 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1092 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1093 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1094 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1095 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1096 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1097 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1098 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1099 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1100 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1101 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1102 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1103 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1104 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1105 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1106 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1107 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1108 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1109 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1110 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1111 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1112 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1113 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1114 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1115 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1116 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1117 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1118 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1119 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1120 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1121 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1122 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1123 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1124 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1125 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1126 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1127 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1128 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1129 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1130 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1131 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1132 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1133 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1134 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1135 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1136 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1137 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1138 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1139 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1140 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1141 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1142 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1143 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1144 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1145 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1146 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1147 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1148 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1149 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1150 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1151 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1152 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1153 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1154 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1155 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1156 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1157 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1158 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1159 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1160 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1161 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1162 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1163 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1164 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1165 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1166 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1167 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1168 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1169 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1170 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1171 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1172 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1173 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1174 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1175 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1176 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1177 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1178 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1179 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1180 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1181 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1182 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1183 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1184 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1185 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1186 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1187 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1188 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1189 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1190 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1191 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1192 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1193 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1194 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1195 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1196 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1197 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1198 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1199 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1200 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1201 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1202 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1203 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1204 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1205 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1206 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1207 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1208 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1209 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1210 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1211 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1212 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1213 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1214 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1215 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1216 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1217 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1218 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1219 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1220 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1221 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1222 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1223 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1224 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1225 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1226 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1227 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1228 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1229 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1230 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1231 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1232 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1233 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1234 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1235 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1236 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1237 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1238 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1239 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1240 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1241 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1242 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1243 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1244 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1245 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1246 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1247 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1248 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1249 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1250 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1251 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1252 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1253 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1254 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1255 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1256 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1257 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1258 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1259 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1260 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1261 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1262 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1263 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1264 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1265 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1266 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1267 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1268 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1269 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1270 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1271 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1272 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1273 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1274 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1275 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1276 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1277 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1278 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1279 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1280 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1281 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1282 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1283 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1284 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1285 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1286 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1287 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1288 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1289 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1290 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1291 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1292 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1293 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1294 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1295 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1296 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1297 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1298 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1299 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1300 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1301 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1302 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1303 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1304 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1305 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1306 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1307 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1308 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1309 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1310 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1311 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1312 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1313 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1314 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1315 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1316 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1317 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1318 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1319 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1320 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1321 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1322 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1323 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1324 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1325 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1326 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1327 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1328 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1329 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1330 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1331 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1332 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1333 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1334 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1335 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1336 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1337 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1338 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1339 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1340 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1341 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1342 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1343 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1344 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1345 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1346 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1347 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1348 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1349 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1350 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1351 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1352 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1353 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1354 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1355 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1356 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1357 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1358 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1359 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1360 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1361 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1362 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1363 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1364 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1365 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1366 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1367 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1368 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1369 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1370 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1371 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1372 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1373 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1374 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1375 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1376 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1377 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1378 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1379 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1380 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1381 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1382 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1383 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1384 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1385 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1386 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1387 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1388 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1389 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1390 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1391 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1392 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1393 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1394 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1395 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1396 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1397 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1398 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1399 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1400 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1401 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1402 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1403 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1404 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1405 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1406 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1407 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1408 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1409 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1410 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1411 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1412 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1413 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1414 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1415 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1416 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1417 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1418 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1419 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1420 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1421 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1422 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1423 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1424 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1425 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1426 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1427 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1428 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1429 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1430 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1431 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1432 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1433 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1434 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1435 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1436 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1437 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1438 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1439 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1440 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1441 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1442 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1443 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1444 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1445 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1446 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1447 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1448 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1449 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1450 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1451 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1452 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1453 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1454 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1455 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1456 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1457 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1458 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1459 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1460 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1461 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1462 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1463 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1464 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1465 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1466 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1467 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1468 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1469 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1470 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1471 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1472 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1473 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1474 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1475 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1476 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1477 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1478 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1479 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1480 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1481 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1482 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1483 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1484 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1485 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1486 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1487 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1488 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1489 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1490 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1491 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1492 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1493 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1494 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1495 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1496 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1497 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1498 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1499 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1500 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1501 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1502 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1503 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1504 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1505 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1506 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1507 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1508 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1509 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1510 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1511 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1512 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1513 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1514 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1515 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1516 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1517 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1518 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1519 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1520 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1521 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1522 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1523 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1524 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1525 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1526 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1527 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1528 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1529 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1530 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1531 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1532 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1533 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1534 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1535 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1536 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1537 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1538 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1539 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1540 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1541 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1542 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1543 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1544 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1545 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1546 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1547 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1548 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1549 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1550 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1551 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1552 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1553 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1554 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1555 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1556 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1557 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1558 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1559 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1560 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1561 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1562 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1563 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1564 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1565 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1566 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1567 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1568 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1569 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1570 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1571 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1572 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1573 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1574 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1575 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1576 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1577 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1578 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1579 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1580 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1581 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1582 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1583 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1584 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1585 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1586 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1587 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1588 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1589 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1590 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1591 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1592 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1593 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1594 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1595 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1596 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1597 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1598 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1599 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1600 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1601 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1602 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1603 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1604 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1605 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1606 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1607 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1608 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1609 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1610 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1611 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1612 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1613 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1614 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1615 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1616 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1617 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1618 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1619 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1620 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1621 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1622 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1623 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1624 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1625 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1626 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1627 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1628 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1629 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1630 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1631 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1632 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1633 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1634 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1635 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1636 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1637 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1638 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1639 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1640 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1641 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1642 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1643 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1644 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1645 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1646 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1647 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1648 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1649 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1650 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1651 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1652 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1653 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1654 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1655 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1656 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1657 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1658 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1659 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1660 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1661 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1662 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1663 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1664 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1665 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1666 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1667 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1668 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1669 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1670 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1671 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1672 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1673 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1674 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1675 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1676 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1677 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1678 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1679 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1680 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1681 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1682 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1683 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1684 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1685 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1686 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1687 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1688 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1689 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1690 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1691 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1692 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1693 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1694 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1695 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1696 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1697 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1698 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1699 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1700 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1701 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1702 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1703 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1704 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1705 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1706 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1707 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1708 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1709 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1710 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1711 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1712 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1713 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1714 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1715 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1716 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1717 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1718 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1719 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1720 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1721 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1722 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1723 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1724 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1725 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1726 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1727 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1728 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1729 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1730 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1731 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1732 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1733 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1734 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1735 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1736 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1737 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1738 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1739 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1740 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1741 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1742 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1743 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1744 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1745 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1746 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1747 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1748 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1749 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1750 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1751 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1752 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1753 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1754 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1755 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1756 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1757 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1758 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1759 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1760 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1761 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1762 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1763 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1764 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1765 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1766 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1767 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1768 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1769 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1770 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1771 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1772 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1773 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1774 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1775 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1776 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1777 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1778 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1779 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1780 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1781 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1782 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1783 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1784 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1785 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1786 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1787 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1788 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1789 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1790 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1791 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1792 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1793 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1794 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1795 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1796 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1797 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1798 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1799 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1800 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1801 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1802 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1803 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1804 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1805 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1806 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1807 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1808 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1809 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1810 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1811 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1812 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1813 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1814 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1815 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1816 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1817 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1818 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1819 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1820 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1821 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1822 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1823 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1824 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1825 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1826 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1827 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1828 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1829 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1830 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1831 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1832 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1833 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1834 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1835 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1836 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1837 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1838 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1839 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1840 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1841 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1842 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1843 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1844 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1845 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1846 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1847 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1848 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1849 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1850 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1851 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1852 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1853 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1854 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1855 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1856 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1857 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1858 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1859 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1860 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1861 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1862 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1863 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1864 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1865 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1866 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1867 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1868 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1869 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1870 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1871 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1872 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1873 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1874 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1875 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1876 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1877 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1878 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1879 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1880 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1881 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1882 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1883 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1884 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1885 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1886 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1887 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1888 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1889 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1890 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1891 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1892 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1893 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1894 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1895 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1896 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1897 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1898 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1899 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1900 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1901 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1902 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1903 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1904 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1905 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1906 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1907 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1908 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1909 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1910 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1911 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1912 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1913 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1914 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1915 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1916 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1917 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1918 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1919 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1920 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1921 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1922 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1923 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1924 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1925 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1926 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1927 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1928 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1929 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1930 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1931 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1932 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1933 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1934 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1935 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1936 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1937 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1938 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1939 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1940 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1941 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1942 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1943 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1944 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1945 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1946 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1947 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1948 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1949 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1950 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1951 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1952 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1953 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1954 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1955 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1956 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1957 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1958 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1959 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1960 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1961 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1962 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1963 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1964 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1965 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1966 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1967 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1968 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1969 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1970 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1971 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1972 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1973 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1974 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1975 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1976 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1977 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1978 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1979 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1980 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1981 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1982 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1983 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1984 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1985 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1986 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1987 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1988 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1989 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1990 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1991 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1992 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1993 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1994 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1995 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1996 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1997 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1998 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [1999 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2000 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2001 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2002 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2003 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2004 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2005 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2006 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2007 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2008 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2009 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2010 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2011 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2012 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2013 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2014 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2015 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2016 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2017 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2018 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2019 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2020 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2021 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2022 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2023 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2024 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2025 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2026 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2027 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2028 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2029 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2030 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2031 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2032 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2033 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2034 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2035 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2036 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2037 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2038 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2039 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2040 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2041 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2042 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2043 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2044 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2045 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2046 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2047 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2048 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2049 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2050 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2051 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2052 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2053 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2054 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2055 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2056 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2057 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2058 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2059 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2060 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2061 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2062 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2063 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2064 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2065 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2066 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2067 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2068 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2069 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2070 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2071 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2072 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2073 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2074 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2075 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2076 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2077 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2078 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2079 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2080 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2081 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2082 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2083 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2084 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2085 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2086 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2087 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2088 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2089 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2090 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2091 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2092 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2093 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2094 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2095 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2096 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2097 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2098 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2099 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2100 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2101 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2102 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2103 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2104 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2105 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2106 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2107 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2108 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2109 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2110 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2111 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2112 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2113 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2114 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2115 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2116 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2117 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2118 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2119 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2120 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2121 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2122 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2123 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2124 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2125 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2126 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2127 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2128 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2129 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2130 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2131 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2132 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2133 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2134 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2135 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2136 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2137 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2138 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2139 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2140 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2141 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2142 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2143 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2144 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2145 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2146 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2147 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2148 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2149 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2150 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2151 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2152 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2153 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2154 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2155 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2156 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2157 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2158 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2159 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2160 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2161 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2162 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2163 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2164 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2165 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2166 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2167 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2168 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2169 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2170 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2171 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2172 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2173 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2174 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2175 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2176 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2177 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2178 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2179 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2180 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2181 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2182 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2183 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2184 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2185 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2186 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2187 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2188 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2189 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2190 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2191 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2192 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2193 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2194 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2195 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2196 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2197 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2198 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2199 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2200 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2201 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2202 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2203 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2204 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2205 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2206 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2207 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2208 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2209 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2210 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2211 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2212 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2213 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2214 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2215 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2216 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2217 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2218 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2219 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2220 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2221 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2222 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2223 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2224 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2225 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2226 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2227 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2228 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2229 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2230 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2231 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2232 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2233 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2234 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2235 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2236 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2237 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2238 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2239 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2240 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2241 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2242 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2243 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2244 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2245 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2246 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2247 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2248 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2249 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2250 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2251 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2252 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2253 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2254 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2255 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2256 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2257 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2258 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2259 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2260 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2261 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2262 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2263 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2264 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2265 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2266 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2267 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2268 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2269 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2270 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2271 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2272 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2273 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2274 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2275 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2276 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2277 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2278 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2279 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2280 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2281 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2282 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2283 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2284 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2285 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2286 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2287 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2288 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2289 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2290 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2291 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2292 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2293 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2294 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2295 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2296 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2297 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2298 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2299 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2300 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2301 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2302 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2303 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2304 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2305 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2306 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2307 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2308 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2309 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2310 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2311 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2312 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2313 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2314 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2315 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2316 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2317 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2318 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2319 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2320 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2321 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2322 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2323 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2324 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2325 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2326 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2327 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2328 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2329 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2330 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2331 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2332 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2333 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2334 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2335 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2336 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2337 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2338 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2339 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2340 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2341 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2342 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2343 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2344 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2345 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2346 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2347 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2348 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2349 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2350 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2351 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2352 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2353 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2354 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2355 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2356 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2357 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2358 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2359 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2360 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2361 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2362 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2363 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2364 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2365 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2366 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2367 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2368 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2369 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2370 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2371 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2372 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2373 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2374 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2375 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2376 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2377 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2378 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2379 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2380 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2381 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2382 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2383 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2384 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2385 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2386 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2387 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2388 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2389 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2390 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2391 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2392 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2393 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2394 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2395 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2396 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2397 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2398 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2399 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2400 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2401 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2402 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2403 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2404 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2405 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2406 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2407 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2408 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2409 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2410 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2411 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2412 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2413 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2414 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2415 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2416 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2417 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2418 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2419 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2420 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2421 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2422 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2423 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2424 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2425 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2426 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2427 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2428 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2429 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2430 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2431 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2432 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2433 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2434 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2435 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2436 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2437 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2438 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2439 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2440 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2441 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2442 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2443 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2444 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2445 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2446 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2447 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2448 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2449 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2450 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2451 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2452 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2453 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2454 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2455 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2456 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2457 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2458 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2459 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2460 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2461 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2462 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2463 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2464 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2465 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2466 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2467 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2468 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2469 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2470 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2471 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2472 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2473 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2474 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2475 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2476 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2477 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2478 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2479 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2480 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2481 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2482 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2483 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2484 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2485 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2486 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2487 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2488 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2489 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2490 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2491 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2492 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2493 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2494 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2495 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2496 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2497 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2498 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2499 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "training epoch: [2500 ] loss: 6.478 correct: 1287.000, total: 3000.000, accuracy: 0.429\n",
            "Finished Training run \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AciJnAh5nfug"
      },
      "source": [
        "columns = [\"epochs\", \"argmax > 0.5\" ,\"argmax < 0.5\", \"focus_true_pred_true\", \"focus_false_pred_true\", \"focus_true_pred_false\", \"focus_false_pred_false\" ]\n",
        "df_train = pd.DataFrame()\n",
        "df_test = pd.DataFrame()\n",
        "df_train[columns[0]] = np.arange(0,epoch+2)\n",
        "df_train[columns[1]] = analysis_data_tr[:,-2]/30\n",
        "df_train[columns[2]] = analysis_data_tr[:,-1]/30\n",
        "df_train[columns[3]] = analysis_data_tr[:,0]/30\n",
        "df_train[columns[4]] = analysis_data_tr[:,1]/30\n",
        "df_train[columns[5]] = analysis_data_tr[:,2]/30\n",
        "df_train[columns[6]] = analysis_data_tr[:,3]/30"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NoQpS_6scRsC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        },
        "outputId": "92d187ea-4278-4018-883f-31aaa40e96c9"
      },
      "source": [
        "df_train"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>epochs</th>\n",
              "      <th>argmax &gt; 0.5</th>\n",
              "      <th>argmax &lt; 0.5</th>\n",
              "      <th>focus_true_pred_true</th>\n",
              "      <th>focus_false_pred_true</th>\n",
              "      <th>focus_true_pred_false</th>\n",
              "      <th>focus_false_pred_false</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>9.800000</td>\n",
              "      <td>90.200000</td>\n",
              "      <td>5.233333</td>\n",
              "      <td>32.533333</td>\n",
              "      <td>6.533333</td>\n",
              "      <td>55.700000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>10.200000</td>\n",
              "      <td>89.800000</td>\n",
              "      <td>5.066667</td>\n",
              "      <td>32.666667</td>\n",
              "      <td>6.400000</td>\n",
              "      <td>55.866667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>10.700000</td>\n",
              "      <td>89.300000</td>\n",
              "      <td>4.966667</td>\n",
              "      <td>32.466667</td>\n",
              "      <td>6.400000</td>\n",
              "      <td>56.166667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>11.000000</td>\n",
              "      <td>89.000000</td>\n",
              "      <td>5.133333</td>\n",
              "      <td>32.100000</td>\n",
              "      <td>6.333333</td>\n",
              "      <td>56.433333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>10.900000</td>\n",
              "      <td>89.100000</td>\n",
              "      <td>5.500000</td>\n",
              "      <td>31.933333</td>\n",
              "      <td>6.233333</td>\n",
              "      <td>56.333333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2496</th>\n",
              "      <td>2496</td>\n",
              "      <td>20.066667</td>\n",
              "      <td>79.933333</td>\n",
              "      <td>11.400000</td>\n",
              "      <td>31.500000</td>\n",
              "      <td>8.366667</td>\n",
              "      <td>48.733333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2497</th>\n",
              "      <td>2497</td>\n",
              "      <td>20.066667</td>\n",
              "      <td>79.933333</td>\n",
              "      <td>11.400000</td>\n",
              "      <td>31.500000</td>\n",
              "      <td>8.366667</td>\n",
              "      <td>48.733333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2498</th>\n",
              "      <td>2498</td>\n",
              "      <td>20.066667</td>\n",
              "      <td>79.933333</td>\n",
              "      <td>11.400000</td>\n",
              "      <td>31.500000</td>\n",
              "      <td>8.366667</td>\n",
              "      <td>48.733333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2499</th>\n",
              "      <td>2499</td>\n",
              "      <td>20.066667</td>\n",
              "      <td>79.933333</td>\n",
              "      <td>11.400000</td>\n",
              "      <td>31.500000</td>\n",
              "      <td>8.366667</td>\n",
              "      <td>48.733333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2500</th>\n",
              "      <td>2500</td>\n",
              "      <td>20.066667</td>\n",
              "      <td>79.933333</td>\n",
              "      <td>11.400000</td>\n",
              "      <td>31.500000</td>\n",
              "      <td>8.366667</td>\n",
              "      <td>48.733333</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2501 rows  7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      epochs  argmax > 0.5  ...  focus_true_pred_false  focus_false_pred_false\n",
              "0          0      9.800000  ...               6.533333               55.700000\n",
              "1          1     10.200000  ...               6.400000               55.866667\n",
              "2          2     10.700000  ...               6.400000               56.166667\n",
              "3          3     11.000000  ...               6.333333               56.433333\n",
              "4          4     10.900000  ...               6.233333               56.333333\n",
              "...      ...           ...  ...                    ...                     ...\n",
              "2496    2496     20.066667  ...               8.366667               48.733333\n",
              "2497    2497     20.066667  ...               8.366667               48.733333\n",
              "2498    2498     20.066667  ...               8.366667               48.733333\n",
              "2499    2499     20.066667  ...               8.366667               48.733333\n",
              "2500    2500     20.066667  ...               8.366667               48.733333\n",
              "\n",
              "[2501 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EY_j8B274vuH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "outputId": "22349384-800f-4162-85c6-0a0c750ee364"
      },
      "source": [
        "%cd /content/\n",
        "plot_analysis(df_train,columns,[0,500,1000,1500,2000,2500])"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCnS6r2_3WdU"
      },
      "source": [
        "aph = []\n",
        "for i in bg:\n",
        "  aph.append(F.softmax(i,dim=1).detach().numpy())\n",
        "  \n",
        "aph = np.concatenate(aph,axis=0)\n",
        "# torch.save({\n",
        "#             'epoch': 500,\n",
        "#             'model_state_dict': what_net.state_dict(),\n",
        "#             #'optimizer_state_dict': optimizer_what.state_dict(),\n",
        "#             \"optimizer_alpha\":optim1,\n",
        "#             \"FTPT_analysis\":analysis_data_tr,\n",
        "#             \"alpha\":aph\n",
        "\n",
        "#             }, \"type4_what_net_500.pt\")"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVzrDOGS4UxU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "645537e2-e0be-466e-fb6a-dbd3e7c7fbb9"
      },
      "source": [
        "aph[0]"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.01161857, 0.0432175 , 0.17611605, 0.06253029, 0.3939581 ,\n",
              "       0.2069044 , 0.05834861, 0.01122377, 0.03608265], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Ut6ZTAXbvqx"
      },
      "source": [
        "avrg = []\n",
        "avrg_lbls = []\n",
        "with torch.no_grad():\n",
        "  for i, data1 in  enumerate(train_loader):\n",
        "          inputs , labels , fore_idx = data1\n",
        "          inputs = inputs.double()\n",
        "          inputs = inputs.to(\"cuda\")\n",
        "          beta  = bg[i]\n",
        "          beta = beta.to(\"cuda\")\n",
        "          avg,alpha = attn_avg(inputs,beta)\n",
        "          \n",
        "          avrg.append(avg.detach().cpu().numpy())\n",
        "          avrg_lbls.append(labels.numpy())\n",
        "avrg= np.concatenate(avrg,axis=0)\n",
        "avrg_lbls = np.concatenate(avrg_lbls,axis=0)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KQFYlmTLG0N",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "52a38467-d6ee-4337-c550-80d23c85d372"
      },
      "source": [
        "%cd /content/drive/MyDrive/Neural_Tangent_Kernel/\n",
        "data = np.load(\"type_4_data.npy\",allow_pickle=True)\n",
        "%cd /content/\n",
        "plot_decision_boundary(what_net,[1,8,2,9],data,bg,avrg,avrg_lbls)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Neural_Tangent_Kernel\n",
            "/content\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbcAAAFlCAYAAABoYabPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdWYxcV37n+e/dY98yconcFyaTpEiJKpVckqpcS1eVy+5yld0ed7sa056G0YYHGGAG8NvMyxgY9DT8MAP0vDRgz2B6ahrjtg2X3V7KrvKmkqpE7SVKXJPJTEaukZkRkbHeiLvfeQgGmaSSIimR4nY+gCBGxI2bJyNJ/vg/95z/lcIwRBAEQRAeJ/KDHoAgCIIg3Gsi3ARBEITHjgg3QRAE4bEjwk0QBEF47IhwEwRBEB47ItwEQRCEx456P06aTmbCofzI/Ti1IAgPgZLbIZtoko/lAKg7DYIgQGX0AY/s0bVy4XwlDMPBBz2Ox8V9Cbeh/Aj/x//yf92PUwuC8BA4P/e7/NzccSbjxwH4w8t/Rjb4nQc8qkfbd557ZvVBj+FxIqYlBUG4K//OeRXgWrAJwsNIhJsgCHfll4+e4jef/s61x394+c84Zy48wBEJwoeJcBME4Y6dn/tdJPn6Xxtr5lkAvhD9zq3eIggPhAg3QRDuyPm53wXg3xz/F9eeO1Va4lTlGw9qSIJwSyLcBEG4Y/unI/u+mXvhAYxEED6aCDdBEG7r/NzvkogZNzz3l+t//YBGIwi3J8JNEISP9B/O9a6rfefQP7vhedO2uVT5jQcxJEG4LRFugiDc0gcXS3z523/FscKNTRn6VdvncpMPYliCcFsi3ARBuCX1m99F01VeGvzyh14TVZvwMBPhJgjCgf7t5k8B+NdHfvVDr5m2Lao24aEmwk0QhAP9yhf/9sDVkWLTtvAoEOEmCMKH9Pe03YrYtC087ES4CYJwoIOqtlPlH336AxGEj0GEmyAIN7i5xdZ+a42aWEgiPBJEuAmCcM1BLbb6+lWbWEgiPApEuAmCcIODpiOhV7WJPpLCo0KEmyAIwMEttvr63f9FH0nhUSHCTRAE/lPmj4EPt9jqe39vTVRtwiNFhJsgPOH+y0+XeG5ghZ+bu/WdtU3bFlWb8EgR4SYIT7jD//x7HCuMMBk/ONzWzLNi07bwyBHhJghPsH7H/4N6R/adKi3R6E5/OgMShHtEhJsgPMG+/O2/uuXqSBALSYRHlwg3QXhC3a7FFvSqNjElKTyKRLgJwhOoPx35UVVbn+gjKTyKRLgJwhPoy9/+q1vuaesT3f+FR5kIN0F4wvR7R95qT9t+omoTHlUi3AThCfLBxRJwcO/I/f5y/a8/jeEIwn0jwk0QniDqN7972+lI6G3azga/8ymMSBDuDxFugvCE6K+OvJPpSEF41IlwE4QnQH868k5WR4qFJMLjQISbIDwB1G9+l2OFkTs+XiwkER51ItwE4TF3Jy22+sRCEuFxIcJNEB5zX/72X31kx//9xEIS4XEhwk0QHmP9RSS36vi/36nyj+7zaATh0yPCTRAeU//OeRW4s0UkAGuNGpcqv3E/hyQInxoRboLwmPrlo6fueDqy73O5yfs0GkH4dIlwE4THUL/F1p1MR4JYSCI8fkS4CcJj5t9u/hS4fYut/cRCEuFxI8JNEB5yy/YSy/bSHR//K1/827va03aq/COxaVt47IhwE4Q7dLch8yD0V0feyZ62vrVGjUZ3+v4MSBAeEPVODpIk6beB3wRC4AzwG2EYWvdzYILwpOsHqRm0b3g8Z8wfePwHF0uoc3e+OhKuL///Zu6FTzBSQXj43LZykyRpDPgfgM+GYXgcUADRm0d4YvQrNjNoYwbth7aCU7/5XYazqbt6z1qjxqnKN+7TiAThwbmjyu3qcVFJklwgBmzdvyEJggDXK7TbVWxwfTryWxP/9I7Pv2b22nKJqk14HN22cgvDcBP434A1oAQ0wjD82/s9MEF4WMwZ88wZ88TlBHE5ce3x/XK3leHddPzfb6NTEVWb8Ni6k2nJLPBLwAwwCsQlSfpXBxz3W5IkvSNJ0juNVv3ej1QQnlC3C1P1m99F0+90Eua6tUZNVG3CY+tOVkt+DbgShmE5DEMX+FPgpZsPCsPw98Mw/GwYhp9NJzP3epyC8MDdj4pteTXO8mq89+uPcW2v32LrXx/51bv6umLTtvC4u5NwWwNekCQpJkmSBHwVuHB/hyUIwp345aOn7no6EsSmbeHxd9u5jDAM35Qk6U+AnwIe8B7w+/d7YILwOOtXa2ZH3ff4JHNT5h0tIIHrLbbuVn8hiSA8zu7oT0YYhr8ThuGRMAyPh2H462EY2vd7YILwJEqyRJy1G547aHqyvzryblps9Z0qLYmOJMJj7+6vQguC8InNTZnA9Qqu/xhg3pikxe2v7X2c6ci+L0TFVlXh8SbCTRAeAkl6lZlG+9rjJXsNk8kPdSixj33vY62OBPjDy3/GOXOBL0TvwaAF4SEmwk0QHqD9FRvA1k4EgMHhg49fKlaYPHb3qyP3E1Wb8CQQjZMF4R5KsnStCrvZ/mX/N2sxT4t57CCJHSSB3vTkzZvHJ//pK1jbJzjzQfaux9bvIykITwJRuQnCQ6AfevFAoVzV2akYDOdt0gWuLTA5P/c9AL408oWP9TXWGjWx/F94YohwE4R74KBrZtCryA5a9p/XFxkvdGkxT5IlDsV3qLvTdC0FgKjSwpBtkiwxYsB/XHZQ1Rl+5fDP02joANeqtxNP1z69b1QQHhEi3IQnyv7Q+TQsr8bZKEUZL3SvPbdRiuLHDcYLXZZX47TVHabzRTItSHtlpNg4RiJNWl2nXYnhyD4LM+dpGQob1VdJql+763H84eU/u5ffliA89ES4CcI90A/Lg8JzvNDtbc6+WrFNZcCQW6S5zLPp9zjb3KbTjDCrr+D4Xd6WJELPIxvGqLvTLNp/z5Fsg2ROxeE90okvAndfsYkpSeFJIsJNeCJ81LThvdILr+i1igxunIp890yGk1MG07nLdJo2PzRbqLRRtDKS77ARymBASnVR1R3gKf52fZ3jR08TaCe5XOmtrPTDFQBO0JuWvN30pOgjKTyJRLgJwj1UcRZIYx742nDeRklNs1iS8VslzvoD7DlTHJv8GyZSi7zekrD8ODE/iSZpSMYWMyfepuPtsdO5iC9LAMjRn1w947duOL/SWAXAT0/d8LzoIyk8iUS4CU+Ej5o2/KQO7hPJtanI/q8/P/UXALxtzlCKnON4RCfWHeGKO0vTNEg5NWK+RFuPIHkB68ElspLHyYEpAknHUHcItQQLvfUk1yq2/gKTpUu9u3DPPn99bD9Z/P8YcB0C/Z59u4LwSBDhJtxXd9oE+NP2SnEDgC9Nj9+3r3HzYpKdisFeXWenorMlTxKNq6RbOqcrTwFP8Y3sMtO5Nd7rQKCEaMnDfMGIELd3SeehEjTZNA9hdJ/Djv6EjeqrjA98kZS3DIB6tZOJ0mgCvQqu6zqs1n6JiVtsCheEx5UIN+GJcr1i2/jE57q5L+RBj8cLXU5OnQag7A9wwd2mMvTXdHyHSvMXacQcFgorjLizTA/vUaRFV60wlN9l4YWQ0WZvinF4LsHhUpKE2wsx27eZHixyfCLFkn21YpvphVtIDLld4nTtHQzf5TOJOK51EYB65Mhtv6/MXRwrCA8rEW7CfdGv2G7ui/igK7h+xVazOjc8/tL0+IFNjA9yqy4jN79udlS2diL4zQzDeZuZQpl2po1XdfE6AblCl8PRt8jrl4h0vsr5dok1r0KileLk2C5HOz+DbJ7htA3vF6uM2Qlkw2TF/DGmJhFIEvABxfY04wNfJNSuV2wfVEu8V9lFsX6Ro8mP/XEJwiNLhJvwRLhdIH3Ue24Ou41Sr+twNu0C8MobeQC+9ELlhsf910vmUZRUl1PrpwDISPO03AqeUgUg1X6KH1ZUynIEX8/RGtvmh47HlcoFftHIYssATdr+AEqgkpY8AgkcJU8QM5icMnlqtgYNkNslAIJEgZ3aBb6YmcPl7io2zW/d8FhUcMKjSISbcE/cXJn1//+wVGx9/WtsN1dsy6sfXhBys/7zXavXImttM8rggEM04t9w3Hihy0YpSq2hAddDrtnSSCVdBgdstNQ54tIlIs04l7shWuIsicQG5cwurfIzKOoQZrrJ7+02OFuFmBwhL3m49gQnzWGmBovMzhk8PVAAwOd6xYZd4jXzCl3X5Y2VTQCOPCMCSniyiHATHhp3Oi14p5IssVGKUnEWbhtc+91cmd38ntFhC4DiegzDCK49fuWNPBday8RjVQbdYzTbKrYtU9bOY5gBhpol6nr8YPfHJNUq/2PqBG/ZZa64HgvRCGebEhvtPNm4RMt28Gtx1moVun6XrHy9UXKDSWy1C/u2HPS3AUiehdStopSX6dpfx2eWHCtkrIu3rcD6r4uKTXgciHATPpHbXVt7GCq2jXCRrqQwGl7/y3r/KsmbF4TEYx5wvZLbf9zyavxaRRaP9Sq2rZ3ItYDrK2vnMUYCJr2j7KgBw3mbMPCuvb6+N8b/c/o3iAz8CVWq/EMzy7If0tYDUobEarDFzqaD5qZJ5RQ6vkynG+G5zDiTUyaThd6Yz+xc4encMOuXeud++vAwK9uL+LVBQscg5hc5Ysi8sbjJHgYvPTPzyT9QQXgEiHATHriP2id2x+fYF6r7u5HMFNp07T12KgZx79Btz9lfun/zHrWDPHOsQTzmsVGKcsr6O3LDDiOZISo1napTIx1zeW5wmnGmmZsy+X/X/rx3vtQQ79Rt/jb6I46l32ZGs3irK7NHmxPGDMPhAGv+Fp5qMqJkSUbHqXUcbrVVTTZ3gAEAJLvBXtfkUvdnyCm7TMhlDGmBHJuk2CRj2XdcwQnCo0yEm/CJPKzX1gAu2b3rau0AOqFPPVwEu3vLMd4qxPaHXP+/m4Pvwl7v2MutLdq2hqluUpF3yAUOw9I0c1yvFNveedI5i8HkOFvdKNZuASleYzjpIhltFjsWcmAwmnEpxEyMmMGR9LNMxseZny/x3vYV3m0DhJSLHbajBjVHZT42QX3pdX5a1vknYyGW0+FSpcqm20CP7jEWHWK5NUTLhum0CDDh8SbCTXjgbrVP7E581LSod/U+aB1yxAdgSO4efI59m61v1WUkry+SpPuhzibL9hKMwEtDGqBxpVkkqsNEJEcNB4CMVqRRgufVeQYHbE53VhhM+Hwr+yz/6/u77ChtPjeZpup0WHVVWrGQiGTgyA5Vq4NhNZiKjQI3bjhX6ivEXBvFmEINe1sb3m0HvNV5kZ/Ph+xFnwJeJcEODhqu3Puekk4REOEmPN5EuAn3xMNUsfXNG5MAmHbv8Z2OcShykajuc3r1JADx4DLRYJ2N0gQVJw4jp2Gkd75l+8b3ppMuDcqU2lk8ySFVqGFLDYpN8LXXqDoeTmgS0KVsf5947hxpaZRkoYG9ofL16DjqdJMVy2Wv6/DZbI6fi8OJQgJ3LAtcX1iiyi3IwYmRWVaWE3iyy/DgFr8x2es5uWPDcyOXOWnA226CjjbCVHIagPrH/1gF4ZEgwk14aHycVZIfNS3aDydGTh/43v3X+vorI2sNjamMz3ihS8XpktcXiQbr6IpJILXI64t0WePVLYMNKcqXpm/8+l8fe5YNZ40fdM7hSTbl6jjFUMcpG8wNKiSCkGg4CsBblsaoqvPV4f+a07kfojpRjqaGWeI99po7VDo1rgQtltwY+C4n6O1fU3c/ACDUe7uzJbuB5nu8Yjlo2QxBcISMdZGkU8SVEzSMETy/ge43xPU04Ykhwk14LGw4vSnIg6qzO63Y8voiftygWXMoyx6fn/oLopTY2htCkzq0Y68BEdpBGhufnbDIK61FxvXJ6yfZPslJPUojVqfoglF9FgOwPRm3+UV03WGDtwg9E0UBN8zweu1v2AtajBgyduISpd09jK5GwVcYkUEKPCTfRe6UAXg6OwhAaKR7X9NuMDllonUv39D9fy45jRYboA0clnsbxtfv7mMVhEeWCDfhsbA/YO509eX+a315fZGFwiLjhUnOn1coVw2mh6FLgcvbMwxEV/FjbU7t6lQ8g5Zj4SCzV95jRzL4FzNfAOCPy0UGo1t0o12qpoLUeZsJrYC9/W3WohqOZ9PNvEvN8oi4cdToMd7vNkmHGrqqM1ewsHwfX4mjAEQ0DqWTBJE0QezGUJPsBgDu2AusmWdh3yXFm/esleP7bhUgCE8AEW7CI+2gBSUbYZRxaeGuz7VYWuC0DZZdYtSf5bXVbwOQzy4S1zQSxueI+VEMiuyyi0GMuJTBpsOyvcTa5Rq6qzEyWidJgqSRJqm08VSX1hY0jWWGY0UwFWbzDXZ4G9lLMS2l8TMeslyjtJtDtYcxPRnIUGkFNLwu6ZEOQaKA3C6hVM4TRgcI1ci1sZ8qLVGvZXiaH9Aypm85/Vhs9MJOrJYUHnci3ITHznihy5xx+31q0OtiktejGHKLctUgCCsk1W3s7jyvvToEwC9/4TLRQKFe6oXmeKHL3uYWlVaLUT3OiyM27coOK50aKW8cbfcwe+Z72LbD0cQA1cYYWWWRiA6H04M0Mq8yozuca0exLIsXUx5m1uVMx2HLghEjS0QuEJWjxOIekwmYLJgE/UErOqEawR17AYDd7X9gwKozY/wa2MUbvj9xjU14UolwEz5V97rF1p3uszvoJqVJloizBixwxV9nM7VNKwBpb5JW6MPIaXL2MXatI+T0IhmtSMVZYM6YZ0OOciHYwsBhfSVDtLpAraay60pEq9CNxzH9AtvtPJoaUIksMxXb4NJmnuFcgolEjoSXZFWp04i7PD8yScyBIDZI2+lib3hkw00K+S4Tk4eQTAtt7RXwXdCiyL6DtvkGQaJAcfMcmhkna5xDDTqogUnSLl6r4IqLEbasC4xO25hurymyqOCEx50IN+Gxsn/P2p10I4EFVutPYRnruFqdtp+htjeH7qtMT3Q4abxDs63i2RazY20S+dP8ZHOHncoCI2qH7fA0Z60O4+o6U2ND7FZ09EyVuBZgWaPsyGscS4wzM5klozX42c/8AaVyCt3f43nVYs9d40eVBqebGwxnVNy9BZBHSRSifCM7Dz5sXbzERL4KvosUuIT0el8qe5eQO2VydpPZYALL3SbubuFLOm19Yt93evL+feCC8JAS4SZ8Ku5Fi62Pcq2Cu+n5RqnXpSRX6F2T61drJpOs1xTWpVUqxpssaOBLGSqAni2R8yOMhkfI6T/AiCroSgeVLnHWSKpVYIG2qaHJw0wkygzEbSK+R8RQ0ZJ5RlPrJNo/ZtWrsCm72CRZzrzJbnGIY8zgZIq8YRlUvDpRxUIObdaqNklTZShVYTpfY/YzJzjzQZaou0GQHCPMH6N6dgUlMMkujAFQsdZoazEC5TieHEcL2nTVQXYSL7FVNHqfeVshzXHY9mlcreBExSY87kS4Cffcg2jF1a/YupaCZSl0LYWNUpRCvMxw3j7wPYMDNm3fpeH7WPIo8VycQXeDyXyLcb0O2x5deQJ7YhGNEh7DLJYWGABSCY9XSmloT/O54QKjKYMf18+RSQyTmtzhXHOdgWzIWCyBI7V4u/FDvjS8RaT82xQrOzQTW7xl6mx5CRypiuSENFCROzJfkEfwfzLGv//RMT77fJXQm6G71duHl5E3aVVdWqtxpsaqXNnbwKofZlw7hy8bRN1tdL+JGnSJodKiQFeWAWhx94tsDiKmNIVHgQg34VPxcVps3SokD7p+drNy1cAwfLrWcSzVAs4BvcUmr555mtM2TOcu00GmUvssrTDJcOoD5o0jN1SBFWeBjRJMRKv4mQanbWD7JGV/Ec+TkFRomyq1UGUwb5N3tpC6W0TkPTQ5wlOaxab3Y8Zjqzyj/go/uJKgtjfAV/I2pnkJ39hD8X1ioU4oh+jaJpoNVneIifQiKa9BU51jb22dNGusKbs0TY2JUpRmcwkGYDyaBq+3j81RMrhKgohXpZAKMaJ5dnZ6S1EGFiymEXcFEJ4MItyEe+Z2t7+5n/oNjV8pbtCsGExmjt7QK/KjDA7YRA/quX+1s0m35GGGFhvVFhdqWxzlJJPKAgQnIXWadmwZ3Z9mKLPNRu0yba+Jojv4ksZ7Vguim2RbQ1RKIWlpjfQApPWvMtVJMprdRfVjjKsRfDT06iRHk1NMf15j5sgKS5dSSIZDPGKytjbJdmMN15VRpRmajcus7X2FLx1PXhty8qbVki1jml3pKADT3HhbnlvJWBdvWJDS16/YxKIU4VEgwk34VN1NxXZzSJ7sXUJC4/r1Mzi4grv5tjUAOxUDQ24xlK7xRT9Js32Iog3PnagzZ4xzc2Pi/k1LN+2LbDY1DBkCv8iOeoHB6BWGIse50lEhBns1Hbtq0HAG8XyJeLhMOexgGuDRpa3ayNYblOVjZKUrnNl+h/OKj+Y6jOhtGmqbmjtB0QnQ4wpfORLgp6cwjSxLrRXk6E8wctO4VgcnkOnmLlHvrDJv5EnaiwBY6gApexktaOPKvV6S0/U/Z3jgFD+s/nd0Ll5hdPr2t7wRhMeBCDfhnnmQt7/pf83scJvsMCDXrzY1/vgrBcelBfL6IuuNNq2WzoiRJi3JVCs7bKpRMq5E3DrBZHKOHdtgWGoxGJ6gXNKRjb9gJlnm8OHzvG8ncT2JWERiazegE4RMRCtksx3mRwzGGzlU32QoE2NhsMkLsxeApwE48XQNdpssb8DklInXdajXdH7SeJXMYI2ZqRlOl3vhdkSFtb0JXOIUhuu09Ul0v0HUKzMUXuh9Tq3itVve3HzH7X7FpgbmgVsK+hWaqNiER4EIN+Ghc6uQbF19/U6uufX1qy8z/RSoYAc2uw0DJTXN4RMmhwEYvHb8zVsJGiWYQsHVfFQkYmqFQ1oJhzSrTgfZXibdvYgUnSIEmm0VwwhohXlWo5s4nkI2PkrEcBmrlRhPbGF5SfCjNKqXWWzK2Kk0p9s1Uuk2T8t5/npljgtqCUol5nMFGnaH/OAhAiOGP1ImH51C2dMI1G9xavMHAIwmpnEV2CHPrnSUKf8/kOueoVKJMYTJgv2n2CR5fWsEl00y92ZtiSA8tES4Cffcg7j9za0CceMTnrfiLDAsG6STRVbcIqW9AonWBM8e3aHmuOy1eysRs2mXve2TXPlpjmjEY34wjmts0FwOWQr3MLKgyFCTTBw1IPQcKn5AXQoIJQu0ACfwKfkudfsy2k6VKQ1kHdDSbF1s0VI91uQa7XCR1FiK1d0YHbdNTEvQ3S1gyRWsroymHqcWqiTYJUELFYcVt4TVKdP1HYy0Rqm6iqOkOZScvKGCq0eO3PKaW5+o2IRHgQg34aF1UEj2rqFdn2q83d2z99/KBuDpo+OkCwDmh25I2t9KcE79G9bra5RXv4Kt5xkvTHNlVaeQUGl2FlhsPcWY8xbTwxu07TSekmMg1sDwL/BrL2zwtgm72iI/o2UI1Bxd5R3aukMyFeOIHMepNwlDiw2pQN1vkirrjKoRlNgYTdtBBuaDZzkJjA+kOG1DS/U4acDkyGH+41qdH72bRpUuMZhLsFMxWe+8TXtohkQ4ywjwlvRbDIUXeDb/MkGgsmZP4PktjHQbfSRPR4uj+vdmj6EgPIxuG26SJC0Af7TvqVngfw7D8N/ft1EJwsd082buLalXlUQ5ceDxeX2RRgk2Sre+NrdRivL3b77IF45pDGQd1pshZy6kmciUmR1aIvS6vFUOmBt7n2+cMNlunGGxVuWyKjEa04lqJqaj0KkrTHppQtUnCGUue0kMWWE0HMb2XOxQQyckK7ucb62x3g5ZeAN8OYLc9XC8QeS1FvnKIMNGFjeIMZOeZLPTQc/qPHN4DLYXgF7rsF2gwyh2aRe1+hk8S8aMLlIuTzMaOcrJqdO4fLj/ZL+CE4RH2W3DLQzDRa7+U1mSJAXYBP7sPo9LeMTdyw4k+6+DLV1JAFCu6tQaGs8cax7Yaqv/eKPoA/ClFyofGpvZUYkHCs321T8GT/0Ru7WLdPYs3EiUVeVvKJVTFEpfw7JHKCkrbNQkJE7ix2YoOjMc1T8goq6Ri7uUDJfX7QKO+i6qmmDXr+N7OiORaXK6TiDXQTWxp0bYqo8SW2lwLDXHQuEFXrvyD2Q7Ic9Gj5PLLnJe38aSAWeYkwa4AXihxd9d3kJyEzyl27xTL7BsyrhOipMjL8H2Au+fSsDIaQYTRUYT06ynf55O6QqDoy5X2sepui7R6y2YBeGxdbfTkl8FlsMwXL0fgxEeb/e65dbN9i806V976+IB8Eqxd/VtXFrg714dYuTQnxKNeMTsQWRscuoi59eblC0D5N5+sE5XwbIVPF/mV5//Y4zxN/m7rXGS+ff5N1/fpV3Z4c3tTRS1ih5Ax9rm7zqr1N0mU+k4cV+iEZhs7MRYUPL83EsGV/bWe7vN6tNo5g5kusT9daY18NU8iy6MKJOEEYuOVac2GPIqXVJ2lONRDS8jMRTOoXUsxtUQT97B5nlGtGOAz9D497Ayq3S9OKbb6q1sLPSuk8UXfeZYYHrBAizucNsbwIdWVgrCw+5uw+07wH8+6AVJkn4L+C2AwYHhTzgs4VH1cXpI3mrrwP5z9a+dXauygGOjZ8ipFhul3tK/k1M3nre3UrLF6LBFuNfr4IG3QC7rkE71zmdIAcmEx5vVFYzGNEP6MzD5j4Rtm9TeIVTzl+jm/hon8y6Z+CqTYw2gw4ZtMqHqjKR32OsMcdndoyZVGQA0LUYjUJjWMmi+gyqlOKxBKjNGzIevpQeIxAZYnOwCKUb2NDTdphRdw5a3aDYa+E6DstWl4VgM5yZZUIdZa7zJF2Y6qPJR3rhc4iuxOp+bPsaPt7/OVlGloZ2lFV3DtiwaWzpVucT4eJXRxPTd/Aiv6Qfa6dWTDIUGo9MHtzEThIfRHYebJEk68G3gfzro9TAMfx/4fYD5mSPhPRmd8Fi4thz/IwIvzhpJri/vv5ubjhbiF8jr9g2bu08agLTAu+UiUWxylUlWN2KcdipY3RoJJvB8aAxtkw5dTl85TjTiMzlb4uxGjKgHEymXF9Pf44p1jpq+hqbvMpXZAWeD96tzKAMGc+08gblAu7OFObjNL0T+GUu+srcAACAASURBVJvuNqnpFbxiipNansKz/xUfLL3OD3/8AVbQwR/Ng7fNTucSg+4w7zf36OhVfNdnMJqhHkCr26Tm+mipCB3PphK7hOOvEza/xbHYGkkqSIAamFjW99mSXiY0M3xeT2EGFpcJicgOo4mfuba6sVexXQ8tzW/d8LgeOfKhPWxJu8hQaGDQRPMdUcEJj4y7qdx+AfhpGIY792swwqPvbnpI7u9EEtJhyV7D5HoFd/NNR6HXTSSvL7JTMYgqLQYHbKaHtwAo7owCYAfXwzStFAnMkGp5Csk3GU9tY0dkamRJJXtTlo2mRrR+gljUp1V1mO6e4JA+yeDAKLr7MgvBAK1Uha4h03BUZjXwZVCwWZVqLMY36BibWFqDN8x3SbKF1uxS9XrjKVwde9IsIIdZfDmKpUwwHdlhIHqBSxEX1evg+l2GIk222xam00XyAzJ2DUmBd6wtxiTYdsvUKioQJT4cp61PMjhaJS97hNYQI3aeSnKdQlpjIj7EwAGbtZdbRQCOxAYO/LkknSIZC6pFmwCHWLeIIZlsr09gIyo44dFwN+H2L7nFlKQgfJRbBd6y3avYQnq3k1GBqvNm75Y0Qe7qMbev4EwmgeuhVnF6xw5FLnIk1evsv9p0mJty2UvXubIWJ6upPHeizcZ7X+fCUhLHlXBcDX8nynAM4hmPd89kuLL2r3hx4R2+MeRwud3l/b08UqTAM6lhknKLirbNRbXMSFwhbcwxdUxF3akxrkRYeDrJ7rbK8O73+Pl4lb3IIV7zirS6El8pzAPP4EhrPI9BsRShaLVIKDYvJiO022kWrQ6HFYeEYhGoEpp7hGkVcKDJGLvWKm/U/pGGlMDxfFzrMnXrLFE54PnRrzGXnKZ+wOe16Fz9uSR7PSmvVWz2RUy3heSaLLeK1G0HjCkU9ohSo8ZT7EpHiUUOvlgnOpcID5M7CjdJkuLA14H/9v4OR3gcLNtLMHL7zdxzxjxJYMleQwUOG+N4xG55/Kl3epXG4MBzALy48C52oHN6tRdmnfIaALGrDUeiEZ9i5RAl8yhSZwVJc1nsJLlSijMz2QvY8UKXnYqB4xpoub/CyxXR7AHaTYe0tMShvEoYn8WSh7GkCi1vlB9ujXNxb5L/ZuECT+kRdp4r8sE70wwpGxx3m+SdJIvtkISxzkl0GjsqK16DIL7Outci4nb5onuOrUSdkVSHZ6IaUs7ClrLMjhzmpAGVpSpWvUY8E4DTotFu4ndCVp1VIv4KVpjCbWXoBCbNwCMhKUi+hK4HDGtZotUFknqvLVd/+rFa7nUz6XpX/3FxtYIbuGmK8bIL0RAWRnUGkgZbxWmGWGehsEgsLe4qIDwa7ijcwjA0gYPnMARhn2V7iQ1njXF98kOvHTRF2WIek14F55Jg3ugt9TevznzNGfMgxT/0voNsto8B8OLYuwAYcouo4lCIX6CRfo2uMUxKzvDFkfeYGVR49czTsJ2h2dKYSF1AyaxTjvp4bZkz5iZ6pEJGPsyP3xzg73/8Wzz9wh8wZASUqgtsqXB6GqLTP6RZnmMwkidqXEBydM61QlY9iUhnD0mDtcw836+UGfOrTKSiSJEsO9EtGuoWTdMjlLNIwIIObvsXWKyuEHHeZ1iJY4XPcKnyA2xTJ9q10LQ2XL0Tt6ad4KnICcrOCjV3k5g8wQvxWaKxgF3pKKPGjdOH62Zv+tbyexvaF53e5/oS3NA3MqrGGU1Mc5Ii2EUMCnTJYNC7Pnfz9TZxtwDhYSQ6lAj3TD/YumEHM2jfcQPlfgV3e711StFIb+9autDr4l85YGNKRiuiyx0YmKDeiXKlPkbDnCaVrzA4YMO+CrEeO8fu8B/hYIM7Szb+Hma8Q23va+S2n8ZMnaEsF9lpFxmyhjGjZ/Bim/zJXhn/gxk+nxvC8fZo7DzDf265ZN0Sgerw/tooPwksUgNlqlIHt6tQxUBKt0Hx2d1x0fwWq60mX5s4xInCYX5aBdOYJZS3gR0q1hpK9Ahf1r+IHvs/2VUdVO8ZWhTIjOrMJW1O22O8v7tKbUmm7SrMHesSi1jUud4QGSCTfQmAar13b7ubV1EWGxfZahfpeiam2+J0CDFni2PDvUbKXhC7dksdsaBEeNiJcBPuif3BZgVdttwNolLswAruIHF6U4oOvWttJ69WcK1bvuPW+tfcMloRO0iyax0hbJzk5MBZulqVjAyJ+ijfGLOJFv6EP1+JUIp2cHAIjQ9QnSopScJ3lthRLpCfPEccqMg+Ve006nCZgUiKqrqGV55hKWzR9bdoVOdobY/y1YEAX7d4e/1L1PWLfFbtMJPK0LIz1AKLiFmGVAzNb+O7ZbpdhaXKBnu7MiODK8wfbrJkjyApHgQBzUaBmFLE8x08xyJmb1NIVpCcAjDNdLrXsb9begVYph750oGfS7+S2moXb3i832hi+loF1tKnAfB67TNp65O4yof/GSLuFiA8jES4CfdOdQFoEcldvhZs97KJ8uBAbyXEeKF7w/P7pzvz+iLjhS4abWLUGKY3NRePLXByqs6SfeNU3ZVgC5hl0p1Hzn3AllpG8VPM+pO0UwFnJv8eGCSnDuCFJhvqFdzARsqt8pTiMZGLULfrrNfjDKZkflYaZ29vFju9yvPP2MAMCxrsJVQGp2PUa+uUK4dIdh3MsIWJSsKXWao0uSAt85uJdzm3tMWFywV2/BQl/SIxb5ZqYHHUOMwho4uebCPlC+wkXuotBlmMADCd7HUeKS5G2Cr2VjVOL1gfqrJute/t4JA6QutqI2VXSYqKTXhkiHAT7on+tbGNcBFuEWxLpsdKx2M2pjIf7/3W63cV6VdsOnsAvGmeuHqsh3zA1+vvndu/EnOjFCW/byP3aRs8+leoetf3Roz53tcchhbjnC7C0ASMDW+wXFMZCFIYZBjQQg6PmLixGg3LZl25CIkdhrRVmq5NxbE5OplFdmrYlQ0i0ZDsQIyovsRK9RhueYpDE6neODaiRMdBiXT56slZNs41yZlvENHgdTskEgYc0SwCVcGyVJY3UjSaOsbEBtlklSHl65S3NMYMyOZXgeS1YMtYF/mZ8C2ajKHJvRZjR8LvMxxV2eEbN3xmt6qsMtZFtooGu9LRa3vh9rvTQBMVm/AwEeEmfGL7O4l0JYVodQGkBdhXUb1ctfn+roUiwWs1l18fizIfV6+FlJZtULJ9sopLzQn4v8udfcdyLQw3StFr77nZeKFLujDO6dU4eX0RL7eHySQjhXEqq71x3ryopb/FIC4votc+zzPBDGmlSDvyBpBmQH6agRgUGzAabXJ4aJhz0TO0FZ1ILIcb5MipHRwsGmGMUqtKt/AKheovsbNjcCX9J5CGYzwFlQVODOmcNFZRd0d49orD6W6ZFlCI6jihxhvbDZrdLGP+KBdqK5iNcaKJBZ43zpCkRMLZxlLzwPVradm8h6G5fFDcxmwoHBrKE00GJM1FOheDW4ZW//03OyikRMUmPGpEuAn31Gh4hLjk3fDckunx/V2LbgAKAAErHY/5uErFWWCt67PpVOgGCu91XkQFAkJie2msIOQHuglE2AoXudBKkrOPkUp4mB2VP3oly67tMzcYMB5RWF6N8265yGcnTqEHTUxyLNtLvBvsMCxNM8f49S4o+zaHL5YW2KhHsd1jjCVk/PAD8JMsN4fIZx2O6OdY5l1Wm23mY/C88QLm3hZndzdJuTaK7BEEJQaTMqO5MSLDZ4nIMSLdXQBGCrNMxk3+9DUTSDE//zSNoEtauYSmS6xFYlSqcSa9JrmoQ+gYmI0ESfUIJGAvvs4eYESPA1xb2HG2a7JKmqnENApnMf08F6VvYvoKI/J5ALasC9CwP7Sa8Wcp8mZxly4ZRoIZkuFHh6EgPEpEuAmf2O26kqx0PBRJQpFC/BD8EPRqhuWOwuKexFv1gHwyhhcAcfCvvq/lhXghbFg+v/euypyq4boyzZZGq6Xx9rrMsutgZGxWcDkZZphOa2zvRVhSZki5NS7vpDHCJJc4x7ZvwPbJA+8iMC4tkM2pDEVO02yrlLoZ8tEunxn+S+rKGnowSCrUkXAZCcY4pEGFNJuygdNKspdcIh7ZpaDpDEUNXnMv4CsJah0bD5fXS2d4V7rEWPgMCbZRpBUWsi5fk/MgK+x6sCtPciw7xra8yPfXba543+GfTD5HbMGiWp4DoBadxipuAhCZHoPuOdY723hKHNu3kdNtkrn/gmVp1DFRMy4LShwcMKXebp567VTv56J02XHqjOldcpLMVnjwbYH6ROst4VEiwk24b16+GGHT8hibkDBkCQAnDJmNKew2fM61XM7v9W7nshH2qik93sTeSaFJkAl0ml6A2yjTNJvsJDqMZ7tI0dNsNWWWqzOo0xWMkSZ+OcVGSeLF0fPkVJmoNIgr19Cj7zMYjrHWyeDIJjvSBS5c0tkozV1rxhyPedf+v6MuQw5mG8/SjlzGtM7xfrAKCiQlnaEgh9RR+APnEjN8C6k2Ttc5TeAlaXUcmm6EoSgkLBUbDdQq4LGxHKAGPvFxC0+yMbeqNA2DdH4IvC4Z1eHQMRtP26G4bDMXv8AvDB7j4shpig2QXJOYu03J2aJmqzQZI2KDFEArgKYDG+4JBvQpcpQO/HnEtd5Kx2R8lHVzi0UX3HQEyfC4Ut+jqAS8dGSG6bu5XYAgPKREuAn3zP5qqD8VGRByqepwPKlyoe1iObDc8TmvVAhj4LR6iy704eYN54rK0NyN0vXBTgRo6S4zc3Uqu1FiYRQ57fO58dchY3LWmkUKIacrN5yj1tDYk3aJymUkIux1R9CNIl1dZadyjK7VO/7FhXd5uVjm3bpKx2gyn83z22Pv8bK7yJWuQ1uT8LUVangM6APESFI3wXOraJaKp11mRmlD6BCTWswkdOJNmREtww+1dcoBOM1naWk7GIkPiKhxSi2drmkxpFnMZ4d5JqbxfrPFpXaHJbNK0hjAir7DVttlQYc2UK+quIpKw8qRlpahuInpdzGYIGZqtNwyBBkWt1KUnRWGsuuowQib/VLYKQK9DiVVq0vEq2D7Dr48iuWAoR/8cz2o0fJyq0hLnxaLSISHlgg34Z5bXo3zZs3GtSCqyNRL8PIWyIMOIb1K7WbHIisAnLdmYeQ0TcDd/TzIEM0MY8jwXt2kbe0iNXJ8bWwGN/023QC8nRSzuRUS2h4r1eOUqzpZZZF8LstA7DiGUkQPruCaKWgfYTTtYGc/oNhRydnHGC90sWtFCFV8fY2L/hW+326xrC6iqhnUIIErVVBknZqq8mtHfonjbpuIW+LS1gbFskQhWcBIXmat6XNxt8Wm72HHpmiu5aj7UPGh5cvM7zTRIl1OTCgMhjYJ2eKEAe9JWS5JYDq7qNEa7e5R3tv1WJiGgc77VBwwfY+0kuekASkJJANO21HKnOCIARFri3q4QodpamWVtl2HEehqIwBE1evXGAeiI8RcKFsVwugcM4PTTN+mtVbC6e1FrEWf+tBrYo+b8LAR4SbcF2MRlXfx6foBgQKEINHvMXLdzRUbgLvX+0s4cHuVlb2Tol2NIxV2iA5uMJ9/j1HHgUqMotLmWLhBp10iSEYY1S2iEZ9auMjQ7BJfH36W8k4CxbUYSpZ41p/rBaB2noXcj6k33uB/P9XmonwOVwFLqdJ0db576QSzo3Wmo20q0h52qJBwMhhxjw3vL8lG40zk49QtD3UvwNcGCKMqQXOL4Rjs+Q3eLvv4TsiQGqKkSsQ9F7mTZMs12em0+UIqy1k7zT9uGWQn4XAqy6a3w7qp0pLHGNNhrKpxulWmrPsUUhkK+jZpO4pGyCvSFXbZ4aVMiVT3KBF3AD25jjFq0w0KLJt5dkONBDASn7w2LXndU4Tt3h27PyqU+tfYknaRS+0tduSr1+/6N0MVhIeQCDfhnpubMpkDdqVVdm2f4cECr9cdnI+o2FJKm1q4xXhkg248pNgdh5HTV4+axYsuQj2KL4/RHr7MOa9ILPSoWSFOpMvwwEXsdpdTuwlyWYft3UO4l8DbzRBVWizk4Upmi2X/ZVo2ILVID9W5qG2x3vEJfAmPEF+RSAKZUEZyZEyji6r5xOUcEXueMaBZHEXLW6SzKv/9fIVjmkHLSLPV2mLN2CWTivOZzgjv+irr8Q4dN07a94hIDk6sjKfUudKCszp4kREu212eJ83xiaf584tt3msc5zPaYTrWBi/vqlz0kwyOOiRC+KDTYMLbRcVmmzJq4PA5pUaNM3TlBayaA+wyYq1hhhJhfYr1skp0LM/nF3ob2E/v28d+u2CD69OSamCiBl2STpFSY5mONnKtGkwbvcUqooITHhYi3IR7rr9ZG0CVJV6vO/gBqFLvN1z3NreyVVMWWsTErybw6lGIgRxz8BpR3K00Z7eeQ1cjzMrbBJ7Mcm2Ohfku44rOurdGbqBBzB1ntTlIGIXPTrzDV4dkNPsIiw6Myot0wg6BnGA+N8JT+Ra7VYPXy6MYxhWOpKt8Jd/hHSIs12aYyWpMDSu48Rwj0WFOtha4wKv8fWmNy2GLYjdNJpKh6Q4RURXObaRolzu4SkBK0Yi4SVS1RlqBsXSZWT3EI86ldodx1tG0LpfqMvjvkAmbHLV+Hqd4lHey26y6W8j6OpFGnPPSEMlMmg3dx/BabDZLmE6c73oemrfGi/FBpoNx1qtPE4+/zKQ2zuHkNO/X165VzEm7yElj+mOteGzrk4zmJhlUkqybW4RXGyzD9abJgvCwEOEmfCI3N0deMj1+b/MsASEuJpYaEgZ1kMAIZ0mq0N23De68NQv0KjiFw2xYszjSCjIgO/NYl8doAfpgGsVV8LUlPG0NvzyIpqY4PniFESmCFzZQdJ85KYuznGajGcHQQ9JKkXVrmXeqEayYjt7IMBJ5n58qZZbW8jiSzdG0gu8bACiBQRgMo8UzPD/3Jp03XmRYj6FpVUqVCJNSnBXvp9TtXRRF4kKmQQsLeAWrlqThLaAaJdxsDc8wWNh7ikzG4EziLACh7iKlFFZaDtsdkxEpThiXmXddXl59k+xgDhuPN0oBWeVnmUgvgZdhRM9Sk18gn3Upd65290fCDuG1LnRtk4zS4mi+goXLxc0BWg2HXGaFXxjcYtmfplq08XHIFIok7SKtOwy5/jHXF5IkqUpp8PbdSf3qlKeo2ISHhQg34Z5a6Xi9DdiKTMUNCfdVaSGw593yrdf0r7nFh5tExuJYfhFnYBHZnMVIWUi+id8OONs4hpq0SU+eQc6UiCl5CtoUOX0VL1ZGN8LevdH8KaotC9+r05Q3WHQM3GgG14/gSRIXdp9i2DnBLw4v0zXWycQ7VOotjlp1fv34e1zydOTIHEddUFqv82p3j12pwdNaEsc36WotJMPH0A4zGRnly8+VKddk3ikplOvP4GYc0oVLhDsFsmEL03YpGBEC3yOQC4TdOkNKgOYMUCn/GhFri7GhMySUZ+laXXajMZyEx1R8lQVgUYNO7BmC9joRHcYTg6yVdcrGSWLJVbr6Iicr0DJAx0STuszJrxKjg00aNXDR/QYxtxeSH6eKG4nfWUNsQXhQRLgJH0u/YjOD9g2PZ2MzJGqHCPyQiLSML4Hvzd72fP0KDkBz5mntljEvhUjlPIxs4NfiwA66nkCX43iFLTzg7WCNYc3iBT2HRcDpxCIvDHqcDLZpmSqR7LP8bEFl40qdpv8GjTBktTaKpgbklRq7yg5tW0aKXiafSjGhRcmnPYypVzH+f/beM8iu+7zTfE4+5+bYOaK70YhEkyAVSFGUTFmWxrLssjxleendWnu2PLarxlsOH7bWta4tb5VDrby1I/nDlMupdteeXQ9tjSV7rGAFSpYokgDRJAAC3UCj8+2+fXM49+Rz9sNFg0gMSpRGuk8VqnH6noRzL/rXv/f/BlnBEDI8oOm852SCizsv8ZUrLTwU6qLHmtrG6Ua0ekk88yiC/wDDUpr9+hKtzUk0bx9iPvGEx6nCOzDERVSjPxvbVkaYMGQscRjT/Rcsr8Jh/xYztBDjJra+Sa3mklfyjBgycwosadBTx+ioMyx7dbq+xbBRRBZkXrZNvrEpk1UUfjIuMp3YISTN5ca7mBrZY7iwSbmzwn63wNHECDFv/5saYdPUj5HXj5FnsLY24PufgbgNeEOe2dgB4ImZiTu+v7NnYOEzNvxq0e9CXOa/HTe40fNp1nNc7nhU798K8h7ccr/mLXRkokDEV7aIhnaQ0jVCwSISuphBgGBlkYIpwsw1AmkfTdFoWB6Sto0Q61IVYxR6Ctn2PGG8RKPqctlu0YkEql6C/ZaPkGhS7ij4Yh63K2MLKucbC3xxb5LM0Df4jccDxsaG2JAfwDxoc+Nck/ORS8ctkBcbVMMQ1fHwA4GHhDgPxOKEKYcLzb/kc5shRnWKZNJDib/IaiNgtfF+IjvO4lGDB5IBKy6U3AN+LH6AqBlcNVOkVJW52BUuBtMMaSnGJjNsF0tMxhdZulmDJjtrvJs12kApnuGVnowtF5BzNkjbuFUZJzTZcFskhQPG9TGsMMVV4SH0xDJy+y/JWZeJaeBKKeTQHMxoG/ADyUDcBnxL3D4FIC7Kd0wAWIj3u/5/qbbIF9sWktHGFfpZkWp05J6/e/U4its/3lOv0b0xAooNepdQ6hLSQQqTyGoOWnH8nooQFBD9YXTmKaeep+qqPDxlMBXaKKGNGXc4FRtiJFchLlVIJUZQwocJXYdM0mQoNs6X6zp+AMc1lwl/lAueQmLkPzNTCDkVDaPWM4Rul5y0Qa0zSk4bY2LUYqVnYbcEZqI4a5GBxxST0SjX10/hGs+RHbMJpV0cwAjypJQmQ7nrXNs8C50HmRs3+xmbwA1rnXnNYFHXWUzmCUSoSC3ajDKXnAH6A0NjAuxVC4j4TOuXSLjbPJHMMqJn2XK3qTptxkY/zMlCnoS7hVKJCEgwOtykOHqJZfcSy84McWmSlL9Gx+riiXA08erQ1m+GgWMb8P3OQNwGvCaHjq1h9+7YPuykfzgFYGfPACF+T4/JG40QwZVxyyk8NY6S679+ROufJwXUmzVWmgWU2M2aN8FEEEMiMUT2x4lKY0S5a0RiiKSOoFiLWO4WQs5E4wh+Wyf0L+HKbV6oZTF7KUYTFQqJKp9xX2HEcHkiHGbImUQTTXpRjFjMZZpxCoFBVdpGVQKcnsQNa5/phWVmtByV5gSdgxxW+gINxSRSnmRc7VKMrrCjpYmHC7xNlDHCiK93R/m/SjalZgljepwZo8Jm6yqaJHEq9jCJeIwTU1/lp4wbfN45xXPKKqZTpWUp7Dk1rtTrnFKHEaL3UeY0icKXEFX1VhgwY1+l52yQZJM4NUQ8rphVVFFnTDRYDYJb75m020bB5JiWQaFv91a7Jaq9ClOxElZoY8t5YqGNFjRIO11qxpnXdW1ff2kdgEfPvH6R9zfDoE/lgO82A3Eb8G1x9xSAa6bP36xfI6dKTBtzCM51ItkjFLuY4rNYnCeBgwDUyz1c18fVQjx3C+HiArr0GIYm48euEjoykjKNPLxPr1sl6KpInoTcO4as9giBMHsJOekg7T9K6G6w2U1QcCeICSLbjgZKlvONx+l5KUYSN4gPm6zsvYuWf5y3DV+h3phiQ/1r9rJVFNXH8OM0rTnW48uYgkXNCpmPsiR1GC8e0KtozOkJxhfglFRiqJZAliO+EdTIzNrIo2tcrrTYadVJSSKPFl5mpx0xXhdIBS1kt0xjt8e6WacjxBAjBSuUqQVJXKeJqx9nKd+3dc2bz3R5c4mhSGPMrqCKGus8yHVHx5dfokFAkHyQrNx3YHvuLh0OKOY65M1J9spxNp0MMM3RIY8bbQspgmO6TsLdRvTKyJp5h9gM1tMG/CAwELcBr8nhGtu9a24ma5txGi3ljg7710yf/3vXoqp12UPgmOHzgAKrikm1BjnPRI2ZTChdAltBVH1c1UOwIkQnwXbvJCTpOzzNwj3op5f7198O1QRoPoHhIWo+oS8S2gryiIORyuNvLyBpIZNKioq4Tq08i9jNY4g9vlbuUHMLLKnHSDU8nOZJtoD3PLFCW26wyiaSFmMyPKBsa7wQXeGVaI33Z3JYdowtM8eY8CU2GnVGmUL3SmRjFbZFkS87FiW6jCYT5Idsnu3VCULQjTQFTacQK6F7AifjEwSyw79K7aOpGv+HV0BPuPR6+6SjJKY4CrE93j65fF83cyAcxwpF8tENdEIS2hGEpEKnV8H02pjVNBP6Iq5TwUNged/CbF1npHiGXtB/f77KUZpBCcPrIkZdFuNH0YIGcti777rboWNrdPtJQ3//whcA+MlHnnzdz83rubL79al8rX0HDPh2GIjbDzGvNaLmW+Uza1tUTR8xdHDCiM/vrlF2ZYRpCMU6epglRRrJWkd0FDLkqPpNgp6KYhf6olZYJthbQtOOoD2wjDfRxD73HkTNR1T64bfQkSF3EWXCQsxVGNItEnMXmYvLjDTmuKKtsLkzRrc1znG+SCC7vLJzjK80H0CSQgo5j0zK44tlFdkHQ0pRMX1cTaALJPQOO6HPV+yA02mXjtNiW1TJKqNU/eM4chVdgyj0MKMCbizP0XyOut2m0opwBIUgsmmLsOFrzMZ7SKGFaSnEMgGFXIczqshW26TqmVjeEunkaTKZVzsX3/5D/3C22sbKIkYUMjm6gq45NPVfRGtdRTv4OohxctwADmibXWQbdtshVfuAhJAFwJY0ckyzmKkA4EpptKCBGrSx5SLLLnRaV28VZFfc/mgdmaHvyOdjwIC3koG4DXhDbs+SvH3q9uHImMMJ10OaxIoZ4IQRIpDI2uzuq1CPE9JgtzvGgTlKhwgp5lENxmh1kuyaeWR3HH3o1axLrxbHPhhHTjiEjgKBiGB4BJaCnLSJIgEx5qIIkDeT/NTJEHDZacBx518Rdw3OexuUD47Ts0QAJCkkk/aZOvE3FGK7DBlthNBlVLPYlDb4qudR0yRSgkwoZjlvnPbRogAAIABJREFUFjgIuhyRExQ67+O/1F/CD3ZZyM1SrtaYSVZR4zVyaoK610aKLI7oGSwEdtwyRBDFR4icfYQooN3N8C92kuN5EdPcIIpanLcjIvmAWHybjd4IHRVwrvI4G8Cdjqa0oVFiidix/trXYfgwoabpGl9jP+qx4R+giAqKJhGLR6SjLFZoMSSMkaGfyLOpzAPQciBndZlPZuloM9zdY+TM0XEAXlrti9z8bOqO694dtnw9V3a3Qxs4tgHfbQbi9kPI7QJ1+/bSdL+X4+Gk6m+Wn12c5iHT5zNrW8iiQCo7zE7bwHVBco7jaauIasC4+iBK0sRz48QxqA9ZTHQOSLRm2T6q4ea+hlePE+1LhJIDI8uETQPBPIactNHHm7jV4yhlm9jMeSZUlTltGoCdkWXKVY2sFCevlXBth8AVKBrrDEsCFXcRRYZOR+GxlExCd/i8H6PrjVHApEyHumwTU0RShkUzsCn5cIYZ1njp1r+10hlCF5PIqX7ITtd95vQcM6xwqRLR8HV0NaS7V6MrgZWIo2o+Qgi1vRBCl04QYQsTjImg+1UAhrsteuoYL5Y2AVhYfFUExmb6a3GH4nI4tHRkfIq2t48hT6C3VAy5RSLWI6WkSRqnqbjXKaoumbH+ve8L/QLsRFSjq05STjzad4g6d5z/ULxeYveb+hy83vSAAQPeKgbiNuCb4o2mbi/EZV7UJL5ad4nLPlKxjQ4I1RStrgqqxbX4MO5mCq92cwSLsUxoKwQ99VZmZehKiFo/USX0JBi+Aj0bIeYyvFSidfFR8orIZJRi43KSZxoFnnhHlYlRi3JVoyRtYKf3kYUMptrDCwXaJ/5PoiDDlvU+oMArwjlSsXP45hSx2CI/Ew+olV9gy3fQdJWRnMxIYoZ6TaSVf5GYX+DgyimutC+Sz3j86IMf5Vqn74bYW0K1r2Hnz+FLKkOqjR3WEUSfvD/KCzceQQnbzBdsdNNg2btGMg7vT88QiiqCPknbqpARt1nS4Bs3B4bOHTxNd6fHpvxvWdtd6j8uu98qbEnrF74fKA7x4pMsabCowFavH3aMeg+j6z/OWe0KALFi3/Gt7H4GgMfyM8Abt+A6XGN7o0ST26cHHG5n7Ktk7KuDNbYBbzkDcfsh5G6BOnRsCn0XkqTfbeTNOLjDc9x+7kxumHjkYoURIf22WzKQco+hBl3a5X7YUcmb/Kh9AdpQIQ+xFnMtF8hwTR/F97cJTRn2lyCpMj9xhciX2L0wRjJUWIyrGPWHaO/HKU99jTWnybU9n4RxiXZ2C8+UqIc+Xc+m0z5OQkkR0wKGlA10LcB3q0SOzKPRAhW1wUVTwgnjiJFFFMgEvTTD+inQKuxbZVJKinpdwzAUXE/i089X+UblOulojH+zeI3K9QZJt8CwHNBzHMphiD4yh2ZorDbqjDLBZLGNkB5hb+0ivlMgq766ziaFNi9tN3kxukBPFhkO8lzaeoVqN8dJ78/5qvyT9IwRCgd9sdpM9ccsCM7NKvnFcQy/wlSsiCulIdlkOrP8ah/Ju9672wVmY6Vv2w7X994Mdx9z+/SAw+3Daw8Y8FYzELcB3xJ3C+TtHInJfL7qEET9hlJBJcWwJpIsBlzYSuP2FLyWgVuL4yVjCHJI4N38IX/zdKLmIwYBYWINGXCkVXqZHfBFcNqkOwmulmS0+gNMZa7QLh/wl1cWqGovMz9WJ67YxIVhevoe3sgyRzMyk0xACEbmWdJ6AzEbsuUusB8UaTXKtP0Cpp9CDmMUtDSYKu0gz+xojKn4BFubcbIxnVTv31DZ0riavEzXmidtTLLd+SwlL06ileHrwUW6UYAmqezUVVIFk6rqIzffhdscY2GozXtO6YxbP8ayC9tmiUUFpmIJLlsKjTAgHi/xEJsowPUwIBnbZsJ9iedrE2i+RDLzam3bUt/IkbUuM24kaGtjyKFJV53Cg1sNkg+d1+F4mq/fdHCPjn/gjvcveXNiN9zprt5saUBXfbXv5OG1B45twFvNQNx+iDkUqEOH9q04tmvriXteE4EfHwr4xwMbSYBAFJiPy3zusord0hGAk/FrIEVUekWkhEMxViXyJD7nP4yS79+XbPWLxWfS6zSDMgYekh4wniyRFXKs7C2ST/jEdB/NmkcLJcaHX2Es26br9zBki47epSS6yPo2u2KbXJRHjDJsUWHYTPPQ8WksNcGnXwiYKzocCUzKHYvx2HEKBZtpeYOe7dBVM+x7NSiI4PqU/ANMYxU37lLzWny6/jIpI81sc5QozJFUR8mPNNA9HbF7jKOjHoZmYwP/sPtpyo1JTrRPsZxcgQwMtWR2rT1kOSIeZvGcKpWgQyZUOSlaaIpFQThDpyRhGwkeHhpj+qao6TfX4rCa9JQROtoMSWcDT0reISaHomUKedrN5+l4Xezuw2x0dcyOxL73CqWXQvTAJT/i0XmNMOShYzM70h3bM4v3Tg8Y1MoN+F4xELcB3xXem9eY0CVu9HyOTPrc6NnI1xX0YpdjsevMRmVuNOaQAh9BDJnO9pMQlOg4CuABUsLB3ThLYkQjtGOMhz0KOY+soZFOjzMtOqST59nYiBCCkNNTu/TiIYE9ylZ0mbbUIK1JzHpnCexTeMlPMiwG1Epj5NNjyMNr7FT2aOsnqOs9hjM6cisgq8Zo+lVwXIb9IezAhxS8bb7veLY2HYZcGyfVI3QPoKKTCGPIskNDvUjKD8gHj+OLLyOm6jwxdRY57EKhxOyxHf7D5RfZPHgMgKJ6hMWMR3WnxqanMxXJTAoam7UxXoxErqoOBa3FtJZnq72Lliyjxvq/UFRKCsUx747nHvP28cV73TRwq53XSm2DjtclpSSQ5QIl+wqWJ5IX+jV0Hj3k0EN/DQc3FPXDouucet3PQKnbP34gcAO+FwzEbcAtvpksydeqjbv9+4c9Jg9JFnq4UYTmB4jBFEZmBrOxgd9N8BX1kX5xtiDjmhph28DXPJamlxGVgG1vmBPZbYazAhVrBl2dY3bqCvWGiuuJuJ7IZfc43d4pZDFCmOvRE0K2KikUKWIhIRDHwMLFK17DiYdktAgxtIisNjMJmJZhoyeTS/hM6xXGtTRRrUjbL+Ppz5IxchSH/zVblCkUHOyky179MpnRbWLOg1SrGrvGCtnhDmZjnO6ezofeu8zCXJv1qyJXuyW+9o02l9em6dkRz7r/Qq2dwT4I6YYQJTOM2DLx3gEZUyATVyhrLUwiXq56HNReZD51gOS+m96axq5Z5n0zDvHNTVqZeW522yLmlegpY/ek3StBh1L9edzmNoLfxWWUWPJfMGQDeC9HtRnGZhw2m01AYfqmGN69VneYtXmw2Q+N3r5Odxj6NIU8lr91S+AOQ6cDBrxVDMRtwDfF7cNJ1zbj7OwZTIxab3jcQlzmtx+5yq7tk+wIiF2Pl1uXKKbPs9OeoJLQiDyJB4xVtKGAy5cepVJJ4AYJ1EIXoXGSiiQBGyxfyTEtp3nBeyeqEpJXV4GISztnaLQUHnrsr5lV86SFFOeEMh2vnyhjbf0C/7HV4KNHn+GhdIghp7jSNPHkGD82nObY0Bme3bmBFZioiQg9LnJuY4daVEdw+uf4kQcaMLLHF5YtXtzUsNw4kZOhEfbo4OF3J5iebNOl1n9OEw8QpKeZPbZJpxbx4otV9u1FZMsiK9Zw1QuEgkHWiJgQ10i741yv1rFsnYd0GbVbYDcKKDsiGbXJUdmmZl7hpPjPEPo8eGkNV0ngx77C+vwH6cVGbj3zjH31vmtcb88dxXIbXPdAlQ0m42N0nOMs21c4cCAp9Z3f3cfeXcd26ODg1Z6TSXeDbbNETUhj+z1i3j5Nr8SmnmY+OTVYexvwljEQtwHfFre333ojpgyJKUNCyXYplUVyVpy93gg78jBJoYnrygiCiqhbzI+8zJiYIEYPzXd47MhFzLbClcYMqhLgugLdrkK5onF6UiYeD2gal/BkkQNrhpW9RdRUneGpELVxDKvT/wF8YmKFn5vLMZO+ykrQZdeP8CKflOJg5VeIJQXcMMFuS6fWUdnwV8gISfzuKOVugz/71N+w1xjDdUrEgxYFLYHfyOBrB6iZDuvXf46NVYfp1D6zsyZfWM7zharLhxdSVOPPkR+r8OHRJ9h8OWTOklgtXmRKfQW5torgR4x7XbKxBHq6gqZYrDspZA/eM7TPlFdntzlOgn3+m9HLACw7gNfEtwTsnX9kPj1GJz5LM7HAcPfrJJ0NttP9hJGMfZVi8Uma+jF6lc+gmiUy2UfJp4+RL9rQcihtaCxpi4zNOPc4ttu53tnCz9eYS96ZhXkY+uy5/e0xJUZCiUPwnemCM2DAm2UgbgPeFIf9JbPDXUplnR12sGyJsejYm27jtbzZr9OKj3yKsq8xVHiY1eopPDfgWOEKu/U4K+0jmI5FQlohnbdRQgukkKpxjSDUGE5nUdImVtPDCARiSY01INs6QSJxiYfGn+GJgsvnLn6A5/w6QTmP99JH8bIu5RMfwwhNLtfezTcaMpPZTZKRyV5rnE/th9i7+0ymJrECh5dKVWQhIh8VKERZ5MYYMVljx3S4tptiZLgCN0Nt6+Iukp9Ac4exLBlbKAOQU1JU7S0sr8Wye5I10eTcxnt5VFFQggNWGk+yJ53kQfl/BzyQ4cfyB9CpIig+EfD/doY4AzyV7oBv8ilfue+zld2IWLtMPmiTqa/TnZ2mFxt7zfdi2+xP4Z5Jv5pFaXod9GCVLSfOgTN+q6j7kNvDnL4Up6PO3Df0eSyWB2psey2ieF88M/ZVPAaObcBbx0DcBtyX28OP39Lxm/1ZbxOj1q1z7Oz167EWX42cIXZ1hgCvaaAjsjB/mRvhGs+/chyhfoq3nXyehQxUrDkKSYmEYlJuidTqCrIMybhPTbpBAxD1Jt38NS6mOow+WOS4C5W6TirrMv6+/0hMCBnn3ey78+xH/ev7UYmKO8ONTpr5kQ4TWpsd5wBN3yMjxZjp/CxZpUQq3cXu6MxIj5FgiMuNkN3mE/zM4jXC/FUiHmdjF6YXvsF7H2uxstPhG+UU01mPprjDM9s+VV/kwa7FWP4CXkZAEldwD7po4gGNZAAEPB0IEEj8LAqIEblYnbXAZ9kUWfJUPpzeB2A5DEAOOSfTL48AIjfElwUEyWR+9x/YGP8QzdT8fUOBmeyjd7xfpQ0NK3SIOSKiIFLf0EDX76l7W974cwyvgh6fxxQ6t4Tx7jW1Qwd3d0uvAQPeKgbiNuB1OXRlWavfSqlRvoxBv99k/zX/dR3bjrvF+VKZYWGGr5rLmOoOWyWVopBnYXQZbTpDvaGy7Aic0CMUaYOMJPL4yAZ6UuAdiy4Pz7h8gAXWnGt8fvcCQXeBTDrH7JRJ1bRotducnPx/MONXUeMmrhaSM/6Jj8R9NhrH+JeYz57xLPO8F9tp00tdZDG2jrp2hB1Bw7WzjBtjvG3+gFeu7+Mk2kzlLYa1CE9fx6GKrhTYa6gEvoGiQKerUA32aLDKuOriKRnWjTXiisKHYj9BFHyOPalFQpBRFY0tZ4txcZ/hTI5syme/PYLUbHG09yWOTK/RAiIdnvcDUMN+cogj88F0lWUvvCVg90UOIYoQRJA9h4qZotNzOWb9FVfnn6KSffDWrre7tNu3lzTYcqpEIkypBQQNiK5w+3raIZZSxFenSLhbJKL7O7jDWXSd1lU2WlcHGZMD3nIG4jbgDg4dmxn2Eyga0QoAWe7tE1hQV27+7c7GyjvRCpYtYdOjUw1Y3WtQN1Ra7Ri2LtFoKRBoLIz2s+1aHYWmcgMxtUmv7VDxUoynmniJHf7z5hgvVSc4W4yzExm4noSh+4xMmQwXHBzBJ9+cYmR4mT3pgLgERT1JYMN+VOboeIILQ2tYzSI5YRsnEiiXT6IkNerbx1l2jjOZXCPm77O1meTlnS5HR8qkYzZWoBAkSjjATPQUs9rz1I3P88LVRyl1H0Gf/SIv9/KYTY/x2EVmp3ZZtdv8SUklEQgcSbRwVI+E+gAfje1iq5MowR6zToIj5jVeVp4jVA2K7ghfcxyKcoVeINCxEvyXdpaLWokZT2TY7/83XSaAMGBJgyUREAUQIxBvil8ockaAldYIdS1HUb9OvF3i2ujP3fPeHfZ/jJSTJN0NFkfBcqFZCtG1kPzMnTPlapXPsG2WcN0G47EhKt2L2F6FM5M/OQg1Dvi+ZCBuA16XW5mQ+/0+jw9MT9zz2v1CTxVhg7KwBlGPptalo16gmx1GChK0RQFLaLLjphguGCSMS6x1q3jKAbpc4biUIilBqzmNoe2QyDzPqpil6meQUdAyLVBXKFcXMRtJSKzzYhDSDXIsqB4LegRGCh2HRqPFe0dGefLE27lwsM/+dZOoq/DQeJytYId4UCOr7FHy0owV1zhqXSOjH7Bh94j8iKnMdbodhXL6z4iOtEgeGAShwHRujZmpXUb1OrtEnMgeUFAMVDXBWKaEJ+SZD6AneWxHOs0wxWqQIe52GW0HxNslIkej2syxIahci1+hEwWYkYAZCtzApSSEzHAfxyaH/Ur5UOhv+yL4IhvNKYzQYL82RizZ5nn7GJfNxzleWCZfu0Qtf4oNub+QllD6jry/HtZ/D/PpYxi1dRygqb/21G3DO8DwfaTQRgk6d7i1Q6F7LYc4cHAD3ioG4vZDxJtJ/DhM8W/ctV7Wus2lHXYyub0X5c6eQdVdZG7aZI4JntkAM9wmnrHpmBladPGUOlLwakeTclVjWICYETDpFdG9EaTMBYaKIQcVkaQ0RHrUglSLSmBB1Ka3PYofScyPOWTUFRiFTadMFdj3NHTRIyc0GVeb1FtQHrrIcFQhY+nIUZacug2jf4OXEDg5NYnktJEFi05NIts7x/FYh0hIsB22CD2YlKClxYm765TbaR7VZLzTX2Az6CFGPaY1j+kRl81eD8FNk+4WOa5Weabr8Xxg8isM8Xz7AGXWoiA9Qcwt8vmDZ1C9HtXuPJanUJF3SCg9hsIYu35IOYTQTtAzk3QCDXLXAXiPJgESWBIoActB37EteSpoARkNnJu/i4iA2UnSjpI8cuF/RYhCjmx9mtKR99JOziAHJjnrEvNCCVdK01X7afqTo/f2odxoXQV1BkXIYzWfp+J3KWTO3EwcGTDg+5M3JW6CIGSAPwVO0e+D+4tRFD373byxAd9fTIxaxNnC5I1r2gAmhEUQwVC+TN4w6binuWbPAPB4qsiEsHgrwcTOSHRMlROpkHTqZVY6Elu1KY6ITRzXINE9SkFYxwn2ybrDJIVTfCjZJJ7c4lOdPZSmihFJJOM9KlHEil/g3cl5ho5+jlhXJikUuX4tSW1viclsEm+iRCD6+MQ4uzBJb3ubltwgT5cnUipS5KK7Mo6vInd9xvMuI2IPo6fTEq+hJCxGejp1R2HPL5OtSYwQYCTqmIGP4e6RDFTkyCJCpBqWmG4n6CZMQt9iP6mTdTeI06Kbr7OaukFKgHFzhMtiC12MSJoFRF8A4zZfrPSTSAhvujmh3zh5efs0RrzL4mi/lddGc4rIizOSKKHq/8BaY5hzQYwT2cuErTW6epHMayzhHfaCfD0spUhHncHrd9+67/6HDm3g2AZ8r3izzu3fA5+JouhnBEFQgdh38Z4GfId5rfltdzu4OwaRchL2fVpq38EpdJHp3RS4KTos3OHY7nfuOSZ4vgOmXkHVDFJuE0toQr7OXLIf3uzKn6UXlpjSH+XsjEXF8ZhojnFm4kHsxh7LLfDVEpErEQ+LOGrATPI6W9clnJ5KOxey0W3QFdoIKmRE8Ejyu5sr/JRsko0Ns7mvE1hrnDcusO0PMx8OkfBN1pUX6TSO80QUY2ruAGNojnO7PeLuOqIiIaIQCAaRliQSPOJpASMSKHYneUd6nGBihet1mSE/z5IGqSGN/68LjW6KaWeejHCdc9YmN4Q0ujXOamOHRF5GT4wRKRq+20O0KswIIouSyFKsyralIfkKI16KtNgjGUosdRZZLFwHMQARlgUPAoGmLYMc0ExWEKWARREQQ5KJFo5jYMS6CNk41AVkv8dq5HFRG2cYUIMWtlygbpxG92vofo1K/JH7fn5uF6p48clXhepmOHLAd4/z588PybJ8aCxeJ6voh5IQuOT7/v9w9uzZg7tffENxEwQhDbwb+O8BoihyAfc7fJMDvo+Js4VMD/mma4uzdetrRtGouov3HNPa2yE0S4ymQkakYZqhy1BiD9eZh9oYz9T6dXI5VUK3Zngi9QIFTFbbMQyxSUbaYN2Oc8N4HiFcY4wiTSdJQq6Q5gCndxxd7vHjxghb+jW+YjuE6giPGTmSos6LhX8CFtncj7G6PU4WH0lr07NktrdjKFNNNisqvWaGo0NjoO6wd5Bma6tAUfXIqD16QYqc+y7Mywqd5Bc4cjTkTGKIq69kOGdDu5zEDGapSSdYKVno1Yhq2mZ0dIgb1cc4HrVQBYeC9EGGtTS6u4chK4wrBXaAqvEsQlRhz5foSD4rfoRjZZDMEbpegtHsAZpiQWj0XRv019kEQIzYbvUnZcckQIbPdbIEocQHJ58H4EZzCtnzCB96nAd9m+f0J/EljYQCR9V4f3LAt/G5eDOJJAPH9u0hy/KfjoyMHC8Wiw1RFKPv9f18PxGGoVCpVE7s7+//KfDhu19/M85tFqgAfyEIwhngPPA/RlF0x6/9giD8EvBLAMX88Ld94wO+c7zRgNG79zss2O4nj0xgYt0StB4TePTXzUymSBRgrmDec+7WXv+c8/IIETZXojaJpEw+PkbVXWRV/As8VaJmxlCUkJf1Z3m5ArqTxnZFXtgy2Y3WaYcyOSWBLbo4sQpmBxIHp8i5s+T1LTbKKWCUMPscdS2gk55i6swneVzMMbp3gmcOurzi7KG6c+jtD9JM/CMXzBWEa8cY5jgxp8in6rvU25u8I5HGispUvA5VzyKKqrxL0okkhziw3nkQNZpAT7xEJShTb4wxLtmQgk44yqZX4mxsmA/NpZib6LCzbtK6/E4E971kjl3i4WS/AFsOQeheY9q16QpJELsgRmwIATPZA8ZjHue1NnW5x4lkncVQAU8CwwMxYkkUIRQoqR6uozNhDmHEumxJDp6jQbJ/EVkKGG6/RHPjn1ib+Sl2YotY5hYr9NP3l26OgXstx3Y3M+lj1CqfoVbZIF/8wBvu37u6DkDs2Gsnpwx4Q04NhO3+iKIYFYvF1v7+/n07eL8ZcZOBh4B/F0XRc4Ig/HvgfwL+l9t3iqLoT4A/AViYPTZ4I36AOGyoHGfrlrDBnQklBbUfnnx1FM5Z4CzvjJ+n6v0ju7LPeOb9VLGAZQynjd1UIX4DRQ8odyawLJETiS0Ooh5/Xz9LEI5StBNUQ5nhoRtYTkBv/0cRhBO09BsMRy0gRdMu8mh4DDd+mYx2GesgjRifYNWpEo/JZAQHI3CY0De4Im0g+SKyWyYdE9mV9mhaPQQliT07it28hhda2FYe3+yRTDfRkhFHjRE6jX2S9fPEDJs5bZrZQpr9vREa3Sly4nESwy8B61xbTcF0m51ApL773xGT28ihyXSn/99tNw4P+C3mxDhXJYsDIhZFiaqbR5I9Nr0EFTNPGGXYcFIs5XbB6SeRoPv9NTdLZsw4AANW9gy00CITK2OoUr8llyNyJraHlQtouhGJzjakzjISn6Ll1Ch1N1i6OUR00O/x+xpxIGyvzc1nc99w7ZsRtx1gJ4qi525uP01f3Ab8V8Ybtcc6rHHLDndvbvfDkHPawhtODJgYtUhj3mqmXKlpFPPOq69LRY5oC7euYdcfo1vTSMf/mZ4LJ5un6Zoy8sQeciCRdxWS1hTbB3lEaY+JYQU5TDGTH+bRUwdcfBFa7gTtcJpEvE25O0MnatDVv0qhPoptT7HptckpY8zYO6zwMutdGTchMiGNIRshV91/Rg7GSCqPMCRJaH6VTVehqOQwewJyYHLFVZFklWmxQIZXuOrUUFQdX2rSlEpsKyHVdhFV2EEIYui+xWcPzrO83mBWLLIW/xKnNYtqucCl0o9QHPPImTfw2h2uHexjZa/Rln0aocgDkswroktb7jGa26XQLVCx8iyz1e8A4kl92yeHEAr9LElgBXAsA3wJ5KD/1ZOIRAEhCDkVZbAm/jXxm2UAcSUJcE/h9etRq/QHm9pO5Y7t+zm4Q8cWWd07tgcObsBbyRuKWxRF+4IgbAuCsBhF0QrwJPDKd//WBny/cbfA3T3c9FDY+kQI6X9gI3So9CbIyB4x5wvEgRHtSXbYYSoPduNB2l2FXHabslzjUmOEjudxJGZixDbZbgTozUW01R8hHuvX2u1cWGTrSupmoFTg3bN/Tzzu8Z/2JhjVxzia0Kg0W2x5EXgNHs6uEWk7yIZMQTE5qZUx0dlteyxoBrOZLE3vKsWgxZ6TwK4tIMvLJBMeQmGCSlXHGLZQ5DTb1T0ks0lRadASNB6YOw1AqnaDC5ZDMt5iNKPx0gsh6/Zj7JtZcpU9lPIQ5zoPcTJlIvS2MMMZjHSW+dBkrOuzqdd5We4hhAqmlcANuJNba25ifxn9cNuRWPRvuumb6YtLogQaBEpIJz7B5SP/lkp+idLNyduHk7gPBap4M6X/bgd3+/ZhL8qifP/elm9EjhvotjNwhwPeMt5stuS/A/7qZqbkDeAXvnu3NOB7xWFN27faV3Jnz8CubeF5cH3jIYK4gapGIN2772EB+M7+B9EVODr2v1Gvd/Cac3Qlm8v+KG4YZ2rCZFqfYkqaYytYodFS0UKNo0dMxoZtGi2FjrEOcR/V1+nIOZ7vlBhP7PNIOk6zNUs+FeNocpJ02mOn3MS22xTDBB9IDnEsbbPvf4Ge0CGTSnGkIpCQGzyvKQhGil7gEHS3uLRjIhsKk0qWPae/JifENIaMBBlJoFLpYjp71DsmJIdpGlukdYcgSlJDJoytcXxonfHmBBeCD5MPrzAcXwMhRmL9FRSpSiSIzLspVl/5MTYcUBIowuggAAAgAElEQVQHvLvQIGNOsQHMZLb6HUluFm9vmOM0q0UAjHgXS7nZuksCbtZ5pyubcKT/zMcSM8CrhdXfDIe9KHPdr/evV/wAGys6nTr39J+MHZslY1/F3jigzhFix2bRbeeecw74r5ff+I3fGEskEsHv/u7vlr9b13j66adTv/VbvzUVhiE///M/X/293/u9/W/m+DclblEULQMPf0t3OOB7wt2u6rvB7Y4NwLIlfE/A7EkockiyfZRZ5zJO0aZlj2Huv69/4LT5ajPlkWUyygaOmEdqv4MHMh7XvRIX/BOknBMc1V3OzjeZm66ytjnFjmSwNN2PkJerGqNxqMoJdmPLJJIRmixQU1RMo4OXrOIQseFDxfFJ9XrccEIcR2I0khAFm4NuDyWUmFJjhOvD+M42Cb0LEXScDBulEqrUZUEFwW4yKbaxvQ7VUGDroIvgfIaGrOENn0GVRXouvFK5yraWRLWz+F6ALeZoJOtkosskHQFGl7GsDbo7Gju6T0pWSPRSZIQmHjbR0Esc6abJxxqMJPqhPccyWDaAKGJJEsGTWNlbpNnKMZyo4CsCyfoiqXwNtAO2a5NEwFCyx3j1C1TyS/fUnuXT/UQS7zUcmxJ0uN7ZQg2+Tty3OIi/ja1ePyx5b37s/Tl0bIcz4AbrewPeDL7v8+u//utTn/3sZ1ePHDninTlz5vhHPvKR5tmzZ+03PrrPoEPJDxmv5cpuF8PbX3uzIhm0N4gFGtelFVxdYjSmkFNvsOru0ag1yERT5HOwE62ws9Ev8p6bNlka/TIaVQw3xqnZTUTBpd5u8PjbavzszJ9Srmp8uTvDzgawv4RlS1i2RLsrU2+oHF/ocClsowG5lIou3CChyuwHXVbrTY5nTFQlg+e5VFoihaCIICUpuSGq5dPuDiF6s2SUIl3WEfwjNLUJMladtHgaJX4eKDIT66B7Po5T52hC4REjzuc7AQk1xkLMwMuPUxg5gdjdx9l5jrb4fmaVx/DaWyzJJZJGgtByEBIpFsdWqJQUzsenSBWu0xVlxH045mcQQom83OLJ4RYAEgEzuS1K5hBr1hBj8j4brSkcy8D0YnTcJK9YNinZ50Nqh76ni7iEQyhKvFfS+lO6v010v0bC3SKTnKdaVuhdXSfZE9kPT7Cx0l/Lm1m07xBGZTKF7tfA2cCWB91M3iq+tHKQeGG9nnhkNtd97+LQt1PtcYs//uM/zn/84x8fFgSB48ePW0eOHLllxf/oj/6o8Bd/8RdFz/OEmZkZ5+mnn15PJpPhn//5n2d///d/f0wUxSiZTAbnzp1bOXfunP4Lv/ALs57nCWEY8rd/+7drp0+fvsfWf/nLX45PT087J06ccAF++qd/uv70009nzp49+6bd20DcfsC4X2ss+PYd3O2p/nen/c9Nm6xehEpNx5UlhpI7HE/XScW67CXaaHaWaVHm9PQyOP3w5U60QtxZQQpL+Lhc99sE1JniCDn3QVAWgRVmChssJh1W9hZZml7GsiW6LQdDtYjH0lxb76LH49T9RfxYGk3ziYUgiSGjyQ3en5vg5Y5EKRJZ69jgCniixZZrk5e7RMI6rheSFraYz3QxwtOUWpNcCZYZL7g4aotuW+KZygjDQoXTcoqN0OdMXCKrQ0fUiNQUUn2VvJ7lnOLSUWJE7V0qsS+RXQCxeR1VivOKukVJVMgeqPQsETd9gbFMQL35BCvWJA8bX+ZnCzf6D1wL+Oz1H0FVXLqFVSqRS1uQifkiGw4gWkyld8mJHWq+hCHbLBausxdl2e5Ns6cu4Msaf2X8Io3mIjPDr/7Ce3ft2d0u6nB7svUZThlxbHmKUt1iKGpxQktzXZvmzQYZOzczMl/rWgO+s3xp5SDxP//dxZkwjIRPXtgt/N5Pn974dgXu3Llz+sc+9rHRZ5999uro6KhfLpelP/zDP7xV7/XUU081fvM3f7MK8Gu/9mtjH//4xwu//du/ffAHf/AHo5/73OdWZ2dnvWq1KgF84hOfKP7qr/5q+Vd+5Vfqtm0Lvu/f95rb29vq+Pj4rXrqiYkJ97nnnkvcd+fXYCBuPyTc3e3/cPtwDtfdYnjI4ffvngBQUFdIYt0Szb2kQyVrIeBA3OMLxjIiLo/oOfwQngtf5LlXciSjDDEjYNuLs7/WJhk9wokjFcTOPh4advA4OWODnPZZLlTWMKQ6ZnDA3PA6ljhLuaqRU8co5h2aPegMr2LvHOFdeYvMZI7/dBChqsssxVT0WMgzzT227IiikiSZSWNZNdTIQQgDekIaVQvB9fDFHJXAZtaJMa9Psd18G71mkSC2D56E6VhU/SJ1XSNM1ACfOUNnzYcVFxZSWQD2u21s/+fJ6n2RKsbGyPZkzMAiZA85tNl3HFDB9VtcdSZpdbaoGHucL84xL25zNvARHImZzBZ7YZ611jSK6ONFEdUgRUyzGYsfsGf3iNIiY66CYxssO/BMGKGPPMrVqB84jLQQ7CvM3Gd0zTdDTxnBl+L4IuRnNJr6LAcrOnGCO9bc7peMcvv2gO8uL6zXE2EYCSlD8duWJ7+wXk98u+L22c9+NvUTP/ETjdHRUR9geHj4jnSn8+fPG7/zO78z3ul0JNM0pSeeeKIF8PDDD3efeuqpmY985CONp556qgHwzne+0/zYxz42urOzo370ox9t3M+1facYiNsPGIdic69ju/YaR7w+pXI/5GTZEpWaSn2zgGVL1Ka22dgNSPhLt/admexRrQU0nDHKqGT0Mo1eil44RNmoYdlw+uYnrhjN0O7Nc3ErjsEl3mWo1O1xLhws8eTUc4ykdml4HSTBRRJAo46KTmTFqakVUhmTF18+g5zP8falM2SkVS43SpjRLnNqyEJCQ1YE5ChJIAW8KzPNyymLcj3PadMl5vWwyJDW2yRjMX40nedqUwZf5W2PVDmpPcm11RTPm1+gwWUe0uEB4nTdEZbSPoqucD0YJha7wEIqy/FTP8HfLf8ZG7tb7DvP0PO6OL5NqXEO10iRSR9l3FRRAocDy8SJfOKejtRYIe1epBiOIV2b4+/iP8dq3WGheAMkBxeFYXsYmYhQ3yeOyAI6M6LIzTp5JtO7kIYAGalYJDGso1X6fSdzM6/9s+ON+j5up/tp/hn7KtOZkzT1Y3S+RZEaOLa3hkdmc91PXtgttC1PFkUhemQ29x0JS74ev/RLvzT79NNPX3/nO99pffzjH88/88wzSYC//uu/3vriF78Y/9SnPpU+e/bsifPnz7/yy7/8y/XHH3/c/OQnP5n+0Ic+tPCJT3xi88Mf/vA9GU6Tk5Pu7u6ueri9s7Nzh5N7MwzE7QecV0OI98+EPPxU3S6Gt1L6R5bpSV9nTBxjb/M9VGoabz96HkcWCSQTVQhfdXTuImYksyrvoMYiHsvOk5Y1GvY6jqvwRGKY4RmHRs3j3H6DCW2bd+cn2T8IaLUVrlqPE8WPsJH+NH9nR3zUOcOUXALfZc0coeXoLGnwYAbWgn5a+tyJK2SybwcPlh2oRyWKWkAmMcSXvDw9a51jcZsSI/x9xybmbzEqw9lhgQZtIskhknViUY4Hc/0/u+UcABde7H9FgEZdoxZl8XNdNNGj0dBY+//Ze7MgR+77zvOTyAuJ+yygUBeqq6uqm81mF9kURdGkqdOWLMkT47VCWitmHNbDrL0btsPhBznCx/oKh/3giLXCL/ZMzGzEjtcezYxHPkeyxJWog6QoNVXNZnfX0dWNulBA4T4Tee8DCtUnRVKiLbGJz0s1gER2ViYK3/z9f8e31mYnCq1Bna5tYR9uc6lSodNNYNi79DwRyenjCQ51wUR0DTJCF8VpMy169F2HpAx+n4MpiewJAQQP5gdxfN4uW4czdMwQdTfDYuIVRNWi2BV5InuBfLCDJ7p8KFoFy8eqDbYsMDHxo5xZ+Knhda63gXsL10jUvhduFak7qyRfbbsx/3K8Z3mi+wc/dbbwZubcfvzHf7z90z/90yd//dd/vZTNZp1yuXxb/XO/3/fNzs5ahmEIf/VXf5WYnJy0AC5fvqy+973v7b33ve/tfelLX4pev35dqdfrzunTp40zZ84c7uzsKKurq9q9xO3pp5/uFQoF/9rampLP562//uu/TvzFX/zF9Tdy3GNxu08ZRWyjvrPXauB+LTS/w+xUn8jSy5SrKrbQJRY30X1DcVtQp2kd7LEYukEglkSvhWnZM1xzOrT7AmHPou859CjRENuE20tslIIEW2fxB05ytSaRnv8fBJLfQu7H2TnMcSZXxBr00ZzrSILHK+0wmdAe37b2MbsHnJROMZPTuHBhj0OKJLJfoD1oUHUS9LwBhq9BWxFA9IEkM+i12DZabEZCzAVDLGkqbijLFeMDdBVYXGpTjqyw2rnOnr0BwGOnkwRaDkbE4FlxC7Pf40ntPfgSOk+eifCdkoir1/F1D5Bcm3+bnOUvy+volk5AcJnzFBYJ4pdhNngS0R3wv6gCm51d1i2Pk7EYjnlAp9fi4Zki/zr4D0iOzmrpLFcaZ6hqWbLxr+O6Bv5ulHPIrLoDcFySnRmMgYYXuYbn+rk29XEscRhp+/PhV72WxW4BuNnv9loR3Fio3lq8Z3niTSskAXj00UcHv/qrv3rw1FNPnfL5fN6DDz7Yn5ubO46ifu3Xfq342GOPnU4kEvYjjzzS7Xa7IsCv/MqvTBcKBdXzPOHJJ59sP/744/pv/MZvZD/72c8mJUny0um09Xu/93sH9/o/ZVnmj//4j3c++MEPLjmOw8/8zM9UH3300dddKQljcbtvubU8vyJf4dmCflShuHjP7RbmbkZsF4X/SZ0DQpUI5VaC+dg+Ee0/4HWfgoNlylWVmWwfG5Ees7ft7wnpCaLhaZ6tDedTLnlPQAgqPZ2XGvvkwxGyYR8dO8r6gYQWfZlcZoZC9IvUfN8hGGjgCC7rwtc5EH0sDlZYVveJhC0umwOwO+h6B1GD8/MZUnM9SgcazYpLxTLo+gxcQeJcZI5eNEOxmeNgv04iVCUeBME2EAyXpXCQc34frgpnJ9dxQ5Ovei5PnW4xPd1jfd3FkyA7qeOPFHC7aYZTjD12Ki+g2QN8osmk6NARhsuBfkFjWplAiE5SU3IAdIUihtzH8OCioJFJZYg6JgdyhJcoI7vwcPYVLFFjPfa/cnnm/+AjzmeIty7R1BaxO1sIGFiOjG1LPOSEeXn556kkV45zW98tYtPt3m0/F2J3u6zf633jIchvX37xF3+x9ou/+Iu1e7326U9/uvLpT3+6cufz//RP/7R153N/8Ad/UHq9/Wof//jHWx//+Mdbb/xoh4zF7T7l2CttIHIobdNr2GCsvO4IrqeLIEg4roPjCHA0mGJaWGY6DcH0y0CArLp4tKS5SWJyeLN44wBicoFmxECrDQsbVLVIqqmSdqeoHBg0DBnLFrC6Mmu9a5AoYKGxXkoQFvpMSEWygziD7VO4aZ2MaFBhE79rU1EF/FGRK7aKcS1I0izj91o0TQkbgZ5aYcs26RnT4AzoOi0G+h6OJBH2fHRcl41OD08KcTYZx9evYE09zqXDbfZqX8W2dRQhQEAIsLM9TclKUIyUcXxxVN9DrBpgthucDAwbqD0tRbN+jYB7gtORBIqT5t39FDtmC9NWeFCAbkeijcxcapuQucvJ0AxNS2OnX0GaTqPMRHEHJs61qwBYiMjveZyQNUmDSdZS/wb1wv+JMCjzneYSqtol7BMYeCqr2m+zY72fPIO7oqxbCzvCZgGAbHB4Q9IyagSsIhMHKofCaYjefWMcG6wRNgt0lPzr+tyMGfPDwljc7lOmJ3X2vHU6joandAjGDBBWh/MiSyvsHWhMT+q3+bB1pS+w/LCBfpCD61nk7D8SUWs8JT5GsncCPeKyMrfK6vYK6wfLx1NG7kXel6eq6FyoqtQbComTkNdmOKwpbLSLTEkRHp3IU6mpwAVO+55Ap0Pb/Q5BQeKsO8FJMcde1MKJWriDKp7Ypi3X8Nkg9aPcaO/hk/Ko7rPklAFt2wHRh88UGdg2odYcNSOLYnq07EMcUyEl+rni8zERMnDQOD3x0PExb9YP8EUrGJaBkNYQRBXkHtmj1ycyAyLyLqqtkk8EOBuJAlAd7GCmMrxLOYts7iK7XUxUTik2CAaa0ABU2kzRl3Nc6+wycKDr6uR8BpKr07F0JsMLyOc+RbR5jf8qB7HVNk8lP0dHzWPh59qpTzHReJFqaI6Ks0+wZxBOnGNqcup1fSYWwnkAOkc1JkE5TNgEXmWxp9AaClvX6tETOuMIbsybTqlUEt/97nffNRPgK1/5yno2m71zCN0bYixu9ykLcz32Ouv0GkNLlXC8B+I6eyZMs/Ka749HLbygjem5qLJLOmlguMfFS0On7RJscbSkaWyyc81iyp3nujzsXakMbtCQZEhDqQVCp0xP3qXTWaCKTFBU2N7zs6SugG2gZVcJhAr06xOo3fOU5q7ialcRxSLNrsTh/jtZONfknXaSXVvhPScWmE6+g7VXSgz0NvP+K6zZFqK3wEwkQLnpUWwMwMoxkwwwGEgk5R3mwio/kht+owtWHwB5/wVO2xUQDTb6LWoNh4yssK3WmPVnWN/8ESrWBUJOn5QeIdeZodjp4ItV2GiX2Am+n0hoBbrPsRCG+sSDyOU1BKuFLk8QiTlYmoXk9vDbFRSnyUCI33bOd3tFCOZYmPswdu05NLtC5WCOVWOfdM4iOfuv0CaWGZSew+j5yEWfYFI9Sy5vEPDfrlC3NlOHzB3CRmHo36bMEjZrBMwiXuc9qEyy0bqM4RXor+U5FE4fN2OHzQKS00Ny+4TMHUJekPWjTMtY4Ma8GWSzWWdtbe2fZVbxWNzuY6aVWYhr7HhN/D4NjpYIe7pEPGoB0GjJrMytHrltB4EgO8kLJJLgl56gWPZjejWasSK90k9S3b7p6N1oDdcqF+Z67B1otNpdiNygIjhQXaIRlDEtgXjE5sA4RPB0ItEO09EIimPT4DJ3TnWLEce07/YD9PlTlM99g75bJYqGEA/SsXU+/5WXUcU0E+Y78GvXmZX85PzvZfFRi+uNMuVvDaA5h6HU0cUSFcflRqdDqjKMxzz5ChudJst+mQs9C8G1+HajQl+QkNQeBX24vFu2hmY/PTeFZE1geTqOTyCZChD2z/Ck+wmao4PtPoftC45WcpEwEF2bgFVEcnUeCERxBYW6N+wZPBHKkVbzbHUK7PaKTHS/iWxW8TkNDKuIag6YbgVQw8Ntti0QPGg5UDbg0IC8/96fgWudHQJWiRORhePnFsJ5wgasdyDGNlGhxKF3d5P/Qjh/PILLFoMshPPHUd+YMT/sjMXtPmZBXWQhD8921oHAccP29quvJlIs+6mZCsnY8BY9lxkw4TPu2mZERb7CZ7/tUmp7BLQ4HemAXl8k0XiIc1M6O/saE/3rWIbOnGCguRZiaBu7E6XcP4Hf76FIHlfVf6S1LaNFT2GKAmr2RfS+jxee/99InH6R0ML/TaqSJR+ymPSimNIkG3vX2G1LfHChRqjuQ7ESLMsD+jWNb39tFljghCfywmGdSq+PbM4yKc1T8X2Tf+x3WcqEWBIVqvUwSvg0O8oGwqCJ5MyTUNrE0x6Z7gJz0UWE8B5Un6a/f56wcI3O4DJ/s1nErrk4s8s8eWSG0PSfYlDYB2DSBJQoLiGMNvhTILtd+vKwsAQLJFfnensL21dEGewhD5rseR2aXQOFPn7PpjPwsW1oXG68iBk6AMfAMysYkV3QFCB/V9P06KctFmiLC+xGP3jbNk3/KQ5ZQ+hW6fU9akKCw0kDWAVO3dZ8bYvDiK1j3By6PF6iHPPDzljc3gZMK8MCglGOrGrapJR1pif1o+rJaToMe90MV2PJ/TlQVtkzd5hWZsmqDw93dFSMMipWiUctKnWoNxTq8jqH4j5iz8OszIF6hUxynd7mo+g+iXl/lrxj0/JMVDdIbLBAuXSOyfAVpiNd1n37dGWJvljBL9fRpSotd5jT2mhfImTsMxtOYKgyGy2obTj0Okt060+Tmf8O+90gsdhTpLUd/otdoKTf4EHtMaandSat5/ALh8S7CWZwCAUGdP1VlpQtzvcD5CKLGJKOoSkIgoySOkf3oEXSGKBFF1kMD8fq71TvPrfbnSrBynkK0eGXfbFbAGOZnP80A2fooRg+6ibMcfsOlkI5JLfHqgG2GMS1dSZCSUKSxoF+gJ8dpgQZnylzaC2AChktjW7rHAKetkAulB8KzB3N1SPxEazhNesc5c9GebfjZUdXR8Ym7m0TNpV7Fo6MI7Yxb0XG4vY24M4RWyllnZhcAG4u/21tB0kpGvpAZHM7RLsSg2SZ6dzwNaddIJMyiE5OQ3YVgKCwzDttl8yJDn/bFtnuqER8UeKpDLPJm32ZWesEQdkmqK1hBxxSgskjJ2ucNve5VLxEJ2KRCwQhAt3gFt22Sb0bRZIs/u07/4QvW9us7z/IIC0RTpvsORU6HZlEzCbpD9N1/z+CMYPFYBJNGpCRbJbi0FMGXDEAaYAuVqkr36Zo7rBgNhHFAS9VDLodmYG7y7za4kuFKUKiSSZXo91zOcEEqck9HkmuszI3yWr4BJtKmxd7l/BiBZTAFSRzgGtcZ704dCpYr4j8WFRmZc6gWMgDsJz6MgDl0NA2JmwUAI6jqQd9BTpqnm2nR7UsM6MGeWSigt1MkhmkOHBNEkGFhAK0JPa0AaJrELCKZLpFYip3Td0f0T26sQne43OxEM4jB5L05R3CosJCOP+qcyZHy57jiG3MW4WxuN3nZHkGjQOqvBOAAHssT9aR6NMneMssyRWq5jKbxjUq8hUQOqSFYZPvnrfOJOrxPo+rJEuQSRnoiXXUgQp6EsI2hMrs7C+hVh/CtHyYPqjuBfDFJaaCOfYKD9Iw1gnGRUT6GH0XV6iST6yxzQ6upKF6EqJgUPSeI6kGCJGg7pY5MLrorsRCPIZpFnEcB1eKoEVavCAV0HsCNSdAoyFxcfBtpKDKnDhL0/CwLQdNs0lJaXTTJBmLYTPA5wxwhDQTviw+XUJtTWJZOxiGSLauUt4qkz43/JUTwiYhSvitElcOrtEmgeo9z77Vw/YFqVkBbuhhBqVt+sZ7yPlP84Xqn1E02pzJRDkZniVgFWlUJQolPytzw/0WCypVY5ilq3OCJ9RvU1RPcanxKG37BVRPRsGjzRRQYcKfYCaYA7N41zUPGwVW1KFQ3SpGsVtqTu5cduwodwvbmDGvxr+En9vHPvax/DPPPBNNJpP25ubm5Tf6/rG4vQ3QmaTDsB/NJkCP2eMobsRo2n+7O/xIhIM2lZrK4KCF5m9iWxm+U7lBufEtYrECZ+JhpOwOIjcIAWEpwkzKxudP0TDrIEnEhSSBeI9tN0zPdYnYp0kqPfzJWUpdi6Brc1qbQlVcqlYN1fHTtwLIkoMeuEG3K2L4g7giLMWvsGtDsTGJIkOhOodhzJPw8viVPLOzPdrW32GWDWrGcDpQWqyiq31Kvi41s80DkSVO56ZQnDqg8CMZg4cFi+JekKYTwZg/oFaTkbuLeE6N3EyFbDLJ/sEE3TUfXn+Xmegu5xNRAmacLUskQZY9vUmz18dwW/g8lWKvQ3EnwYeiEu/MrvPlkoaDQUJ/hbr+CkUpRYIZJrxhT9tu9IOsvhBlSpYJxRxudB8gwXDS0Jl39BjsprAI4s9PsWrAoLHNTDBHMj2c/djk9n62rU4BzALJI7EKmwVig7uju5GY3StiezXGEduYfyk+9alPVX/5l3/58Od+7ue+p6nfY3G7zxjNjvwRdQcAP8OhArdGcCOhg7utcFZUMIB9wpieRNZKEpRt9sXL1H0HcEsEB1Aw6+SVBA9PJNmJHaAjolYzMGEQSV6G+km0o49ZtqOi+R2mH36eC5diUFphZW6V2Yl13qE7SMIsEU9Dcg5Jexaiz2bKDmJbU8g+kZlglI47iZC5jhEcLqkuT77EB5Z6uKFJlrtLXDmweaV3DcXvcDb6GDuNXb6l7xGVAqQ6j9HdStDy75CLl2hVXJ71FHTTYlYRCDhFdoPQE/47lr4NQpJmsEoz6CNjTRF0hjcEql2m2r3KMiYCTRQZJNVPy5FxDJtJx084ssi+ecj/s3OFmuNgOC7/1Nij1jWZ1LI8Is0z8HxcfC7EvhVF7/sYqD6i9RtIpkwvlAIgbR/QIcia8GHy/gEYN5cd7xSp0XJny+kdv76iAnfYztzK6L2jCG+0hD2O4t7CbH4xxPZzIeae6LL4gbeknxvAhz70oe76+rpyr9deD77v9Y1j3nqMIrhXY2Gux/mzTR7KX2UyvMX5s00e/9EogfQs1wJX8cWucX5eIhBSaNp1NA6QaVEw63TdPn1vQNkejorLCHk0IUC7moLSCk/npxEjeb5w4Z1cuBTDMHwYRxFWWCoS00ookosS6mFKMl9uqxQUWJrz0IICDRbQkw3qYpG9Q49aT6ffFykdaKwXl3Gic1hTj3MtESUWbLAgWXTMMkGrz+Oin1N+jbbXotlUaLcVFK/JmUAU1+dncrLP6cUij4UdTkUiLCzUkP0y5WYWEEjHG8xkDtkQbTZEm2BoG59aYsk/LK+fUoM8kEihBGN0vBgB30l+ZmkOOZ2g5V/CFRS6Zo2SA1U3TdeFTaPK6tGf9JR8meyMiT2xSMs3j9/vkp0xyc4MK1bTkxZkVym01uhZHeTQWTrKsC0AborcqglbnQJdu0/X7rPVKbDVKRxXR1piGEsM0/SfYtX4/gYow/D93+8+xrzJbH4xxN/9cp6L/2+av/vlPJtffEMeaPdi5Of27LPPbqyvr1/5sz/7s51bX//kJz/ZeOWVV66ur69fWV5e1j/zmc+kAEZ+buvr61c+//nPX4Obfm5ra2tXXn755avz8/NvaNL/G2Ecud0n3OnX9g1jWEjwI0d34t/Y/kng5gDlO0XuVpNTxddnRQ0QUmHV2GTP0zAFA8MzuVRp0KRDVNSpCy18osfAca03LogAACAASURBVKn5r9LpyPQdhXh0DV1uULHKqEkVBIOt7WX2vHXqahizPUASPdTEy+z7bpCu5jEjw4SQ6OkENXDSNUKGH1Xsk40c8LiqsqOALGe5ZtxAd7/NynSOsL9NaScBNFjOrZNPV5gPRMh3BD7X2Ie4QMyf4YY5QPfqxCc77FsG17oOF6JB/LE6zZ7OqgEvtgdMh/xMxKa4Huvh+upsNioshcK4gTS6MuxLc9UN+pJGR5hHd5rMBrP4axU004/kSOCzMA4OeZw5Vk69j9XCf+Sa1eBk6lHk+bOEzB30iouGy7mHh/nL1W2H4uAqrSTk/KexxOF1rATfMbxAt4jIRO9FQpKGKUaRnc7Q7RqOKx2PKySPHn83D+zjqsreMKUxahOZiw1/jiO4txjbz4XwHAF/zGbQlNh+LvT9Rm9jP7cxb3mC7CDRJ5LQsTG4bBS43K8STZ5lzskCWUp1CEhhFniSppmn3RuKopDbx3YU4kKMtGywUW0ACkvmAjpw4VqMG2YCzW8TCDk02xKnU1/jgcwOWd8ikwk/m0aBcH9ALTTgXfEYkcrjGNUUYU1GDVXJdTQ8dZamukdtV8d9JcHkSYvodI/L3SZGo8K3ewaCo1GoiKS8HGeTWf6uOIN+aLBnVynHqpR1naYdZOB1ycWLvDvix0ksEfFVMFoZqgc6fRNUocfedhBHmeTBfJSP54cOCH+5cZWsZ/AOucUj0QB9OcRFscUZ6wxnVINw1GZADIUe6dbnWbcqxLQM8/4oinEJAE99hFluThA55e0zAPz5KfLRwV0jsUa5rkJrjZCkHbcRYN68iV456rXbMoe1kcl7OG4XWmtgrB33q7WMoTBOCa//czISxHHP2w8hc090efm/pBg0JQTRY+6Jt6Sf25vBWNzuE0b+bHf6tX1jezhq69YZknBUQHLLtqNILsjwy7LPNDoBshLowE59QFzIYlbihPwKrepD7DTP4J+o0utJBPQHCRo+0kmTdm2LSgmmtAy0VpgKXWFh4u8JFJdp6GcgukrIJyFJ4A9HCappXjF2+Fq/z+fbOitZiMx8gLLZJdxJsVdcYDt6ke3uOnF5h740YMMeUGiWsas7RJU6q3aGdSvPfusiqXAVI5gj241xWA9jehqH0kNUnB3MioillonJHhlPxzbj2Gaac5EoDyUn2dyIUGheoCr0WFADlA2Vrmtwaf8yiCoPxdN05QB+92nqmobk6hwGH+OV6CUapkOs4+KTPHZjwwkwURTeP3GOgZTkWmeHnX6F2UCaXN5gq1PgWqdHwCphh1tokShdK/KqYjHqTTPFKAMpSUIfVkrWtbO3bfd6IrYRudBwW+/o8dw45/bWZvEDXT76J4U3M+f2g/BzezMYi9uYu4pL9oxvohPg0E3glxIUBpepsEOWEyzpHwEdvvrKcN3KLy3S6YqcsRfA8FFJfZaeV0QN2QTDKm3nFQbWHh/ObONXPb5y7RRh3w7TOYtUXOWaOaBgPotCk44bIDfVJRJI8eJ6lzkJspEdAl6JjVacdLgIFBlsy5wUBGqpFhW3g72zwLZvg6nUIX6riefJmGKCQ/LkFPjXcz2mz1xndXuFy19tcy18DdHS+UgwRT0u4ngmvl4JT9JYXAK9FuK5r/kJWxHEcIegs0PtQOQw/CSNzEWmYnEi7vsoMyzi2N5N0TfeQ9wpYFjX2a3N4Igq6Zx17GY90/o8itPCEkOY4rA5PXBLGf+WoXO9v0vaXyKYft9d16hwSxP2qOpxNOnEEofebXf2pN36XhiK5a0R4Oi5Wx+/Hl5tH2N+SFj8wJtWSAI/GD83gI9+9KPzL7zwQrjRaEiZTOahX/u1Xyv+yq/8yj1GKdwbwfO8197qDbI4f8r7k9/9D2/6fse8fraMTYLssKjOHrtrw+0R2yg/N+GrA5BVh1+qJeMZAFaN4VDhilWm4dQ5p52nXB3e2tevnSVhPEAkNBySvDjf5UKlQF28QbVnoLs656YP0d1d4kaYTyQCiF6P+mCGy66HoKWZTnyFbavNP3TLRHwag1SPrmny0ZNP8sorcR7UHsNrlRD6FdpyiJXkM6xRZXXPz0nJjx5sERKhOJjAkfeo+Q6xBImUlCVlh5gTw9SdE+SsII9mkkiL76C2Weaq/6vUqn5Odp7miXcUOJndwpP8uKFJnOiw8eyv/9sc88IzHChDX7rHozZN7Twvxr9C9+BpzkazNP2niA3WKBaGljHh3jod66sM/Dlm1RPk8sbxUmCm+xxdW+eGMaBtdUkFZ5gSdPx2jZjP46KXYt/oktHSrOQ/dVslZKG1RrFbOJ5GcmeV5J2P7+ReIvRmCNNY3N5cPnH+3AXP824btnrx4sXCuXPnXvcX+tuRixcvps6dO5e/8/lx5HafcGv0tWVssmfusPwaRbQBhl/cow/BcUP3kci1+/95+LwYISxGALjRrhB1pwgGbCKyTbsrUVevsJg9JONpUJ3H9X+ZOhX8ZJkPO5yIW8TVHtZgwHLsFRwxxDfNCJal03dBEKYZIFPTL5HwJymVNDK+Abn0Kld7XZBaiGGLbatHV5ZpBiw2bA+95yOiuFjaDRynj+U6CJ6I4g6Y9BSmbImBUkP2icSEGywlq+y3fZyQl3mlnqM8ELm+l0F2mkyfiRwL26XDbRZ/dJtHW1d5ZkunL8+RneuTkdaplnbxjoQNhoKSy6+RY5VawcAx+mRS20wmp24Tm++QQ6eH4BXoe7BtwYYDftsj4um0hBptIUhroNPf/zzLyrD/bCRsut2jZ3Vui+BGvJao3Ss39mYI0ljUxvwwMxa3+4wtY5Oa+U08b8DAjXDD6GOzw8rc7PGy4ygfVzKG+bUlNQGAdce+MtLQnVr3+lTsMqZnooVdJKGIGWhTYQO1c5LE6A2lFTJAT36evtfn6u4JpFSC/Ilv8VW7QMcL8D7rNI8FrgOX+E+NAobnIJlxLLVKY6DTNao8MAGJ5LCIKm5OEaPDhlsGoc85ZgipbaquzZWBn4TUJazp7JsuGV8A0bZRhT4dO0CrmyQmmDhqjw1J4MaVLhOdCdreMIqNRCz6fZH1g2WqoQwrc8OxYlt7LwNwPhnmrF8Aqlwq7rJmNVmSwXMLdwlK2CiwYWwjM+CBQAr7qOes6b8pJMVuAUH0o4l+NAnKdhdRANExUD2TKS2IJoJmFZHEKNvNyzQHLQRbZyBEKPV20KQgudB3b7oel+ePeasw9nMb86rcWsK/YexRM1s4XhHHNSlbJWpClill2OB/69IkQI9hu8BI1Fa3V44GKg/3+ZHwUBA/2/jP9Jwus/55Iqk+A1en03Apei8xv7DFKf8Zei6UIv+esHJIyOoS7ssEhAMKepeHSKBwmbi/z9fYQe00+HA4RsDxU7YHVAd1Ap6A6Msg+x10XWIHicrmu5juqCSSIXBSVMx1AiZMOksESdOLFwj69ggLE/gFE4Q6GVFCVFJUOhlGK+6Wp7JVncf0T/OTiz0CQGn7BD1HIpPvEo0O0wcv14bL/x1rKKyrRhifVmYlIFNqDrAcmx37A8wHe8y0Pg8Mp4scN1GnikiuTVeZZa1fu21KyLHAAZV+kTkgIocQXYmAlqJvdEmocWaCudusZmaCObpWj74F3cMYmnKC/NSrD2x4bv/zVPpFzk08Mc6NjfmhZ+znNuZ1M6tM0nWj7Fll/EKXjDJPVn0f9ypHGkVwHM+XvDcZaZKMNMl0/SfZ89bRkuvEhWVUAmSkm20q8zM9VHTMSpy4kEH0p1jz7bJln+QBqY5Mi693K2z3RLrtR8lYbbaaTZraIborEXazyJbBRuUQWVDpbVVpK3CoiHxzZ4ZM+hpSoMsl2yXjPE4uV2Otp+EGzxKSS1R6L1IVojw5978z8fwEabXA7sxV9GqH94RcpMUMi0fRWU9ts7kRIRo1jyO2/74xtKrpWUOx2+h22GkP8IITvDy4TluQcJUEqwY8qN08P8e9YkKUgKvT7tdYN4eViPeqWDQcnZqSpWbrqHaPCZ+flhBiYEHfhORRLm8udoam/xSd1hpat4CmnCDnP82rWWcXWmtU+kUMRz9ewhwz5u3KWNze4oyWGsNsMq8O78yf6XyTrlPn0UCCkyqsH+wNhyLfuDmsYGR502GR1e0VutIXMJwvsSD0MOsueFAL7dBlhympT59p9rx1yl4B7AMySTglJYAga6UeYanIwxOTSES53u+jOi4xawHBPqTktbkeckmELDpuh64t8d86l0EtYcgudj+GpNk4Whs8ME2Rc6kz6FPDEVurrX0E5RKuZBBVYzhaj5D0LDlDou8t8I4ZHWHg8g83NNSgxsfz63xu+1kMgojuUHz1gQ+3pbB61Bpx9qHGXedyKTqUok6jhs9osqyEETQ/OCa241AzHyUttgAYSMOod1Se31HydJVZiv0S7epFWkKIqJq8K2rKhfJMCcPIuWtqRK0+J+UkkdhjBOXw8bGMpo8k/acoFlRgmaj1ID0LCuvDcsj88k2RG0VsbbOB6Qy4XH0RVdRui+DGjHk7MRa3+5RJKcGsep4e3/f0Hb5R6NAymiz5s0RYoN2EMhAfmllT94ocuBskzTwnlCjBgIPi2hiHIifb7yFk2nxNvETP0/D5ksiuzqHR4sBQCNkx8vIy+qCKFigRi5oIlRRZeZZ+Rqdc9mMMRAiBI4XRvSgZScYBLtfC2BGNq41v0TYHlAYhqu0gv/O157neMjmnJsm4KpoU55LyAv3DHf6V/xw9dejPNhI4h2EhyahbbM314+srPDjzEGezfbrFL7Mh+ZgNP8LSUZHOrbHTaPBwobVGS1sgrIFk9+46jyORqZkFAM5NnCFsDsv5g3L4NhHqKHmKBZVc3Q+loSCTtL/rdUoHckg+mfrgEFXUSAdyr1vYxkuXY+43xuJ2nzCqkgTwS2cIsMcVoz4UN/NmvnYqdIWUoqP6OsgMKBnPEMxCz02wfiVKT69y+mSHRXWWSebpsIg+eBEFyHnDLz5NcCh7BdZKPTJCnqA1wLREXqj02JcanFBPsHGwjHFwFtMUmfLrzDpnoQdlfwfBq3Pa+RFk/5cJhGXmIi66PgH2BFOhCpmUn9loj7WrwwrN9y/0uGaGaYoaQVUhK2dpMcu6/C0S8mU2SwY+OY0qDajoOhcNlZqlcEL2UJUSpmihKBP4fTaLS22c6FDULh1uA3B2Yu62c7mYmMSngK87zME9WynzXDPHOQ06R1Yyd5bqYwyrGgGiahLd7tEyarSM2nGj9Og9LWfYuL2igCQNzUpHk/tH8yZ7VgfdNSgOrkKWo+XIIbdGbCOemBr20z23/3ls1+LcxNA7rtBaGwvWmLclY3F7G1Ms+ynbKpnUzbxZXN1HowfM8vX9Ms8b3+EgUGSWB3mu/0Vavn3OTUkYNZWN7g1uBC8iEsUSBgyoYro6J+J1loH10gqN1tCj7GHnSQBW419AaeRYePQ6STOAqNuczupkYufZ2Q4ym+mxuNjm7EScS+E4ACtzLn//UpxndzSmZJhXZ9npnCbvqHj9ryPrJ5BiSRxnh4Gsc1LxExM0/JKFKXfwVI+MkKGlLbNquHC4fZegAcetAGcBUQXf4bBqUjV6nJclHlQA4+5KyREjERuV3r8aJ8OzhEywb3lsiWHCRoGwCevF5aGwbauo3rCvUEdDC7jAvcXtVkYR2+vJuY1HaY35Xvjn9nO7du2a/MlPfnK+Wq3KgiDwsz/7s5Xf/M3fPHwj+xiL233E7SO4TpEdFYzM3bpENkt0skeYTfYONEL2o/RK0PDWWY7v8oCaRa/PsGou0/TWgSJhUiS9WdZ9lzmU1qjYaQxSpIMaeuA6tf4hSaaQgzaKEMQgxWzKIHS2yYVLMdT037EP+Do/wflAk7+0Vlkw1/nAqRhbezJOYwJikEiYeM0SlYv7XI9FCNJgZyfIDd2H6PZ5epjmYtVaZ8a/RdRxaPsaJFWVWKpDoyIRtGOcc5KUpAZ+1yHoJnEHMtnJEm2fzmZ9+vhMdGpDz7RLR4/PTswh778AgKdG+afrPnaNCkEfyHKI1aOZDEnuPadxlDMLyuHjkv2e1bmtuGPUhN3QztwVAQ6vIXT8pykOrqJ6KknOHB9vLv/aM2afmPrg8bT+sWCNeasiyzJ//Md/vPfkk0/2G42G7+GHH37gJ37iJ9rnz5//7nd2tzAWt7chW9tBUorGzr7GfjeE5ndww0WikSKKGMUVOmz4/hPJmEnWUbhW77POF4kmLbTOFNt7Eppvl+lAHEF/P5d6lwkFY2SkbbKywgklSqXmYEpfQIgGqdZlQGBwGGB6coV3Tr/EU9FJqMETJ2yK+w00U8BzJBYDM6RVGx1YXGqzsxNkby/ITzwdwZMDvFw7QDdBOwjS7ynkEml+9DEdT/KTD7yPCzstcqkik8Egpi/G2eQMq4bHgXpIGHCBnVYFgCywtxdEqkfwaV/H1z049jMb4jGwbUwvTfyO6shM9zkyQDn0xPd9PUY+bKOxWitzq4Q7BWCZnF8/3u61IrbvhXG7wP3H1/a+Fnrp8KXQIxOPdJ+afuot6ec2Nzdnzc3NWQDxeNxdWFjQd3Z2lLG4vc1ZUIf5ty1j85ZyfyA7LHmntELVXGa/G6JSU3jn0kssZ0wUIYzs6zIdv8Z6p4jB0DCzSx0HG81TCAgR2vSp+YrEkcgJSdLOElDB75N5QM2hNk+iN1VuxP4nbljFZ8xyKBziZfbZU76DVlriwewwn1Zzdolle6wE2tR2dfpyh7p6jXJ1G+3yHFdawz7Oi9/cx5AzPHo6yoQMHWyqwauk4yHOZoa/o1aLEMjuMBEoMi0nsUSViUyZ5ysNClachBZCaO3ja5fwyRALhen6PE4qYLoNvD54kZNc3wohO00WhRCb1hKKT6bXg8XpPHAzL7YUylEGrt1ok1ZOcObc3f1no2hutEx4Zw7s1ZY4F8J5Orfk2d4IY8F6+/K1va+Ffvf53807niP8/dbfp37rXb9V+H4FbuTn9vzzz69NTk7a5XJZ/KM/+qPM6PVPfvKTjV/91V+tAvzSL/1S7jOf+Uzq13/91w9Hfm7z8/NWtVoV4aaf2y/8wi/UB4OBYNvfvUgKYH19Xbly5Urg6aeffkO/x1jc7jPCDGdKlrjZpP3dGXqdNK08qq9DVNrFRuTJ8IfpsMiznWfIhouc9p9Fr+XQxW8xEQqyr+eImVGioV3eky0RFPz0pQAWUWpcgiTUjDA1WyUe3cN2GyStYeXmY5Mr1MzhPMuDwzYAK3mdy/0WA6tCRoPvlHoc1loYRpSSW0QvdfGzx7bfwXF/DOfAoN5Q2Hc0NjeGQrmcW+d0eIDgTLCxHuHyoMd8yKBjWfQ9k8LBdQy9wYQIX3+xhF84YD7kYvd1FKeJLdrgmBw2VZbCc+i+DKtGhazYYUqZYNWATPc5graO6TTZdhoYxkVyiISB2MD4rpNDmo3nhv+4hxUN3D0j8p8jUns1xgJ4f/DS4Ushx3OEiBqx20ZbeunwpdD3K24/SD+3Vqvl+6mf+qmFP/zDP9xNJBLuGznusRP3fcbX98s8V67Rdfv03C5bxibPdp5hy9ik53bpuV32vHUuVApUaiqG6eN67UFWt1cw3DDxpEiP2dvMTBNSimllFo0wfsFPlhwfsD7KycZTDFoxFCvItD3Dico70LfOYJkilinSrTxJfZDDCuh4PpnDcJuNmMhqeB1LCHFjP028ned0MIOnhCjFfDTDNVqmzr5mcSE44KvOF9jwLiHEZcRgn3q3xmbteST6LPmmGHgDvnB4gVUT3NAkbngKN5QjmhAIJwScxBL5E08wI3lo7gC/TwB52CeWlRQyShrdl0FXpoCh0efElMfkE2d5TlaoSHEezCV4Z37itvO8O2jySrlFuWYTsCMMdB8vrO/z3MUbFFprbDUvs9UcGoAWuwUKrTV0W0e39X8xB+s3a4bkmLcOj0w80hUF0WsbbUkURO+RiUf+Rfzc/vRP/3RnY2Pjyqc//emiYRg+GPq5/f7v/35xd3dXOX/+/AOlUkn8+Z//+frf/M3fXNM0zf3IRz6y+Ld/+7fhV9uvYRjChz/84YWPfexj9Z/92Z9tvtHjGkdu9wmjiG1X3kcSBkAFP2Uk7l4qm548yuPYffSBSC4zIBgYLg/cKmwl4xmWFYhLZ+i5XcpeGewcS+L7SWWGxp2ea9Iycry8N0Nc2SWRqtIsv5tGS6YXfBb8DnkJbvh2AJsnUw8yO9sj1LEIBBSykzrNSIH2/j69Totd22HNGtBVIiiaQc07xOc1mHRKtAWTVtnFMa8zIxfReJD9jkKt6iNz5By9sx0EgngmTEon6Hef4LBznUcmWzySnua/Xthko37AQBowEcqRtjK0WzKPn/EQBh7nonHcQJqXDrfpHX6JYF9lszX0bQv7YTL5BKsGyObfEFYmsZ0YW70SYWGTWbIkuE7YvH1KZ6d9kaZVxLSGf5/fLYIbFancy8tttM2YMa/GU9NPdX/rXb9VeDNzbj8IPzfXdfnEJz4xt7S0NPjt3/7t76ki83WJmyAIBaADOIB9py3DmB88ewcaMVllxs2h+PpsdZpYnp/TmdnjqslbzUkX8rAl6OwdaAQD9tG8yel7jukCKFp7hCWdnC8HNpSrwxaCJ6cy7B1obMsek5kBJ+ZdKpdcZuNbHPjKmMY5rMIUyVOXkYJtEkqMbinG3xaD9Psi/f4DlEWN5ViThSg4UprGoIfariMYHSaCPjxbwI+FgI2mpkj4FnCcPWqeRdOaZU5MUW5e4K8vFwn3cihOnUUhgumLEzSuo5l7uKHhEGjHt4PuCeieyjUnzsCCZbmEMGjhGzTAGXqt1avfJJpusKSeAvfmysmoXD8TSLM0McGqCa2Cn1klxuMnJ4czJe8go6WZCeYwzWF/3Uwwd9c2o6kjZF/f9R7n08a8Gk9NP/WmFZLAD8bP7Ytf/GLoc5/7XHJxcVE/derUAwC/8zu/s//xj3+89XqP+3X5uR2J26Oe570uX6Gxn9uby53u2vfc5mgoctC9RkIpcBWHppXn6fzN0vd77efOYcrDSsrhaK7RMOZvHQ4jvaQ1w6yQp+c7yd6BxvTkUBzLVZWEtEEsYrEn3GDXOuCpTAvLK/Fy148QuURBq3Mm+RATcoJKL0dePUUiYZKqaAQCDg8nv8iBVmJNHi7/7XQaYHbQe8PcnBZMIFg9okacdOxxovpFXq7q9GI5npge8K1aCVyLD2SnEaw+9UOBrrrEhx4dqoUTneP/+ubfUS5fRfH5uNQIE5SCnD8R5QPpNCsq+Dp7eFqSi50uXylfQtfnsW0fqtviTDDNYeidrCg3G7lHdjR7eyYnZZibUY5nSm41L1PTS0yHF4592FYL/xGAlfyn7rp+z128AUB0avgnNipEGVVvjiopR8ako6KWsbjdP4z93L43xn5u9zkjcWodOKiyw5OpDB1uCluYTVZUbsul3Sls92LbLNMVSkR8Afy+NqXuNg17F7F3ms0bJ2l3hx+hWGS4FJcM7NGzqtiWyMB2OOQGQa/LgjDPI+FTXOnewJ84ZNb6KDtXguy4X+T06TaGlKbcBNLDnPF0Zolea4+UYFEzTTwlzGwwTHmvjDnYOT6+nlGk0jbRzQGC2eY7peHyqjMIYNrb+LrecdQ2G03jGzS40a5TNRoEpSCz0TQwXM680dSJT9VBVOgYBlHCtBieG8VpETCLSFIU2ekcz5TMhfJEFzqEzB1AORYy4LjXbfT4u0VsAb0AQGl3aCC0cOLe12M0c7InDOdgjiO4MWPuzesVNw/4J0EQPODPPM/783/GYxpzxJ2O2a8ngquay1TNZRZSry5Y9/y/joSu15fY1EXagzLLCnTCkI5kabQUClaZFa1PRPXR0xfY3laYndKZntS5UBmK28R8mVS3g9zw8x2vjueFyItz4Ib4h0uHWIE+/+ZDUyyabV7c3mPfhFlxhlf0F8Cn0m4vU2m/zMnUBmL3EMHoMi+JNGpdwuElJLGPpbfB7xEMmew6VfS2RsvxaHa6XOl0SKga5ydCqL4Olw42WDc3OJldIuvpTCYnOOjWyYQM3jHpspiY5CEVNmsR1s1VOqVdfEEFJRQlLD5Cs1ckHcyRUkBV83SPIqiRyIxK/D35DMnoKZJwT4PRW52078WMMlSzvu9oCkl0mCsdZdFHObeOcpR/f40pKK+XsTiO+UHyw+Dn9qTnefuCIEwAXxQEYc3zvK/euoEgCP8O+HcA6WTmXvsY8y/AnVHYrX5vo8d7BxpVc5lef3j5t7aDx8uMdzItD6+ljo6EyooGtgzfeJX/f7c2i6tX6eglSmKHw06GhGzQDBQp2y0WSNEtTfEX299iXysTDEbZq2pYkTqp1ADMZWa0NEsRl029znJqknPhMIUNkU7bT0Posm722NTb6K7NQipNyzLp2g7ZcBJVlJjUgpyIpvG0BJ5Zh0ETX7/C9dohe6bFYNBBdE2ut+sUn9/kGS1Dwk1SMyRqA4W1sgDJLB+L3fy97pwpORKZkaXNaDbkqOBjNJ3ktRjZ7dQKw3XG5eQBu70ihZZxT8EZ97C97XBd1xV8Pt9r54/egny/fm6u6woMZzPcxesSN8/z9o9+HgqC8D+Ax4Cv3rHNnwN/DsOc2/d6sGNusnCPQpB/Dva89WEhQ1ancaABHU5lgjTsOs8f6LxLzXECk1jQRRIsShUPv7vDZNCPPjjB3oFGJmuwPLnO84clBj6bltYnredJ+97B9eYl4nKTB9IxphIi39h8nkI7QE7rExFu0LNKHDbizAQj2IMuEMZ2fxzZ91XccAEnnuZaYAdTHGBXM2zpBxxIHUKSSNDnZ9/os9+pYvo1VmJxSs0yF/pV3j81z7n4BA9LJp4CG7JMABl8IvFAnHB4gnbdoKSXuaHXmBEgKvmx9T4zLNFR8rw7nAduVjI2G88xE8zdtSy4cLQd3C5AxW6BoBwmqibvGsM1YqtToGmY5NW54/3cKYtvpEry9QjfeKbkgptZKwAAIABJREFUW4ZXKpXKA+l0unW/Ctz3iuu6QqVSiQKv3Ov11xQ3QRCCgM/zvM7Rv38M+N039zDH/HNxq9/b6HF0EqL0bhahZO+O2G6laeV5lywi9ES6Wh2fEiKVsIiErlAWcgQDNntHf3YBsYqoOET8QU6eOMvlyi679SZPJTTmchG+VZbp0UBVVZJikohSIzkBC/4Ip/7/9u41OM7zOvD8/+nb2/dGN+4gbgQIghRFCrorsiIrlu0otqPxKNfZTXYqNSnnw9RUUpmtqc3OB+9UZVKe/TCb2trK1KaczXoySRzHlryx4jjxOLJlm5Eli6ZI8QKCBIFGA2hcG33vty/vsx9eNAiAIAmQIJsAzq+KJTTQaDwEKRye5znnPMEu3ivlmJszUMUwI33d+DIQz0Cm0kax1s7lVIaU1YKzNc1Mucz8dBaratIR9KGdq/1r4RiWw432RtEuA5weLH8rR8MQmvVxNTdH1V/mqM9g8LifiOHj7Us5nFaIaniB48Ui4ZVlaLv5e+GrLOAvA0YzwXKcNjNNwdOF22UHu52W7K94j5E1oalrgvP59yksdRExmuEWgbBOgtDBUK1WfzOZTH4xmUw+ivQlb2YBH1ar1d/c6oPbydzagTeUUvXn/4XW+lu7tz5xJ7uRsQWwizDWF5QktN2rFl0900tVl6EZMosGmSQc64jxfGeMgKPIpcUavVYXHf4MsU6LC7NdpIsGgfBqG4FZJE8vHdpg1PgbnNZh0slXwfmnnAwN8Gosw4cLafSCmxd6y1ixKuPL11m0Qjzpb+fTR/OMzgRRxRfoCNtzJQdORXl9df/zJ8leFmoJqmqMsqOKt9ZLwZzBKtU45GriU22Kt+cDVLMVnhloYm4xwlIiz7mWFI8F/WgjAtgzJX2eZpzuZVKlPPNuDz/dNYCvLczVzBXeK50l4jzKp4JtlA27SvFqNk7VOYGq5Il57CkraXOJSiVNm3s1a6vdvAW5vol6q0C1PnsKFK6xkp0i7bWvzLkbO8nGZHtzb3jyySfngVcbvY696I7BTWs9Djz2ANYi7qOtRnHVz9jytxhq4ycBQGL2ceYWhzk3+QSfPP46HRWTxfIwzvCN5+YW51he8aBqNQg5aQmXCTj/lKfCk6yUmxmvJHm/kqQaMHAaWVaUi6xvgVhzjt4++6qZeDzAu5MJwpEybekuXv9hirmV93nxeJC33zlFARfP9R6hYJU5l2lh2GEQ9UZxqTK1zDzhopsWNywseOtTxezszRdi/FqQv0gUmcnN41Iuuj0DzKyEsbxFapE+8n2TuD7I0pxppVR7jnQJ1MQ0FnlogubCBwTNObRyEqp56CpfwWGZlGP2VT7e6tLaudxW6ne9bRVEkvk4mQoYvh6C7ghp0+6VO9Hy9NpzpIlbiJ2RVoB97JppTy0ZMno3FJTAxgwuVbV7yYpLdql62LQrKfLL11he8eDIjvBkZ5GxXJm3l1y0aE1v0C5esQc0Q0EXiKs41eBVytqPqxakbC3grEBxuZdS1U/GW8a0DM6kDCbzhzD8JXzRMt++GmA00E0ue5xw5AoAb3+vHf/AFMOd7ZxqDtPScoYW4DO9z3B95RzzuWmGXcs84+jnQmYe73KMF4sv0uc7xzsTMZQvyNG+ZoyiwWQmgZsVurwxMpU2qM7gdxqcaH6SocP2bMux5VlSxWX6vWEKVh/LGhRBYozT13SIkBlgauUD3DX7+f2+KIVKhlJxgotzdhHPI8P9t/yzqN/1tl79+psVncblgKgnTNUBqfIMBffNbQN3cjfZmGRsYr+S4CZuEvNMAOBRBQxHheMt3yQWKDPGLzCbPcJSycIR8m2ozAw0t2INfh3vQoDDVitWtpsmJ3hbxrh6rYl+Twm371E+LDuw/Am6mgJEw2FOtfkIVLt59+oS4eoVop0LJObP42n6O2pBRaEc5L+8PcmUvspjgR4SiUdYLnt4pbebJ90tjM20Muh2MqkX+NCEIG0sp53kqn4Ou30cHra4lJ8jG5ghHKrQUo7yXnmFaTPB4eZmoJPz85PkykV+4k/wnvdxjpfm6TUG8B87jLdkEjInyBr9pI08seKH1BxelvyPkavYN2pnOHTL7+V2tgp7Al24avb3suoM8FjsxE3DlOtN3JLBCbE9Etz2ofX9cXli5E0IsMyQYc+NtD8+ttY/1+1Z3bK0e51pWe7hzPIMIS88EjjEQrLAN1cu8N78WwRoIRopcyl7jbnr53myq51BY4g/+eA79H4iwq+3/jLvj11gIuvnqY4ATcOzzC+PUnFDa6sHT8JNPmDSGvqAsMeL9nUz1JEhni/CYo1aap4mJnD7PeRL7XywlGU2U6bq7iIQO0w6NI2XDj72MydxpifJLXUSMeLEwibBhVbi5RfpDIEnN0HcMUrJfZkp7zKqXCWoK7hrK7Q4weG1i2nGlu3pP/mKiVmr0lUZZrY8Ta9h952tDyJHQr20OZJUHT4KbvjRTB5w0WO1EWCR0sQ0E2qE/uHSWhC6kxXvMZpXWwyuZSfIem69tXknSwv2UXh/6yuAnKeJg02C2wFi97cFoMMuMNEUKKybYlK3WB5moWjQ1/IWfr2E8nSSL4QJe2ZoduVweztYnHMzfTlCu/Ix2A/e57+Ox93E2ckRpgmQUQbfGp0lND3Iux88xujgeWLmEmPLs7R4y7gKrSznymjTRzAbJmY101JzMp0tciQa5njLEH+7cp2scZmj4TaSupNpV5VIZJqhplbOz0/iyM1SjGUoAk2RNAuJ7/Hh8hwDkS5GIjBZyvKdH83yXHeSADUqrhBp1zJHA36aQm3k09OMppIkLCflWpVsOU9XDA4ffQZ/5MbA6fVZ1HT45bV+twxuYlwlpiYB8GDRpi/RVLoxi3I7BSX19w2Gbg5st7oORwhxexLc9qGt++OGuFYOrD0OAWNmHEVw46xJcww6znKsLUc+XeK/F88xV5kiXa5S1CUyWrNUXsSX/zidLU3E9UV+3/p/OQx8pv/jjMWh1zOE4fAxi50VHfYP0t62QLTtEn7fNR453E57rpuJwgJH3FmOqwQLxmEWwl/HG5gj5nESc03gLszQ7wfDG+RiskB6tsyxyAuceCLK+flJrGAn3SfsdWeXYsSaC9QcJpdSF0m4LFacSSYKPmJVD6FqhpjHBLcfvdqAnSykubA8zYzykp6Joh0BnmzVW2Y6E2l73Fa9D+2sCd7+QzxJhYVZN3laCPeUGa6OggmlTa0Bd3IvQauesZVMuxq0PsPSHTy5tnaQDE4cLBLcDoD1o7XavJdJz9YoOaq4mgoEiBPiRoFJomy3DERdMaZLT3N+uoQuzjPvjLNAkIX5fiqBKZ7wTtAZnmM0u0hhAp79xBOkzQLDT/wVYysLmOWfJVMK442VWPZc5vrCGR6LLjO1YlKNF/m4sYw2M+C1e9OCwQrvZWvkA2WavFUMs8YjniqGN8iF/BIhb4rHWiA92cHrQFtvwV5nJc3YygLUynSEFjgWuspXSznezbbha5vD8lhcqBj04uHngx7coTa+vZLhcipFr9fL8YAPd6HEsivPTC1APHeFzrSdtW0OBlttGWaNfpaxpxtXnObaYGP3ptaAW7UArH98u2tuJGMTYmckuO1jaxncpvfPLRr4nGUea++mQnDLzw04ggx1wsTYAF3eeZa9E+SLToLzx/C6juLsv8RYdYwp4xpdtABwfeYizQHPTa/V4RtHuxfo8fhZdNrP9ddmOOJpouoI8X9Pn6XdP0diKUo5WGZUzaADmuZImLFsimS2QMVRwduUJeOcIJ0v0rbpr+580qBc9mB4Zhh2G1yvRilaM7gdTlwuDwtOH+9kl8kvf8gHpiZbrVL0GYRSR1guzmEut2GVDuNoOcpM3qCr395a/Kfk3zFXjBN2N9Pu7eP0tJ0l1XvRvplbghCMZIeZmTDwH7MD44PcRmxePWOrZ3Aj3XLmJoQEtwNg/XU2eY7gDEPYM8r12VZ7yPJqST9AvHKd1IqHS+kQsaYytKRYqJbJVw2Uu8Tzz/8p0UgZf/EYPzSSDHcu8rsvHGG+mEPV8jwWauZkoJ0rtTzvc4H/byZDs+Gmw2WSWHQTC6YIBS2Mqgens0i0ZQozO0XckSHn82FWKvxjvsrpGkQ8QMVF2uEnWQzz3mIHbeoQ3c4e5uP2oOYXPnKcU+lJFnJjXM55cLvdtLUsQe0nnClqUu4oA1YONMw5HJSUk7DhpMkwcCgnpvIzlXycyvwhvLEs13JnMRxBno+8wGT+Em/PfQ0LC8uy+KnWz+BVQfyVGYIqT85zo3dwXh0HoJ/SHf887lSyLxWSQtw7CW4HSGLWR2fgEuFglWLJSSbnYjZvl/TXtyNNy6SMRbsxTYcxQ6ilFVclyfxKGZdRo7c1yZQGV9rJ070OWgfa+HHiIv1uOOrzoqolHPk53po9z2ixALSRKrXh83kxKGEpF3gMzlbb+SA1iceMs5RROJ2KpeIyTlcLPu8pdNkLHrtQIxIp4y8rNOCz5gjUIJ63r7Y5f26AgJmClRnmc0lSOkOoBu3BMqcMF3+vLSrUQENJ3xgybgHLpQLl4hy9/hKVSI24o0SieI2FaT8O7Sd6OIHfCOB3hZjPz5CpLPGxw79IU+ky302epqCXSCTsOxuLlj3thVF7wHn/8LptyNVrbfqH7cC33XO47VoLklIlKcQaCW4HSHdnkRaPSXzax3TukbX3X5sMcGl5tcjC5aNSVTRbTlYcbqzUEM0hTYs3jiOcpcs5wFJe0XL0O0QGxlnxnoBqnqHmKKdCQTSgylmmzTThYJlhB2Q986imZYJNiucHeqi2ncI1fw4znmfchIjRTr+/nYu1JGUVYqR0jMNGL80nZ9dK9a1wgu5QgNalZqDKyydWx6Mk4UzBz/iMot0VIOiZJV11Matv/NUexQfAsC5ueLwy2095sYuYcZ2CJ0ElcwSMdkyy/GDhK7zQcRSHclCoZnHg4LjLRVPpMu5aFpdVxF+ZIUSFLJ0sJd2scA2f36LLe3xbfx63Cj53UyG5+VYCIQ46CW77yO1uDwgxxkifffVNJhXBMD4kHKwyX7J/GMZq9llRyfsONaXoNgdpVXn6WiscaTVxmQGSmWaejcXIGjnah64TDfQTrqwwGGpCe0KcS15jLL2E9gQYLdVwV9MkTQcly8lJIpSdMSx/kLHlWRKzcYq1KhFVI20kyZpuThIk1FSjzzdBb1MzeWAoZjffJeauEK5mmcrZvXmRJbtIJtQBX43/Lc06S5kyh1w5ii4H8+bqWaIBRbO49vZ6/zyYZcF3hgvFMiFXgLZaGdyaKW8as+jhg8sLPNb060QPJ+gODPGYU619bo+/g6ozQLH5Gh53hdyyhUdbdPWb9EfsDK2eseWzTpKVi5Q+uEqvMUBrz+5sN64vTFGVvJ1NuifWzgMlgxMHmQS3A6i12cS07MKP1JybgL/KscARAELBDiZyKQL+Gs+2NhONZQlSpnV+kLR5jLJVxjfyDcY8LTwZOwpzFza8tvYEsMK9DPRkWUkGUVaMztYyv/vxZ6lF+nCmJ3FMnSNhVQAnlLM4ahYGboZqzRytHMXj81HImgQm7exs4Oko1xJRANo66j1kdnD788s/4mpmnkKlmamKnyd8SQBmlP37K5pFzIqJ4TYYxccwRYYpMoqPldAEiWKRQtlPxBnGFUphhmdw5AYwdDPlqsmFldP8auBl+gLHN1wcWnUGyHr6mVoqs1KZhdJ1KqrItXGTGcf11YKUEQCSlYssVcdvM8dka9vN2FQlj8sqrGWTm88DhTiIJLjtA9u5sbs+mSTAMkPtvWTpXmsRWM9fex5VmsH0wLzZhUk3IcZ4pA3KxRE+jP0nAP6Hzsc51dzJeUcJVVxkRGWZL7YTr7VyqElzKTVPyWmgc50Yrsra659bmuXiuSiduplKbIapaohht5uXdBtlp8WZchFzpsxPdUGNAUZzCfLzGTIuOxNTFXuosKXsrcUr2QV8ho/UIbs1YNm0s6U5o2ktsNWsGjWrRtEssoJJF2X8lSyTyiKjFSEFkeASY+E5AJzVAk7lIXx4hnL8EX5yLkPfT238PtUbrme8BoYjQLCjl5KVYf29if3DJSbSl8llpsGRp3aok3m3SbGwtGXD9p1szsTs2ZT2fXCuGoxEj1FxhriWnUC7Q5KxiQNNgtsBNtiX31BJmdCjdHcWiba78ZOg6MhSNEdRWYOVSj+np98hXWvjlebHSJtXObc0y1h6ieFN1f/xyQDLywbx+S46ci8QbM1zdrIdOs4yVoZpniGiSxjuJSxVoeoMUKn4cfvKxCJlcksVPIFW/ik0StxaoJtTt/w9tPsiLJRya4+TGLicai2wWZaFpS1ilTwuh8LvceJD46mWaHE7KTqcWL4cFWcBiNAUbKK4ZA+ONmsFnMpBu29jFrQ+KHX1m3RxCJLDzJQurW5L2lu89ctKTauMaRVI5uP4XIGbvl/3on4fXKg8QcUZWnssxEEnwW0fqGdo35uwr6g51b9xpNY1c4zErI8iVbra7VmTMHbLe+Jm5rxcsVJ0RNO86O9kdHaY2fxxqsP/O5F8P4lEADjC449YDAHG0jA/VgNoPUWHA64uncAZH8FjxjFNF7HyccauVHn32wPQUsGdMdCuo3Q7vLxcdNPT5+dvRx0EHXP4QkWWrSBnTYinF+iNtHKyrW/d6gY2rPXvpy5RNIv4DDuTmzUi9gdqRQy3Qc2qUa6Uca27xHjSclBweDF1lWxNs6h8eF1eDLd9KOfrG6U4OYyafooW/SjFnPumise6tbL+5NZ/Nl3BfiKD2bXA1hXspzlybG2Lczvu1PRdz+A2r0mIg0yC2wGyoKegYjJk3PzDb7Avz+DqnMkECZpcWUK5VprKXXg9FxlPXqItmcfp8EMgwvJMEGLtnOqDsaWtv14m7aFccJOoXsFfrlGiirYKrFiTLJShiQ7SVYMWd5D3Sm/jc6zwM7Rw0UiSdcywvFLAcLl5/fI7xNMLvHzYzuDWKiiV4uziFADD3KiErBeQ+Az7bYfDwZLbrgYNaZOKViy7fUQdVZzVvB3Ylp61F903urb+QjXLpHmZZGkCX/BnAZief4/uwBB9gY0Vkf3DJfo5vPF96/rZ6oHtfgUeqZIUYiMJbvtA/ewsWjyx+tjuARvsy69+bIRo0UXcMUVmyQ+OkQ3X1ax/nfetvyenz1NlhkTRx/eNb+NyJTl8/DBZfGRMi+HOK2TSbsauDHHyVB8DTwOkeP2H9tlaPPwmfcfmcOo+rs2lybivkwlM8OQLK+QrZS6PPYrX4eeZ5xY51dxJLRLlmVkT8HFkuJnJqSV6Q1E8Thf1fKs30grJEcay49A8y+WlaRK5FUrlIhWrxgpO+4mrGdx6httYy+yagBXTxImTCZyYq/8HmHoFQzWtfY5vNcgVJ4cxcfKu5z9SpYJn2cvZ1Ft8pvtzNwW49TZnV/cS2OTWbCF2ToLbAZDQoxSVk3mu4dYWCT0KZvGW25IttLFiuXEHqnh0niVjnP7oMkcDbcynS0Q9Xtpjy3Q1N3N+3r68c+PWoS0aNWkqm3S4e8FXJFWaoVCtcMQNAQeoaomvXfkxOM+RKiWpOoJ8Z+wdAH65v5e/LsyjVYVf6OykFunj9R+OE88niIVKFCtlXEoRtUycgF9BulKhnRUqlkUcz4ZKybpRfBSBfuyDqdFFu+m6VvJT0yGKk/Zj37oMrkaNbC0FgGu1CjORH7ttcFtPgpEQD54Et31gfVHI+sdrb5tFErM+2nQbrc0m3UZxy9cZ6TvLCO0szDVzxvoW0WCaXm2SrgXoDYTBmMakSHmxFYevik4nSVyoMloBRibJha6jiovUSj5KVom85aClJc2v9Icpurs5xxXiuRVeHpnkVLQVbbTz36+fBYebcLAIbge+7M3VFmNXwuSNKLlsjhiH8c4VaSqGefzoYVooc35xkmbDRXW5k1zNhYoltvV9S5klSqRQQM1RAAucurBlBqdwoLFYyicJuCN0B7b+h8F2hyJv1/qzPgmSQmyfBDex1jowYsB3sj8iU7HQZSdV3cqx5jinok+TzzmZyLroDuc5HijzT7kQVwsTJDNuri66ASgEluhxQVt7CVUrE3AWqDksVsLLWP487YUAuYrJt5dSjJbhFwY7+cShQSx/K2NL0wA8GzgKQK0pzGtNh6lF+jh/zu5xGwgMApCtXsfvNBiKdXI6OU6mUqZGGace5HoFqDmAKk6HE5fTtdYGUP89rmDiVzUKyqI3FgcUU8s9OClgdE6sbWFupFE4OOQf4lTsp7edtQkhGkOC2z6y1TlaXXdnkaJ5+xrxLEMUiROOzDKce413237IbKXIYeUFKphEqCn7PM/nzlBxRgm0N3MML12BbuJ5CAS6GRrK4MjN4nMPkzcGqLadBUCbRQainfiVj4n0AueWZkk7g1juCO/H7dmWA7HDBGpTJC5kKLq7GXgaTp6ytwTH37P/W4os0YeLkwZ8VxW5ku6n4srbW4sm1Ba7ADA6t650aTIMQsqeM1nQ9lndPFEM1bRlYAs4I3gJ0RMY5nDg5jvS1k8a2a3zsfXTTdY/3lytKYTYmgS3fazeGvDR/o1baJsvMx0zL3OldAlPrp8cK5yIVRgNfpf58I/4aPbfcCULPZFrHKWbuXQ738q+j0EeT/Yxrjq7KBZdhKsDJJdaoLPI0JB9Djd0NEMtkuL8vP1154t2P1pbawe5cpHRMuQqigHg+ZidCWVcdnam3BXyxgCQ4vy8PUA5gD2x5FSsHYC3zr3J9cQoJi18WClRZozhQI2SI0SiEgWWcDrs4FAPWvWTtGFdpKCdTKzO5Lp1xgZd/gGejX4GuLHdKIR4uElw24fqQQu2/mG9NpmkY+P70yxQIIvJAGMd38fldLKs7UBYmHWuPS9f8zOh8tRMwA8LC158aT9hThAr5yHZjsUItYHUhte3/K1rbw9EO4gYfsaWZ4kYfj76BDhys4zOXMZl5Rg4nEO7M5C+8fl2sMN+P3AhX2ShpnCFl3BUq6icG+W08ESTGI40d9JkGPhu8T2qUziYLV5HtWr6AsdvzthWBykDW2Zwd6ueoUnGJsTdkeC2D70/Y4+R8phtwM0Z3ObLSxeWDKKMEK2EmOQsCccow5kX+PwvPgvYWdPrXx0gXh6jJ3INTyWLdrhxd64wGAiSSXvwd77PUz09AHa5PjB22c6T2gJ2Y/VQrJOxsTDzSzDcOcqII43Ds35g1Ubnlux+trTb/vz5rF0t6XJMAFBwesGq0pxXNONgQnkoA04rx4ivnqltHbxu9f6NFFFPGzVd3VF1pBCi8SS47SP1jM3EnrOYVfY5VhQ7i6hnbPmC/ceemrV/wKe0PV6jzDwzxTiOapVI+XHOn4uunXcNHc3gWxplojBHiTzBphx+o0hq7hLBli56+04QCdl3m0WDdlAbLYOjsIBDF7GCm9LEVSMGjE6GuW7lGBiMMHTUPq+z3J1YQT8A4xcd9pNLbgpFFxML/QCciFU4O3PjbjR3eAm/24l5q2i5Aw6c+F0harqKQznWqiPXZ2R3czXNTknGJsTdkeC2Dz3WYW//XU7aBSYfXR3HdW1y4/O6ld3TRasdjOKLNRzto7z0ZJCu7M3FKcNB+3XOxjWg8HdatLSUWAiMkgtV+E7SzhCbI3blo/Y1oyp2EcvEVJWXO0Y4Up4iUJsiNOPgH3MJnFae1mKVYLsHCG34evXeubGxlN3fRpSoJ8qCqtCqVwgsVPm4e4BLlClZLkLFGq2VIInmhW1mZltzKQ8f6/gVaonHmCvGefxU+LZZW8icAG4EN9lKFKLxJLjtI5sLRdpV68aPb+qHo8OuYsROuPh+doZDOkpX9mfIZd2kLc9aGf7YlTAwwmu/GIW3YCabIuT2MRRtIbs4iy4uczg4xUyqi8SlAtFAimDbCsqqQrAFozJHwBwnh3ttPamUh/FyhWateD4S4sr1Ds5OjjB0NMPJQ3bGeH5+Eppn6W0GlipQqTAwmCNayZO7mOZYyEtNR5kpmtQ8oMo3btu+6+9j8BTPtLzCxJKXTt9h+gK3D1JZo/+ev6YQYndJcNvHPrppgPKtdHt6+cq1qxw7+V2aKoMbPmYHNcjlXCQSAV7/ah/eYo3OpiUmZ6v4vT6UIwMOF0eb2/CY7ZQMTXtAEW2OQK3MyUOPoLrSwCh/NTFMPF9luOJjqeYlUfJxudRD1qrySOTGFTz1Ckmwz+pOttn9bmPZcSKGxYme4zibO3HkZumZDFB0d5Ppz/Hv33mDTHlj1rZ+1uR2LI53MGF671iGX9+OrBeUFC5fByCfe/S2nyeEuP8kuO1DtxqrtWY1YxubtXvWhjpdPPXS25Rp58nmIV47FuX1r/Yxn4OYsrPAufwpikUn8XiAQmGEF/u7aHdcplZOkDCLLGZbqEwfp1hy0EGGxYqH0ZXj9LVO8Gi/H+32b1jCRGGOSiCJ1xUip7OcSYUpe+GpnhLptIcfJ+2KyKeO239Fz89PQsckQx12VaXTAFUpoKoljKrdYjDQepJHY12cTm4umdmZnsDwPX2+EKLxJLgJ/q/Ctxi86ufR1qPkQkX+7q3zXJuZYrDrBXp77a3M69evMRCFpz4eZOxKmC7vZQYrCTCg7A4xV2lixAFFDSGrGbe7RtLdyyljAkdulg/ydvbSEdN0xCCpgsTTEfxLbfRWO/GHg/T25RnP2oGpYNnZ1njKLo4JZh8D4LWPRLmQm+Xc0iyPhSNYgXZaH7MDZw1o9gZxoLDQaxlbzbK3KuuPg0aIdm8vIXeM67kLlPWNcWTdviE+c/IJoHTHzGtzQYn/mH0rQGC0dtvPE0LcfxLcDqLkiP3f0lUAnmpXDPErxPxzJM4FmFvO0xGOc6T5AlczVwDoDtnZ0UifRcC0tyov5ecwXe1kDR/+5gze8CVSsz5CmTZi7UVOnJzFkQtiBTshf33DEoZinQDE0356Dyte+4gHZ3qWN89kKHoMBzAOAAAXQUlEQVS6ebE3xHgqyXIxR2+klaFQ/R63lP16gHbb7QJW0L/2e/pk7wlOJ69RqJhUHWUUYFq1tdsFHDjxuUI8Gv0IpxfepKJvBCCfM8jPdP7Kbn6nhRANIsHtgLtovUt7coRY7Di58QT5RIVjZcWgS5ObmSdTmMYZDNETCvJW8SKz/7jCT1nHibR7qBSrFJwuQqUIpquVUe8cs2VYX8ZiBe2J/idXH59dnQB2sq3PDkiB8dWJJnbhynCw227WNjIEPT6Wky1Q7SZteRjPX2MsWyFXcTAwmONcLs3YygKDq8EOYKS1h//tmVf5cGmaR5sPcXVlnv/jzN9SsRQ+j5em1b61q9mzVC0Th3JiaY0DxbHw0xuqIrebeW1uAZCMTYjGk+B2ANWrJidOJ3imz+TnXjzJN969QnJliTa/QaXo4HtzHlxVNz2xKu6mMkuVODm9RIe2cKkiRjXNiOEk54TZap6sq5fu7lMMdtu9axChFrn5Gpw6Z3qSgJliKDTAiHEW0pOoSoGjhwt8kBllLLFAzhUi5jkEwHh+4znaeGqOoMfL5WQ7mYwDSlOrH+nBSZRfO2U3lI+09vBu8l0uXXyKtJmh1j+BQzk4EhphvpTArJZQsJbNCSH2BwluB1jHcz/C5XJy8lSK73xYo6fQwYloB9niNBHXAqWiiyIxJjyTlHMWGWVSDDk454ujzCwfNY5woTLNh84Fujt7Sa/e/HkuZ4++Gp21J4y8duy5tUB3Enj9q30sqTFisTKZkmetIvPo4cLa2oaaWkm5IxDNETEsxsbCDIUGOHkqxfn5DGPLXoZineSSjtv+HuP5D3mis4PBlX/NbPE6c8bf0O7r5ZmWVzjp1FzNnCXubOXR6Ed2PIFELg8V4uElwW0fW5sxuXoeVc/YQozxVtP/iYHFs4ee5sL4O3S1Ori8GOFCagalZng+EiHvbOXsciveoSKRkg9PMQ61ZXBE1r6G0zLpC3g54oF0LokV7Fg7E2N59qY1OdOTxFQZDzncuky4amdkeWNgbWbkiQE7yNTbAewtzOiG16mf2QU7pu23k3amVp+oUvdX361wrfgSR8tOwhxBZz6JzgAd0Ok7TKfv8H2ZLCKEaCwJbgfQ2MQijMBPH3p67X1mOI7rSA8tK07mM17yOoAjECbgXuJwoZ/unhe5Vv2vqJKL1058lA+nzjGDxoOLNg4RnwwwUcjxwtP2MGRHYYH5Qhbti/GHP/oGAC/7/icCZopYrIxbl0mlPJTx8PgTS9QiqQ1DkjdbH7Tqk0vW98LdzuPGz3Itf9F+4MwQ1ZMsLYySruU5Eurd8fis3b6QVAix+yS47UNrV9ms9rHVqyKZPEJCj1J+5m/weWOs1DQRw4cV7CRQhmMRzfHqJc5mFYZziNauIq2qjVbjOuTeZtyaRxs3bqm2/K3o1fL6oqcbs3LnteWNATIlD+HqNcp4WNZD1CL27dubz+jqQWyzelBLm4W1ocx0nL3p+fH8h/QMTxC1Ssx8YA+cjBwqEyxXgJtv/BZC7B8S3PaZEGMEiJOnd8uPJ9q/ThvwfNtzkEsytrKAVYZcucjCRIEURcrdMZ5ts28UqBo9dPct4cjN0lcdxAq0o4FHe05Ri/Txd2+dB6DNdYw23zECk5cZIczRwxH+8/nTULzGs80d1FofAcNuHo8wQsCsrGZsdx9kxlP2wOfHOwa2/Pjp2TGuLP4Gz8agy7t6nuY+i3afoHn1ypoKOx94vFsXkgoh7p9tBzellBP4MTCttf7M/VuSuFdDRi9ZhqCzfuZ2BLDP3MzBHL956rfs7McYWBtVfPb6j3HXihS1SVNLFdP3A2ZnffSvJmraiKCqRZSZRrv9a1lW0WOP+ArtcBJ/3hiwtyLvwsm2Ps7PT671wNUzNmfazujWZ4DPxuwgv3Y/2p2veRNC7AM7ydx+G7gEq9chi4dKCDuQucmtPd6cwV0c/ALT2fSGs6qTbXb2dX1xmXYClKoWK8kis/k22geDDDVnbvt1X/uIXehx/pw9fXnglP1YTY/yb4+eQBsRzi3PoStpToVXg2Kbfbv2+fmNW483naGtFsJsLhI5Pz/J2PIshapJrlxa+zy7BcH2jalvbrnera6suVuSsQnx8Lp9HfUqpVQ38Gngi/d3OWI3DRm9DBpDDPbl+UvX9wF4begVwA4E64OBo3Cc5EoHy1YNnytEf9OTnBiwS/hrkT60289Paj7OuDtv2b8WMMfXsqf7pR7YcuUSUW8Q0FxLnMORm7VnTVYKONOTeNNzRK3P39e1CCEeXtvN3P4Q+HdsvnBrHaXU54DPAbQ2t9/7ysSOZLGHJdczuPrjumcf/2ue7XiFtGn3kq1N9+iG0BEXjqKb+dlFirUsR9pbyfdlOD+f2ZBZ1c/n6hP64UZWdfJUCmf6RpZXOfQcYG8Vnmw/bAdJNhaDwI1gNRTr5CfJ66jiIvNzPkxXK92OG43Z67/WUKyTtFlYa+QeCvo41dwJlRt9ckKIg+2OwU0p9RlgXmv9vlLqpVs9T2v9x8AfAwwdPqZv9Tzx4P1Z01eo34PtyNlFGMqpoFYmcSHDxWyceM6BaZbRtU6mlroJRGbXesnqASnjCkG5xOuX3yGeDPByx4vAjbMutRpctjr72i3r2wCCHruR+0SbHTjrX/fPF85wofYsL+z6VxdC7BXbydw+AryqlPoU4AXCSqn/prX+tfu7NHE3Nmdsvz99htcGx/nNU78FwNcT5wB4sfcIp2LtXLnewUz2EsNGG96+x0mpOH6Xj6FYeC2QfOe6/TmGy80H4wWqukrAaiPt8zD+XgpfJUP3ia2PYm9V3r++QftkWx/O9CSOoBflbYEoaJeFb6lC3hi46cytrn7PG9iBzZGbXWsgf8H3q7f9Pu20t00IsbfcMbhprX8P+D2A1cztf5bAtne89uI/8Ehnx1owsfz2WOP6/WoDT0cpjvcydy2H6VqgvdNaC2z1z+mN2J9zeSlBvloiZkQJ6zDj+WtkqwmGg93UItH7mrHV3dgOvfljVrCTr2cu3LevLYTYO6TPbR+7OPgF3B4Xz7e+xOuX3wFYa3o+t2RPEXkUeCwcYcbn4FsL7xAxDE4es8/LxlbHZ7UFIoyn5hhfnkP5KhzraaEwPUu+tsxTh1wcjbnQ6cyGzOlONjdc1yJ9nIj0bQyQAwC3bxfYvCXqTc8xl/pnRG9x7Lv59mzJ4ITYn3YU3LTW3wW+e19WInbVucuzuAbhidjTvP7DFJfTDrq781xeSgDw80P26C1HbhZVLeLUDo4YXmIezYXxd7CCneTK9vSRXLnEt6+fxXC4OdbaQ8BtgLNGv7+dU7Ebf4Xq19ts14bZkXd67mrGlk57Njwe6bu5OvNkpIOVba9CCLEfSea2j9THbg0aQ7g+/SXaozfOwbr9nQxEcywX7T64k2190NaHlZ7kwtj3WM4qUtqFQ2vmklfQ3gUGWu3m7/l8mkPBGM8cOspQrJOx5Vkef9o+79LpybULQ7cTpO5kp1ua9ec705N8L/kDlrxNWNats7DNt2dLxibE/iTBbR+6OPgFAPpT/yMAuewU769cY7ncTKHaRnd38UbDdHKEhdQYupImuezj/PVO2mMhnnluEcvwrxWTuJ0u5vIrxNMLgH2Nzd3a3A5Q3zK93WvWi0o2tyCsV6yUiar/dNfrEkLsHxLc9oF6xpa3cvxgLMFQT9pu1k7e/NxufxcD0dyG97W2/xJ6aYqZ1Ac4rRYinhc4MTC5oaDEcLnX3q5bC1LuCJiFHW0z7rbvl6+z5G0ius0xYJKxCbG/SXDbZ3o++jYul9d+0GEPKmbZwZMdgzz+iMXY8jQR40YJPW0pzp+L8o8/eYaclSYQqJHLuXj9q30MHY3yO8/az9tOdrVd9a+9uchlO8HxVm0B8XRqbUiyEEJIcNsHBg27t+33pv6Gj/W5+PfP/itg6/vO6tuKdesDiaP0Cl5fBdjG3TVs3bPWCKcXvgvcGJIshBAS3PaRjz35T2uzI2F9sLl98Dl5KsXJU3YGN3YlzNDRzE0Z0m5kbJvVX/Neg2M8neL04s/yacnahBCrJLjtExcHv0C3I3rLAFHvWftJ8vrqezZOSLuXrKtRGdt6n47dOfjK/WtCHBwS3PaBP2v6Ck8C/+rRX97y49sNPvUM7kG7l+D45atvSNYmhLiJBLd94MnmcX7z1O1nKd5yS/IhyLru1Z2ytnrGlq9kNzyWDE6I/Wtb97mJh1d9xNZBVC8kEUKIzQ7mT8V9or4d+S+P/eK2P2c/ZGp12y3/r2dokrEJcXBI5raHbWc7cr+K5z8EpPxfCLE1ydz2qPqIrd2wuR/ubrO7B9nvdnp2jAv5YV7wbf9zJGMT4uCQzG0P+voZe9zWQc/a7nQhqRDi4JLMbQ86+ktf45HOjnt+nXqmda+9b5sHId/vDO5usjYhxMEimdse8/vTZwB4vvWlxi6kwSRrE0LcjmRue8gfXfiQ1179h13bjtyt3rcHOWPyy1ffkKxNCHFHkrntIS+9+uaGC0gPKsnahBB3IpnbHlHvafv5nk/t+mvvVqa1n3rohBB7m2Rue8RB7mmr+/LVNxq9BCHEHiHBbQ+4OPgFlEP+qACi1ucbvQQhxB4gPzEfcvVm7VtN/D8ovjH1zUYvQQixh0hw2wMO+nYkQN40JWsTQmybBLeHmGxHCiHE3ZGfnA+p+oitg74dCfaW5IX8cKOXIYTYQyS4PaR2a8TWfpA3TeltE0LsiAS3h1C9iOSgj9iCG0OShRBiJyS4PWTqsyOliMR2enaMK4u/0ehlCCH2GAluD5nXXty92ZH7hVxIKoTYKQluD5E/a/pKo5fwUJHeNiHE3ZLg9pD4g/LbMmJrk7xpypakEOKuSHB7SHz2+Gk+Ofhoo5fx0Di98F1AtiSFEHdHgttDoF4d2RuQ4FYXT6c4vfizjV6GEGKPkuD2kJDtyJt9OvZco5cghNijJLg12MXBLxD0G41exkPl9MJ3JWsTQtwTCW4N9EcX7AblXz3yzxu8kodLPJ1q9BKEEHucBLcG+fqZMV569U0ZsbVJfSKJbEkKIe6FBLcGOfpLXyPoN2TE1ianZ8dkS1IIcc/uGNyUUl6l1LtKqQ+UUheUUv/hQSxsP6uP2JLtyK1J1iaEuFfbydxM4GNa68eAEeAVpZT89LlL5y7PyoitW5AhyUKI3XLH4KZtudWH7tVf+r6uah9zffpLcgHpLZyeHZN724QQu2JbP2WVUk6l1FlgHvi21vpHWzznc0qpHyulfpzOruz2OveF+uxIuYD0ZvWsTe5tE0Lshm0FN611TWs9AnQDzyilbhqlobX+Y631U1rrpyKhpt1e574gsyNvTbI2IcRu2tH+mNZ6BXgLeOX+LGf/ujj4BdmOvAPJ2oQQu2U71ZKtSqmm1bd9wCeAy/d7YfuRbEduTQpJhBC7bTupRCfwllLqHPAe9pnbm/d3WfuLjNi6PeltE0LsNtednqC1Pgc8/gDWsi/VJ/5LT9vtSW+bEGI3ySHQffT1M2OATPy/nfq9bUIIsZskuN1HR3/pazI78g7i6RRR6/ONXoYQYp+R4Haf/EH5bQCZHSmEEA0gwe0++P3pM3z2+Gk+OSg3a9/Ol6++Ib1tQoj7QoLbffDai/9AezRMb0CC251Ib5sQ4n6Q4LbL6tuRP9/zqQavRAghDi4Jbrvss8dPS3XkNnz56hvS2yaEuG8kuO0iGbG1M9LbJoS4X+Qn8S6pN2vLiK07+8bUNxu9BCHEPifBbRfJduT25E2TK4u/0ehlCCH2MQluu0BmR25ffSLJs7Hexi5ECLGvSXC7R+cuzwIyO3K7lkoFydqEEPedBLd75Pr0l2TE1g7kTVOyNiHEfSfB7R7Ui0hkxNb2yJBkIcSDIsHtLv3RBfuCTSki2b54OiVbkkKIB0KC21166dU3JbDtQP22bdmSFEI8CBLc7sKfNX2l0UvYcxKFRZlIIoR4YCS47dAflN/myeZxydp2KJ5O0UJXo5chhDggJLjtkFxls3PS2yaEeNAkuO1AfXakXGWzM/F0SrYkhRAPlAS3bfr96TOAzI68WzIkWQjxIElw26bXXvwHada+C1+++kajlyCEOIAkuG2DNGvfm6j1+UYvQQhxwEhwu4P67Eipjty5em+bEEI8aBLc7kBmR96907NjUkgihGgICW63UR+xJduRO1fP2qSQRAjRCBLcbuHc5VleevVN6Wm7S6dnx7iQH270MoQQB5QEt1twffpLuD0u6Wm7By/45JxSCNEYEty28AfltwH4l8d+scEr2ZvkahshRKNJcNvCZ4+flurIexBPp6T8XwjRUBLcNqmP2BJCCLF3yU/xLciIrbv35atvSCGJEKLhJLitc3HwC7g9rkYvY8+TQhIhRKNJcFtVH7ElRSR3T+ZICiEeFhLckBFbu0kKSYQQDwMJbtg9be3RcKOXsad9Y+qbjV6CEEKsOfDBrd7T9vM9n2rwSva2vGlK1iaEeGjcMbgppXqUUm8ppS4qpS4opX77QSzsQfijCx/y2eOnZcTWPZKsTQjxsNlO5lYF/q3W+hHgOeBfK6Ueub/LejBeevVNHunskBFb9yhvmlxZ/I1GL0MIIdbcMbhprWe11mdW384Cl4BD93th91t9O1Im/t+b+qitZ2O9jV2IEEKss6MzN6VUP/A48KP7sZgHSUZs7Y54OiVZmxDioaO01tt7olJB4HvAf9Rav77Fxz8HfG714aPAXr2GuQVYbPQi7oGsv7Fk/Y21l9ffp7VubfQi9ottBTellBt4E/h7rfV/3sbzf6y1fmoX1vfA7eW1g6y/0WT9jbXX1y92z3aqJRXwJ8Cl7QQ2IYQQotG2c+b2EeDXgY8ppc6u/pKmMCGEEA+tO04J1lr/AFA7fN0/vrvlPBT28tpB1t9osv7G2uvrF7tk2wUlQgghxF5x4MdvCSGE2H92Nbgppf4fpdS8UmrPtQHs9TFjSimvUupdpdQHq+v/D41e091QSjmVUj9RSr3Z6LXslFJqQil1fvVc+seNXs9OKaWalFJfVUpdVkpdUkr9VKPXtF1KqeF1NQFnlVIZpdTvNHpdonF2dVtSKfUikAP+q9Z6T820Ukp1Ap1a6zNKqRDwPvBZrfXFBi9tW1arWgNa69xq68YPgN/WWr/T4KXtiFLqd4GngLDW+jONXs9OKKUmgKe01nuyz0op9SXg+1rrLyqlPIBfa73S6HXtlFLKCUwDz2qtJxu9HtEYu5q5aa3fBpZ38zUflL0+ZkzbcqsP3au/9tSBqlKqG/g08MVGr+WgUUpFgBex237QWpf3YmBb9TJwTQLbwSZnblvYq2PGVrf0zgLzwLe11ntq/cAfAv8OsBq9kLukgX9QSr2/OrFnLzkMLAB/urot/EWlVKDRi7pLvwr8ZaMXIRpLgtsmq2PGvgb8jtY60+j17ITWuqa1HgG6gWeUUntma1gp9RlgXmv9fqPXcg9e0Fo/Afwc9u0ZLzZ6QTvgAp4A/ovW+nEgD/wvjV3Szq1up74K/HWj1yIaS4LbOqtnVV8D/nyr+Zl7xep20lvAK41eyw58BHh19dzqy9hDA/5bY5e0M1rr6dX/zgNvAM80dkU7kgAS67L9r2IHu73m54AzWuu5Ri9ENJYEt1V7fcyYUqpVKdW0+rYP+ARwubGr2j6t9e9prbu11v3Y20r/qLX+tQYva9uUUoHVQiRWt/M+yR4aHq61TgJTSqnh1Xe9DOyJYqpN/gWyJSnYxoSSnVBK/SXwEtCilEoAn9da/8lufo37qD5m7PzquRXA/6q13ivXTHcCX1qtFHMAX9Fa77ly+j2sHXjD/jcSLuAvtNbfauySduzfAH++urU3Duypu4xW/1HxCeC3Gr0W0XgyoUQIIcS+I9uSQggh9h0JbkIIIfYdCW5CCCH2HQluQggh9h0JbkIIIfYdCW5CCCH2HQluQggh9h0JbkIIIfad/x+16lCxuLKkGwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqS70WNvOcIw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "d7276d75-9830-4de4-bf18-05c1e8d3da10"
      },
      "source": [
        "plt.plot(loss_curi_tr)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f96ea0bf8d0>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GmyEWKD92_T"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ZQMzXXJa8lB"
      },
      "source": [
        ""
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6WUeyvNO3iP"
      },
      "source": [
        ""
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQRNesx7O3Xr"
      },
      "source": [
        ""
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLPSDVK_QWId"
      },
      "source": [
        ""
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hw2rtHfzFyFA"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3lfGywVHL6S"
      },
      "source": [
        ""
      ],
      "execution_count": 31,
      "outputs": []
    }
  ]
}
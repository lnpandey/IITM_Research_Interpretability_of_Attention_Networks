{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "blob_only_atttention_weights_lr_1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWIyC9Ip_bcq"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGVy-1EllAc_"
      },
      "source": [
        "train_data = np.load(\"train_blob_data.npy\",allow_pickle=True)\n",
        "\n",
        "test_data = np.load(\"test_blob_data.npy\",allow_pickle=True)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uL771xuGZC5Q"
      },
      "source": [
        "mosaic_list_of_images = train_data[0][\"mosaic_list\"]\n",
        "mosaic_label = train_data[0][\"mosaic_label\"]\n",
        "fore_idx = train_data[0][\"fore_idx\"]\n",
        "\n",
        "\n",
        "test_mosaic_list_of_images = test_data[0][\"mosaic_list\"]\n",
        "test_mosaic_label = test_data[0][\"mosaic_label\"]\n",
        "test_fore_idx = test_data[0][\"fore_idx\"]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2qfRXfNZCao"
      },
      "source": [
        "class MosaicDataset1(Dataset):\n",
        "  \"\"\"MosaicDataset dataset.\"\"\"\n",
        "\n",
        "  def __init__(self, mosaic_list, mosaic_label,fore_idx):\n",
        "    \"\"\"\n",
        "      Args:\n",
        "        csv_file (string): Path to the csv file with annotations.\n",
        "        root_dir (string): Directory with all the images.\n",
        "        transform (callable, optional): Optional transform to be applied\n",
        "            on a sample.\n",
        "    \"\"\"\n",
        "    self.mosaic = mosaic_list\n",
        "    self.label = mosaic_label\n",
        "    self.fore_idx = fore_idx\n",
        "    \n",
        "  def __len__(self):\n",
        "    return len(self.label)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.mosaic[idx] , self.label[idx] , self.fore_idx[idx]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uf76JwkxZCT0"
      },
      "source": [
        "batch = 250\n",
        "train_dataset = MosaicDataset1(mosaic_list_of_images, mosaic_label, fore_idx)\n",
        "train_loader = DataLoader( train_dataset,batch_size= batch ,shuffle=False)\n",
        "test_dataset = MosaicDataset1(test_mosaic_list_of_images, test_mosaic_label, test_fore_idx)\n",
        "test_loader = DataLoader(test_dataset,batch_size= batch ,shuffle=False)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOpZfj1bq7wN"
      },
      "source": [
        "bg = []\n",
        "for i in range(12):\n",
        "  torch.manual_seed(i)\n",
        "  betag = torch.randn(250,9)#torch.ones((250,9))/9\n",
        "  bg.append( betag.requires_grad_() )"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzb3ii4drXpu",
        "outputId": "81e4950f-f919-48b8-a2c1-f681f352ea40"
      },
      "source": [
        "bg"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([[-1.1258, -1.1524, -0.2506,  ..., -0.3160, -2.1152,  0.3223],\n",
              "         [-1.2633,  0.3500,  0.3081,  ..., -0.2473, -1.3527, -1.6959],\n",
              "         [ 0.5667,  0.7935,  0.5988,  ...,  0.7502, -0.5855, -0.1734],\n",
              "         ...,\n",
              "         [ 0.8374, -0.7942, -0.3622,  ...,  0.0121,  0.8032, -0.6962],\n",
              "         [-1.0645,  0.2384, -0.3385,  ...,  0.9635, -1.0340,  0.1894],\n",
              "         [ 0.8253,  1.1038, -1.2491,  ..., -0.5940, -1.7125,  0.3617]],\n",
              "        requires_grad=True),\n",
              " tensor([[-1.5256, -0.7502, -0.6540,  ..., -0.9798, -1.6091, -0.7121],\n",
              "         [ 0.3037, -0.7773, -0.2515,  ...,  0.4676, -0.6970, -1.1608],\n",
              "         [ 0.6995,  0.1991,  0.8657,  ...,  1.1017, -0.1759, -2.2456],\n",
              "         ...,\n",
              "         [-0.4302,  0.1508,  0.6937,  ...,  0.0314,  2.6645,  0.1189],\n",
              "         [ 1.4484, -0.0213, -1.3367,  ...,  0.6279, -1.4719, -1.0291],\n",
              "         [ 0.9081, -1.2433,  1.6062,  ..., -0.1177, -0.5548, -0.0595]],\n",
              "        requires_grad=True),\n",
              " tensor([[-1.0408,  0.9166, -1.3042,  ..., -1.0574, -0.1188, -0.9078],\n",
              "         [ 0.3452, -0.5713, -0.2351,  ..., -0.4327, -1.5071, -0.4586],\n",
              "         [-0.8480,  0.5266,  0.0299,  ...,  0.4640, -0.4986,  0.1289],\n",
              "         ...,\n",
              "         [ 1.5719,  1.0154, -2.1620,  ..., -1.0790,  1.5801, -1.6557],\n",
              "         [-1.1613,  0.3672, -0.3078,  ..., -1.2456, -0.1125,  0.6222],\n",
              "         [ 0.4521, -0.2505,  2.3728,  ..., -0.1377, -0.8815, -0.1671]],\n",
              "        requires_grad=True),\n",
              " tensor([[-0.0766,  0.3599, -0.7820,  ...,  1.6206, -1.5967, -0.0517],\n",
              "         [-0.3060,  0.2485, -0.2226,  ...,  0.4163,  0.2615,  0.9311],\n",
              "         [-0.5145, -1.6517,  1.0460,  ...,  0.5638,  2.2566,  1.8693],\n",
              "         ...,\n",
              "         [ 2.1181,  0.1464, -0.0447,  ...,  1.3816,  0.4975,  0.2814],\n",
              "         [-0.7639, -1.4938, -1.1430,  ...,  0.6355,  0.6700,  1.5335],\n",
              "         [-0.0191, -0.3568,  0.4536,  ..., -0.9493,  2.0439, -0.3827]],\n",
              "        requires_grad=True),\n",
              " tensor([[-0.9414,  1.2632, -0.1838,  ..., -2.6021,  0.6245, -0.8684],\n",
              "         [-0.2051,  0.3976,  0.6699,  ..., -2.1205,  1.5191, -0.6682],\n",
              "         [ 0.0031, -0.1535,  1.1396,  ..., -0.7588, -0.1853, -0.8558],\n",
              "         ...,\n",
              "         [ 1.6794, -0.5509,  0.4118,  ...,  0.9084, -0.8626, -0.6553],\n",
              "         [ 0.6058, -0.5888,  0.9448,  ...,  0.0072, -0.2579,  1.7659],\n",
              "         [-1.2965,  0.2970, -0.5833,  ...,  1.7838, -0.4794,  0.5579]],\n",
              "        requires_grad=True),\n",
              " tensor([[ 1.8423,  0.5189, -1.7119,  ..., -0.1307, -1.4374,  0.3908],\n",
              "         [-0.0190, -1.3527, -0.7308,  ..., -0.7823,  2.7799,  1.2220],\n",
              "         [-0.3364, -0.9651, -0.1297,  ..., -0.4374,  0.7792, -0.0583],\n",
              "         ...,\n",
              "         [ 0.6700, -0.5400,  0.2353,  ..., -1.0840, -0.6141, -0.0155],\n",
              "         [ 0.4779, -0.4648, -0.1366,  ...,  0.1162,  3.0351, -0.2885],\n",
              "         [-0.6777, -0.1373, -0.7330,  ...,  0.6185, -0.3036, -1.0850]],\n",
              "        requires_grad=True),\n",
              " tensor([[-1.2113,  0.6304, -1.4713,  ...,  0.3295,  0.3264, -0.4806],\n",
              "         [ 1.1032,  2.5485,  0.3006,  ..., -1.6279, -1.4801, -1.0631],\n",
              "         [ 0.3630,  0.3995,  0.1457,  ..., -1.3437,  0.8535,  0.8811],\n",
              "         ...,\n",
              "         [-0.5519,  0.2253,  0.4891,  ..., -0.0110, -0.6023, -0.7230],\n",
              "         [-1.1593, -0.6551,  1.6578,  ...,  0.4795, -1.3562,  0.2920],\n",
              "         [ 0.3474, -0.9874, -0.0130,  ...,  0.6061,  0.8639, -0.9552]],\n",
              "        requires_grad=True),\n",
              " tensor([[-0.8201,  0.3956,  0.8989,  ..., -0.6411, -0.8937,  0.9265],\n",
              "         [-0.5355, -1.1597, -0.4602,  ...,  1.0902, -1.5827, -0.3246],\n",
              "         [ 1.9264, -0.3300,  0.1984,  ..., -0.2093, -0.2153, -1.8157],\n",
              "         ...,\n",
              "         [-0.6910,  0.3328,  2.2102,  ..., -0.0383,  0.4400, -0.8350],\n",
              "         [-0.2194, -0.7611, -0.0921,  ..., -0.3143, -0.4196,  1.1570],\n",
              "         [-0.8934, -1.7705,  0.3805,  ...,  0.1963, -0.7307,  1.3581]],\n",
              "        requires_grad=True),\n",
              " tensor([[-1.1892,  1.3932,  2.1059,  ...,  2.1414,  0.1317, -0.6388],\n",
              "         [ 1.3384, -1.1908, -0.7601,  ..., -0.1051,  0.4414,  0.6590],\n",
              "         [-0.7585, -0.6001, -0.3948,  ..., -1.7526,  0.3920,  0.8295],\n",
              "         ...,\n",
              "         [-0.0557, -0.1032, -0.4624,  ..., -0.1339, -1.6662, -0.4955],\n",
              "         [ 1.0884, -0.4479, -0.0847,  ...,  1.7487, -1.6152, -1.8258],\n",
              "         [ 1.7062,  1.1041, -1.3736,  ..., -1.5244,  0.4869, -1.7420]],\n",
              "        requires_grad=True),\n",
              " tensor([[-1.0674, -0.7172,  1.0897,  ..., -0.7737, -2.4656,  0.9968],\n",
              "         [ 0.4524, -0.3464, -0.7245,  ...,  0.2331, -1.1433,  0.8289],\n",
              "         [ 0.9534,  0.2948,  1.5159,  ...,  0.3971,  0.4058, -0.5274],\n",
              "         ...,\n",
              "         [-0.3297, -0.3700,  1.9490,  ..., -0.0443,  1.8073, -0.6388],\n",
              "         [ 0.0977,  0.1862,  1.4303,  ..., -1.9735, -1.1663,  1.7066],\n",
              "         [-0.8396, -2.5271, -1.0791,  ...,  0.1053,  1.2463, -0.7709]],\n",
              "        requires_grad=True),\n",
              " tensor([[-0.8173, -0.5556, -0.8267,  ..., -0.5133,  2.6278, -0.7465],\n",
              "         [ 1.0051, -0.2568,  0.4765,  ..., -0.2496,  0.8298,  1.1209],\n",
              "         [ 0.9999,  1.1167,  1.0763,  ...,  0.0562,  0.2456,  0.9535],\n",
              "         ...,\n",
              "         [-1.0042, -0.7732,  0.9129,  ..., -0.4342,  1.3256, -0.6357],\n",
              "         [-0.5979,  1.2285,  1.0288,  ..., -1.4067,  0.2403,  0.5257],\n",
              "         [-1.7332, -0.2443,  0.1425,  ..., -0.9291,  1.4324, -0.2338]],\n",
              "        requires_grad=True),\n",
              " tensor([[-0.5108,  1.0283, -0.3532,  ...,  0.1421, -0.5243, -0.2487],\n",
              "         [-0.5252,  2.8922, -0.5947,  ..., -0.0080,  0.2479,  1.5727],\n",
              "         [-1.6395, -1.5925, -0.1546,  ..., -0.3935,  0.6171,  0.7528],\n",
              "         ...,\n",
              "         [-0.3538,  0.1294,  1.1873,  ..., -0.2866, -0.3111,  0.2674],\n",
              "         [ 1.7757, -0.1730,  0.6679,  ..., -0.2519,  0.8360, -0.4348],\n",
              "         [ 0.4242,  0.7649, -0.5807,  ..., -0.7654, -0.1086,  0.4636]],\n",
              "        requires_grad=True)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HbrMidFCla6h"
      },
      "source": [
        "class Module2(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Module2, self).__init__()\n",
        "    self.linear1 = nn.Linear(5,100)\n",
        "    self.linear2 = nn.Linear(100,3)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = F.relu(self.linear1(x))\n",
        "    x = self.linear2(x)\n",
        "    return x"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRqj2VELllkX"
      },
      "source": [
        "torch.manual_seed(1234)\n",
        "what_net = Module2().double()\n",
        "\n",
        "what_net.load_state_dict(torch.load(\"blob_what_net.pt\"))\n",
        "what_net = what_net.to(\"cuda\")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6d8Wch99l4yB"
      },
      "source": [
        "def attn_avg(x,beta):\n",
        "  y = torch.zeros([batch,5], dtype=torch.float64)\n",
        "  y = y.to(\"cuda\")\n",
        "  alpha = F.softmax(beta,dim=1)   # alphas\n",
        "  #print(alpha[0],x[0,:])\n",
        "  for i in range(9):            \n",
        "    alpha1 = alpha[:,i]      \n",
        "    y = y + torch.mul(alpha1[:,None],x[:,i])\n",
        "  return y,alpha\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rz1Kpw12loV6"
      },
      "source": [
        "def calculate_attn_loss(dataloader,what,criter):\n",
        "  what.eval()\n",
        "  r_loss = 0\n",
        "  alphas = []\n",
        "  lbls = []\n",
        "  pred = []\n",
        "  fidices = []\n",
        "  correct = 0\n",
        "  tot = 0\n",
        "  with torch.no_grad():\n",
        "    for i, data in enumerate(dataloader, 0):\n",
        "      inputs, labels,fidx= data\n",
        "      lbls.append(labels)\n",
        "      fidices.append(fidx)\n",
        "      inputs = inputs.double()\n",
        "      beta = bg[i]  # beta for ith batch\n",
        "      inputs, labels,beta = inputs.to(\"cuda\"),labels.to(\"cuda\"),beta.to(\"cuda\")\n",
        "      avg,alpha = attn_avg(inputs,beta)\n",
        "      alpha = alpha.to(\"cuda\")\n",
        "      outputs = what(avg)\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      correct += sum(predicted == labels)\n",
        "      tot += len(predicted)\n",
        "      pred.append(predicted.cpu().numpy())\n",
        "      alphas.append(alpha.cpu().numpy())\n",
        "      loss = criter(outputs, labels)\n",
        "      r_loss += loss.item()\n",
        "  alphas = np.concatenate(alphas,axis=0)\n",
        "  pred = np.concatenate(pred,axis=0)\n",
        "  lbls = np.concatenate(lbls,axis=0)\n",
        "  fidices = np.concatenate(fidices,axis=0)\n",
        "  #print(alphas.shape,pred.shape,lbls.shape,fidices.shape) \n",
        "  analysis = analyse_data(alphas,lbls,pred,fidices)\n",
        "  return r_loss/i,analysis,correct.item(),tot,correct.item()/tot"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAY-x6UAwrwE"
      },
      "source": [
        "for param in what_net.parameters():\n",
        "    param.requires_grad = False"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_toCktPanH0S"
      },
      "source": [
        "\n",
        "def analyse_data(alphas,lbls,predicted,f_idx):\n",
        "    '''\n",
        "       analysis data is created here\n",
        "    '''\n",
        "    batch = len(predicted)\n",
        "    amth,alth,ftpt,ffpt,ftpf,ffpf = 0,0,0,0,0,0\n",
        "    for j in range (batch):\n",
        "      focus = np.argmax(alphas[j])\n",
        "      if(alphas[j][focus] >= 0.5):\n",
        "        amth +=1\n",
        "      else:\n",
        "        alth +=1\n",
        "      if(focus == f_idx[j] and predicted[j] == lbls[j]):\n",
        "        ftpt += 1\n",
        "      elif(focus != f_idx[j] and predicted[j] == lbls[j]):\n",
        "        ffpt +=1\n",
        "      elif(focus == f_idx[j] and predicted[j] != lbls[j]):\n",
        "        ftpf +=1\n",
        "      elif(focus != f_idx[j] and predicted[j] != lbls[j]):\n",
        "        ffpf +=1\n",
        "    #print(sum(predicted==lbls),ftpt+ffpt)\n",
        "    return [ftpt,ffpt,ftpf,ffpf,amth,alth]"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S633XgMToeN3"
      },
      "source": [
        "optim1 = []\n",
        "for i in range(12):\n",
        "  optim1.append(optim.RMSprop([bg[i]], lr=1))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qPaYaojinMTA",
        "outputId": "c2a902b8-db62-4918-e9bb-b212f2951fa2"
      },
      "source": [
        "# instantiate optimizer\n",
        "#optimizer_what = optim.RMSprop(what_net.parameters(), lr=0.001)#, momentum=0.9)#,nesterov=True)\n",
        "\n",
        "\n",
        " \n",
        "\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "acti = []\n",
        "analysis_data_tr = []\n",
        "analysis_data_tst = []\n",
        "loss_curi_tr = []\n",
        "loss_curi_tst = []\n",
        "epochs = 200\n",
        "\n",
        "\n",
        "# calculate zeroth epoch loss and FTPT values\n",
        "running_loss,anlys_data,correct,total,accuracy = calculate_attn_loss(train_loader,what_net,criterion)\n",
        "print('training epoch: [%d ] loss: %.3f correct: %.3f, total: %.3f, accuracy: %.3f' %(0,running_loss,correct,total,accuracy)) \n",
        "loss_curi_tr.append(running_loss)\n",
        "analysis_data_tr.append(anlys_data)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# training starts \n",
        "for epoch in range(epochs): # loop over the dataset multiple times\n",
        "  ep_lossi = []\n",
        "  running_loss = 0.0\n",
        "  #what_net.train()\n",
        "  for i, data in enumerate(train_loader, 0):\n",
        "    # get the inputs\n",
        "    inputs, labels,_  = data\n",
        "    inputs = inputs.double()\n",
        "    beta = bg[i] # alpha for ith batch\n",
        "    #print(labels)\n",
        "    inputs, labels,beta = inputs.to(\"cuda\"),labels.to(\"cuda\"),beta.to(\"cuda\")\n",
        "        \n",
        "    # zero the parameter gradients\n",
        "    #optimizer_what.zero_grad()\n",
        "    optim1[i].zero_grad()\n",
        "      \n",
        "    # forward + backward + optimize\n",
        "    avg,alpha = attn_avg(inputs,beta)\n",
        "    outputs = what_net(avg)     \n",
        "    loss = criterion(outputs, labels)\n",
        "\n",
        "    # print statistics\n",
        "    running_loss += loss.item()\n",
        "    #alpha.retain_grad()\n",
        "    loss.backward(retain_graph=False)\n",
        "    #optimizer_what.step()\n",
        "    optim1[i].step()\n",
        "\n",
        "\n",
        "  running_loss_tr,anls_data,correct,total,accuracy = calculate_attn_loss(train_loader,what_net,criterion)\n",
        "  analysis_data_tr.append(anls_data)\n",
        "  loss_curi_tr.append(running_loss_tr)   #loss per epoch\n",
        "  print('training epoch: [%d ] loss: %.3f correct: %.3f, total: %.3f, accuracy: %.3f' %(epoch+1,running_loss_tr,correct,total,accuracy)) \n",
        "\n",
        "\n",
        "  \n",
        "  if running_loss_tr<=0.08:\n",
        "    break\n",
        "print('Finished Training run ')\n",
        "analysis_data_tr = np.array(analysis_data_tr)\n",
        "\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training epoch: [0 ] loss: 12.050 correct: 1071.000, total: 3000.000, accuracy: 0.357\n",
            "training epoch: [1 ] loss: 3.357 correct: 2279.000, total: 3000.000, accuracy: 0.760\n",
            "training epoch: [2 ] loss: 1.290 correct: 2771.000, total: 3000.000, accuracy: 0.924\n",
            "training epoch: [3 ] loss: 0.459 correct: 2899.000, total: 3000.000, accuracy: 0.966\n",
            "training epoch: [4 ] loss: 0.345 correct: 2920.000, total: 3000.000, accuracy: 0.973\n",
            "training epoch: [5 ] loss: 0.308 correct: 2928.000, total: 3000.000, accuracy: 0.976\n",
            "training epoch: [6 ] loss: 0.283 correct: 2931.000, total: 3000.000, accuracy: 0.977\n",
            "training epoch: [7 ] loss: 0.278 correct: 2933.000, total: 3000.000, accuracy: 0.978\n",
            "training epoch: [8 ] loss: 0.277 correct: 2934.000, total: 3000.000, accuracy: 0.978\n",
            "training epoch: [9 ] loss: 0.279 correct: 2934.000, total: 3000.000, accuracy: 0.978\n",
            "training epoch: [10 ] loss: 0.279 correct: 2934.000, total: 3000.000, accuracy: 0.978\n",
            "training epoch: [11 ] loss: 0.275 correct: 2935.000, total: 3000.000, accuracy: 0.978\n",
            "training epoch: [12 ] loss: 0.275 correct: 2935.000, total: 3000.000, accuracy: 0.978\n",
            "training epoch: [13 ] loss: 0.274 correct: 2935.000, total: 3000.000, accuracy: 0.978\n",
            "training epoch: [14 ] loss: 0.274 correct: 2935.000, total: 3000.000, accuracy: 0.978\n",
            "training epoch: [15 ] loss: 0.268 correct: 2936.000, total: 3000.000, accuracy: 0.979\n",
            "training epoch: [16 ] loss: 0.266 correct: 2937.000, total: 3000.000, accuracy: 0.979\n",
            "training epoch: [17 ] loss: 0.247 correct: 2939.000, total: 3000.000, accuracy: 0.980\n",
            "training epoch: [18 ] loss: 0.246 correct: 2940.000, total: 3000.000, accuracy: 0.980\n",
            "training epoch: [19 ] loss: 0.243 correct: 2940.000, total: 3000.000, accuracy: 0.980\n",
            "training epoch: [20 ] loss: 0.234 correct: 2941.000, total: 3000.000, accuracy: 0.980\n",
            "training epoch: [21 ] loss: 0.234 correct: 2941.000, total: 3000.000, accuracy: 0.980\n",
            "training epoch: [22 ] loss: 0.233 correct: 2941.000, total: 3000.000, accuracy: 0.980\n",
            "training epoch: [23 ] loss: 0.233 correct: 2941.000, total: 3000.000, accuracy: 0.980\n",
            "training epoch: [24 ] loss: 0.229 correct: 2942.000, total: 3000.000, accuracy: 0.981\n",
            "training epoch: [25 ] loss: 0.229 correct: 2942.000, total: 3000.000, accuracy: 0.981\n",
            "training epoch: [26 ] loss: 0.229 correct: 2942.000, total: 3000.000, accuracy: 0.981\n",
            "training epoch: [27 ] loss: 0.229 correct: 2942.000, total: 3000.000, accuracy: 0.981\n",
            "training epoch: [28 ] loss: 0.229 correct: 2942.000, total: 3000.000, accuracy: 0.981\n",
            "training epoch: [29 ] loss: 0.229 correct: 2942.000, total: 3000.000, accuracy: 0.981\n",
            "training epoch: [30 ] loss: 0.228 correct: 2942.000, total: 3000.000, accuracy: 0.981\n",
            "training epoch: [31 ] loss: 0.228 correct: 2942.000, total: 3000.000, accuracy: 0.981\n",
            "training epoch: [32 ] loss: 0.228 correct: 2942.000, total: 3000.000, accuracy: 0.981\n",
            "training epoch: [33 ] loss: 0.224 correct: 2944.000, total: 3000.000, accuracy: 0.981\n",
            "training epoch: [34 ] loss: 0.236 correct: 2944.000, total: 3000.000, accuracy: 0.981\n",
            "training epoch: [35 ] loss: 0.222 correct: 2946.000, total: 3000.000, accuracy: 0.982\n",
            "training epoch: [36 ] loss: 0.222 correct: 2945.000, total: 3000.000, accuracy: 0.982\n",
            "training epoch: [37 ] loss: 0.220 correct: 2945.000, total: 3000.000, accuracy: 0.982\n",
            "training epoch: [38 ] loss: 0.208 correct: 2946.000, total: 3000.000, accuracy: 0.982\n",
            "training epoch: [39 ] loss: 0.208 correct: 2946.000, total: 3000.000, accuracy: 0.982\n",
            "training epoch: [40 ] loss: 0.208 correct: 2946.000, total: 3000.000, accuracy: 0.982\n",
            "training epoch: [41 ] loss: 0.207 correct: 2946.000, total: 3000.000, accuracy: 0.982\n",
            "training epoch: [42 ] loss: 0.209 correct: 2946.000, total: 3000.000, accuracy: 0.982\n",
            "training epoch: [43 ] loss: 0.209 correct: 2946.000, total: 3000.000, accuracy: 0.982\n",
            "training epoch: [44 ] loss: 0.209 correct: 2946.000, total: 3000.000, accuracy: 0.982\n",
            "training epoch: [45 ] loss: 0.207 correct: 2946.000, total: 3000.000, accuracy: 0.982\n",
            "training epoch: [46 ] loss: 0.205 correct: 2947.000, total: 3000.000, accuracy: 0.982\n",
            "training epoch: [47 ] loss: 0.205 correct: 2947.000, total: 3000.000, accuracy: 0.982\n",
            "training epoch: [48 ] loss: 0.205 correct: 2947.000, total: 3000.000, accuracy: 0.982\n",
            "training epoch: [49 ] loss: 0.205 correct: 2947.000, total: 3000.000, accuracy: 0.982\n",
            "training epoch: [50 ] loss: 0.200 correct: 2947.000, total: 3000.000, accuracy: 0.982\n",
            "training epoch: [51 ] loss: 0.200 correct: 2948.000, total: 3000.000, accuracy: 0.983\n",
            "training epoch: [52 ] loss: 0.200 correct: 2948.000, total: 3000.000, accuracy: 0.983\n",
            "training epoch: [53 ] loss: 0.200 correct: 2948.000, total: 3000.000, accuracy: 0.983\n",
            "training epoch: [54 ] loss: 0.200 correct: 2948.000, total: 3000.000, accuracy: 0.983\n",
            "training epoch: [55 ] loss: 0.200 correct: 2948.000, total: 3000.000, accuracy: 0.983\n",
            "training epoch: [56 ] loss: 0.200 correct: 2948.000, total: 3000.000, accuracy: 0.983\n",
            "training epoch: [57 ] loss: 0.200 correct: 2948.000, total: 3000.000, accuracy: 0.983\n",
            "training epoch: [58 ] loss: 0.200 correct: 2948.000, total: 3000.000, accuracy: 0.983\n",
            "training epoch: [59 ] loss: 0.200 correct: 2948.000, total: 3000.000, accuracy: 0.983\n",
            "training epoch: [60 ] loss: 0.200 correct: 2948.000, total: 3000.000, accuracy: 0.983\n",
            "training epoch: [61 ] loss: 0.200 correct: 2948.000, total: 3000.000, accuracy: 0.983\n",
            "training epoch: [62 ] loss: 0.200 correct: 2948.000, total: 3000.000, accuracy: 0.983\n",
            "training epoch: [63 ] loss: 0.200 correct: 2948.000, total: 3000.000, accuracy: 0.983\n",
            "training epoch: [64 ] loss: 0.200 correct: 2948.000, total: 3000.000, accuracy: 0.983\n",
            "training epoch: [65 ] loss: 0.200 correct: 2948.000, total: 3000.000, accuracy: 0.983\n",
            "training epoch: [66 ] loss: 0.200 correct: 2948.000, total: 3000.000, accuracy: 0.983\n",
            "training epoch: [67 ] loss: 0.200 correct: 2948.000, total: 3000.000, accuracy: 0.983\n",
            "training epoch: [68 ] loss: 0.199 correct: 2948.000, total: 3000.000, accuracy: 0.983\n",
            "training epoch: [69 ] loss: 0.200 correct: 2948.000, total: 3000.000, accuracy: 0.983\n",
            "training epoch: [70 ] loss: 0.200 correct: 2948.000, total: 3000.000, accuracy: 0.983\n",
            "training epoch: [71 ] loss: 0.200 correct: 2948.000, total: 3000.000, accuracy: 0.983\n",
            "training epoch: [72 ] loss: 0.200 correct: 2948.000, total: 3000.000, accuracy: 0.983\n",
            "training epoch: [73 ] loss: 0.200 correct: 2948.000, total: 3000.000, accuracy: 0.983\n",
            "training epoch: [74 ] loss: 0.200 correct: 2948.000, total: 3000.000, accuracy: 0.983\n",
            "training epoch: [75 ] loss: 0.200 correct: 2948.000, total: 3000.000, accuracy: 0.983\n",
            "training epoch: [76 ] loss: 0.200 correct: 2948.000, total: 3000.000, accuracy: 0.983\n",
            "training epoch: [77 ] loss: 0.199 correct: 2948.000, total: 3000.000, accuracy: 0.983\n",
            "training epoch: [78 ] loss: 0.200 correct: 2948.000, total: 3000.000, accuracy: 0.983\n",
            "training epoch: [79 ] loss: 0.200 correct: 2948.000, total: 3000.000, accuracy: 0.983\n",
            "training epoch: [80 ] loss: 0.199 correct: 2948.000, total: 3000.000, accuracy: 0.983\n",
            "training epoch: [81 ] loss: 0.200 correct: 2948.000, total: 3000.000, accuracy: 0.983\n",
            "training epoch: [82 ] loss: 0.199 correct: 2948.000, total: 3000.000, accuracy: 0.983\n",
            "training epoch: [83 ] loss: 0.199 correct: 2948.000, total: 3000.000, accuracy: 0.983\n",
            "training epoch: [84 ] loss: 0.199 correct: 2948.000, total: 3000.000, accuracy: 0.983\n",
            "training epoch: [85 ] loss: 0.199 correct: 2948.000, total: 3000.000, accuracy: 0.983\n",
            "training epoch: [86 ] loss: 0.199 correct: 2948.000, total: 3000.000, accuracy: 0.983\n",
            "training epoch: [87 ] loss: 0.198 correct: 2949.000, total: 3000.000, accuracy: 0.983\n",
            "training epoch: [88 ] loss: 0.198 correct: 2949.000, total: 3000.000, accuracy: 0.983\n",
            "training epoch: [89 ] loss: 0.199 correct: 2949.000, total: 3000.000, accuracy: 0.983\n",
            "training epoch: [90 ] loss: 0.199 correct: 2949.000, total: 3000.000, accuracy: 0.983\n",
            "training epoch: [91 ] loss: 0.198 correct: 2949.000, total: 3000.000, accuracy: 0.983\n",
            "training epoch: [92 ] loss: 0.198 correct: 2949.000, total: 3000.000, accuracy: 0.983\n",
            "training epoch: [93 ] loss: 0.197 correct: 2950.000, total: 3000.000, accuracy: 0.983\n",
            "training epoch: [94 ] loss: 0.197 correct: 2951.000, total: 3000.000, accuracy: 0.984\n",
            "training epoch: [95 ] loss: 0.197 correct: 2951.000, total: 3000.000, accuracy: 0.984\n",
            "training epoch: [96 ] loss: 0.196 correct: 2951.000, total: 3000.000, accuracy: 0.984\n",
            "training epoch: [97 ] loss: 0.197 correct: 2951.000, total: 3000.000, accuracy: 0.984\n",
            "training epoch: [98 ] loss: 0.197 correct: 2951.000, total: 3000.000, accuracy: 0.984\n",
            "training epoch: [99 ] loss: 0.197 correct: 2951.000, total: 3000.000, accuracy: 0.984\n",
            "training epoch: [100 ] loss: 0.197 correct: 2951.000, total: 3000.000, accuracy: 0.984\n",
            "training epoch: [101 ] loss: 0.208 correct: 2950.000, total: 3000.000, accuracy: 0.983\n",
            "training epoch: [102 ] loss: 0.197 correct: 2951.000, total: 3000.000, accuracy: 0.984\n",
            "training epoch: [103 ] loss: 0.204 correct: 2950.000, total: 3000.000, accuracy: 0.983\n",
            "training epoch: [104 ] loss: 0.197 correct: 2951.000, total: 3000.000, accuracy: 0.984\n",
            "training epoch: [105 ] loss: 0.196 correct: 2951.000, total: 3000.000, accuracy: 0.984\n",
            "training epoch: [106 ] loss: 0.197 correct: 2951.000, total: 3000.000, accuracy: 0.984\n",
            "training epoch: [107 ] loss: 0.196 correct: 2951.000, total: 3000.000, accuracy: 0.984\n",
            "training epoch: [108 ] loss: 0.197 correct: 2951.000, total: 3000.000, accuracy: 0.984\n",
            "training epoch: [109 ] loss: 0.197 correct: 2951.000, total: 3000.000, accuracy: 0.984\n",
            "training epoch: [110 ] loss: 0.197 correct: 2951.000, total: 3000.000, accuracy: 0.984\n",
            "training epoch: [111 ] loss: 0.197 correct: 2951.000, total: 3000.000, accuracy: 0.984\n",
            "training epoch: [112 ] loss: 0.197 correct: 2951.000, total: 3000.000, accuracy: 0.984\n",
            "training epoch: [113 ] loss: 0.197 correct: 2951.000, total: 3000.000, accuracy: 0.984\n",
            "training epoch: [114 ] loss: 0.197 correct: 2951.000, total: 3000.000, accuracy: 0.984\n",
            "training epoch: [115 ] loss: 0.197 correct: 2951.000, total: 3000.000, accuracy: 0.984\n",
            "training epoch: [116 ] loss: 0.197 correct: 2951.000, total: 3000.000, accuracy: 0.984\n",
            "training epoch: [117 ] loss: 0.197 correct: 2951.000, total: 3000.000, accuracy: 0.984\n",
            "training epoch: [118 ] loss: 0.197 correct: 2951.000, total: 3000.000, accuracy: 0.984\n",
            "training epoch: [119 ] loss: 0.197 correct: 2951.000, total: 3000.000, accuracy: 0.984\n",
            "training epoch: [120 ] loss: 0.197 correct: 2951.000, total: 3000.000, accuracy: 0.984\n",
            "training epoch: [121 ] loss: 0.197 correct: 2951.000, total: 3000.000, accuracy: 0.984\n",
            "training epoch: [122 ] loss: 0.197 correct: 2951.000, total: 3000.000, accuracy: 0.984\n",
            "training epoch: [123 ] loss: 0.197 correct: 2951.000, total: 3000.000, accuracy: 0.984\n",
            "training epoch: [124 ] loss: 0.196 correct: 2951.000, total: 3000.000, accuracy: 0.984\n",
            "training epoch: [125 ] loss: 0.197 correct: 2951.000, total: 3000.000, accuracy: 0.984\n",
            "training epoch: [126 ] loss: 0.197 correct: 2951.000, total: 3000.000, accuracy: 0.984\n",
            "training epoch: [127 ] loss: 0.196 correct: 2951.000, total: 3000.000, accuracy: 0.984\n",
            "training epoch: [128 ] loss: 0.196 correct: 2951.000, total: 3000.000, accuracy: 0.984\n",
            "training epoch: [129 ] loss: 0.197 correct: 2951.000, total: 3000.000, accuracy: 0.984\n",
            "training epoch: [130 ] loss: 0.197 correct: 2951.000, total: 3000.000, accuracy: 0.984\n",
            "training epoch: [131 ] loss: 0.196 correct: 2951.000, total: 3000.000, accuracy: 0.984\n",
            "training epoch: [132 ] loss: 0.197 correct: 2951.000, total: 3000.000, accuracy: 0.984\n",
            "training epoch: [133 ] loss: 0.196 correct: 2951.000, total: 3000.000, accuracy: 0.984\n",
            "training epoch: [134 ] loss: 0.197 correct: 2951.000, total: 3000.000, accuracy: 0.984\n",
            "training epoch: [135 ] loss: 0.197 correct: 2951.000, total: 3000.000, accuracy: 0.984\n",
            "training epoch: [136 ] loss: 0.197 correct: 2951.000, total: 3000.000, accuracy: 0.984\n",
            "training epoch: [137 ] loss: 0.196 correct: 2951.000, total: 3000.000, accuracy: 0.984\n",
            "training epoch: [138 ] loss: 0.197 correct: 2951.000, total: 3000.000, accuracy: 0.984\n",
            "training epoch: [139 ] loss: 0.196 correct: 2951.000, total: 3000.000, accuracy: 0.984\n",
            "training epoch: [140 ] loss: 0.196 correct: 2951.000, total: 3000.000, accuracy: 0.984\n",
            "training epoch: [141 ] loss: 0.197 correct: 2951.000, total: 3000.000, accuracy: 0.984\n",
            "training epoch: [142 ] loss: 0.196 correct: 2951.000, total: 3000.000, accuracy: 0.984\n",
            "training epoch: [143 ] loss: 0.196 correct: 2951.000, total: 3000.000, accuracy: 0.984\n",
            "training epoch: [144 ] loss: 0.196 correct: 2951.000, total: 3000.000, accuracy: 0.984\n",
            "training epoch: [145 ] loss: 0.197 correct: 2951.000, total: 3000.000, accuracy: 0.984\n",
            "training epoch: [146 ] loss: 0.197 correct: 2951.000, total: 3000.000, accuracy: 0.984\n",
            "training epoch: [147 ] loss: 0.196 correct: 2951.000, total: 3000.000, accuracy: 0.984\n",
            "training epoch: [148 ] loss: 0.197 correct: 2951.000, total: 3000.000, accuracy: 0.984\n",
            "training epoch: [149 ] loss: 0.196 correct: 2951.000, total: 3000.000, accuracy: 0.984\n",
            "training epoch: [150 ] loss: 0.196 correct: 2951.000, total: 3000.000, accuracy: 0.984\n",
            "training epoch: [151 ] loss: 0.197 correct: 2951.000, total: 3000.000, accuracy: 0.984\n",
            "training epoch: [152 ] loss: 0.197 correct: 2951.000, total: 3000.000, accuracy: 0.984\n",
            "training epoch: [153 ] loss: 0.197 correct: 2951.000, total: 3000.000, accuracy: 0.984\n",
            "training epoch: [154 ] loss: 0.197 correct: 2951.000, total: 3000.000, accuracy: 0.984\n",
            "training epoch: [155 ] loss: 0.196 correct: 2951.000, total: 3000.000, accuracy: 0.984\n",
            "training epoch: [156 ] loss: 0.196 correct: 2951.000, total: 3000.000, accuracy: 0.984\n",
            "training epoch: [157 ] loss: 0.196 correct: 2951.000, total: 3000.000, accuracy: 0.984\n",
            "training epoch: [158 ] loss: 0.187 correct: 2951.000, total: 3000.000, accuracy: 0.984\n",
            "training epoch: [159 ] loss: 0.187 correct: 2951.000, total: 3000.000, accuracy: 0.984\n",
            "training epoch: [160 ] loss: 0.186 correct: 2951.000, total: 3000.000, accuracy: 0.984\n",
            "training epoch: [161 ] loss: 0.187 correct: 2951.000, total: 3000.000, accuracy: 0.984\n",
            "training epoch: [162 ] loss: 0.187 correct: 2951.000, total: 3000.000, accuracy: 0.984\n",
            "training epoch: [163 ] loss: 0.187 correct: 2951.000, total: 3000.000, accuracy: 0.984\n",
            "training epoch: [164 ] loss: 0.187 correct: 2951.000, total: 3000.000, accuracy: 0.984\n",
            "training epoch: [165 ] loss: 0.187 correct: 2951.000, total: 3000.000, accuracy: 0.984\n",
            "training epoch: [166 ] loss: 0.187 correct: 2951.000, total: 3000.000, accuracy: 0.984\n",
            "training epoch: [167 ] loss: 0.187 correct: 2951.000, total: 3000.000, accuracy: 0.984\n",
            "training epoch: [168 ] loss: 0.186 correct: 2951.000, total: 3000.000, accuracy: 0.984\n",
            "training epoch: [169 ] loss: 0.187 correct: 2951.000, total: 3000.000, accuracy: 0.984\n",
            "training epoch: [170 ] loss: 0.187 correct: 2951.000, total: 3000.000, accuracy: 0.984\n",
            "training epoch: [171 ] loss: 0.187 correct: 2951.000, total: 3000.000, accuracy: 0.984\n",
            "training epoch: [172 ] loss: 0.187 correct: 2951.000, total: 3000.000, accuracy: 0.984\n",
            "training epoch: [173 ] loss: 0.187 correct: 2951.000, total: 3000.000, accuracy: 0.984\n",
            "training epoch: [174 ] loss: 0.186 correct: 2951.000, total: 3000.000, accuracy: 0.984\n",
            "training epoch: [175 ] loss: 0.186 correct: 2951.000, total: 3000.000, accuracy: 0.984\n",
            "training epoch: [176 ] loss: 0.187 correct: 2951.000, total: 3000.000, accuracy: 0.984\n",
            "training epoch: [177 ] loss: 0.187 correct: 2951.000, total: 3000.000, accuracy: 0.984\n",
            "training epoch: [178 ] loss: 0.185 correct: 2951.000, total: 3000.000, accuracy: 0.984\n",
            "training epoch: [179 ] loss: 0.184 correct: 2951.000, total: 3000.000, accuracy: 0.984\n",
            "training epoch: [180 ] loss: 0.184 correct: 2951.000, total: 3000.000, accuracy: 0.984\n",
            "training epoch: [181 ] loss: 0.184 correct: 2951.000, total: 3000.000, accuracy: 0.984\n",
            "training epoch: [182 ] loss: 0.185 correct: 2951.000, total: 3000.000, accuracy: 0.984\n",
            "training epoch: [183 ] loss: 0.184 correct: 2951.000, total: 3000.000, accuracy: 0.984\n",
            "training epoch: [184 ] loss: 0.184 correct: 2951.000, total: 3000.000, accuracy: 0.984\n",
            "training epoch: [185 ] loss: 0.184 correct: 2951.000, total: 3000.000, accuracy: 0.984\n",
            "training epoch: [186 ] loss: 0.184 correct: 2951.000, total: 3000.000, accuracy: 0.984\n",
            "training epoch: [187 ] loss: 0.184 correct: 2951.000, total: 3000.000, accuracy: 0.984\n",
            "training epoch: [188 ] loss: 0.185 correct: 2951.000, total: 3000.000, accuracy: 0.984\n",
            "training epoch: [189 ] loss: 0.184 correct: 2951.000, total: 3000.000, accuracy: 0.984\n",
            "training epoch: [190 ] loss: 0.184 correct: 2951.000, total: 3000.000, accuracy: 0.984\n",
            "training epoch: [191 ] loss: 0.184 correct: 2951.000, total: 3000.000, accuracy: 0.984\n",
            "training epoch: [192 ] loss: 0.184 correct: 2951.000, total: 3000.000, accuracy: 0.984\n",
            "training epoch: [193 ] loss: 0.185 correct: 2951.000, total: 3000.000, accuracy: 0.984\n",
            "training epoch: [194 ] loss: 0.184 correct: 2951.000, total: 3000.000, accuracy: 0.984\n",
            "training epoch: [195 ] loss: 0.184 correct: 2951.000, total: 3000.000, accuracy: 0.984\n",
            "training epoch: [196 ] loss: 0.184 correct: 2951.000, total: 3000.000, accuracy: 0.984\n",
            "training epoch: [197 ] loss: 0.184 correct: 2951.000, total: 3000.000, accuracy: 0.984\n",
            "training epoch: [198 ] loss: 0.185 correct: 2951.000, total: 3000.000, accuracy: 0.984\n",
            "training epoch: [199 ] loss: 0.184 correct: 2951.000, total: 3000.000, accuracy: 0.984\n",
            "training epoch: [200 ] loss: 0.184 correct: 2951.000, total: 3000.000, accuracy: 0.984\n",
            "Finished Training run \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AciJnAh5nfug"
      },
      "source": [
        "columns = [\"epochs\", \"argmax > 0.5\" ,\"argmax < 0.5\", \"focus_true_pred_true\", \"focus_false_pred_true\", \"focus_true_pred_false\", \"focus_false_pred_false\" ]\n",
        "df_train = pd.DataFrame()\n",
        "df_test = pd.DataFrame()\n",
        "df_train[columns[0]] = np.arange(0,epoch+2)\n",
        "df_train[columns[1]] = analysis_data_tr[:,-2]\n",
        "df_train[columns[2]] = analysis_data_tr[:,-1]\n",
        "df_train[columns[3]] = analysis_data_tr[:,0]\n",
        "df_train[columns[4]] = analysis_data_tr[:,1]\n",
        "df_train[columns[5]] = analysis_data_tr[:,2]\n",
        "df_train[columns[6]] = analysis_data_tr[:,3]"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "NoQpS_6scRsC",
        "outputId": "ab348f47-a82d-4b89-c496-d14cc146ebf9"
      },
      "source": [
        "df_train"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>epochs</th>\n",
              "      <th>argmax &gt; 0.5</th>\n",
              "      <th>argmax &lt; 0.5</th>\n",
              "      <th>focus_true_pred_true</th>\n",
              "      <th>focus_false_pred_true</th>\n",
              "      <th>focus_true_pred_false</th>\n",
              "      <th>focus_false_pred_false</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>302</td>\n",
              "      <td>2698</td>\n",
              "      <td>135</td>\n",
              "      <td>936</td>\n",
              "      <td>179</td>\n",
              "      <td>1750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1189</td>\n",
              "      <td>1811</td>\n",
              "      <td>425</td>\n",
              "      <td>1854</td>\n",
              "      <td>121</td>\n",
              "      <td>600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>1660</td>\n",
              "      <td>1340</td>\n",
              "      <td>612</td>\n",
              "      <td>2159</td>\n",
              "      <td>23</td>\n",
              "      <td>206</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>1684</td>\n",
              "      <td>1316</td>\n",
              "      <td>677</td>\n",
              "      <td>2222</td>\n",
              "      <td>5</td>\n",
              "      <td>96</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>1683</td>\n",
              "      <td>1317</td>\n",
              "      <td>695</td>\n",
              "      <td>2225</td>\n",
              "      <td>2</td>\n",
              "      <td>78</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>196</th>\n",
              "      <td>196</td>\n",
              "      <td>1714</td>\n",
              "      <td>1286</td>\n",
              "      <td>702</td>\n",
              "      <td>2249</td>\n",
              "      <td>0</td>\n",
              "      <td>49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>197</th>\n",
              "      <td>197</td>\n",
              "      <td>1714</td>\n",
              "      <td>1286</td>\n",
              "      <td>702</td>\n",
              "      <td>2249</td>\n",
              "      <td>0</td>\n",
              "      <td>49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>198</th>\n",
              "      <td>198</td>\n",
              "      <td>1713</td>\n",
              "      <td>1287</td>\n",
              "      <td>702</td>\n",
              "      <td>2249</td>\n",
              "      <td>0</td>\n",
              "      <td>49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199</th>\n",
              "      <td>199</td>\n",
              "      <td>1713</td>\n",
              "      <td>1287</td>\n",
              "      <td>702</td>\n",
              "      <td>2249</td>\n",
              "      <td>0</td>\n",
              "      <td>49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>200</th>\n",
              "      <td>200</td>\n",
              "      <td>1714</td>\n",
              "      <td>1286</td>\n",
              "      <td>702</td>\n",
              "      <td>2249</td>\n",
              "      <td>0</td>\n",
              "      <td>49</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>201 rows Ã— 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     epochs  argmax > 0.5  ...  focus_true_pred_false  focus_false_pred_false\n",
              "0         0           302  ...                    179                    1750\n",
              "1         1          1189  ...                    121                     600\n",
              "2         2          1660  ...                     23                     206\n",
              "3         3          1684  ...                      5                      96\n",
              "4         4          1683  ...                      2                      78\n",
              "..      ...           ...  ...                    ...                     ...\n",
              "196     196          1714  ...                      0                      49\n",
              "197     197          1714  ...                      0                      49\n",
              "198     198          1713  ...                      0                      49\n",
              "199     199          1713  ...                      0                      49\n",
              "200     200          1714  ...                      0                      49\n",
              "\n",
              "[201 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "id": "IMAhRdxOcVf6",
        "outputId": "b7d93cc1-2b2d-44f7-c46b-0520bed4a7ca"
      },
      "source": [
        "fig= plt.figure(figsize=(6,6))\n",
        "plt.plot(df_train[columns[0]],df_train[columns[3]]/30, label =\"focus_true_pred_true \")\n",
        "plt.plot(df_train[columns[0]],df_train[columns[4]]/30, label =\"focus_false_pred_true \")\n",
        "plt.plot(df_train[columns[0]],df_train[columns[5]]/30, label =\"focus_true_pred_false \")\n",
        "plt.plot(df_train[columns[0]],df_train[columns[6]]/30, label =\"focus_false_pred_false \")\n",
        "plt.title(\"On Train set\")\n",
        "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"percentage of data\")\n",
        "plt.xticks([0,5,10])\n",
        "#plt.vlines(vline_list,min(min(df_train[columns[3]]/300),min(df_train[columns[4]]/300),min(df_train[columns[5]]/300),min(df_train[columns[6]]/300)), max(max(df_train[columns[3]]/300),max(df_train[columns[4]]/300),max(df_train[columns[5]]/300),max(df_train[columns[6]]/300)),linestyles='dotted')\n",
        "plt.show()\n",
        "fig.savefig(\"train_analysis.pdf\")\n",
        "fig.savefig(\"train_analysis.png\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCnS6r2_3WdU"
      },
      "source": [
        "aph = []\n",
        "for i in bg:\n",
        "  aph.append(F.softmax(i,dim=1).detach().numpy())\n",
        "  \n",
        "aph = np.concatenate(aph,axis=0)\n",
        "torch.save({\n",
        "            'epoch': 500,\n",
        "            'model_state_dict': what_net.state_dict(),\n",
        "            #'optimizer_state_dict': optimizer_what.state_dict(),\n",
        "            \"optimizer_alpha\":optim1,\n",
        "            \"FTPT_analysis\":analysis_data_tr,\n",
        "            \"alpha\":aph\n",
        "\n",
        "            }, \"type4_what_net_500.pt\")"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVzrDOGS4UxU",
        "outputId": "0127b6ae-2a4b-4281-a30b-97ac54e8ebed"
      },
      "source": [
        "aph[0]"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.6747610e-01, 6.7559682e-02, 1.5346459e-01, 2.6536290e-10,\n",
              "       9.5572339e-10, 4.1449943e-01, 2.9989486e-10, 4.9353074e-11,\n",
              "       1.9700019e-01], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BwTDpx6STIPh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
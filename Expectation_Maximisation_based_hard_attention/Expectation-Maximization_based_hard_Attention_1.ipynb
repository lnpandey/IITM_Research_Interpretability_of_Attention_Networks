{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51664248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "from tqdm import tqdm\n",
    "%matplotlib inline\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm as tqdm\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91400c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Focus(nn.Module):\n",
    "    def __init__(self,input_dims):\n",
    "        super(Focus,self).__init__()\n",
    "        self.input_dims = input_dims\n",
    "        self.fc1 = nn.Linear(input_dims,1,bias=False)\n",
    "    def forward(self,x):\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00fa5275",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classification(nn.Module):\n",
    "    def __init__(self,input_dims,output_dims):\n",
    "        super(Classification,self).__init__()\n",
    "        self.input_dims = input_dims\n",
    "        self.output_dims = output_dims\n",
    "        self.fc1 = nn.Linear(input_dims,output_dims)\n",
    "    def forward(self,x):\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        if self.output_dims > 1:\n",
    "            x = x\n",
    "        else:\n",
    "            x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "574d6007",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss_focus(gamma,focus_output):\n",
    "    #print(gamma.shape,focus_output.shape)\n",
    "    log_outputs = torch.log(focus_output)\n",
    "    \n",
    "    loss_ = gamma*log_outputs\n",
    "    \n",
    "    #print(loss_.shape)\n",
    "    loss_ = torch.sum(loss_,dim=1)\n",
    "    #print(loss_.shape)\n",
    "    #print(torch.sum(-loss_>1))\n",
    "    \n",
    "    loss_ = -torch.mean(loss_,dim=0)\n",
    "    \n",
    "    #print(loss_.shape,loss_)\n",
    "    \n",
    "    return loss_ \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d200411",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss_classification(gamma,classification_output,label,criterion,n_patches):\n",
    "    \n",
    "    batch = label.size(0)\n",
    "    classes = classification_output.size(2)\n",
    "    #print(classification_output)\n",
    "    label = label.repeat_interleave(n_patches)\n",
    "    classification_output = classification_output.reshape((batch*n_patches,classes))\n",
    "    loss_ = criterion(classification_output,label)\n",
    "    \n",
    "    loss_ = loss_.reshape((batch,n_patches))\n",
    "    \n",
    "    loss_ = gamma*loss_\n",
    "    loss_ = torch.sum(loss_,dim=1)\n",
    "    loss_ = torch.mean(loss_,dim=0)\n",
    "    #print(loss_,loss_.shape)\n",
    "    \n",
    "    return loss_\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e873cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expectation_step(fc,cl,data,labels):\n",
    "    batch= data.size(0)\n",
    "    with torch.no_grad():\n",
    "        outputs_f = F.softmax(fc(data),dim=1)\n",
    "        #print(\"sds\",cl(data).shape)\n",
    "        outputs_g = F.softmax(cl(data),dim=2)\n",
    "        #print(outputs_f.shape,outputs_g.shape,outputs_g[0])\n",
    "        \n",
    "    outputs_g = outputs_g[np.arange(batch),:,labels]\n",
    "    #print(outputs_g.shape,outputs_g[0],outputs_f.shape)\n",
    "    \n",
    "    \n",
    "    \n",
    "    p_x_y_z = outputs_f[:,:,0]*outputs_g   #(1-outputs_g)\n",
    "    #print(p_x_y_z[0])\n",
    "    #print(torch.sum(p_x_y_z,dim=1,keepdims=True))\n",
    "    \n",
    "    normalized_p = p_x_y_z/torch.sum(p_x_y_z,dim=1,keepdims=True)\n",
    "    #print(normalized_p)\n",
    "    return normalized_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8cbcd09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def maximization_step(p_z,focus,classification,data,labels,focus_optimizer,classification_optimizer,Criterion):    \n",
    "    \n",
    "    patches = data.size(1)\n",
    "    focus_optimizer.zero_grad()\n",
    "    classification_optimizer.zero_grad()\n",
    "    \n",
    "    focus_outputs = F.softmax(focus(data),dim=1)[:,:,0]\n",
    "    classification_outputs = classification(data) # classification returns output after sigmoid/softmax\n",
    "    \n",
    "    \n",
    "    #print(focus_outputs,classification_outputs)\n",
    "    \n",
    "    loss_focus = calculate_loss_focus(p_z,focus_outputs)\n",
    "    loss_classification = calculate_loss_classification(p_z,classification_outputs,\n",
    "                                                        labels,Criterion,patches)\n",
    "    \n",
    "    print(\"Focus loss\",loss_focus.item())\n",
    "    print(\"Classification loss\",loss_classification.item())\n",
    "    loss_focus.backward() \n",
    "    loss_classification.backward()\n",
    "    focus_optimizer.step()\n",
    "    classification_optimizer.step()\n",
    "    \n",
    "    return focus,classification,focus_optimizer,classification_optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e032d3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3, 1]), torch.Size([2]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.tensor([[[3.],[3.],[-1.]],[[3.],[+1.],[3.]]])\n",
    "labels = torch.tensor([0,1])\n",
    "data.shape,labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4427b8f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Focus loss 1.0986123085021973\n",
      "Classification loss 0.7376129627227783\n",
      "Focus loss 1.103293538093567\n",
      "Classification loss 0.6849931478500366\n",
      "Focus loss 1.0956631898880005\n",
      "Classification loss 0.6763752698898315\n",
      "Focus loss 1.082407832145691\n",
      "Classification loss 0.6508828401565552\n",
      "Focus loss 1.0344674587249756\n",
      "Classification loss 0.6073635816574097\n",
      "Focus loss 0.92075514793396\n",
      "Classification loss 0.5168488025665283\n",
      "Focus loss 0.7539455890655518\n",
      "Classification loss 0.3891769051551819\n",
      "Focus loss 0.6281040906906128\n",
      "Classification loss 0.28865480422973633\n",
      "Focus loss 0.5569226741790771\n",
      "Classification loss 0.2299833595752716\n",
      "Focus loss 0.5112577080726624\n",
      "Classification loss 0.19291278719902039\n",
      "Focus loss 0.47777020931243896\n",
      "Classification loss 0.1666490137577057\n",
      "Focus loss 0.451302707195282\n",
      "Classification loss 0.14668597280979156\n",
      "Focus loss 0.4294686019420624\n",
      "Classification loss 0.13086259365081787\n",
      "Focus loss 0.41096049547195435\n",
      "Classification loss 0.11797450482845306\n",
      "Focus loss 0.39496952295303345\n",
      "Classification loss 0.10727103054523468\n",
      "Focus loss 0.3809526860713959\n",
      "Classification loss 0.09824717044830322\n",
      "Focus loss 0.3685239851474762\n",
      "Classification loss 0.09054488688707352\n",
      "Focus loss 0.35739850997924805\n",
      "Classification loss 0.08390174806118011\n",
      "Focus loss 0.3473590016365051\n",
      "Classification loss 0.07811959087848663\n",
      "Focus loss 0.33823657035827637\n",
      "Classification loss 0.07304637879133224\n",
      "Focus loss 0.3298972249031067\n",
      "Classification loss 0.0685630589723587\n",
      "Focus loss 0.32223302125930786\n",
      "Classification loss 0.06457549333572388\n",
      "Focus loss 0.3151557445526123\n",
      "Classification loss 0.061007991433143616\n",
      "Focus loss 0.30859267711639404\n",
      "Classification loss 0.05779927223920822\n",
      "Focus loss 0.3024832308292389\n",
      "Classification loss 0.0548991858959198\n",
      "Focus loss 0.29677629470825195\n",
      "Classification loss 0.052266426384449005\n",
      "Focus loss 0.2914285957813263\n",
      "Classification loss 0.04986638203263283\n",
      "Focus loss 0.28640303015708923\n",
      "Classification loss 0.04767027124762535\n",
      "Focus loss 0.2816678583621979\n",
      "Classification loss 0.045653775334358215\n",
      "Focus loss 0.2771952748298645\n",
      "Classification loss 0.043796055018901825\n",
      "Focus loss 0.2729616165161133\n",
      "Classification loss 0.04207944869995117\n",
      "Focus loss 0.2689454257488251\n",
      "Classification loss 0.0404888391494751\n",
      "Focus loss 0.26512840390205383\n",
      "Classification loss 0.039011016488075256\n",
      "Focus loss 0.26149415969848633\n",
      "Classification loss 0.03763467073440552\n",
      "Focus loss 0.2580283284187317\n",
      "Classification loss 0.036349810659885406\n",
      "Focus loss 0.254717618227005\n",
      "Classification loss 0.03514784574508667\n",
      "Focus loss 0.2515507638454437\n",
      "Classification loss 0.03402096778154373\n",
      "Focus loss 0.24851715564727783\n",
      "Classification loss 0.03296266496181488\n",
      "Focus loss 0.24560759961605072\n",
      "Classification loss 0.03196676820516586\n",
      "Focus loss 0.2428135871887207\n",
      "Classification loss 0.031028106808662415\n",
      "Focus loss 0.24012726545333862\n",
      "Classification loss 0.030141867697238922\n",
      "Focus loss 0.23754200339317322\n",
      "Classification loss 0.029303893446922302\n",
      "Focus loss 0.23505127429962158\n",
      "Classification loss 0.02851034700870514\n",
      "Focus loss 0.2326492965221405\n",
      "Classification loss 0.02775794267654419\n",
      "Focus loss 0.2303306609392166\n",
      "Classification loss 0.027043472975492477\n",
      "Focus loss 0.22809073328971863\n",
      "Classification loss 0.026364238932728767\n",
      "Focus loss 0.22592483460903168\n",
      "Classification loss 0.025717727839946747\n",
      "Focus loss 0.2238290011882782\n",
      "Classification loss 0.025101549923419952\n",
      "Focus loss 0.2217993587255478\n",
      "Classification loss 0.024513833224773407\n",
      "Focus loss 0.2198324054479599\n",
      "Classification loss 0.023952506482601166\n",
      "Focus loss 0.21792490780353546\n",
      "Classification loss 0.023415900766849518\n",
      "Focus loss 0.21607379615306854\n",
      "Classification loss 0.022902484983205795\n",
      "Focus loss 0.2142762541770935\n",
      "Classification loss 0.022410765290260315\n",
      "Focus loss 0.21252961456775665\n",
      "Classification loss 0.021939367055892944\n",
      "Focus loss 0.2108316570520401\n",
      "Classification loss 0.021487150341272354\n",
      "Focus loss 0.20917978882789612\n",
      "Classification loss 0.021052882075309753\n",
      "Focus loss 0.20757216215133667\n",
      "Classification loss 0.020635593682527542\n",
      "Focus loss 0.2060067504644394\n",
      "Classification loss 0.02023429609835148\n",
      "Focus loss 0.2044815570116043\n",
      "Classification loss 0.01984817162156105\n",
      "Focus loss 0.2029949277639389\n",
      "Classification loss 0.019476214423775673\n",
      "Focus loss 0.20154507458209991\n",
      "Classification loss 0.019117793068289757\n",
      "Focus loss 0.20013068616390228\n",
      "Classification loss 0.01877215877175331\n",
      "Focus loss 0.19875016808509827\n",
      "Classification loss 0.018438678234815598\n",
      "Focus loss 0.19740213453769684\n",
      "Classification loss 0.01811664178967476\n",
      "Focus loss 0.1960853636264801\n",
      "Classification loss 0.01780555583536625\n",
      "Focus loss 0.1947985291481018\n",
      "Classification loss 0.017504820600152016\n",
      "Focus loss 0.19354064762592316\n",
      "Classification loss 0.017213929444551468\n",
      "Focus loss 0.1923104077577591\n",
      "Classification loss 0.016932491213083267\n",
      "Focus loss 0.19110682606697083\n",
      "Classification loss 0.016659997403621674\n",
      "Focus loss 0.1899290531873703\n",
      "Classification loss 0.016395993530750275\n",
      "Focus loss 0.18877601623535156\n",
      "Classification loss 0.01614011824131012\n",
      "Focus loss 0.18764697015285492\n",
      "Classification loss 0.015892084687948227\n",
      "Focus loss 0.1865409016609192\n",
      "Classification loss 0.01565142720937729\n",
      "Focus loss 0.18545718491077423\n",
      "Classification loss 0.015417834743857384\n",
      "Focus loss 0.18439500033855438\n",
      "Classification loss 0.015191120095551014\n",
      "Focus loss 0.18335354328155518\n",
      "Classification loss 0.014970915392041206\n",
      "Focus loss 0.18233218789100647\n",
      "Classification loss 0.014756864868104458\n",
      "Focus loss 0.18133032321929932\n",
      "Classification loss 0.01454880926758051\n",
      "Focus loss 0.1803472340106964\n",
      "Classification loss 0.014346454292535782\n",
      "Focus loss 0.1793823540210724\n",
      "Classification loss 0.014149637892842293\n",
      "Focus loss 0.17843516170978546\n",
      "Classification loss 0.013958103954792023\n",
      "Focus loss 0.17750492691993713\n",
      "Classification loss 0.013771599158644676\n",
      "Focus loss 0.17659135162830353\n",
      "Classification loss 0.013589959591627121\n",
      "Focus loss 0.17569397389888763\n",
      "Classification loss 0.013412988744676113\n",
      "Focus loss 0.17481206357479095\n",
      "Classification loss 0.013240586966276169\n",
      "Focus loss 0.17394521832466125\n",
      "Classification loss 0.013072424568235874\n",
      "Focus loss 0.17309314012527466\n",
      "Classification loss 0.012908577919006348\n",
      "Focus loss 0.17225533723831177\n",
      "Classification loss 0.01274860743433237\n",
      "Focus loss 0.1714312881231308\n",
      "Classification loss 0.012592584826052189\n",
      "Focus loss 0.170620858669281\n",
      "Classification loss 0.012440293096005917\n",
      "Focus loss 0.16982337832450867\n",
      "Classification loss 0.012291581369936466\n",
      "Focus loss 0.16903874278068542\n",
      "Classification loss 0.012146390974521637\n",
      "Focus loss 0.1682664304971695\n",
      "Classification loss 0.01200457289814949\n",
      "Focus loss 0.16750626266002655\n",
      "Classification loss 0.01186596229672432\n",
      "Focus loss 0.1667577624320984\n",
      "Classification loss 0.011730456724762917\n",
      "Focus loss 0.16602067649364471\n",
      "Classification loss 0.011598007753491402\n",
      "Focus loss 0.1652947962284088\n",
      "Classification loss 0.011468568816781044\n",
      "Focus loss 0.16457967460155487\n",
      "Classification loss 0.011341860517859459\n",
      "Focus loss 0.16387519240379333\n",
      "Classification loss 0.011217949911952019\n",
      "Focus loss 0.16318099200725555\n",
      "Classification loss 0.011096671223640442\n",
      "Focus loss 0.16249682009220123\n",
      "Classification loss 0.010978036560118198\n",
      "Focus loss 0.16182248294353485\n",
      "Classification loss 0.010861760005354881\n",
      "Focus loss 0.16115769743919373\n",
      "Classification loss 0.010748026892542839\n",
      "Focus loss 0.16050222516059875\n",
      "Classification loss 0.010636551305651665\n",
      "Focus loss 0.159855917096138\n",
      "Classification loss 0.010527342557907104\n",
      "Focus loss 0.15921849012374878\n",
      "Classification loss 0.010420408099889755\n",
      "Focus loss 0.15858975052833557\n",
      "Classification loss 0.010315575636923313\n",
      "Focus loss 0.1579694300889969\n",
      "Classification loss 0.010212743654847145\n",
      "Focus loss 0.15735751390457153\n",
      "Classification loss 0.010112026706337929\n",
      "Focus loss 0.15675362944602966\n",
      "Classification loss 0.010013198480010033\n",
      "Focus loss 0.1561576873064041\n",
      "Classification loss 0.009916326031088829\n",
      "Focus loss 0.155569389462471\n",
      "Classification loss 0.009821240790188313\n",
      "Focus loss 0.15498875081539154\n",
      "Classification loss 0.00972793996334076\n",
      "Focus loss 0.15441550314426422\n",
      "Classification loss 0.00963643565773964\n",
      "Focus loss 0.1538494974374771\n",
      "Classification loss 0.009546561166644096\n",
      "Focus loss 0.15329056978225708\n",
      "Classification loss 0.009458368644118309\n",
      "Focus loss 0.15273864567279816\n",
      "Classification loss 0.009371806867420673\n",
      "Focus loss 0.15219339728355408\n",
      "Classification loss 0.009286770597100258\n",
      "Focus loss 0.1516548991203308\n",
      "Classification loss 0.009203257970511913\n",
      "Focus loss 0.15112288296222687\n",
      "Classification loss 0.009121213108301163\n",
      "Focus loss 0.1505972146987915\n",
      "Classification loss 0.009040642529726028\n",
      "Focus loss 0.15007781982421875\n",
      "Classification loss 0.008961431682109833\n",
      "Focus loss 0.14956453442573547\n",
      "Classification loss 0.0088835833594203\n",
      "Focus loss 0.14905723929405212\n",
      "Classification loss 0.008807160891592503\n",
      "Focus loss 0.14855580031871796\n",
      "Classification loss 0.008731934241950512\n",
      "Focus loss 0.148060142993927\n",
      "Classification loss 0.008657963946461678\n",
      "Focus loss 0.1475701630115509\n",
      "Classification loss 0.008585304021835327\n",
      "Focus loss 0.14708568155765533\n",
      "Classification loss 0.008513737469911575\n",
      "Focus loss 0.1466066837310791\n",
      "Classification loss 0.008443433791399002\n",
      "Focus loss 0.14613297581672668\n",
      "Classification loss 0.00837421789765358\n",
      "Focus loss 0.14566448330879211\n",
      "Classification loss 0.008306213654577732\n",
      "Focus loss 0.14520113170146942\n",
      "Classification loss 0.008239246904850006\n",
      "Focus loss 0.14474284648895264\n",
      "Classification loss 0.008173265494406223\n",
      "Focus loss 0.14428946375846863\n",
      "Classification loss 0.008108435198664665\n",
      "Focus loss 0.1438409388065338\n",
      "Classification loss 0.008044535294175148\n",
      "Focus loss 0.14339716732501984\n",
      "Classification loss 0.007981739938259125\n",
      "Focus loss 0.14295805990695953\n",
      "Classification loss 0.00791981816291809\n",
      "Focus loss 0.1425236165523529\n",
      "Classification loss 0.007858828641474247\n",
      "Focus loss 0.14209359884262085\n",
      "Classification loss 0.007798830047249794\n",
      "Focus loss 0.14166806638240814\n",
      "Classification loss 0.007739711552858353\n",
      "Focus loss 0.14124684035778046\n",
      "Classification loss 0.007681472226977348\n",
      "Focus loss 0.14082998037338257\n",
      "Classification loss 0.007624051067978144\n",
      "Focus loss 0.140417218208313\n",
      "Classification loss 0.007567574270069599\n",
      "Focus loss 0.14000864326953888\n",
      "Classification loss 0.007511867210268974\n",
      "Focus loss 0.1396040916442871\n",
      "Classification loss 0.007456927560269833\n",
      "Focus loss 0.1392035037279129\n",
      "Classification loss 0.00740287359803915\n",
      "Focus loss 0.13880695402622223\n",
      "Classification loss 0.007349530700594187\n",
      "Focus loss 0.13841411471366882\n",
      "Classification loss 0.007296964526176453\n",
      "Focus loss 0.1380252242088318\n",
      "Classification loss 0.007245169952511787\n",
      "Focus loss 0.13763996958732605\n",
      "Classification loss 0.007194034289568663\n",
      "Focus loss 0.1372583657503128\n",
      "Classification loss 0.007143673021346331\n",
      "Focus loss 0.13688036799430847\n",
      "Classification loss 0.007093916647136211\n",
      "Focus loss 0.13650600612163544\n",
      "Classification loss 0.007044933270663023\n",
      "Focus loss 0.1361350417137146\n",
      "Classification loss 0.006996558979153633\n",
      "Focus loss 0.1357676088809967\n",
      "Classification loss 0.0069489069283008575\n",
      "Focus loss 0.135403573513031\n",
      "Classification loss 0.0069018579088151455\n",
      "Focus loss 0.1350427269935608\n",
      "Classification loss 0.006855417974293232\n",
      "Focus loss 0.13468527793884277\n",
      "Classification loss 0.006809646263718605\n",
      "Focus loss 0.13433103263378143\n",
      "Classification loss 0.006764479447156191\n",
      "Focus loss 0.13398000597953796\n",
      "Classification loss 0.006719808094203472\n",
      "Focus loss 0.13363203406333923\n",
      "Classification loss 0.006675862707197666\n",
      "Focus loss 0.13328716158866882\n",
      "Classification loss 0.006632412783801556\n",
      "Focus loss 0.13294537365436554\n",
      "Classification loss 0.00658946018666029\n",
      "Focus loss 0.13260656595230103\n",
      "Classification loss 0.006547175347805023\n",
      "Focus loss 0.1322707235813141\n",
      "Classification loss 0.0065053291618824005\n",
      "Focus loss 0.13193772733211517\n",
      "Classification loss 0.006464099511504173\n",
      "Focus loss 0.1316075325012207\n",
      "Classification loss 0.0064233094453811646\n",
      "Focus loss 0.13128025829792023\n",
      "Classification loss 0.00638301856815815\n",
      "Focus loss 0.13095583021640778\n",
      "Classification loss 0.00634328369051218\n",
      "Focus loss 0.13063403964042664\n",
      "Classification loss 0.006303988862782717\n",
      "Focus loss 0.1303148865699768\n",
      "Classification loss 0.006265254225581884\n",
      "Focus loss 0.12999852001667023\n",
      "Classification loss 0.006226908415555954\n",
      "Focus loss 0.1296846568584442\n",
      "Classification loss 0.00618906132876873\n",
      "Focus loss 0.1293734312057495\n",
      "Classification loss 0.0061516594141721725\n",
      "Focus loss 0.12906479835510254\n",
      "Classification loss 0.006114699877798557\n",
      "Focus loss 0.12875866889953613\n",
      "Classification loss 0.006078187841922045\n",
      "Focus loss 0.12845496833324432\n",
      "Classification loss 0.00604217778891325\n",
      "Focus loss 0.1281536966562271\n",
      "Classification loss 0.0060064420104026794\n",
      "Focus loss 0.1278548389673233\n",
      "Classification loss 0.005971209146082401\n",
      "Focus loss 0.1275584101676941\n",
      "Classification loss 0.0059364233165979385\n",
      "Focus loss 0.12726430594921112\n",
      "Classification loss 0.005901968106627464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Focus loss 0.126972496509552\n",
      "Classification loss 0.00586796086281538\n",
      "Focus loss 0.12668296694755554\n",
      "Classification loss 0.005834284704178572\n",
      "Focus loss 0.12639571726322174\n",
      "Classification loss 0.005801002029329538\n",
      "Focus loss 0.12611067295074463\n",
      "Classification loss 0.0057681649923324585\n",
      "Focus loss 0.1258278340101242\n",
      "Classification loss 0.0057356031611561775\n",
      "Focus loss 0.1255471259355545\n",
      "Classification loss 0.005703491624444723\n",
      "Focus loss 0.12526853382587433\n",
      "Classification loss 0.005671655759215355\n",
      "Focus loss 0.12499206513166428\n",
      "Classification loss 0.0056402115151286125\n",
      "Focus loss 0.12471773475408554\n",
      "Classification loss 0.005609102547168732\n",
      "Focus loss 0.12444540858268738\n",
      "Classification loss 0.005578329786658287\n",
      "Focus loss 0.12417513877153397\n",
      "Classification loss 0.00554788950830698\n",
      "Focus loss 0.12390682101249695\n",
      "Classification loss 0.005517784506082535\n",
      "Focus loss 0.1236405298113823\n",
      "Classification loss 0.005488013848662376\n",
      "Focus loss 0.12337622046470642\n",
      "Classification loss 0.005458523984998465\n",
      "Focus loss 0.12311379611492157\n",
      "Classification loss 0.0054294271394610405\n",
      "Focus loss 0.12285321205854416\n",
      "Classification loss 0.005400551483035088\n",
      "Focus loss 0.12259452044963837\n",
      "Classification loss 0.005372010171413422\n",
      "Focus loss 0.12233779579401016\n",
      "Classification loss 0.005343803204596043\n",
      "Focus loss 0.12208281457424164\n",
      "Classification loss 0.005315821617841721\n"
     ]
    }
   ],
   "source": [
    "focus = Focus(1)\n",
    "#print(focus.fc1.weight.data)\n",
    "focus.fc1.weight.data = torch.tensor([[0.]])\n",
    "classification = Classification(1,2)\n",
    "#print(classification.fc1.weight.data)\n",
    "classification.fc1.weight.data = torch.tensor([[0.1],[-0.1]])\n",
    "classification.fc1.bias.data = torch.tensor([0.,0.])\n",
    "\n",
    "Criterion = nn.CrossEntropyLoss(reduction=\"none\") #nn.BCELoss(reduction=\"none\")\n",
    "focus_optimizer = optim.SGD(focus.parameters(), lr=0.5)\n",
    "classification_optimizer = optim.SGD(classification.parameters(),lr=0.5)\n",
    "\n",
    "for i in range(200):\n",
    "    p_z = expectation_step(focus,classification,data,labels)\n",
    "    #print(p_z.shape)\n",
    "    focus,classification,focus_optimizer,classification_optimizer=maximization_step(p_z\n",
    "                                                                                ,focus,classification,data,\n",
    "                                                                                labels,focus_optimizer,\n",
    "                                                                                classification_optimizer,\n",
    "                                                                                Criterion)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7688a5ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-1.7961]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for params in focus.parameters():\n",
    "    print(params)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "43a91ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-2.6065],\n",
      "        [ 2.6065]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0283, -0.0283], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for params in classification.parameters():\n",
    "    print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "15a421eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Focus True 100.0\n",
      "Accuracy 100.0\n"
     ]
    }
   ],
   "source": [
    "# method 1\n",
    "batch = data.size(0)\n",
    "indexes = torch.argmax(F.softmax(focus(data),dim=1),dim=1)[:,0].numpy()\n",
    "print(\"Focus True\",(np.sum(indexes == fore_idx,axis=0).item()/len(fore_idx))*100)\n",
    "outputs = F.softmax(classification(data[np.arange(batch),indexes,:]),dim=1)\n",
    "prediction = torch.argmax(outputs,dim=1)\n",
    "accuracy = (torch.sum(prediction == labels,dim=0)/len(labels) )*100\n",
    "print(\"Accuracy\", accuracy.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f20f7cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Focus True 100.0\n",
      "Accuracy 100.0\n"
     ]
    }
   ],
   "source": [
    "# method 2\n",
    "focus_output = F.softmax(focus(data),dim=1)\n",
    "indexes = torch.argmax(F.softmax(focus(data),dim=1),dim=1)[:,0].numpy()\n",
    "classification_output = F.softmax(classification(data),dim=2)\n",
    "print(\"Focus True\",(np.sum(indexes == fore_idx,axis=0).item()/len(fore_idx))*100)\n",
    "prediction = torch.argmax(torch.sum(focus_output*classification_output,dim=1),dim=1)\n",
    "accuracy = (torch.sum(prediction == labels,dim=0)/len(labels) )*100\n",
    "print(\"Accuracy\", accuracy.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e07ca3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198dfab7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d46185",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

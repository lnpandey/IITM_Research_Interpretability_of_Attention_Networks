{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ff4e98c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-21T05:49:13.855862Z",
     "iopub.status.busy": "2022-03-21T05:49:13.843825Z",
     "iopub.status.idle": "2022-03-21T05:49:15.676965Z",
     "shell.execute_reply": "2022-03-21T05:49:15.677471Z",
     "shell.execute_reply.started": "2022-03-21T05:46:08.434237Z"
    },
    "papermill": {
     "duration": 1.858124,
     "end_time": "2022-03-21T05:49:15.677730",
     "exception": false,
     "start_time": "2022-03-21T05:49:13.819606",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "from tqdm import tqdm\n",
    "%matplotlib inline\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm as tqdm\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fb01174",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-21T05:49:15.728382Z",
     "iopub.status.busy": "2022-03-21T05:49:15.727602Z",
     "iopub.status.idle": "2022-03-21T05:49:15.736442Z",
     "shell.execute_reply": "2022-03-21T05:49:15.735984Z",
     "shell.execute_reply.started": "2022-03-21T05:46:11.875614Z"
    },
    "papermill": {
     "duration": 0.036371,
     "end_time": "2022-03-21T05:49:15.736549",
     "exception": false,
     "start_time": "2022-03-21T05:49:15.700178",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Focus(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Focus, self).__init__()\n",
    "\n",
    "    self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=0, bias=False)\n",
    "    self.pool = nn.MaxPool2d(2, 2)\n",
    "    self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=0, bias=False)\n",
    "    self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=0, bias=False)\n",
    "    self.fc1 = nn.Linear(1024, 512, bias=False)\n",
    "    self.fc2 = nn.Linear(512, 64, bias=False)\n",
    "    self.fc3 = nn.Linear(64, 10, bias=False)\n",
    "    self.fc4 = nn.Linear(10,1, bias=False)\n",
    "\n",
    "    torch.nn.init.xavier_normal_(self.conv1.weight)\n",
    "    torch.nn.init.xavier_normal_(self.conv2.weight)\n",
    "    torch.nn.init.xavier_normal_(self.conv3.weight)\n",
    "    torch.nn.init.xavier_normal_(self.fc1.weight)\n",
    "    torch.nn.init.xavier_normal_(self.fc2.weight)\n",
    "    torch.nn.init.xavier_normal_(self.fc3.weight)\n",
    "    torch.nn.init.xavier_normal_(self.fc4.weight)\n",
    "\n",
    "  def forward(self,z):  #y is avg image #z batch of list of 9 images\n",
    "    batch = z.size(0)\n",
    "    patches = z.size(1)\n",
    "    z = z.view(batch*patches,3,32,32)\n",
    "    alpha =  self.helper(z)\n",
    "    alpha = alpha.view(batch,patches,-1)\n",
    "    return alpha[:,:,0] # scores \n",
    "    \n",
    "  def helper(self, x):\n",
    "    x = self.pool(F.relu(self.conv1(x)))\n",
    "    x = self.pool(F.relu(self.conv2(x)))\n",
    "    # print(x.shape)\n",
    "    x = (F.relu(self.conv3(x)))\n",
    "    x =  x.view(x.size(0), -1)\n",
    "    # print(x.shape)\n",
    "    x = F.relu(self.fc1(x))\n",
    "    x = F.relu(self.fc2(x))\n",
    "    x = F.relu(self.fc3(x))\n",
    "    x = self.fc4(x)\n",
    "    return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6abee423",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-21T05:49:15.788433Z",
     "iopub.status.busy": "2022-03-21T05:49:15.787752Z",
     "iopub.status.idle": "2022-03-21T05:49:15.790320Z",
     "shell.execute_reply": "2022-03-21T05:49:15.789872Z",
     "shell.execute_reply.started": "2022-03-21T05:46:12.240560Z"
    },
    "papermill": {
     "duration": 0.033762,
     "end_time": "2022-03-21T05:49:15.790429",
     "exception": false,
     "start_time": "2022-03-21T05:49:15.756667",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Classification(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Classification, self).__init__()\n",
    "    self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "    self.pool = nn.MaxPool2d(2, 2)\n",
    "    self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "    self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "    self.fc2 = nn.Linear(120, 84)\n",
    "    self.fc3 = nn.Linear(84, 10)\n",
    "    self.fc4 = nn.Linear(10,3)\n",
    "\n",
    "    torch.nn.init.xavier_normal_(self.conv1.weight)\n",
    "    torch.nn.init.zeros_(self.conv1.bias)\n",
    "    torch.nn.init.xavier_normal_(self.conv2.weight)\n",
    "    torch.nn.init.zeros_(self.conv2.bias)\n",
    "    torch.nn.init.xavier_normal_(self.fc1.weight)\n",
    "    torch.nn.init.zeros_(self.fc1.bias)\n",
    "    torch.nn.init.xavier_normal_(self.fc2.weight)\n",
    "    torch.nn.init.zeros_(self.fc2.bias)\n",
    "    torch.nn.init.xavier_normal_(self.fc3.weight)\n",
    "    torch.nn.init.zeros_(self.fc3.bias)\n",
    "    torch.nn.init.xavier_normal_(self.fc4.weight)\n",
    "    torch.nn.init.zeros_(self.fc4.bias)\n",
    "\n",
    "  def forward(self,z): \n",
    "    y1 = self.pool(F.relu(self.conv1(z)))\n",
    "    y1 = self.pool(F.relu(self.conv2(y1)))\n",
    "    y1 = y1.view(-1, 16 * 5 * 5)\n",
    "\n",
    "    y1 = F.relu(self.fc1(y1))\n",
    "    y1 = F.relu(self.fc2(y1))\n",
    "    y1 = F.relu(self.fc3(y1))\n",
    "    y1 = self.fc4(y1)\n",
    "    return y1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ae02f25",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-21T05:49:15.835256Z",
     "iopub.status.busy": "2022-03-21T05:49:15.834481Z",
     "iopub.status.idle": "2022-03-21T05:49:15.836867Z",
     "shell.execute_reply": "2022-03-21T05:49:15.836477Z",
     "shell.execute_reply.started": "2022-03-21T05:46:12.655795Z"
    },
    "papermill": {
     "duration": 0.026515,
     "end_time": "2022-03-21T05:49:15.836995",
     "exception": false,
     "start_time": "2022-03-21T05:49:15.810480",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_loss_focus(gamma,focus_output):\n",
    "    \n",
    "    m = torch.nn.LogSoftmax(dim=1)\n",
    "    log_outputs = m(focus_output)    \n",
    "    loss_ = gamma*log_outputs\n",
    "    loss_ = torch.sum(loss_,dim=1)\n",
    "    loss_ = -torch.mean(loss_,dim=0)  \n",
    "    return loss_ \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5eb4e19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-21T05:49:15.882931Z",
     "iopub.status.busy": "2022-03-21T05:49:15.882199Z",
     "iopub.status.idle": "2022-03-21T05:49:15.884090Z",
     "shell.execute_reply": "2022-03-21T05:49:15.884473Z",
     "shell.execute_reply.started": "2022-03-21T05:46:13.078654Z"
    },
    "papermill": {
     "duration": 0.027612,
     "end_time": "2022-03-21T05:49:15.884598",
     "exception": false,
     "start_time": "2022-03-21T05:49:15.856986",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_loss_classification(gamma,classification_output,label,criterion,n_patches):\n",
    "    #print(classification_output.shape)\n",
    "    \n",
    "    batch = label.size(0)\n",
    "    classes = classification_output.size(2)\n",
    "    label = label.repeat_interleave(n_patches)\n",
    "    classification_output = classification_output.reshape((batch*n_patches,classes))\n",
    "    loss_ = criterion(classification_output,label)\n",
    "    \n",
    "    loss_ = loss_.reshape((batch,n_patches))\n",
    "    \n",
    "    loss_ = gamma*loss_\n",
    "    loss_ = torch.sum(loss_,dim=1)\n",
    "    loss_ = torch.mean(loss_,dim=0)\n",
    "    \n",
    "    return loss_\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a76cd8ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-21T05:49:15.932049Z",
     "iopub.status.busy": "2022-03-21T05:49:15.931557Z",
     "iopub.status.idle": "2022-03-21T05:49:15.934649Z",
     "shell.execute_reply": "2022-03-21T05:49:15.935044Z",
     "shell.execute_reply.started": "2022-03-21T05:46:13.635998Z"
    },
    "papermill": {
     "duration": 0.030144,
     "end_time": "2022-03-21T05:49:15.935170",
     "exception": false,
     "start_time": "2022-03-21T05:49:15.905026",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def expectation_step(fc,cl,data,labels):\n",
    "    batch= data.size(0)\n",
    "    patches = data.size(1)\n",
    "    with torch.no_grad():\n",
    "        outputs_f = torch.softmax(fc(data),dim=1)\n",
    "        data = data.reshape(batch*patches,3,32,32)\n",
    "        outputs_g = cl(data)\n",
    "        \n",
    "    outputs_g = torch.softmax(outputs_g.reshape(batch,patches,3),dim=2)\n",
    "    \n",
    "        \n",
    "        \n",
    "    #print(\"Focus output\",outputs_f.shape,torch.sum(outputs_f,dim=1))\n",
    "    #print(\"Classification output\",outputs_g.shape,torch.sum(outputs_g,dim=1),torch.sum(outputs_g,dim=2))\n",
    "    outputs_g = outputs_g[np.arange(batch),:,labels]\n",
    "    p_x_y_z = outputs_f*outputs_g   #(1-outputs_g)    \n",
    "    \n",
    "    \n",
    "    normalized_p = p_x_y_z/torch.sum(p_x_y_z,dim=1,keepdims=True)\n",
    "#     print(outputs_f[0],outputs_g[0],normalized_p[0])\n",
    "    return normalized_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39a7dee7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-21T05:49:15.984197Z",
     "iopub.status.busy": "2022-03-21T05:49:15.983403Z",
     "iopub.status.idle": "2022-03-21T05:49:15.985353Z",
     "shell.execute_reply": "2022-03-21T05:49:15.985763Z",
     "shell.execute_reply.started": "2022-03-21T05:46:14.056114Z"
    },
    "papermill": {
     "duration": 0.029053,
     "end_time": "2022-03-21T05:49:15.985877",
     "exception": false,
     "start_time": "2022-03-21T05:49:15.956824",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def maximization_step(p_z,focus,classification,data,labels,focus_optimizer,classification_optimizer,Criterion):    \n",
    "    batch = data.size(0)\n",
    "    patches = data.size(1)\n",
    "    focus_optimizer.zero_grad()\n",
    "    classification_optimizer.zero_grad()\n",
    "    \n",
    "    focus_outputs = focus(data)\n",
    "    data = data.reshape(batch*patches,3,32,32)\n",
    "    classification_outputs = classification(data) \n",
    "    classification_outputs = classification_outputs.reshape(batch,patches,3)\n",
    "    \n",
    "    \n",
    "    #print(focus_outputs,classification_outputs)\n",
    "    \n",
    "    loss_focus = calculate_loss_focus(p_z,focus_outputs)\n",
    "    loss_classification = calculate_loss_classification(p_z,classification_outputs,\n",
    "                                                        labels,Criterion,patches)\n",
    "    \n",
    "    #print(\"Focus loss\",loss_focus.item())\n",
    "    #print(\"Classification loss\",loss_classification.item())\n",
    "    loss_focus.backward() \n",
    "    loss_classification.backward()\n",
    "    focus_optimizer.step()\n",
    "    classification_optimizer.step()\n",
    "    \n",
    "    return focus,classification,focus_optimizer,classification_optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9cfddcdd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-21T05:49:16.033350Z",
     "iopub.status.busy": "2022-03-21T05:49:16.032610Z",
     "iopub.status.idle": "2022-03-21T05:49:16.036361Z",
     "shell.execute_reply": "2022-03-21T05:49:16.035830Z",
     "shell.execute_reply.started": "2022-03-21T05:46:14.532144Z"
    },
    "papermill": {
     "duration": 0.03031,
     "end_time": "2022-03-21T05:49:16.036500",
     "exception": false,
     "start_time": "2022-03-21T05:49:16.006190",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_mosaic_img(bg_idx,fg_idx,fg,m): \n",
    "    \"\"\"\n",
    "      bg_idx : list of indexes of background_data[] to be used as background images in mosaic\n",
    "      fg_idx : index of image to be used as foreground image from foreground data\n",
    "      fg : at what position/index foreground image has to be stored out of 0-8\n",
    "    \"\"\"\n",
    "    image_list=[]\n",
    "    j=0\n",
    "    for i in range(m):  # m value \n",
    "        if i != fg:\n",
    "            image_list.append(background_data[bg_idx[j]])\n",
    "            j+=1\n",
    "        else: \n",
    "            image_list.append(foreground_data[fg_idx])\n",
    "            label = foreground_label[fg_idx] - fg1  # minus fg1 because our fore ground classes are fg1,fg2,fg3 but we have to store it as 0,1,2\n",
    "    #image_list = np.concatenate(image_list ,axis=0)\n",
    "    image_list = torch.stack(image_list) \n",
    "    return image_list,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e90ea8aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-21T05:49:16.091505Z",
     "iopub.status.busy": "2022-03-21T05:49:16.084335Z",
     "iopub.status.idle": "2022-03-21T05:50:24.573911Z",
     "shell.execute_reply": "2022-03-21T05:50:24.571978Z",
     "shell.execute_reply.started": "2022-03-21T05:46:15.364902Z"
    },
    "papermill": {
     "duration": 68.517427,
     "end_time": "2022-03-21T05:50:24.574034",
     "exception": false,
     "start_time": "2022-03-21T05:49:16.056607",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a25917df86dd4a7a99cb7ce4f4499630",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/170498071 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:30<00:00, 161.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Foreground Background Data created\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [00:12<00:00, 3230.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mosaic Data Created\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "fg1, fg2, fg3 = 0,1,2\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=10, shuffle=False)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=10, shuffle=False)\n",
    "\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "foreground_classes = {'plane', 'car', 'bird'}\n",
    "\n",
    "background_classes = {'cat', 'deer', 'dog', 'frog', 'horse','ship', 'truck'}\n",
    "\n",
    "# print(type(foreground_classes))\n",
    "\n",
    "dataiter = iter(trainloader)\n",
    "background_data=[]\n",
    "background_label=[]\n",
    "foreground_data=[]\n",
    "foreground_label=[]\n",
    "batch_size=10\n",
    "\n",
    "for i in tqdm(range(5000)):   #5000*batch_size = 50000 data points\n",
    "    images, labels = dataiter.next()\n",
    "    for j in range(batch_size):\n",
    "        if(classes[labels[j]] in background_classes):\n",
    "            img = images[j].tolist()\n",
    "            background_data.append(img)\n",
    "            background_label.append(labels[j])\n",
    "        else:\n",
    "            img = images[j].tolist()\n",
    "            foreground_data.append(img)\n",
    "            foreground_label.append(labels[j])\n",
    "            \n",
    "foreground_data = torch.tensor(foreground_data)\n",
    "foreground_label = torch.tensor(foreground_label)\n",
    "background_data = torch.tensor(background_data)\n",
    "background_label = torch.tensor(background_label)\n",
    "print(\"Foreground Background Data created\")\n",
    "\n",
    "\n",
    "\n",
    "m = 20\n",
    "desired_num = 40000\n",
    "mosaic_data =[]      # list of mosaic images, each mosaic image is saved as list of 9 images\n",
    "fore_idx =[]                   # list of indexes at which foreground image is present in a mosaic image i.e from 0 to 9               \n",
    "mosaic_label=[]                # label of mosaic image = foreground class present in that mosaic\n",
    "list_set_labels = [] \n",
    "for i in tqdm(range(desired_num)):\n",
    "    set_idx = set()\n",
    "    np.random.seed(i)\n",
    "    bg_idx = np.random.randint(0,35000,m-1)\n",
    "    set_idx = set(background_label[bg_idx].tolist())\n",
    "    fg_idx = np.random.randint(0,15000)\n",
    "    set_idx.add(foreground_label[fg_idx].item())\n",
    "    fg = np.random.randint(0,m)\n",
    "    fore_idx.append(fg)\n",
    "    image_list,label = create_mosaic_img(bg_idx,fg_idx,fg,m)\n",
    "    mosaic_data.append(image_list)\n",
    "    mosaic_label.append(label)\n",
    "    list_set_labels.append(set_idx)\n",
    "print(\"Mosaic Data Created\")\n",
    "# mosaic_data = torch.stack(mosaic_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9629674c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-21T05:50:24.825886Z",
     "iopub.status.busy": "2022-03-21T05:50:24.824971Z",
     "iopub.status.idle": "2022-03-21T05:50:25.000213Z",
     "shell.execute_reply": "2022-03-21T05:50:25.000682Z",
     "shell.execute_reply.started": "2022-03-21T05:47:40.526265Z"
    },
    "papermill": {
     "duration": 0.306565,
     "end_time": "2022-03-21T05:50:25.000828",
     "exception": false,
     "start_time": "2022-03-21T05:50:24.694263",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, (40000,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mosaic_data),np.shape(mosaic_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3197a209",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-21T05:50:25.251904Z",
     "iopub.status.busy": "2022-03-21T05:50:25.250334Z",
     "iopub.status.idle": "2022-03-21T05:50:25.252497Z",
     "shell.execute_reply": "2022-03-21T05:50:25.252891Z",
     "shell.execute_reply.started": "2022-03-21T05:47:42.427916Z"
    },
    "papermill": {
     "duration": 0.128619,
     "end_time": "2022-03-21T05:50:25.253018",
     "exception": false,
     "start_time": "2022-03-21T05:50:25.124399",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MosaicDataset(Dataset):\n",
    "  \"\"\"MosaicDataset dataset.\"\"\"\n",
    "\n",
    "  def __init__(self, mosaic_list_of_images, mosaic_label, fore_idx):\n",
    "    \"\"\"\n",
    "      Args:\n",
    "        csv_file (string): Path to the csv file with annotations.\n",
    "        root_dir (string): Directory with all the images.\n",
    "        transform (callable, optional): Optional transform to be applied\n",
    "            on a sample.\n",
    "    \"\"\"\n",
    "    self.mosaic = mosaic_list_of_images\n",
    "    self.label = mosaic_label\n",
    "    self.fore_idx = fore_idx\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.label)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return self.mosaic[idx] , self.label[idx], self.fore_idx[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "855ccd56",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-21T05:50:25.502943Z",
     "iopub.status.busy": "2022-03-21T05:50:25.502030Z",
     "iopub.status.idle": "2022-03-21T05:50:25.504571Z",
     "shell.execute_reply": "2022-03-21T05:50:25.503957Z",
     "shell.execute_reply.started": "2022-03-21T05:47:43.552108Z"
    },
    "papermill": {
     "duration": 0.130228,
     "end_time": "2022-03-21T05:50:25.504690",
     "exception": false,
     "start_time": "2022-03-21T05:50:25.374462",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch = 250\n",
    "tr = 30000\n",
    "msd = MosaicDataset(mosaic_data[0:tr], mosaic_label[0:tr] , fore_idx[0:tr])\n",
    "train_loader = DataLoader( msd,batch_size= batch ,shuffle=False)\n",
    "\n",
    "batch = 250\n",
    "msd1 = MosaicDataset(mosaic_data[tr:], mosaic_label[tr:] , fore_idx[tr:])\n",
    "test_loader = DataLoader( msd1,batch_size= batch ,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce754b86",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-21T05:50:25.752782Z",
     "iopub.status.busy": "2022-03-21T05:50:25.751963Z",
     "iopub.status.idle": "2022-03-21T05:50:25.753937Z",
     "shell.execute_reply": "2022-03-21T05:50:25.754314Z",
     "shell.execute_reply.started": "2022-03-21T05:47:46.551655Z"
    },
    "papermill": {
     "duration": 0.128381,
     "end_time": "2022-03-21T05:50:25.754451",
     "exception": false,
     "start_time": "2022-03-21T05:50:25.626070",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# torch.manual_seed(12)\n",
    "# focus = Focus()\n",
    "# focus = focus.to(device)\n",
    "# #print(focus.fc1.weight.data)\n",
    "# # focus.fc1.weight.data = torch.tensor([[0.,0.]])\n",
    "# torch.manual_seed(12)\n",
    "# classification = Classification()\n",
    "# classification = classification.to(device)\n",
    "# #print(classification.fc1.bias.data)\n",
    "# # classification.fc1.weight.data = torch.tensor([[0.1,0.1],[-0.1,-0.1],[0.,0.]])\n",
    "# # classification.fc1.bias.data = torch.tensor([0.,0.,0.])\n",
    "\n",
    "# Criterion = nn.CrossEntropyLoss(reduction=\"none\") #nn.BCELoss(reduction=\"none\")\n",
    "# focus_optimizer = optim.SGD(focus.parameters(), lr=0.07,momentum=0.9)\n",
    "# classification_optimizer = optim.SGD(classification.parameters(),lr=0.07,momentum=0.9)\n",
    "\n",
    "# # focus_optimizer = optim.Adam(focus.parameters(), lr=0.0001,weight_decay=0.00001)\n",
    "# # classification_optimizer = optim.Adam(classification.parameters(),lr=0.00005,weight_decay=0.001)\n",
    "\n",
    "# for i in range(100):\n",
    "#     focus_epoch_loss = []\n",
    "#     classification_epoch_loss = []\n",
    "#     for j,data in enumerate(train_loader):\n",
    "#         images,labels,foreground_index = data\n",
    "#         images = images.to(device)\n",
    "#         labels = labels.to(device)\n",
    "#         p_z = expectation_step(focus,classification,images,labels)\n",
    "#         focus,classification,focus_optimizer,classification_optimizer=maximization_step(p_z,focus,classification,\n",
    "#                                                                                         images,labels,\n",
    "#                                                                                         focus_optimizer,\n",
    "#                                                                                         classification_optimizer,\n",
    "#                                                                                         Criterion)\n",
    "#         with torch.no_grad():\n",
    "#             p_z = expectation_step(focus,classification,images,labels)\n",
    "#             batch = images.size(0)\n",
    "#             patches = images.size(1)\n",
    "#             focus_outputs = focus(images)\n",
    "#             images = images.reshape(batch*patches,3,32,32)\n",
    "#             classification_outputs = classification(images) # classification returns output after sigmoid/softmax]\n",
    "#             classification_outputs = classification_outputs.reshape(batch,patches,3)\n",
    "#             #print(focus_outputs,classification_outputs)\n",
    "#             loss_focus = calculate_loss_focus(p_z,focus_outputs)\n",
    "#             loss_classification = calculate_loss_classification(p_z,classification_outputs,\n",
    "#                                                         labels,Criterion,patches)\n",
    "#             focus_epoch_loss.append(loss_focus.item())\n",
    "#             classification_epoch_loss.append(loss_classification.item())\n",
    "#     print(\"*\"*60)\n",
    "#     print(\"Epoch: \" + str(i+1)+\", Focus Loss: \"+str(np.mean(focus_epoch_loss)))\n",
    "#     print(\"Epoch: \" + str(i+1)+\", Classification Loss: \"+str(np.mean(classification_epoch_loss))) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1f15ecc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-21T05:50:26.012427Z",
     "iopub.status.busy": "2022-03-21T05:50:26.011814Z",
     "iopub.status.idle": "2022-03-21T07:19:55.368594Z",
     "shell.execute_reply": "2022-03-21T07:19:55.368110Z"
    },
    "papermill": {
     "duration": 5369.49336,
     "end_time": "2022-03-21T07:19:55.368739",
     "exception": false,
     "start_time": "2022-03-21T05:50:25.875379",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************************\n",
      "EM Step: 1 Epoch: 1, Focus Loss: 2.9951861441135406\n",
      "EM Step: 1 Epoch: 1, Classification Loss: 1.0974576234817506\n",
      "************************************************************\n",
      "EM Step: 1 Epoch: 2, Focus Loss: 2.995185649394989\n",
      "EM Step: 1 Epoch: 2, Classification Loss: 1.096885214249293\n",
      "************************************************************\n",
      "EM Step: 1 Epoch: 3, Focus Loss: 2.9951852401097616\n",
      "EM Step: 1 Epoch: 3, Classification Loss: 1.0965149104595184\n",
      "************************************************************\n",
      "EM Step: 1 Epoch: 4, Focus Loss: 2.995184882481893\n",
      "EM Step: 1 Epoch: 4, Classification Loss: 1.0962236255407334\n",
      "************************************************************\n",
      "EM Step: 1 Epoch: 5, Focus Loss: 2.99518461227417\n",
      "EM Step: 1 Epoch: 5, Classification Loss: 1.095926829179128\n",
      "************************************************************\n",
      "EM Step: 1 Epoch: 6, Focus Loss: 2.9951843361059827\n",
      "EM Step: 1 Epoch: 6, Classification Loss: 1.0956660081942877\n",
      "************************************************************\n",
      "EM Step: 1 Epoch: 7, Focus Loss: 2.9951840658982594\n",
      "EM Step: 1 Epoch: 7, Classification Loss: 1.0954238692919414\n",
      "************************************************************\n",
      "EM Step: 1 Epoch: 8, Focus Loss: 2.9951838413874308\n",
      "EM Step: 1 Epoch: 8, Classification Loss: 1.095193836092949\n",
      "************************************************************\n",
      "EM Step: 1 Epoch: 9, Focus Loss: 2.995183598995209\n",
      "EM Step: 1 Epoch: 9, Classification Loss: 1.0949626902739207\n",
      "************************************************************\n",
      "EM Step: 1 Epoch: 10, Focus Loss: 2.9951834042867023\n",
      "EM Step: 1 Epoch: 10, Classification Loss: 1.094751740495364\n",
      "************************************************************\n",
      "EM Step: 2 Epoch: 1, Focus Loss: 2.9950954993565877\n",
      "EM Step: 2 Epoch: 1, Classification Loss: 1.0865665207306543\n",
      "************************************************************\n",
      "EM Step: 2 Epoch: 2, Focus Loss: 2.995066400369008\n",
      "EM Step: 2 Epoch: 2, Classification Loss: 1.0856252481540045\n",
      "************************************************************\n",
      "EM Step: 2 Epoch: 3, Focus Loss: 2.9950522740681964\n",
      "EM Step: 2 Epoch: 3, Classification Loss: 1.0850895414749782\n",
      "************************************************************\n",
      "EM Step: 2 Epoch: 4, Focus Loss: 2.9950417041778565\n",
      "EM Step: 2 Epoch: 4, Classification Loss: 1.0846453924973807\n",
      "************************************************************\n",
      "EM Step: 2 Epoch: 5, Focus Loss: 2.9950330813725787\n",
      "EM Step: 2 Epoch: 5, Classification Loss: 1.0842296838760377\n",
      "************************************************************\n",
      "EM Step: 2 Epoch: 6, Focus Loss: 2.9950255672136943\n",
      "EM Step: 2 Epoch: 6, Classification Loss: 1.083876101175944\n",
      "************************************************************\n",
      "EM Step: 2 Epoch: 7, Focus Loss: 2.9950189252694446\n",
      "EM Step: 2 Epoch: 7, Classification Loss: 1.0835142135620117\n",
      "************************************************************\n",
      "EM Step: 2 Epoch: 8, Focus Loss: 2.9950128237406415\n",
      "EM Step: 2 Epoch: 8, Classification Loss: 1.083157334725062\n",
      "************************************************************\n",
      "EM Step: 2 Epoch: 9, Focus Loss: 2.995007143417994\n",
      "EM Step: 2 Epoch: 9, Classification Loss: 1.0828088124593098\n",
      "************************************************************\n",
      "EM Step: 2 Epoch: 10, Focus Loss: 2.9950017909208935\n",
      "EM Step: 2 Epoch: 10, Classification Loss: 1.0824861794710159\n",
      "************************************************************\n",
      "EM Step: 3 Epoch: 1, Focus Loss: 2.993977348009745\n",
      "EM Step: 3 Epoch: 1, Classification Loss: 1.0589356005191803\n",
      "************************************************************\n",
      "EM Step: 3 Epoch: 2, Focus Loss: 2.9937177995840707\n",
      "EM Step: 3 Epoch: 2, Classification Loss: 1.0576817909876506\n",
      "************************************************************\n",
      "EM Step: 3 Epoch: 3, Focus Loss: 2.993569399913152\n",
      "EM Step: 3 Epoch: 3, Classification Loss: 1.0568808009227117\n",
      "************************************************************\n",
      "EM Step: 3 Epoch: 4, Focus Loss: 2.993449435631434\n",
      "EM Step: 3 Epoch: 4, Classification Loss: 1.0561973889668783\n",
      "************************************************************\n",
      "EM Step: 3 Epoch: 5, Focus Loss: 2.9933407167593638\n",
      "EM Step: 3 Epoch: 5, Classification Loss: 1.0556134064992269\n",
      "************************************************************\n",
      "EM Step: 3 Epoch: 6, Focus Loss: 2.993241335948308\n",
      "EM Step: 3 Epoch: 6, Classification Loss: 1.0550574759642284\n",
      "************************************************************\n",
      "EM Step: 3 Epoch: 7, Focus Loss: 2.9931478599707284\n",
      "EM Step: 3 Epoch: 7, Classification Loss: 1.054572136203448\n",
      "************************************************************\n",
      "EM Step: 3 Epoch: 8, Focus Loss: 2.9930590828259787\n",
      "EM Step: 3 Epoch: 8, Classification Loss: 1.0540742784738542\n",
      "************************************************************\n",
      "EM Step: 3 Epoch: 9, Focus Loss: 2.9929682075977326\n",
      "EM Step: 3 Epoch: 9, Classification Loss: 1.0536198884248733\n",
      "************************************************************\n",
      "EM Step: 3 Epoch: 10, Focus Loss: 2.9928730368614196\n",
      "EM Step: 3 Epoch: 10, Classification Loss: 1.0531811594963074\n",
      "************************************************************\n",
      "EM Step: 4 Epoch: 1, Focus Loss: 2.985165367523829\n",
      "EM Step: 4 Epoch: 1, Classification Loss: 1.0177860409021378\n",
      "************************************************************\n",
      "EM Step: 4 Epoch: 2, Focus Loss: 2.983677065372467\n",
      "EM Step: 4 Epoch: 2, Classification Loss: 1.0159872243801753\n",
      "************************************************************\n",
      "EM Step: 4 Epoch: 3, Focus Loss: 2.9828764140605926\n",
      "EM Step: 4 Epoch: 3, Classification Loss: 1.0150600209832192\n",
      "************************************************************\n",
      "EM Step: 4 Epoch: 4, Focus Loss: 2.9822862644990287\n",
      "EM Step: 4 Epoch: 4, Classification Loss: 1.0142608856161435\n",
      "************************************************************\n",
      "EM Step: 4 Epoch: 5, Focus Loss: 2.9817887604236604\n",
      "EM Step: 4 Epoch: 5, Classification Loss: 1.0135537793238958\n",
      "************************************************************\n",
      "EM Step: 4 Epoch: 6, Focus Loss: 2.9813519855340322\n",
      "EM Step: 4 Epoch: 6, Classification Loss: 1.0129272669553757\n",
      "************************************************************\n",
      "EM Step: 4 Epoch: 7, Focus Loss: 2.980955622593562\n",
      "EM Step: 4 Epoch: 7, Classification Loss: 1.0123208661874135\n",
      "************************************************************\n",
      "EM Step: 4 Epoch: 8, Focus Loss: 2.980650403102239\n",
      "EM Step: 4 Epoch: 8, Classification Loss: 1.0117123638590177\n",
      "************************************************************\n",
      "EM Step: 4 Epoch: 9, Focus Loss: 2.980373960733414\n",
      "EM Step: 4 Epoch: 9, Classification Loss: 1.0111834029356639\n",
      "************************************************************\n",
      "EM Step: 4 Epoch: 10, Focus Loss: 2.9801275014877318\n",
      "EM Step: 4 Epoch: 10, Classification Loss: 1.0106760760148366\n",
      "************************************************************\n",
      "EM Step: 5 Epoch: 1, Focus Loss: 2.9458723803361258\n",
      "EM Step: 5 Epoch: 1, Classification Loss: 0.9472266798218091\n",
      "************************************************************\n",
      "EM Step: 5 Epoch: 2, Focus Loss: 2.9418482979138694\n",
      "EM Step: 5 Epoch: 2, Classification Loss: 0.9458337451020876\n",
      "************************************************************\n",
      "EM Step: 5 Epoch: 3, Focus Loss: 2.9408998747666675\n",
      "EM Step: 5 Epoch: 3, Classification Loss: 0.9446166942516963\n",
      "************************************************************\n",
      "EM Step: 5 Epoch: 4, Focus Loss: 2.9399327596028644\n",
      "EM Step: 5 Epoch: 4, Classification Loss: 0.9435044407844544\n",
      "************************************************************\n",
      "EM Step: 5 Epoch: 5, Focus Loss: 2.9392510890960692\n",
      "EM Step: 5 Epoch: 5, Classification Loss: 0.9424208586414655\n",
      "************************************************************\n",
      "EM Step: 5 Epoch: 6, Focus Loss: 2.93886484503746\n",
      "EM Step: 5 Epoch: 6, Classification Loss: 0.9414505342642466\n",
      "************************************************************\n",
      "EM Step: 5 Epoch: 7, Focus Loss: 2.9391803165276844\n",
      "EM Step: 5 Epoch: 7, Classification Loss: 0.9405450617273649\n",
      "************************************************************\n",
      "EM Step: 5 Epoch: 8, Focus Loss: 2.938121012846629\n",
      "EM Step: 5 Epoch: 8, Classification Loss: 0.9398092473546664\n",
      "************************************************************\n",
      "EM Step: 5 Epoch: 9, Focus Loss: 2.9375046332677206\n",
      "EM Step: 5 Epoch: 9, Classification Loss: 0.9392418379584948\n",
      "************************************************************\n",
      "EM Step: 5 Epoch: 10, Focus Loss: 2.9370463053385416\n",
      "EM Step: 5 Epoch: 10, Classification Loss: 0.9386777261892955\n",
      "************************************************************\n",
      "EM Step: 6 Epoch: 1, Focus Loss: 2.8621337076028186\n",
      "EM Step: 6 Epoch: 1, Classification Loss: 0.8477383549014728\n",
      "************************************************************\n",
      "EM Step: 6 Epoch: 2, Focus Loss: 2.857687783241272\n",
      "EM Step: 6 Epoch: 2, Classification Loss: 0.8460849021871885\n",
      "************************************************************\n",
      "EM Step: 6 Epoch: 3, Focus Loss: 2.8567973415056866\n",
      "EM Step: 6 Epoch: 3, Classification Loss: 0.8446519826849301\n",
      "************************************************************\n",
      "EM Step: 6 Epoch: 4, Focus Loss: 2.853213572502136\n",
      "EM Step: 6 Epoch: 4, Classification Loss: 0.8435780043403308\n",
      "************************************************************\n",
      "EM Step: 6 Epoch: 5, Focus Loss: 2.8526079217592875\n",
      "EM Step: 6 Epoch: 5, Classification Loss: 0.8427394693096478\n",
      "************************************************************\n",
      "EM Step: 6 Epoch: 6, Focus Loss: 2.8524298826853434\n",
      "EM Step: 6 Epoch: 6, Classification Loss: 0.8421514848868052\n",
      "************************************************************\n",
      "EM Step: 6 Epoch: 7, Focus Loss: 2.852353141705195\n",
      "EM Step: 6 Epoch: 7, Classification Loss: 0.8417284364501635\n",
      "************************************************************\n",
      "EM Step: 6 Epoch: 8, Focus Loss: 2.8522864242394763\n",
      "EM Step: 6 Epoch: 8, Classification Loss: 0.8414627025524776\n",
      "************************************************************\n",
      "EM Step: 6 Epoch: 9, Focus Loss: 2.852012272675832\n",
      "EM Step: 6 Epoch: 9, Classification Loss: 0.8416267365217209\n",
      "************************************************************\n",
      "EM Step: 6 Epoch: 10, Focus Loss: 2.8515023827552795\n",
      "EM Step: 6 Epoch: 10, Classification Loss: 0.842326553662618\n",
      "************************************************************\n",
      "EM Step: 7 Epoch: 1, Focus Loss: 2.6504553417364756\n",
      "EM Step: 7 Epoch: 1, Classification Loss: 0.726834678153197\n",
      "************************************************************\n",
      "EM Step: 7 Epoch: 2, Focus Loss: 2.6401778598626455\n",
      "EM Step: 7 Epoch: 2, Classification Loss: 0.723748205602169\n",
      "************************************************************\n",
      "EM Step: 7 Epoch: 3, Focus Loss: 2.6394683917363486\n",
      "EM Step: 7 Epoch: 3, Classification Loss: 0.7224436342716217\n",
      "************************************************************\n",
      "EM Step: 7 Epoch: 4, Focus Loss: 2.634236437082291\n",
      "EM Step: 7 Epoch: 4, Classification Loss: 0.7215664823849995\n",
      "************************************************************\n",
      "EM Step: 7 Epoch: 5, Focus Loss: 2.6322767476240796\n",
      "EM Step: 7 Epoch: 5, Classification Loss: 0.7209412495295207\n",
      "************************************************************\n",
      "EM Step: 7 Epoch: 6, Focus Loss: 2.6311733444531757\n",
      "EM Step: 7 Epoch: 6, Classification Loss: 0.7206141223510106\n",
      "************************************************************\n",
      "EM Step: 7 Epoch: 7, Focus Loss: 2.630284337202708\n",
      "EM Step: 7 Epoch: 7, Classification Loss: 0.7204864025115967\n",
      "************************************************************\n",
      "EM Step: 7 Epoch: 8, Focus Loss: 2.6303168376286825\n",
      "EM Step: 7 Epoch: 8, Classification Loss: 0.720367181301117\n",
      "************************************************************\n",
      "EM Step: 7 Epoch: 9, Focus Loss: 2.6308485686779024\n",
      "EM Step: 7 Epoch: 9, Classification Loss: 0.7212351113557816\n",
      "************************************************************\n",
      "EM Step: 7 Epoch: 10, Focus Loss: 2.629211715857188\n",
      "EM Step: 7 Epoch: 10, Classification Loss: 0.7216496686140697\n",
      "************************************************************\n",
      "EM Step: 8 Epoch: 1, Focus Loss: 2.4155817230542502\n",
      "EM Step: 8 Epoch: 1, Classification Loss: 0.6135554358363151\n",
      "************************************************************\n",
      "EM Step: 8 Epoch: 2, Focus Loss: 2.4115983188152312\n",
      "EM Step: 8 Epoch: 2, Classification Loss: 0.6103702823321024\n",
      "************************************************************\n",
      "EM Step: 8 Epoch: 3, Focus Loss: 2.4126212100187936\n",
      "EM Step: 8 Epoch: 3, Classification Loss: 0.608951264123122\n",
      "************************************************************\n",
      "EM Step: 8 Epoch: 4, Focus Loss: 2.414176869392395\n",
      "EM Step: 8 Epoch: 4, Classification Loss: 0.6079307188590367\n",
      "************************************************************\n",
      "EM Step: 8 Epoch: 5, Focus Loss: 2.412280074755351\n",
      "EM Step: 8 Epoch: 5, Classification Loss: 0.607311371465524\n",
      "************************************************************\n",
      "EM Step: 8 Epoch: 6, Focus Loss: 2.4111919045448302\n",
      "EM Step: 8 Epoch: 6, Classification Loss: 0.6068194970488549\n",
      "************************************************************\n",
      "EM Step: 8 Epoch: 7, Focus Loss: 2.410167201360067\n",
      "EM Step: 8 Epoch: 7, Classification Loss: 0.6063991978764534\n",
      "************************************************************\n",
      "EM Step: 8 Epoch: 8, Focus Loss: 2.409759436051051\n",
      "EM Step: 8 Epoch: 8, Classification Loss: 0.606274793545405\n",
      "************************************************************\n",
      "EM Step: 8 Epoch: 9, Focus Loss: 2.4080435673395795\n",
      "EM Step: 8 Epoch: 9, Classification Loss: 0.6062144488096237\n",
      "************************************************************\n",
      "EM Step: 8 Epoch: 10, Focus Loss: 2.4067398567994434\n",
      "EM Step: 8 Epoch: 10, Classification Loss: 0.6066435227791468\n",
      "************************************************************\n",
      "EM Step: 9 Epoch: 1, Focus Loss: 2.1349739332993827\n",
      "EM Step: 9 Epoch: 1, Classification Loss: 0.484380333373944\n",
      "************************************************************\n",
      "EM Step: 9 Epoch: 2, Focus Loss: 2.1334431648254393\n",
      "EM Step: 9 Epoch: 2, Classification Loss: 0.4816037411491076\n",
      "************************************************************\n",
      "EM Step: 9 Epoch: 3, Focus Loss: 2.1384310285250345\n",
      "EM Step: 9 Epoch: 3, Classification Loss: 0.4805825471878052\n",
      "************************************************************\n",
      "EM Step: 9 Epoch: 4, Focus Loss: 2.1394119719664255\n",
      "EM Step: 9 Epoch: 4, Classification Loss: 0.47987140143911045\n",
      "************************************************************\n",
      "EM Step: 9 Epoch: 5, Focus Loss: 2.1304872115453084\n",
      "EM Step: 9 Epoch: 5, Classification Loss: 0.47995491946736973\n",
      "************************************************************\n",
      "EM Step: 9 Epoch: 6, Focus Loss: 2.1281235764424005\n",
      "EM Step: 9 Epoch: 6, Classification Loss: 0.4800202044347922\n",
      "************************************************************\n",
      "EM Step: 9 Epoch: 7, Focus Loss: 2.127598637342453\n",
      "EM Step: 9 Epoch: 7, Classification Loss: 0.4799442914625009\n",
      "************************************************************\n",
      "EM Step: 9 Epoch: 8, Focus Loss: 2.127426467339198\n",
      "EM Step: 9 Epoch: 8, Classification Loss: 0.4805175046126048\n",
      "************************************************************\n",
      "EM Step: 9 Epoch: 9, Focus Loss: 2.129349818825722\n",
      "EM Step: 9 Epoch: 9, Classification Loss: 0.4817644231021404\n",
      "************************************************************\n",
      "EM Step: 9 Epoch: 10, Focus Loss: 2.133465171853701\n",
      "EM Step: 9 Epoch: 10, Classification Loss: 0.48248814369241394\n",
      "************************************************************\n",
      "EM Step: 10 Epoch: 1, Focus Loss: 1.9014490356047948\n",
      "EM Step: 10 Epoch: 1, Classification Loss: 0.383638825515906\n",
      "************************************************************\n",
      "EM Step: 10 Epoch: 2, Focus Loss: 1.9031462699174881\n",
      "EM Step: 10 Epoch: 2, Classification Loss: 0.3828631120423476\n",
      "************************************************************\n",
      "EM Step: 10 Epoch: 3, Focus Loss: 1.9029364695151647\n",
      "EM Step: 10 Epoch: 3, Classification Loss: 0.38330849409103396\n",
      "************************************************************\n",
      "EM Step: 10 Epoch: 4, Focus Loss: 1.9025298297405242\n",
      "EM Step: 10 Epoch: 4, Classification Loss: 0.3863641048471133\n",
      "************************************************************\n",
      "EM Step: 10 Epoch: 5, Focus Loss: 1.9001803775628408\n",
      "EM Step: 10 Epoch: 5, Classification Loss: 0.3885228842496872\n",
      "************************************************************\n",
      "EM Step: 10 Epoch: 6, Focus Loss: 1.8924623409907022\n",
      "EM Step: 10 Epoch: 6, Classification Loss: 0.3894949565331141\n",
      "************************************************************\n",
      "EM Step: 10 Epoch: 7, Focus Loss: 1.8902505258719127\n",
      "EM Step: 10 Epoch: 7, Classification Loss: 0.3887387154002984\n",
      "************************************************************\n",
      "EM Step: 10 Epoch: 8, Focus Loss: 1.8892724543809891\n",
      "EM Step: 10 Epoch: 8, Classification Loss: 0.38583847930034004\n",
      "************************************************************\n",
      "EM Step: 10 Epoch: 9, Focus Loss: 1.8891656567653021\n",
      "EM Step: 10 Epoch: 9, Classification Loss: 0.3853929800291856\n",
      "************************************************************\n",
      "EM Step: 10 Epoch: 10, Focus Loss: 1.8907054354747137\n",
      "EM Step: 10 Epoch: 10, Classification Loss: 0.38438638697067895\n",
      "************************************************************\n",
      "EM Step: 11 Epoch: 1, Focus Loss: 1.6370349715153376\n",
      "EM Step: 11 Epoch: 1, Classification Loss: 0.28574372207125026\n",
      "************************************************************\n",
      "EM Step: 11 Epoch: 2, Focus Loss: 1.6373013069232305\n",
      "EM Step: 11 Epoch: 2, Classification Loss: 0.2858094576746225\n",
      "************************************************************\n",
      "EM Step: 11 Epoch: 3, Focus Loss: 1.6355643024047215\n",
      "EM Step: 11 Epoch: 3, Classification Loss: 0.2814120208223661\n",
      "************************************************************\n",
      "EM Step: 11 Epoch: 4, Focus Loss: 1.6340609649817148\n",
      "EM Step: 11 Epoch: 4, Classification Loss: 0.27969803450008235\n",
      "************************************************************\n",
      "EM Step: 11 Epoch: 5, Focus Loss: 1.634558767080307\n",
      "EM Step: 11 Epoch: 5, Classification Loss: 0.2779815418024858\n",
      "************************************************************\n",
      "EM Step: 11 Epoch: 6, Focus Loss: 1.634661422173182\n",
      "EM Step: 11 Epoch: 6, Classification Loss: 0.2774047503868739\n",
      "************************************************************\n",
      "EM Step: 11 Epoch: 7, Focus Loss: 1.6368483821551005\n",
      "EM Step: 11 Epoch: 7, Classification Loss: 0.27641969472169875\n",
      "************************************************************\n",
      "EM Step: 11 Epoch: 8, Focus Loss: 1.633461140592893\n",
      "EM Step: 11 Epoch: 8, Classification Loss: 0.27523412990073365\n",
      "************************************************************\n",
      "EM Step: 11 Epoch: 9, Focus Loss: 1.6286204487085343\n",
      "EM Step: 11 Epoch: 9, Classification Loss: 0.2736664233108362\n",
      "************************************************************\n",
      "EM Step: 11 Epoch: 10, Focus Loss: 1.6263042837381363\n",
      "EM Step: 11 Epoch: 10, Classification Loss: 0.27278979967037836\n",
      "************************************************************\n",
      "EM Step: 12 Epoch: 1, Focus Loss: 1.4449806183576583\n",
      "EM Step: 12 Epoch: 1, Classification Loss: 0.18778874141474564\n",
      "************************************************************\n",
      "EM Step: 12 Epoch: 2, Focus Loss: 1.443973531325658\n",
      "EM Step: 12 Epoch: 2, Classification Loss: 0.18489208705723287\n",
      "************************************************************\n",
      "EM Step: 12 Epoch: 3, Focus Loss: 1.4429875959952672\n",
      "EM Step: 12 Epoch: 3, Classification Loss: 0.18373773582279682\n",
      "************************************************************\n",
      "EM Step: 12 Epoch: 4, Focus Loss: 1.4431000967820486\n",
      "EM Step: 12 Epoch: 4, Classification Loss: 0.18332502109309037\n",
      "************************************************************\n",
      "EM Step: 12 Epoch: 5, Focus Loss: 1.443132472038269\n",
      "EM Step: 12 Epoch: 5, Classification Loss: 0.18297724276781083\n",
      "************************************************************\n",
      "EM Step: 12 Epoch: 6, Focus Loss: 1.4432168841361999\n",
      "EM Step: 12 Epoch: 6, Classification Loss: 0.18309123578170935\n",
      "************************************************************\n",
      "EM Step: 12 Epoch: 7, Focus Loss: 1.4410291413466136\n",
      "EM Step: 12 Epoch: 7, Classification Loss: 0.1836296102652947\n",
      "************************************************************\n",
      "EM Step: 12 Epoch: 8, Focus Loss: 1.4395891378323238\n",
      "EM Step: 12 Epoch: 8, Classification Loss: 0.1844892218708992\n",
      "************************************************************\n",
      "EM Step: 12 Epoch: 9, Focus Loss: 1.4385711669921875\n",
      "EM Step: 12 Epoch: 9, Classification Loss: 0.1847878514478604\n",
      "************************************************************\n",
      "EM Step: 12 Epoch: 10, Focus Loss: 1.4365640570720037\n",
      "EM Step: 12 Epoch: 10, Classification Loss: 0.18505208517114322\n",
      "************************************************************\n",
      "EM Step: 13 Epoch: 1, Focus Loss: 1.2859384049971898\n",
      "EM Step: 13 Epoch: 1, Classification Loss: 0.13238245174288749\n",
      "************************************************************\n",
      "EM Step: 13 Epoch: 2, Focus Loss: 1.2829467356204987\n",
      "EM Step: 13 Epoch: 2, Classification Loss: 0.1496584568793575\n",
      "************************************************************\n",
      "EM Step: 13 Epoch: 3, Focus Loss: 1.280436646938324\n",
      "EM Step: 13 Epoch: 3, Classification Loss: 0.1321706476310889\n",
      "************************************************************\n",
      "EM Step: 13 Epoch: 4, Focus Loss: 1.2807584355274837\n",
      "EM Step: 13 Epoch: 4, Classification Loss: 0.12930867889275152\n",
      "************************************************************\n",
      "EM Step: 13 Epoch: 5, Focus Loss: 1.2820991595586142\n",
      "EM Step: 13 Epoch: 5, Classification Loss: 0.12715990723421175\n",
      "************************************************************\n",
      "EM Step: 13 Epoch: 6, Focus Loss: 1.2860584606726964\n",
      "EM Step: 13 Epoch: 6, Classification Loss: 0.12571626814703146\n",
      "************************************************************\n",
      "EM Step: 13 Epoch: 7, Focus Loss: 1.2893390516440073\n",
      "EM Step: 13 Epoch: 7, Classification Loss: 0.12673539041231077\n",
      "************************************************************\n",
      "EM Step: 13 Epoch: 8, Focus Loss: 1.2886067579189937\n",
      "EM Step: 13 Epoch: 8, Classification Loss: 0.12503645742932956\n",
      "************************************************************\n",
      "EM Step: 13 Epoch: 9, Focus Loss: 1.2864960829416912\n",
      "EM Step: 13 Epoch: 9, Classification Loss: 0.1245143952469031\n",
      "************************************************************\n",
      "EM Step: 13 Epoch: 10, Focus Loss: 1.2842964380979538\n",
      "EM Step: 13 Epoch: 10, Classification Loss: 0.12371610092620054\n",
      "************************************************************\n",
      "EM Step: 14 Epoch: 1, Focus Loss: 1.136081051826477\n",
      "EM Step: 14 Epoch: 1, Classification Loss: 0.08563602153832714\n",
      "************************************************************\n",
      "EM Step: 14 Epoch: 2, Focus Loss: 1.1331407601634662\n",
      "EM Step: 14 Epoch: 2, Classification Loss: 0.08951390590518712\n",
      "************************************************************\n",
      "EM Step: 14 Epoch: 3, Focus Loss: 1.1336794416109721\n",
      "EM Step: 14 Epoch: 3, Classification Loss: 0.0851636102112631\n",
      "************************************************************\n",
      "EM Step: 14 Epoch: 4, Focus Loss: 1.1379156996806463\n",
      "EM Step: 14 Epoch: 4, Classification Loss: 0.08585418180252115\n",
      "************************************************************\n",
      "EM Step: 14 Epoch: 5, Focus Loss: 1.1358796348174414\n",
      "EM Step: 14 Epoch: 5, Classification Loss: 0.15009917511294285\n",
      "************************************************************\n",
      "EM Step: 14 Epoch: 6, Focus Loss: 1.1352324773867926\n",
      "EM Step: 14 Epoch: 6, Classification Loss: 0.17916054396579664\n",
      "************************************************************\n",
      "EM Step: 14 Epoch: 7, Focus Loss: 1.137558748324712\n",
      "EM Step: 14 Epoch: 7, Classification Loss: 0.100774612929672\n",
      "************************************************************\n",
      "EM Step: 14 Epoch: 8, Focus Loss: 1.1381301735838254\n",
      "EM Step: 14 Epoch: 8, Classification Loss: 0.08895133752375842\n",
      "************************************************************\n",
      "EM Step: 14 Epoch: 9, Focus Loss: 1.1349355593323707\n",
      "EM Step: 14 Epoch: 9, Classification Loss: 0.08363275177155932\n",
      "************************************************************\n",
      "EM Step: 14 Epoch: 10, Focus Loss: 1.1327743848164877\n",
      "EM Step: 14 Epoch: 10, Classification Loss: 0.08158821361139416\n",
      "************************************************************\n",
      "EM Step: 15 Epoch: 1, Focus Loss: 1.0523731604218483\n",
      "EM Step: 15 Epoch: 1, Classification Loss: 0.05571849974803626\n",
      "************************************************************\n",
      "EM Step: 15 Epoch: 2, Focus Loss: 1.0513083294034005\n",
      "EM Step: 15 Epoch: 2, Classification Loss: 0.054726301102588575\n",
      "************************************************************\n",
      "EM Step: 15 Epoch: 3, Focus Loss: 1.049624856809775\n",
      "EM Step: 15 Epoch: 3, Classification Loss: 0.053474390398090085\n",
      "************************************************************\n",
      "EM Step: 15 Epoch: 4, Focus Loss: 1.0483670130372047\n",
      "EM Step: 15 Epoch: 4, Classification Loss: 0.05278536019225915\n",
      "************************************************************\n",
      "EM Step: 15 Epoch: 5, Focus Loss: 1.0474553674459457\n",
      "EM Step: 15 Epoch: 5, Classification Loss: 0.052457608825837575\n",
      "************************************************************\n",
      "EM Step: 15 Epoch: 6, Focus Loss: 1.0490188534061113\n",
      "EM Step: 15 Epoch: 6, Classification Loss: 0.05227441533158223\n",
      "************************************************************\n",
      "EM Step: 15 Epoch: 7, Focus Loss: 1.0497011298934618\n",
      "EM Step: 15 Epoch: 7, Classification Loss: 0.05204045840849479\n",
      "************************************************************\n",
      "EM Step: 15 Epoch: 8, Focus Loss: 1.0474116270740828\n",
      "EM Step: 15 Epoch: 8, Classification Loss: 0.05165507233080765\n",
      "************************************************************\n",
      "EM Step: 15 Epoch: 9, Focus Loss: 1.0455572361747423\n",
      "EM Step: 15 Epoch: 9, Classification Loss: 0.05146105575064818\n",
      "************************************************************\n",
      "EM Step: 15 Epoch: 10, Focus Loss: 1.0434601267178854\n",
      "EM Step: 15 Epoch: 10, Classification Loss: 0.05136598306708038\n",
      "************************************************************\n",
      "EM Step: 16 Epoch: 1, Focus Loss: 0.9372153739134471\n",
      "EM Step: 16 Epoch: 1, Classification Loss: 0.039767964541291195\n",
      "************************************************************\n",
      "EM Step: 16 Epoch: 2, Focus Loss: 0.9338634207844734\n",
      "EM Step: 16 Epoch: 2, Classification Loss: 0.041041657545914254\n",
      "************************************************************\n",
      "EM Step: 16 Epoch: 3, Focus Loss: 0.9325379624962806\n",
      "EM Step: 16 Epoch: 3, Classification Loss: 0.039955590308333434\n",
      "************************************************************\n",
      "EM Step: 16 Epoch: 4, Focus Loss: 0.9329379603266716\n",
      "EM Step: 16 Epoch: 4, Classification Loss: 0.03911629044450819\n",
      "************************************************************\n",
      "EM Step: 16 Epoch: 5, Focus Loss: 0.9278881296515464\n",
      "EM Step: 16 Epoch: 5, Classification Loss: 0.038424375482524434\n",
      "************************************************************\n",
      "EM Step: 16 Epoch: 6, Focus Loss: 0.9308917398254076\n",
      "EM Step: 16 Epoch: 6, Classification Loss: 0.03794230648006002\n",
      "************************************************************\n",
      "EM Step: 16 Epoch: 7, Focus Loss: 0.9300164485971133\n",
      "EM Step: 16 Epoch: 7, Classification Loss: 0.037651552244399984\n",
      "************************************************************\n",
      "EM Step: 16 Epoch: 8, Focus Loss: 0.9272194355726242\n",
      "EM Step: 16 Epoch: 8, Classification Loss: 0.0370398737800618\n",
      "************************************************************\n",
      "EM Step: 16 Epoch: 9, Focus Loss: 0.9259500607848168\n",
      "EM Step: 16 Epoch: 9, Classification Loss: 0.036944450577721\n",
      "************************************************************\n",
      "EM Step: 16 Epoch: 10, Focus Loss: 0.924072439968586\n",
      "EM Step: 16 Epoch: 10, Classification Loss: 0.03675038395449519\n",
      "************************************************************\n",
      "EM Step: 17 Epoch: 1, Focus Loss: 0.8311315755049388\n",
      "EM Step: 17 Epoch: 1, Classification Loss: 0.03000944246693204\n",
      "************************************************************\n",
      "EM Step: 17 Epoch: 2, Focus Loss: 0.829911008477211\n",
      "EM Step: 17 Epoch: 2, Classification Loss: 0.15568647637652855\n",
      "************************************************************\n",
      "EM Step: 17 Epoch: 3, Focus Loss: 0.8316385279099147\n",
      "EM Step: 17 Epoch: 3, Classification Loss: 0.10515306728581587\n",
      "************************************************************\n",
      "EM Step: 17 Epoch: 4, Focus Loss: 0.8326715404788653\n",
      "EM Step: 17 Epoch: 4, Classification Loss: 0.0604798245554169\n",
      "************************************************************\n",
      "EM Step: 17 Epoch: 5, Focus Loss: 0.8325753887494405\n",
      "EM Step: 17 Epoch: 5, Classification Loss: 0.043288687523454425\n",
      "************************************************************\n",
      "EM Step: 17 Epoch: 6, Focus Loss: 0.8333064193526903\n",
      "EM Step: 17 Epoch: 6, Classification Loss: 0.03834039865372082\n",
      "************************************************************\n",
      "EM Step: 17 Epoch: 7, Focus Loss: 0.8322317605217298\n",
      "EM Step: 17 Epoch: 7, Classification Loss: 0.033605489018373194\n",
      "************************************************************\n",
      "EM Step: 17 Epoch: 8, Focus Loss: 0.8289419084787368\n",
      "EM Step: 17 Epoch: 8, Classification Loss: 0.03163771772136291\n",
      "************************************************************\n",
      "EM Step: 17 Epoch: 9, Focus Loss: 0.8277135084072749\n",
      "EM Step: 17 Epoch: 9, Classification Loss: 0.030471936582277218\n",
      "************************************************************\n",
      "EM Step: 17 Epoch: 10, Focus Loss: 0.8286606753865878\n",
      "EM Step: 17 Epoch: 10, Classification Loss: 0.029550013551488518\n",
      "************************************************************\n",
      "EM Step: 18 Epoch: 1, Focus Loss: 0.8062608947356542\n",
      "EM Step: 18 Epoch: 1, Classification Loss: 0.02670150901346157\n",
      "************************************************************\n",
      "EM Step: 18 Epoch: 2, Focus Loss: 0.8043261388937633\n",
      "EM Step: 18 Epoch: 2, Classification Loss: 0.032147278015812235\n",
      "************************************************************\n",
      "EM Step: 18 Epoch: 3, Focus Loss: 0.799816274146239\n",
      "EM Step: 18 Epoch: 3, Classification Loss: 0.049556696352859336\n",
      "************************************************************\n",
      "EM Step: 18 Epoch: 4, Focus Loss: 0.7971729243795077\n",
      "EM Step: 18 Epoch: 4, Classification Loss: 0.057331770999977986\n",
      "************************************************************\n",
      "EM Step: 18 Epoch: 5, Focus Loss: 0.7955435494581858\n",
      "EM Step: 18 Epoch: 5, Classification Loss: 0.05122210821136832\n",
      "************************************************************\n",
      "EM Step: 18 Epoch: 6, Focus Loss: 0.7957070703307788\n",
      "EM Step: 18 Epoch: 6, Classification Loss: 0.03542447155341506\n",
      "************************************************************\n",
      "EM Step: 18 Epoch: 7, Focus Loss: 0.7989339724183082\n",
      "EM Step: 18 Epoch: 7, Classification Loss: 0.029624880695094665\n",
      "************************************************************\n",
      "EM Step: 18 Epoch: 8, Focus Loss: 0.7927756433685621\n",
      "EM Step: 18 Epoch: 8, Classification Loss: 0.027744497040597102\n",
      "************************************************************\n",
      "EM Step: 18 Epoch: 9, Focus Loss: 0.790306356549263\n",
      "EM Step: 18 Epoch: 9, Classification Loss: 0.026094672386534512\n",
      "************************************************************\n",
      "EM Step: 18 Epoch: 10, Focus Loss: 0.7888636747996013\n",
      "EM Step: 18 Epoch: 10, Classification Loss: 0.02508884222091486\n",
      "************************************************************\n",
      "EM Step: 19 Epoch: 1, Focus Loss: 0.7331607153018316\n",
      "EM Step: 19 Epoch: 1, Classification Loss: 0.019790835049934685\n",
      "************************************************************\n",
      "EM Step: 19 Epoch: 2, Focus Loss: 0.730812831223011\n",
      "EM Step: 19 Epoch: 2, Classification Loss: 0.021310825645923615\n",
      "************************************************************\n",
      "EM Step: 19 Epoch: 3, Focus Loss: 0.7282980566223463\n",
      "EM Step: 19 Epoch: 3, Classification Loss: 0.02262872722155104\n",
      "************************************************************\n",
      "EM Step: 19 Epoch: 4, Focus Loss: 0.7272586097319921\n",
      "EM Step: 19 Epoch: 4, Classification Loss: 0.02082910869115343\n",
      "************************************************************\n",
      "EM Step: 19 Epoch: 5, Focus Loss: 0.7231962889432907\n",
      "EM Step: 19 Epoch: 5, Classification Loss: 0.019628347425411145\n",
      "************************************************************\n",
      "EM Step: 19 Epoch: 6, Focus Loss: 0.7213138341903687\n",
      "EM Step: 19 Epoch: 6, Classification Loss: 0.01933851479552686\n",
      "************************************************************\n",
      "EM Step: 19 Epoch: 7, Focus Loss: 0.7210722183187802\n",
      "EM Step: 19 Epoch: 7, Classification Loss: 0.019045071575480202\n",
      "************************************************************\n",
      "EM Step: 19 Epoch: 8, Focus Loss: 0.7206253503759702\n",
      "EM Step: 19 Epoch: 8, Classification Loss: 0.018252020919074616\n",
      "************************************************************\n",
      "EM Step: 19 Epoch: 9, Focus Loss: 0.7202117944757144\n",
      "EM Step: 19 Epoch: 9, Classification Loss: 0.017949930027437708\n",
      "************************************************************\n",
      "EM Step: 19 Epoch: 10, Focus Loss: 0.720206285516421\n",
      "EM Step: 19 Epoch: 10, Classification Loss: 0.017892782335790496\n",
      "************************************************************\n",
      "EM Step: 20 Epoch: 1, Focus Loss: 0.7037163625160853\n",
      "EM Step: 20 Epoch: 1, Classification Loss: 0.019166287209372967\n",
      "************************************************************\n",
      "EM Step: 20 Epoch: 2, Focus Loss: 0.6957138662536939\n",
      "EM Step: 20 Epoch: 2, Classification Loss: 0.017819820860556015\n",
      "************************************************************\n",
      "EM Step: 20 Epoch: 3, Focus Loss: 0.6939332713683446\n",
      "EM Step: 20 Epoch: 3, Classification Loss: 0.01820265598362312\n",
      "************************************************************\n",
      "EM Step: 20 Epoch: 4, Focus Loss: 0.6913345540563266\n",
      "EM Step: 20 Epoch: 4, Classification Loss: 0.01625044222843523\n",
      "************************************************************\n",
      "EM Step: 20 Epoch: 5, Focus Loss: 0.689006732404232\n",
      "EM Step: 20 Epoch: 5, Classification Loss: 0.016403218098760892\n",
      "************************************************************\n",
      "EM Step: 20 Epoch: 6, Focus Loss: 0.6866705159346262\n",
      "EM Step: 20 Epoch: 6, Classification Loss: 0.01597402853658423\n",
      "************************************************************\n",
      "EM Step: 20 Epoch: 7, Focus Loss: 0.681880175570647\n",
      "EM Step: 20 Epoch: 7, Classification Loss: 0.01625954845221713\n",
      "************************************************************\n",
      "EM Step: 20 Epoch: 8, Focus Loss: 0.6811203559239706\n",
      "EM Step: 20 Epoch: 8, Classification Loss: 0.015461773258478691\n",
      "************************************************************\n",
      "EM Step: 20 Epoch: 9, Focus Loss: 0.6799378246068954\n",
      "EM Step: 20 Epoch: 9, Classification Loss: 0.01520498759734134\n",
      "************************************************************\n",
      "EM Step: 20 Epoch: 10, Focus Loss: 0.6814663494626682\n",
      "EM Step: 20 Epoch: 10, Classification Loss: 0.015199791345124444\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(12)\n",
    "focus = Focus()\n",
    "focus = focus.to(device)\n",
    "\n",
    "\n",
    "torch.manual_seed(12)\n",
    "classification = Classification()\n",
    "classification = classification.to(device)\n",
    "\n",
    "\n",
    "Criterion = nn.CrossEntropyLoss(reduction=\"none\") #nn.BCELoss(reduction=\"none\")\n",
    "\n",
    "\n",
    "em_steps = 20\n",
    "\n",
    "\n",
    "for i in range(em_steps):\n",
    "    # calculate p_z\n",
    "    gamma_ = []\n",
    "    focus_optimizer = optim.SGD(focus.parameters(), lr=0.03,momentum=0.9)\n",
    "    classification_optimizer = optim.SGD(classification.parameters(),lr=0.03,momentum=0.9)\n",
    "    for j,data in enumerate(train_loader):\n",
    "        images,labels,foreground_index = data\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        p_z = expectation_step(focus,classification,images,labels)\n",
    "        gamma_.append(p_z)\n",
    "    for epoch in range(10):\n",
    "        focus_epoch_loss = []\n",
    "        classification_epoch_loss = []\n",
    "        for j,data in enumerate(train_loader):\n",
    "            images,labels,foreground_index = data\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            focus,classification,focus_optimizer,classification_optimizer=maximization_step(gamma_[j],\n",
    "                                                                                            focus,classification,\n",
    "                                                                                            images,labels,\n",
    "                                                                                            focus_optimizer,\n",
    "                                                                                            classification_optimizer,\n",
    "                                                                                            Criterion)\n",
    "            with torch.no_grad():\n",
    "                batch = images.size(0)\n",
    "                patches = images.size(1)\n",
    "                focus_outputs = focus(images)\n",
    "                images = images.reshape(batch*patches,3,32,32)\n",
    "                classification_outputs = classification(images) # classification returns output after sigmoid/softmax]\n",
    "                classification_outputs = classification_outputs.reshape(batch,patches,3)\n",
    "                #print(focus_outputs,classification_outputs)\n",
    "                loss_focus = calculate_loss_focus(gamma_[j],focus_outputs)\n",
    "                loss_classification = calculate_loss_classification(gamma_[j],classification_outputs,\n",
    "                                                        labels,Criterion,patches)\n",
    "                focus_epoch_loss.append(loss_focus.item())\n",
    "                classification_epoch_loss.append(loss_classification.item())\n",
    "        print(\"*\"*60)\n",
    "        print(\"EM Step: \"+str(i+1)+\" Epoch: \" + str(epoch+1)+\", Focus Loss: \"+str(np.mean(focus_epoch_loss)))\n",
    "        print(\"EM Step: \"+str(i+1)+\" Epoch: \" + str(epoch+1)+\", Classification Loss: \"+str(np.mean(classification_epoch_loss))) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa304ad9",
   "metadata": {
    "papermill": {
     "duration": 0.182138,
     "end_time": "2022-03-21T07:19:55.732753",
     "exception": false,
     "start_time": "2022-03-21T07:19:55.550615",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b1d98dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-21T07:19:56.107799Z",
     "iopub.status.busy": "2022-03-21T07:19:56.106821Z",
     "iopub.status.idle": "2022-03-21T07:20:04.445036Z",
     "shell.execute_reply": "2022-03-21T07:20:04.445584Z",
     "shell.execute_reply.started": "2022-03-20T09:05:57.37659Z"
    },
    "papermill": {
     "duration": 8.531994,
     "end_time": "2022-03-21T07:20:04.445733",
     "exception": false,
     "start_time": "2022-03-21T07:19:55.913739",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Focus True 68.73666666666666\n",
      "Accuracy 95.35666666666667\n"
     ]
    }
   ],
   "source": [
    "# method 1\n",
    "predicted_indexes = []\n",
    "foreground_index_list = []\n",
    "prediction_list = []\n",
    "labels_list = []\n",
    "with torch.no_grad():\n",
    "    for j,data in enumerate(train_loader):\n",
    "        images,labels,foreground_index = data\n",
    "        images = images.to(device)\n",
    "        foreground_index_list.append(foreground_index.numpy())\n",
    "        labels_list.append(labels.numpy())\n",
    "        batch = images.size(0)\n",
    "        scores = focus(images)\n",
    "        indexes = torch.argmax(F.softmax(scores,dim=1),dim=1).cpu().numpy()\n",
    "        predicted_indexes.append(indexes)\n",
    "        outputs = F.softmax(classification(images[np.arange(batch),indexes,:]),dim=1)\n",
    "        prediction = torch.argmax(outputs,dim=1)\n",
    "        prediction_list.append(prediction.cpu().numpy())\n",
    "    \n",
    "predicted_indexes = np.concatenate(predicted_indexes,axis=0)\n",
    "foreground_index_list = np.concatenate(foreground_index_list,axis=0)\n",
    "prediction_list = np.concatenate(prediction_list,axis=0)\n",
    "labels_list = np.concatenate(labels_list,axis=0)\n",
    "    \n",
    "\n",
    "print(\"Focus True\",(np.sum(predicted_indexes == foreground_index_list,axis=0).item()/\n",
    "                    len(foreground_index_list))*100)\n",
    "accuracy = (np.sum(prediction_list == labels_list,axis=0)/len(labels_list) )*100\n",
    "print(\"Accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c8f90b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-21T07:20:04.818539Z",
     "iopub.status.busy": "2022-03-21T07:20:04.817548Z",
     "iopub.status.idle": "2022-03-21T07:20:13.947685Z",
     "shell.execute_reply": "2022-03-21T07:20:13.948791Z",
     "shell.execute_reply.started": "2022-03-20T09:06:00.879083Z"
    },
    "papermill": {
     "duration": 9.323471,
     "end_time": "2022-03-21T07:20:13.949010",
     "exception": false,
     "start_time": "2022-03-21T07:20:04.625539",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Focus True 68.73666666666666\n",
      "Accuracy 95.86333333333333\n"
     ]
    }
   ],
   "source": [
    "# method 2\n",
    "predicted_indexes = []\n",
    "foreground_index_list = []\n",
    "prediction_list = []\n",
    "labels_list = []\n",
    "with torch.no_grad():\n",
    "    for j,data in enumerate(train_loader):\n",
    "        images,labels,foreground_index = data\n",
    "        images = images.to(device)\n",
    "        batch = images.size(0)\n",
    "        patches = images.size(1)\n",
    "        foreground_index_list.append(foreground_index.numpy())\n",
    "        labels_list.append(labels.numpy())\n",
    "        batch = images.size(0)\n",
    "        focus_outputs = F.softmax(focus(images),dim=1)\n",
    "        indexes = torch.argmax(focus_outputs,dim=1).cpu().numpy()\n",
    "        predicted_indexes.append(indexes)\n",
    "        images = images.reshape(batch*patches,3,32,32)\n",
    "        classification_outputs = F.softmax(classification(images),dim=1)\n",
    "        classification_outputs = classification_outputs.reshape(batch,patches,3)\n",
    "\n",
    "        #print(classification_outputs.shape,focus_outputs.shape)\n",
    "\n",
    "        prediction = torch.argmax(torch.sum(focus_outputs[:,:,None]*classification_outputs,dim=1),dim=1)\n",
    "        prediction_list.append(prediction.cpu().numpy())\n",
    "    \n",
    "predicted_indexes = np.concatenate(predicted_indexes,axis=0)\n",
    "foreground_index_list = np.concatenate(foreground_index_list,axis=0)\n",
    "prediction_list = np.concatenate(prediction_list,axis=0)\n",
    "labels_list = np.concatenate(labels_list,axis=0)\n",
    "    \n",
    "\n",
    "print(\"Focus True\",(np.sum(predicted_indexes == foreground_index_list,axis=0).item()/\n",
    "                    len(foreground_index_list))*100)\n",
    "accuracy = (np.sum(prediction_list == labels_list,axis=0)/len(labels_list) )*100\n",
    "print(\"Accuracy\", accuracy)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# method 2\n",
    "# focus_output = F.softmax(focus(data),dim=1)\n",
    "# indexes = torch.argmax(F.softmax(focus(data),dim=1),dim=1)[:,0].numpy()\n",
    "# classification_output = F.softmax(classification(data),dim=2)\n",
    "# print(\"Focus True\",(np.sum(indexes == fore_idx,axis=0).item()/len(fore_idx))*100)\n",
    "# prediction = torch.argmax(torch.sum(focus_output*classification_output,dim=1),dim=1)\n",
    "# accuracy = (torch.sum(prediction == labels,dim=0)/len(labels) )*100\n",
    "# print(\"Accuracy\", accuracy.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e9bb2205",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-21T07:20:14.412896Z",
     "iopub.status.busy": "2022-03-21T07:20:14.411880Z",
     "iopub.status.idle": "2022-03-21T07:20:14.415268Z",
     "shell.execute_reply": "2022-03-21T07:20:14.414777Z",
     "shell.execute_reply.started": "2022-03-20T09:06:07.629136Z"
    },
    "papermill": {
     "duration": 0.206934,
     "end_time": "2022-03-21T07:20:14.415384",
     "exception": false,
     "start_time": "2022-03-21T07:20:14.208450",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7131161689758301, 0.6468906998634338, 0.6420168876647949, 0.6079952716827393, 0.7196786999702454, 0.6138648986816406, 0.6683324575424194, 0.6153679490089417, 0.6345533132553101, 0.7420426607131958, 0.5048795342445374, 0.5942814350128174, 0.7885680198669434, 0.6982541680335999, 0.6999250650405884, 0.561603307723999, 0.6495387554168701, 0.7503591775894165, 0.7243794202804565, 0.7342938780784607, 0.586621880531311, 0.7976404428482056, 0.680903971195221, 0.6138706207275391, 0.7974025011062622, 0.7387398481369019, 0.6559605002403259, 0.689844012260437, 0.7251850962638855, 0.6912002563476562, 0.6955007910728455, 0.658417284488678, 0.6759337186813354, 0.6579760909080505, 0.7189950346946716, 0.6171526312828064, 0.7065195441246033, 0.7136853933334351, 0.7150242328643799, 0.6627944707870483, 0.6064757108688354, 0.7407604455947876, 0.6211341619491577, 0.8663938045501709, 0.5691202878952026, 0.767224133014679, 0.7762392163276672, 0.7315568923950195, 0.7132584452629089, 0.6578229069709778, 0.6289902925491333, 0.6126300096511841, 0.7125966548919678, 0.7142258882522583, 0.7243813872337341, 0.6950640082359314, 0.6772284507751465, 0.6558465957641602, 0.5464513301849365, 0.5475951433181763, 0.727564811706543, 0.7187638282775879, 0.8585294485092163, 0.6898024082183838, 0.7497780919075012, 0.6815438270568848, 0.6759278774261475, 0.5876147747039795, 0.6775528192520142, 0.7097446918487549, 0.628652811050415, 0.7050899863243103, 0.7100620865821838, 0.6663122177124023, 0.6495315432548523, 0.6316094994544983, 0.7308253049850464, 0.7507222294807434, 0.7172329425811768, 0.7132961750030518, 0.6581526398658752, 0.7542963027954102, 0.7435819506645203, 0.7289777994155884, 0.6017261743545532, 0.6918852925300598, 0.7378733158111572, 0.5630432367324829, 0.6316465139389038, 0.6951881647109985, 0.6952277421951294, 0.5964018106460571, 0.7228379249572754, 0.7232678532600403, 0.6939557194709778, 0.6157136559486389, 0.6657856702804565, 0.6956678032875061, 0.6305578351020813, 0.7874347567558289, 0.6585797667503357, 0.661969780921936, 0.6556428670883179, 0.6289679408073425, 0.6139261722564697, 0.743042528629303, 0.6962873339653015, 0.5795719027519226, 0.7300136089324951, 0.7333364486694336, 0.6649499535560608, 0.7215778827667236, 0.617416262626648, 0.7024601101875305, 0.5839982032775879, 0.7057368755340576, 0.6940123438835144, 0.8163416981697083, 0.7109940052032471, 0.569450855255127]\n"
     ]
    }
   ],
   "source": [
    "print(focus_epoch_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950de0cd",
   "metadata": {
    "papermill": {
     "duration": 0.180448,
     "end_time": "2022-03-21T07:20:14.776543",
     "exception": false,
     "start_time": "2022-03-21T07:20:14.596095",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "> **Test data Evaluation**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "11cddc9b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-21T07:20:15.149756Z",
     "iopub.status.busy": "2022-03-21T07:20:15.148568Z",
     "iopub.status.idle": "2022-03-21T07:20:17.872698Z",
     "shell.execute_reply": "2022-03-21T07:20:17.873070Z",
     "shell.execute_reply.started": "2022-03-20T09:06:13.618167Z"
    },
    "papermill": {
     "duration": 2.915798,
     "end_time": "2022-03-21T07:20:17.873247",
     "exception": false,
     "start_time": "2022-03-21T07:20:14.957449",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Focus True 63.9\n",
      "Accuracy 83.52000000000001\n"
     ]
    }
   ],
   "source": [
    "# method 1\n",
    "predicted_indexes = []\n",
    "foreground_index_list = []\n",
    "prediction_list = []\n",
    "labels_list = []\n",
    "with torch.no_grad():\n",
    "    for j,data in enumerate(test_loader):\n",
    "        images,labels,foreground_index = data\n",
    "        images = images.to(device)\n",
    "        foreground_index_list.append(foreground_index.numpy())\n",
    "        labels_list.append(labels.numpy())\n",
    "        batch = images.size(0)\n",
    "        scores = focus(images)\n",
    "        indexes = torch.argmax(F.softmax(scores,dim=1),dim=1).cpu().numpy()\n",
    "        predicted_indexes.append(indexes)\n",
    "        outputs = F.softmax(classification(images[np.arange(batch),indexes,:]),dim=1)\n",
    "        prediction = torch.argmax(outputs,dim=1)\n",
    "        prediction_list.append(prediction.cpu().numpy())\n",
    "    \n",
    "predicted_indexes = np.concatenate(predicted_indexes,axis=0)\n",
    "foreground_index_list = np.concatenate(foreground_index_list,axis=0)\n",
    "prediction_list = np.concatenate(prediction_list,axis=0)\n",
    "labels_list = np.concatenate(labels_list,axis=0)\n",
    "    \n",
    "\n",
    "print(\"Focus True\",(np.sum(predicted_indexes == foreground_index_list,axis=0).item()/\n",
    "                    len(foreground_index_list))*100)\n",
    "accuracy = (np.sum(prediction_list == labels_list,axis=0)/len(labels_list) )*100\n",
    "print(\"Accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9e4f961e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-21T07:20:18.249455Z",
     "iopub.status.busy": "2022-03-21T07:20:18.248527Z",
     "iopub.status.idle": "2022-03-21T07:20:21.201011Z",
     "shell.execute_reply": "2022-03-21T07:20:21.200589Z",
     "shell.execute_reply.started": "2022-03-20T09:06:18.105584Z"
    },
    "papermill": {
     "duration": 3.146191,
     "end_time": "2022-03-21T07:20:21.201154",
     "exception": false,
     "start_time": "2022-03-21T07:20:18.054963",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Focus True 63.9\n",
      "Accuracy 84.91\n"
     ]
    }
   ],
   "source": [
    "# method 2\n",
    "predicted_indexes = []\n",
    "foreground_index_list = []\n",
    "prediction_list = []\n",
    "labels_list = []\n",
    "with torch.no_grad():\n",
    "    for j,data in enumerate(test_loader):\n",
    "        images,labels,foreground_index = data\n",
    "        images = images.to(device)\n",
    "        batch = images.size(0)\n",
    "        patches = images.size(1)\n",
    "        foreground_index_list.append(foreground_index.numpy())\n",
    "        labels_list.append(labels.numpy())\n",
    "        batch = images.size(0)\n",
    "        focus_outputs = F.softmax(focus(images),dim=1)\n",
    "        indexes = torch.argmax(focus_outputs,dim=1).cpu().numpy()\n",
    "        predicted_indexes.append(indexes)\n",
    "        images = images.reshape(batch*patches,3,32,32)\n",
    "        classification_outputs = F.softmax(classification(images),dim=1)\n",
    "        classification_outputs = classification_outputs.reshape(batch,patches,3)\n",
    "\n",
    "        #print(classification_outputs.shape,focus_outputs.shape)\n",
    "\n",
    "        prediction = torch.argmax(torch.sum(focus_outputs[:,:,None]*classification_outputs,dim=1),dim=1)\n",
    "        prediction_list.append(prediction.cpu().numpy())\n",
    "    \n",
    "predicted_indexes = np.concatenate(predicted_indexes,axis=0)\n",
    "foreground_index_list = np.concatenate(foreground_index_list,axis=0)\n",
    "prediction_list = np.concatenate(prediction_list,axis=0)\n",
    "labels_list = np.concatenate(labels_list,axis=0)\n",
    "    \n",
    "\n",
    "print(\"Focus True\",(np.sum(predicted_indexes == foreground_index_list,axis=0).item()/\n",
    "                    len(foreground_index_list))*100)\n",
    "accuracy = (np.sum(prediction_list == labels_list,axis=0)/len(labels_list) )*100\n",
    "print(\"Accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d3f03f",
   "metadata": {
    "papermill": {
     "duration": 0.182001,
     "end_time": "2022-03-21T07:20:21.565705",
     "exception": false,
     "start_time": "2022-03-21T07:20:21.383704",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b0282c",
   "metadata": {
    "papermill": {
     "duration": 0.184817,
     "end_time": "2022-03-21T07:20:21.931530",
     "exception": false,
     "start_time": "2022-03-21T07:20:21.746713",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3a2723",
   "metadata": {
    "papermill": {
     "duration": 0.288573,
     "end_time": "2022-03-21T07:20:22.553356",
     "exception": false,
     "start_time": "2022-03-21T07:20:22.264783",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 5479.687585,
   "end_time": "2022-03-21T07:20:25.203948",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-03-21T05:49:05.516363",
   "version": "2.3.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "08f4372ab59744558039bca24b877b9c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_3a405d8f19674d45ab3cf7da07918307",
       "max": 170498071.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_1fc16dd9d2c74df5920e58b7c37a251c",
       "value": 170498071.0
      }
     },
     "1fc16dd9d2c74df5920e58b7c37a251c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "2160d472f3b6453ca495cfd3817fd5f5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3a405d8f19674d45ab3cf7da07918307": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4e8836ff02314f5fb148bcc7aeafb00d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "6d3ccc1b9cbe43279b5cb2b4a0a4f7e3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "7a7f9f246ca64408934f15b9d0883cad": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_2160d472f3b6453ca495cfd3817fd5f5",
       "placeholder": "​",
       "style": "IPY_MODEL_6d3ccc1b9cbe43279b5cb2b4a0a4f7e3",
       "value": " 170499072/? [00:03&lt;00:00, 55176318.55it/s]"
      }
     },
     "9c72c8d10ec14bb8840b86c95e7f295b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a25917df86dd4a7a99cb7ce4f4499630": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_eaefa4750b8c489cac2780f6d58585b8",
        "IPY_MODEL_08f4372ab59744558039bca24b877b9c",
        "IPY_MODEL_7a7f9f246ca64408934f15b9d0883cad"
       ],
       "layout": "IPY_MODEL_9c72c8d10ec14bb8840b86c95e7f295b"
      }
     },
     "cddf2c77bf45400fae42f566e340cbbe": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "eaefa4750b8c489cac2780f6d58585b8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_cddf2c77bf45400fae42f566e340cbbe",
       "placeholder": "​",
       "style": "IPY_MODEL_4e8836ff02314f5fb148bcc7aeafb00d",
       "value": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

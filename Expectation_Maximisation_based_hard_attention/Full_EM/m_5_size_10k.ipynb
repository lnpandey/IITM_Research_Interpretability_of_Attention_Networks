{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a3ccba2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-21T04:37:45.968282Z",
     "iopub.status.busy": "2022-03-21T04:37:45.966784Z",
     "iopub.status.idle": "2022-03-21T04:37:47.638758Z",
     "shell.execute_reply": "2022-03-21T04:37:47.639449Z",
     "shell.execute_reply.started": "2022-03-20T08:28:27.859219Z"
    },
    "papermill": {
     "duration": 1.698173,
     "end_time": "2022-03-21T04:37:47.639871",
     "exception": false,
     "start_time": "2022-03-21T04:37:45.941698",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "from tqdm import tqdm\n",
    "%matplotlib inline\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm as tqdm\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbcf64b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-21T04:37:47.696563Z",
     "iopub.status.busy": "2022-03-21T04:37:47.695891Z",
     "iopub.status.idle": "2022-03-21T04:37:47.698111Z",
     "shell.execute_reply": "2022-03-21T04:37:47.698512Z",
     "shell.execute_reply.started": "2022-03-20T08:28:37.120300Z"
    },
    "papermill": {
     "duration": 0.036263,
     "end_time": "2022-03-21T04:37:47.698645",
     "exception": false,
     "start_time": "2022-03-21T04:37:47.662382",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Focus(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Focus, self).__init__()\n",
    "\n",
    "    self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=0, bias=False)\n",
    "    self.pool = nn.MaxPool2d(2, 2)\n",
    "    self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=0, bias=False)\n",
    "    self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=0, bias=False)\n",
    "    self.fc1 = nn.Linear(1024, 512, bias=False)\n",
    "    self.fc2 = nn.Linear(512, 64, bias=False)\n",
    "    self.fc3 = nn.Linear(64, 10, bias=False)\n",
    "    self.fc4 = nn.Linear(10,1, bias=False)\n",
    "\n",
    "    torch.nn.init.xavier_normal_(self.conv1.weight)\n",
    "    torch.nn.init.xavier_normal_(self.conv2.weight)\n",
    "    torch.nn.init.xavier_normal_(self.conv3.weight)\n",
    "    torch.nn.init.xavier_normal_(self.fc1.weight)\n",
    "    torch.nn.init.xavier_normal_(self.fc2.weight)\n",
    "    torch.nn.init.xavier_normal_(self.fc3.weight)\n",
    "    torch.nn.init.xavier_normal_(self.fc4.weight)\n",
    "\n",
    "  def forward(self,z):  #y is avg image #z batch of list of 9 images\n",
    "    batch = z.size(0)\n",
    "    patches = z.size(1)\n",
    "    z = z.view(batch*patches,3,32,32)\n",
    "    alpha =  self.helper(z)\n",
    "    alpha = alpha.view(batch,patches,-1)\n",
    "    return alpha[:,:,0] # scores \n",
    "    \n",
    "  def helper(self, x):\n",
    "    x = self.pool(F.relu(self.conv1(x)))\n",
    "    x = self.pool(F.relu(self.conv2(x)))\n",
    "    # print(x.shape)\n",
    "    x = (F.relu(self.conv3(x)))\n",
    "    x =  x.view(x.size(0), -1)\n",
    "    # print(x.shape)\n",
    "    x = F.relu(self.fc1(x))\n",
    "    x = F.relu(self.fc2(x))\n",
    "    x = F.relu(self.fc3(x))\n",
    "    x = self.fc4(x)\n",
    "    return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c43600f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-21T04:37:47.750669Z",
     "iopub.status.busy": "2022-03-21T04:37:47.749911Z",
     "iopub.status.idle": "2022-03-21T04:37:47.752243Z",
     "shell.execute_reply": "2022-03-21T04:37:47.751849Z",
     "shell.execute_reply.started": "2022-03-20T08:28:40.017216Z"
    },
    "papermill": {
     "duration": 0.033318,
     "end_time": "2022-03-21T04:37:47.752346",
     "exception": false,
     "start_time": "2022-03-21T04:37:47.719028",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Classification(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Classification, self).__init__()\n",
    "    self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "    self.pool = nn.MaxPool2d(2, 2)\n",
    "    self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "    self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "    self.fc2 = nn.Linear(120, 84)\n",
    "    self.fc3 = nn.Linear(84, 10)\n",
    "    self.fc4 = nn.Linear(10,3)\n",
    "\n",
    "    torch.nn.init.xavier_normal_(self.conv1.weight)\n",
    "    torch.nn.init.zeros_(self.conv1.bias)\n",
    "    torch.nn.init.xavier_normal_(self.conv2.weight)\n",
    "    torch.nn.init.zeros_(self.conv2.bias)\n",
    "    torch.nn.init.xavier_normal_(self.fc1.weight)\n",
    "    torch.nn.init.zeros_(self.fc1.bias)\n",
    "    torch.nn.init.xavier_normal_(self.fc2.weight)\n",
    "    torch.nn.init.zeros_(self.fc2.bias)\n",
    "    torch.nn.init.xavier_normal_(self.fc3.weight)\n",
    "    torch.nn.init.zeros_(self.fc3.bias)\n",
    "    torch.nn.init.xavier_normal_(self.fc4.weight)\n",
    "    torch.nn.init.zeros_(self.fc4.bias)\n",
    "\n",
    "  def forward(self,z): \n",
    "    y1 = self.pool(F.relu(self.conv1(z)))\n",
    "    y1 = self.pool(F.relu(self.conv2(y1)))\n",
    "    y1 = y1.view(-1, 16 * 5 * 5)\n",
    "\n",
    "    y1 = F.relu(self.fc1(y1))\n",
    "    y1 = F.relu(self.fc2(y1))\n",
    "    y1 = F.relu(self.fc3(y1))\n",
    "    y1 = self.fc4(y1)\n",
    "    return y1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "736fb48e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-21T04:37:47.799744Z",
     "iopub.status.busy": "2022-03-21T04:37:47.796492Z",
     "iopub.status.idle": "2022-03-21T04:37:47.801928Z",
     "shell.execute_reply": "2022-03-21T04:37:47.801503Z",
     "shell.execute_reply.started": "2022-03-20T08:36:06.558404Z"
    },
    "papermill": {
     "duration": 0.029463,
     "end_time": "2022-03-21T04:37:47.802035",
     "exception": false,
     "start_time": "2022-03-21T04:37:47.772572",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_loss_focus(gamma,focus_output):\n",
    "    \n",
    "    m = torch.nn.LogSoftmax(dim=1)\n",
    "    log_outputs = m(focus_output)    \n",
    "    loss_ = gamma*log_outputs\n",
    "    loss_ = torch.sum(loss_,dim=1)\n",
    "    loss_ = -torch.mean(loss_,dim=0)  \n",
    "    return loss_ \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0bd1980",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-21T04:37:47.850816Z",
     "iopub.status.busy": "2022-03-21T04:37:47.849342Z",
     "iopub.status.idle": "2022-03-21T04:37:47.851634Z",
     "shell.execute_reply": "2022-03-21T04:37:47.852093Z",
     "shell.execute_reply.started": "2022-03-20T08:28:42.037522Z"
    },
    "papermill": {
     "duration": 0.028415,
     "end_time": "2022-03-21T04:37:47.852213",
     "exception": false,
     "start_time": "2022-03-21T04:37:47.823798",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_loss_classification(gamma,classification_output,label,criterion,n_patches):\n",
    "    #print(classification_output.shape)\n",
    "    \n",
    "    batch = label.size(0)\n",
    "    classes = classification_output.size(2)\n",
    "    label = label.repeat_interleave(n_patches)\n",
    "    classification_output = classification_output.reshape((batch*n_patches,classes))\n",
    "    loss_ = criterion(classification_output,label)\n",
    "    \n",
    "    loss_ = loss_.reshape((batch,n_patches))\n",
    "    \n",
    "    loss_ = gamma*loss_\n",
    "    loss_ = torch.sum(loss_,dim=1)\n",
    "    loss_ = torch.mean(loss_,dim=0)\n",
    "    \n",
    "    return loss_\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "816e2193",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-21T04:37:47.900692Z",
     "iopub.status.busy": "2022-03-21T04:37:47.899894Z",
     "iopub.status.idle": "2022-03-21T04:37:47.902391Z",
     "shell.execute_reply": "2022-03-21T04:37:47.901982Z",
     "shell.execute_reply.started": "2022-03-20T08:28:42.857391Z"
    },
    "papermill": {
     "duration": 0.029696,
     "end_time": "2022-03-21T04:37:47.902489",
     "exception": false,
     "start_time": "2022-03-21T04:37:47.872793",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def expectation_step(fc,cl,data,labels):\n",
    "    batch= data.size(0)\n",
    "    patches = data.size(1)\n",
    "    with torch.no_grad():\n",
    "        outputs_f = torch.softmax(fc(data),dim=1)\n",
    "        data = data.reshape(batch*patches,3,32,32)\n",
    "        outputs_g = cl(data)\n",
    "        \n",
    "    outputs_g = torch.softmax(outputs_g.reshape(batch,patches,3),dim=2)\n",
    "    \n",
    "        \n",
    "        \n",
    "    #print(\"Focus output\",outputs_f.shape,torch.sum(outputs_f,dim=1))\n",
    "    #print(\"Classification output\",outputs_g.shape,torch.sum(outputs_g,dim=1),torch.sum(outputs_g,dim=2))\n",
    "    outputs_g = outputs_g[np.arange(batch),:,labels]\n",
    "    p_x_y_z = outputs_f*outputs_g   #(1-outputs_g)    \n",
    "    \n",
    "    \n",
    "    normalized_p = p_x_y_z/torch.sum(p_x_y_z,dim=1,keepdims=True)\n",
    "#     print(outputs_f[0],outputs_g[0],normalized_p[0])\n",
    "    return normalized_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b088754",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-21T04:37:47.951672Z",
     "iopub.status.busy": "2022-03-21T04:37:47.951139Z",
     "iopub.status.idle": "2022-03-21T04:37:47.954568Z",
     "shell.execute_reply": "2022-03-21T04:37:47.954153Z",
     "shell.execute_reply.started": "2022-03-20T08:28:43.596869Z"
    },
    "papermill": {
     "duration": 0.030148,
     "end_time": "2022-03-21T04:37:47.954695",
     "exception": false,
     "start_time": "2022-03-21T04:37:47.924547",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def maximization_step(p_z,focus,classification,data,labels,focus_optimizer,classification_optimizer,Criterion):    \n",
    "    batch = data.size(0)\n",
    "    patches = data.size(1)\n",
    "    focus_optimizer.zero_grad()\n",
    "    classification_optimizer.zero_grad()\n",
    "    \n",
    "    focus_outputs = focus(data)\n",
    "    data = data.reshape(batch*patches,3,32,32)\n",
    "    classification_outputs = classification(data) \n",
    "    classification_outputs = classification_outputs.reshape(batch,patches,3)\n",
    "    \n",
    "    \n",
    "    #print(focus_outputs,classification_outputs)\n",
    "    \n",
    "    loss_focus = calculate_loss_focus(p_z,focus_outputs)\n",
    "    loss_classification = calculate_loss_classification(p_z,classification_outputs,\n",
    "                                                        labels,Criterion,patches)\n",
    "    \n",
    "    #print(\"Focus loss\",loss_focus.item())\n",
    "    #print(\"Classification loss\",loss_classification.item())\n",
    "    loss_focus.backward() \n",
    "    loss_classification.backward()\n",
    "    focus_optimizer.step()\n",
    "    classification_optimizer.step()\n",
    "    \n",
    "    return focus,classification,focus_optimizer,classification_optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f7a8084",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-21T04:37:48.001375Z",
     "iopub.status.busy": "2022-03-21T04:37:48.000639Z",
     "iopub.status.idle": "2022-03-21T04:37:48.002475Z",
     "shell.execute_reply": "2022-03-21T04:37:48.002903Z",
     "shell.execute_reply.started": "2022-03-20T08:28:44.173461Z"
    },
    "papermill": {
     "duration": 0.028103,
     "end_time": "2022-03-21T04:37:48.003021",
     "exception": false,
     "start_time": "2022-03-21T04:37:47.974918",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_mosaic_img(bg_idx,fg_idx,fg,m): \n",
    "    \"\"\"\n",
    "      bg_idx : list of indexes of background_data[] to be used as background images in mosaic\n",
    "      fg_idx : index of image to be used as foreground image from foreground data\n",
    "      fg : at what position/index foreground image has to be stored out of 0-8\n",
    "    \"\"\"\n",
    "    image_list=[]\n",
    "    j=0\n",
    "    for i in range(m):  # m value \n",
    "        if i != fg:\n",
    "            image_list.append(background_data[bg_idx[j]])\n",
    "            j+=1\n",
    "        else: \n",
    "            image_list.append(foreground_data[fg_idx])\n",
    "            label = foreground_label[fg_idx] - fg1  # minus fg1 because our fore ground classes are fg1,fg2,fg3 but we have to store it as 0,1,2\n",
    "    #image_list = np.concatenate(image_list ,axis=0)\n",
    "    image_list = torch.stack(image_list) \n",
    "    return image_list,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d690775b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-21T04:37:48.059561Z",
     "iopub.status.busy": "2022-03-21T04:37:48.058786Z",
     "iopub.status.idle": "2022-03-21T04:38:50.823054Z",
     "shell.execute_reply": "2022-03-21T04:38:50.822497Z",
     "shell.execute_reply.started": "2022-03-20T08:28:45.159285Z"
    },
    "papermill": {
     "duration": 62.799236,
     "end_time": "2022-03-21T04:38:50.823191",
     "exception": false,
     "start_time": "2022-03-21T04:37:48.023955",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "877104c962454b9d9ef8ac300d12380a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/170498071 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:31<00:00, 159.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Foreground Background Data created\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [00:02<00:00, 7587.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mosaic Data Created\n"
     ]
    }
   ],
   "source": [
    "fg1, fg2, fg3 = 0,1,2\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=10, shuffle=False)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=10, shuffle=False)\n",
    "\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "foreground_classes = {'plane', 'car', 'bird'}\n",
    "\n",
    "background_classes = {'cat', 'deer', 'dog', 'frog', 'horse','ship', 'truck'}\n",
    "\n",
    "# print(type(foreground_classes))\n",
    "\n",
    "dataiter = iter(trainloader)\n",
    "background_data=[]\n",
    "background_label=[]\n",
    "foreground_data=[]\n",
    "foreground_label=[]\n",
    "batch_size=10\n",
    "\n",
    "for i in tqdm(range(5000)):   #5000*batch_size = 50000 data points\n",
    "    images, labels = dataiter.next()\n",
    "    for j in range(batch_size):\n",
    "        if(classes[labels[j]] in background_classes):\n",
    "            img = images[j].tolist()\n",
    "            background_data.append(img)\n",
    "            background_label.append(labels[j])\n",
    "        else:\n",
    "            img = images[j].tolist()\n",
    "            foreground_data.append(img)\n",
    "            foreground_label.append(labels[j])\n",
    "            \n",
    "foreground_data = torch.tensor(foreground_data)\n",
    "foreground_label = torch.tensor(foreground_label)\n",
    "background_data = torch.tensor(background_data)\n",
    "background_label = torch.tensor(background_label)\n",
    "print(\"Foreground Background Data created\")\n",
    "\n",
    "\n",
    "\n",
    "m = 5\n",
    "desired_num = 20000\n",
    "mosaic_data =[]      # list of mosaic images, each mosaic image is saved as list of 9 images\n",
    "fore_idx =[]                   # list of indexes at which foreground image is present in a mosaic image i.e from 0 to 9               \n",
    "mosaic_label=[]                # label of mosaic image = foreground class present in that mosaic\n",
    "list_set_labels = [] \n",
    "for i in tqdm(range(desired_num)):\n",
    "    set_idx = set()\n",
    "    np.random.seed(i)\n",
    "    bg_idx = np.random.randint(0,35000,m-1)\n",
    "    set_idx = set(background_label[bg_idx].tolist())\n",
    "    fg_idx = np.random.randint(0,15000)\n",
    "    set_idx.add(foreground_label[fg_idx].item())\n",
    "    fg = np.random.randint(0,m)\n",
    "    fore_idx.append(fg)\n",
    "    image_list,label = create_mosaic_img(bg_idx,fg_idx,fg,m)\n",
    "    mosaic_data.append(image_list)\n",
    "    mosaic_label.append(label)\n",
    "    list_set_labels.append(set_idx)\n",
    "print(\"Mosaic Data Created\")\n",
    "mosaic_data = torch.stack(mosaic_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b9a8ac5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-21T04:38:51.079851Z",
     "iopub.status.busy": "2022-03-21T04:38:51.059754Z",
     "iopub.status.idle": "2022-03-21T04:38:51.107322Z",
     "shell.execute_reply": "2022-03-21T04:38:51.107745Z",
     "shell.execute_reply.started": "2022-03-20T08:29:53.921244Z"
    },
    "papermill": {
     "duration": 0.188848,
     "end_time": "2022-03-21T04:38:51.107894",
     "exception": false,
     "start_time": "2022-03-21T04:38:50.919046",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([20000, 5, 3, 32, 32]), (20000,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mosaic_data.shape,np.shape(mosaic_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5bd3d0dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-21T04:38:51.304990Z",
     "iopub.status.busy": "2022-03-21T04:38:51.304236Z",
     "iopub.status.idle": "2022-03-21T04:38:51.306222Z",
     "shell.execute_reply": "2022-03-21T04:38:51.306580Z",
     "shell.execute_reply.started": "2022-03-20T08:29:54.013924Z"
    },
    "papermill": {
     "duration": 0.10379,
     "end_time": "2022-03-21T04:38:51.306749",
     "exception": false,
     "start_time": "2022-03-21T04:38:51.202959",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MosaicDataset(Dataset):\n",
    "  \"\"\"MosaicDataset dataset.\"\"\"\n",
    "\n",
    "  def __init__(self, mosaic_list_of_images, mosaic_label, fore_idx):\n",
    "    \"\"\"\n",
    "      Args:\n",
    "        csv_file (string): Path to the csv file with annotations.\n",
    "        root_dir (string): Directory with all the images.\n",
    "        transform (callable, optional): Optional transform to be applied\n",
    "            on a sample.\n",
    "    \"\"\"\n",
    "    self.mosaic = mosaic_list_of_images\n",
    "    self.label = mosaic_label\n",
    "    self.fore_idx = fore_idx\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.label)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return self.mosaic[idx] , self.label[idx], self.fore_idx[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eca53db9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-21T04:38:51.504273Z",
     "iopub.status.busy": "2022-03-21T04:38:51.503532Z",
     "iopub.status.idle": "2022-03-21T04:38:51.506036Z",
     "shell.execute_reply": "2022-03-21T04:38:51.505593Z",
     "shell.execute_reply.started": "2022-03-20T08:29:56.548543Z"
    },
    "papermill": {
     "duration": 0.104722,
     "end_time": "2022-03-21T04:38:51.506143",
     "exception": false,
     "start_time": "2022-03-21T04:38:51.401421",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch = 250\n",
    "msd = MosaicDataset(mosaic_data[0:10000], mosaic_label[0:10000] , fore_idx[0:10000])\n",
    "train_loader = DataLoader( msd,batch_size= batch ,shuffle=False)\n",
    "\n",
    "batch = 250\n",
    "msd1 = MosaicDataset(mosaic_data[10000:], mosaic_label[10000:] , fore_idx[10000:])\n",
    "test_loader = DataLoader( msd1,batch_size= batch ,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67fdbe25",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-21T04:38:51.701062Z",
     "iopub.status.busy": "2022-03-21T04:38:51.700323Z",
     "iopub.status.idle": "2022-03-21T04:38:51.702450Z",
     "shell.execute_reply": "2022-03-21T04:38:51.702838Z",
     "shell.execute_reply.started": "2022-03-20T08:30:03.599894Z"
    },
    "papermill": {
     "duration": 0.101914,
     "end_time": "2022-03-21T04:38:51.702966",
     "exception": false,
     "start_time": "2022-03-21T04:38:51.601052",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# torch.manual_seed(12)\n",
    "# focus = Focus()\n",
    "# focus = focus.to(device)\n",
    "# #print(focus.fc1.weight.data)\n",
    "# # focus.fc1.weight.data = torch.tensor([[0.,0.]])\n",
    "# torch.manual_seed(12)\n",
    "# classification = Classification()\n",
    "# classification = classification.to(device)\n",
    "# #print(classification.fc1.bias.data)\n",
    "# # classification.fc1.weight.data = torch.tensor([[0.1,0.1],[-0.1,-0.1],[0.,0.]])\n",
    "# # classification.fc1.bias.data = torch.tensor([0.,0.,0.])\n",
    "\n",
    "# Criterion = nn.CrossEntropyLoss(reduction=\"none\") #nn.BCELoss(reduction=\"none\")\n",
    "# focus_optimizer = optim.SGD(focus.parameters(), lr=0.07,momentum=0.9)\n",
    "# classification_optimizer = optim.SGD(classification.parameters(),lr=0.07,momentum=0.9)\n",
    "\n",
    "# # focus_optimizer = optim.Adam(focus.parameters(), lr=0.0001,weight_decay=0.00001)\n",
    "# # classification_optimizer = optim.Adam(classification.parameters(),lr=0.00005,weight_decay=0.001)\n",
    "\n",
    "# for i in range(100):\n",
    "#     focus_epoch_loss = []\n",
    "#     classification_epoch_loss = []\n",
    "#     for j,data in enumerate(train_loader):\n",
    "#         images,labels,foreground_index = data\n",
    "#         images = images.to(device)\n",
    "#         labels = labels.to(device)\n",
    "#         p_z = expectation_step(focus,classification,images,labels)\n",
    "#         focus,classification,focus_optimizer,classification_optimizer=maximization_step(p_z,focus,classification,\n",
    "#                                                                                         images,labels,\n",
    "#                                                                                         focus_optimizer,\n",
    "#                                                                                         classification_optimizer,\n",
    "#                                                                                         Criterion)\n",
    "#         with torch.no_grad():\n",
    "#             p_z = expectation_step(focus,classification,images,labels)\n",
    "#             batch = images.size(0)\n",
    "#             patches = images.size(1)\n",
    "#             focus_outputs = focus(images)\n",
    "#             images = images.reshape(batch*patches,3,32,32)\n",
    "#             classification_outputs = classification(images) # classification returns output after sigmoid/softmax]\n",
    "#             classification_outputs = classification_outputs.reshape(batch,patches,3)\n",
    "#             #print(focus_outputs,classification_outputs)\n",
    "#             loss_focus = calculate_loss_focus(p_z,focus_outputs)\n",
    "#             loss_classification = calculate_loss_classification(p_z,classification_outputs,\n",
    "#                                                         labels,Criterion,patches)\n",
    "#             focus_epoch_loss.append(loss_focus.item())\n",
    "#             classification_epoch_loss.append(loss_classification.item())\n",
    "#     print(\"*\"*60)\n",
    "#     print(\"Epoch: \" + str(i+1)+\", Focus Loss: \"+str(np.mean(focus_epoch_loss)))\n",
    "#     print(\"Epoch: \" + str(i+1)+\", Classification Loss: \"+str(np.mean(classification_epoch_loss))) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5329f70",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-21T04:38:51.913510Z",
     "iopub.status.busy": "2022-03-21T04:38:51.912832Z",
     "iopub.status.idle": "2022-03-21T04:45:18.369022Z",
     "shell.execute_reply": "2022-03-21T04:45:18.369697Z"
    },
    "papermill": {
     "duration": 386.571678,
     "end_time": "2022-03-21T04:45:18.369994",
     "exception": false,
     "start_time": "2022-03-21T04:38:51.798316",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************************\n",
      "EM Step: 1 Epoch: 1, Focus Loss: 1.6089856445789337\n",
      "EM Step: 1 Epoch: 1, Classification Loss: 1.0928608983755113\n",
      "************************************************************\n",
      "EM Step: 1 Epoch: 2, Focus Loss: 1.6089825063943863\n",
      "EM Step: 1 Epoch: 2, Classification Loss: 1.0836345672607421\n",
      "************************************************************\n",
      "EM Step: 1 Epoch: 3, Focus Loss: 1.6089802533388138\n",
      "EM Step: 1 Epoch: 3, Classification Loss: 1.074713096022606\n",
      "************************************************************\n",
      "EM Step: 1 Epoch: 4, Focus Loss: 1.6089783638715744\n",
      "EM Step: 1 Epoch: 4, Classification Loss: 1.068159282207489\n",
      "************************************************************\n",
      "EM Step: 1 Epoch: 5, Focus Loss: 1.608976709842682\n",
      "EM Step: 1 Epoch: 5, Classification Loss: 1.0643538028001784\n",
      "************************************************************\n",
      "EM Step: 1 Epoch: 6, Focus Loss: 1.6089751809835433\n",
      "EM Step: 1 Epoch: 6, Classification Loss: 1.0611683547496795\n",
      "************************************************************\n",
      "EM Step: 1 Epoch: 7, Focus Loss: 1.6089737862348557\n",
      "EM Step: 1 Epoch: 7, Classification Loss: 1.0581663370132446\n",
      "************************************************************\n",
      "EM Step: 1 Epoch: 8, Focus Loss: 1.608972492814064\n",
      "EM Step: 1 Epoch: 8, Classification Loss: 1.0555223554372788\n",
      "************************************************************\n",
      "EM Step: 1 Epoch: 9, Focus Loss: 1.6089712858200074\n",
      "EM Step: 1 Epoch: 9, Classification Loss: 1.0530775487422943\n",
      "************************************************************\n",
      "EM Step: 1 Epoch: 10, Focus Loss: 1.6089701324701309\n",
      "EM Step: 1 Epoch: 10, Classification Loss: 1.0506840735673904\n",
      "************************************************************\n",
      "EM Step: 2 Epoch: 1, Focus Loss: 1.6064008057117463\n",
      "EM Step: 2 Epoch: 1, Classification Loss: 0.9659864723682403\n",
      "************************************************************\n",
      "EM Step: 2 Epoch: 2, Focus Loss: 1.60420301258564\n",
      "EM Step: 2 Epoch: 2, Classification Loss: 0.9588406980037689\n",
      "************************************************************\n",
      "EM Step: 2 Epoch: 3, Focus Loss: 1.6031221181154252\n",
      "EM Step: 2 Epoch: 3, Classification Loss: 0.953266179561615\n",
      "************************************************************\n",
      "EM Step: 2 Epoch: 4, Focus Loss: 1.6022233337163925\n",
      "EM Step: 2 Epoch: 4, Classification Loss: 0.9478610932826996\n",
      "************************************************************\n",
      "EM Step: 2 Epoch: 5, Focus Loss: 1.6014575213193893\n",
      "EM Step: 2 Epoch: 5, Classification Loss: 0.9437665656208992\n",
      "************************************************************\n",
      "EM Step: 2 Epoch: 6, Focus Loss: 1.6007933825254441\n",
      "EM Step: 2 Epoch: 6, Classification Loss: 0.9391717180609703\n",
      "************************************************************\n",
      "EM Step: 2 Epoch: 7, Focus Loss: 1.600195762515068\n",
      "EM Step: 2 Epoch: 7, Classification Loss: 0.9348832637071609\n",
      "************************************************************\n",
      "EM Step: 2 Epoch: 8, Focus Loss: 1.5996518403291702\n",
      "EM Step: 2 Epoch: 8, Classification Loss: 0.9309217989444732\n",
      "************************************************************\n",
      "EM Step: 2 Epoch: 9, Focus Loss: 1.5991752058267594\n",
      "EM Step: 2 Epoch: 9, Classification Loss: 0.9266858533024788\n",
      "************************************************************\n",
      "EM Step: 2 Epoch: 10, Focus Loss: 1.5987304031848908\n",
      "EM Step: 2 Epoch: 10, Classification Loss: 0.9224603310227394\n",
      "************************************************************\n",
      "EM Step: 3 Epoch: 1, Focus Loss: 1.5595955729484559\n",
      "EM Step: 3 Epoch: 1, Classification Loss: 0.7890826657414436\n",
      "************************************************************\n",
      "EM Step: 3 Epoch: 2, Focus Loss: 1.5511281758546829\n",
      "EM Step: 3 Epoch: 2, Classification Loss: 0.7816404700279236\n",
      "************************************************************\n",
      "EM Step: 3 Epoch: 3, Focus Loss: 1.5471390157938003\n",
      "EM Step: 3 Epoch: 3, Classification Loss: 0.7755372881889343\n",
      "************************************************************\n",
      "EM Step: 3 Epoch: 4, Focus Loss: 1.5448156505823136\n",
      "EM Step: 3 Epoch: 4, Classification Loss: 0.7663498923182488\n",
      "************************************************************\n",
      "EM Step: 3 Epoch: 5, Focus Loss: 1.5426268875598907\n",
      "EM Step: 3 Epoch: 5, Classification Loss: 0.7578225716948509\n",
      "************************************************************\n",
      "EM Step: 3 Epoch: 6, Focus Loss: 1.540861177444458\n",
      "EM Step: 3 Epoch: 6, Classification Loss: 0.7495387122035027\n",
      "************************************************************\n",
      "EM Step: 3 Epoch: 7, Focus Loss: 1.5393011569976807\n",
      "EM Step: 3 Epoch: 7, Classification Loss: 0.743295930325985\n",
      "************************************************************\n",
      "EM Step: 3 Epoch: 8, Focus Loss: 1.5378847867250443\n",
      "EM Step: 3 Epoch: 8, Classification Loss: 0.7409203007817269\n",
      "************************************************************\n",
      "EM Step: 3 Epoch: 9, Focus Loss: 1.5366216272115707\n",
      "EM Step: 3 Epoch: 9, Classification Loss: 0.7366280123591423\n",
      "************************************************************\n",
      "EM Step: 3 Epoch: 10, Focus Loss: 1.535208097100258\n",
      "EM Step: 3 Epoch: 10, Classification Loss: 0.7311713516712188\n",
      "************************************************************\n",
      "EM Step: 4 Epoch: 1, Focus Loss: 1.391245922446251\n",
      "EM Step: 4 Epoch: 1, Classification Loss: 0.5514053240418434\n",
      "************************************************************\n",
      "EM Step: 4 Epoch: 2, Focus Loss: 1.3713084667921067\n",
      "EM Step: 4 Epoch: 2, Classification Loss: 0.5386061549186707\n",
      "************************************************************\n",
      "EM Step: 4 Epoch: 3, Focus Loss: 1.3656729876995086\n",
      "EM Step: 4 Epoch: 3, Classification Loss: 0.535835436731577\n",
      "************************************************************\n",
      "EM Step: 4 Epoch: 4, Focus Loss: 1.362532502412796\n",
      "EM Step: 4 Epoch: 4, Classification Loss: 0.5388146616518498\n",
      "************************************************************\n",
      "EM Step: 4 Epoch: 5, Focus Loss: 1.3612761199474335\n",
      "EM Step: 4 Epoch: 5, Classification Loss: 0.5469191461801529\n",
      "************************************************************\n",
      "EM Step: 4 Epoch: 6, Focus Loss: 1.3533505260944367\n",
      "EM Step: 4 Epoch: 6, Classification Loss: 0.5248174734413624\n",
      "************************************************************\n",
      "EM Step: 4 Epoch: 7, Focus Loss: 1.348989138007164\n",
      "EM Step: 4 Epoch: 7, Classification Loss: 0.5213734716176986\n",
      "************************************************************\n",
      "EM Step: 4 Epoch: 8, Focus Loss: 1.3461044669151305\n",
      "EM Step: 4 Epoch: 8, Classification Loss: 0.5184952966868878\n",
      "************************************************************\n",
      "EM Step: 4 Epoch: 9, Focus Loss: 1.3440318167209626\n",
      "EM Step: 4 Epoch: 9, Classification Loss: 0.5116824731230736\n",
      "************************************************************\n",
      "EM Step: 4 Epoch: 10, Focus Loss: 1.3448041558265686\n",
      "EM Step: 4 Epoch: 10, Classification Loss: 0.5009129382669926\n",
      "************************************************************\n",
      "EM Step: 5 Epoch: 1, Focus Loss: 1.1505084067583085\n",
      "EM Step: 5 Epoch: 1, Classification Loss: 0.3204615607857704\n",
      "************************************************************\n",
      "EM Step: 5 Epoch: 2, Focus Loss: 1.1369503259658813\n",
      "EM Step: 5 Epoch: 2, Classification Loss: 0.30593394599854945\n",
      "************************************************************\n",
      "EM Step: 5 Epoch: 3, Focus Loss: 1.134447020292282\n",
      "EM Step: 5 Epoch: 3, Classification Loss: 0.30399953909218314\n",
      "************************************************************\n",
      "EM Step: 5 Epoch: 4, Focus Loss: 1.1417702376842498\n",
      "EM Step: 5 Epoch: 4, Classification Loss: 0.3073642924427986\n",
      "************************************************************\n",
      "EM Step: 5 Epoch: 5, Focus Loss: 1.1401077687740326\n",
      "EM Step: 5 Epoch: 5, Classification Loss: 0.31725822687149047\n",
      "************************************************************\n",
      "EM Step: 5 Epoch: 6, Focus Loss: 1.1258694380521774\n",
      "EM Step: 5 Epoch: 6, Classification Loss: 0.3465144857764244\n",
      "************************************************************\n",
      "EM Step: 5 Epoch: 7, Focus Loss: 1.125253438949585\n",
      "EM Step: 5 Epoch: 7, Classification Loss: 0.3512603290379047\n",
      "************************************************************\n",
      "EM Step: 5 Epoch: 8, Focus Loss: 1.1390503704547883\n",
      "EM Step: 5 Epoch: 8, Classification Loss: 0.38106933832168577\n",
      "************************************************************\n",
      "EM Step: 5 Epoch: 9, Focus Loss: 1.1337016701698304\n",
      "EM Step: 5 Epoch: 9, Classification Loss: 0.37261138409376143\n",
      "************************************************************\n",
      "EM Step: 5 Epoch: 10, Focus Loss: 1.127364245057106\n",
      "EM Step: 5 Epoch: 10, Classification Loss: 0.3410362660884857\n",
      "************************************************************\n",
      "EM Step: 6 Epoch: 1, Focus Loss: 0.921114693582058\n",
      "EM Step: 6 Epoch: 1, Classification Loss: 0.22211902365088462\n",
      "************************************************************\n",
      "EM Step: 6 Epoch: 2, Focus Loss: 0.9172872185707093\n",
      "EM Step: 6 Epoch: 2, Classification Loss: 0.23937729522585868\n",
      "************************************************************\n",
      "EM Step: 6 Epoch: 3, Focus Loss: 0.9229457512497902\n",
      "EM Step: 6 Epoch: 3, Classification Loss: 0.25486642457544806\n",
      "************************************************************\n",
      "EM Step: 6 Epoch: 4, Focus Loss: 0.9238314300775528\n",
      "EM Step: 6 Epoch: 4, Classification Loss: 0.2266638409346342\n",
      "************************************************************\n",
      "EM Step: 6 Epoch: 5, Focus Loss: 0.919113664329052\n",
      "EM Step: 6 Epoch: 5, Classification Loss: 0.22765111401677132\n",
      "************************************************************\n",
      "EM Step: 6 Epoch: 6, Focus Loss: 0.9064501360058784\n",
      "EM Step: 6 Epoch: 6, Classification Loss: 0.23378875404596328\n",
      "************************************************************\n",
      "EM Step: 6 Epoch: 7, Focus Loss: 0.9046508148312569\n",
      "EM Step: 6 Epoch: 7, Classification Loss: 0.22419990822672844\n",
      "************************************************************\n",
      "EM Step: 6 Epoch: 8, Focus Loss: 0.9003357946872711\n",
      "EM Step: 6 Epoch: 8, Classification Loss: 0.23266190625727176\n",
      "************************************************************\n",
      "EM Step: 6 Epoch: 9, Focus Loss: 0.8978716060519218\n",
      "EM Step: 6 Epoch: 9, Classification Loss: 0.2142881065607071\n",
      "************************************************************\n",
      "EM Step: 6 Epoch: 10, Focus Loss: 0.894078603386879\n",
      "EM Step: 6 Epoch: 10, Classification Loss: 0.18717301599681377\n",
      "************************************************************\n",
      "EM Step: 7 Epoch: 1, Focus Loss: 0.7710990518331527\n",
      "EM Step: 7 Epoch: 1, Classification Loss: 0.09530127495527267\n",
      "************************************************************\n",
      "EM Step: 7 Epoch: 2, Focus Loss: 0.7708725422620774\n",
      "EM Step: 7 Epoch: 2, Classification Loss: 0.09137407764792442\n",
      "************************************************************\n",
      "EM Step: 7 Epoch: 3, Focus Loss: 0.7755950585007667\n",
      "EM Step: 7 Epoch: 3, Classification Loss: 0.0881879759952426\n",
      "************************************************************\n",
      "EM Step: 7 Epoch: 4, Focus Loss: 0.7735282152891159\n",
      "EM Step: 7 Epoch: 4, Classification Loss: 0.08948039207607508\n",
      "************************************************************\n",
      "EM Step: 7 Epoch: 5, Focus Loss: 0.770647968351841\n",
      "EM Step: 7 Epoch: 5, Classification Loss: 0.09478949196636677\n",
      "************************************************************\n",
      "EM Step: 7 Epoch: 6, Focus Loss: 0.7672078669071197\n",
      "EM Step: 7 Epoch: 6, Classification Loss: 0.09765448197722434\n",
      "************************************************************\n",
      "EM Step: 7 Epoch: 7, Focus Loss: 0.7707598328590393\n",
      "EM Step: 7 Epoch: 7, Classification Loss: 0.09787366893142461\n",
      "************************************************************\n",
      "EM Step: 7 Epoch: 8, Focus Loss: 0.7764047011733055\n",
      "EM Step: 7 Epoch: 8, Classification Loss: 0.09951711930334568\n",
      "************************************************************\n",
      "EM Step: 7 Epoch: 9, Focus Loss: 0.7787416040897369\n",
      "EM Step: 7 Epoch: 9, Classification Loss: 0.09568559043109418\n",
      "************************************************************\n",
      "EM Step: 7 Epoch: 10, Focus Loss: 0.7734148293733597\n",
      "EM Step: 7 Epoch: 10, Classification Loss: 0.09236540533602237\n",
      "************************************************************\n",
      "EM Step: 8 Epoch: 1, Focus Loss: 0.6490527257323265\n",
      "EM Step: 8 Epoch: 1, Classification Loss: 0.04488623114302755\n",
      "************************************************************\n",
      "EM Step: 8 Epoch: 2, Focus Loss: 0.6496635749936104\n",
      "EM Step: 8 Epoch: 2, Classification Loss: 0.04703392619267106\n",
      "************************************************************\n",
      "EM Step: 8 Epoch: 3, Focus Loss: 0.6487726181745529\n",
      "EM Step: 8 Epoch: 3, Classification Loss: 0.05152415386401117\n",
      "************************************************************\n",
      "EM Step: 8 Epoch: 4, Focus Loss: 0.651762193441391\n",
      "EM Step: 8 Epoch: 4, Classification Loss: 0.05029129581525922\n",
      "************************************************************\n",
      "EM Step: 8 Epoch: 5, Focus Loss: 0.6519454762339592\n",
      "EM Step: 8 Epoch: 5, Classification Loss: 0.05115475319325924\n",
      "************************************************************\n",
      "EM Step: 8 Epoch: 6, Focus Loss: 0.6533057793974877\n",
      "EM Step: 8 Epoch: 6, Classification Loss: 0.05138659961521626\n",
      "************************************************************\n",
      "EM Step: 8 Epoch: 7, Focus Loss: 0.6531216993927955\n",
      "EM Step: 8 Epoch: 7, Classification Loss: 0.052384997811168434\n",
      "************************************************************\n",
      "EM Step: 8 Epoch: 8, Focus Loss: 0.6490349680185318\n",
      "EM Step: 8 Epoch: 8, Classification Loss: 0.06002911208197474\n",
      "************************************************************\n",
      "EM Step: 8 Epoch: 9, Focus Loss: 0.6473662570118904\n",
      "EM Step: 8 Epoch: 9, Classification Loss: 0.07302899518981576\n",
      "************************************************************\n",
      "EM Step: 8 Epoch: 10, Focus Loss: 0.6461029201745987\n",
      "EM Step: 8 Epoch: 10, Classification Loss: 0.07728695441037417\n",
      "************************************************************\n",
      "EM Step: 9 Epoch: 1, Focus Loss: 0.5897691205143929\n",
      "EM Step: 9 Epoch: 1, Classification Loss: 0.04328422285616398\n",
      "************************************************************\n",
      "EM Step: 9 Epoch: 2, Focus Loss: 0.5853873580694199\n",
      "EM Step: 9 Epoch: 2, Classification Loss: 0.04683890379965305\n",
      "************************************************************\n",
      "EM Step: 9 Epoch: 3, Focus Loss: 0.5867963597178459\n",
      "EM Step: 9 Epoch: 3, Classification Loss: 0.04825644390657544\n",
      "************************************************************\n",
      "EM Step: 9 Epoch: 4, Focus Loss: 0.5870665222406387\n",
      "EM Step: 9 Epoch: 4, Classification Loss: 0.051270753238350154\n",
      "************************************************************\n",
      "EM Step: 9 Epoch: 5, Focus Loss: 0.587488804757595\n",
      "EM Step: 9 Epoch: 5, Classification Loss: 0.050672985054552554\n",
      "************************************************************\n",
      "EM Step: 9 Epoch: 6, Focus Loss: 0.5863467827439308\n",
      "EM Step: 9 Epoch: 6, Classification Loss: 0.04692809227854013\n",
      "************************************************************\n",
      "EM Step: 9 Epoch: 7, Focus Loss: 0.5873329401016235\n",
      "EM Step: 9 Epoch: 7, Classification Loss: 0.04532469660043716\n",
      "************************************************************\n",
      "EM Step: 9 Epoch: 8, Focus Loss: 0.5840677946805954\n",
      "EM Step: 9 Epoch: 8, Classification Loss: 0.042987794056534766\n",
      "************************************************************\n",
      "EM Step: 9 Epoch: 9, Focus Loss: 0.584042102098465\n",
      "EM Step: 9 Epoch: 9, Classification Loss: 0.038213238632306454\n",
      "************************************************************\n",
      "EM Step: 9 Epoch: 10, Focus Loss: 0.5837226122617721\n",
      "EM Step: 9 Epoch: 10, Classification Loss: 0.0359451356343925\n",
      "************************************************************\n",
      "EM Step: 10 Epoch: 1, Focus Loss: 0.5319161064922809\n",
      "EM Step: 10 Epoch: 1, Classification Loss: 0.014476367621682584\n",
      "************************************************************\n",
      "EM Step: 10 Epoch: 2, Focus Loss: 0.5339518323540687\n",
      "EM Step: 10 Epoch: 2, Classification Loss: 0.012090473098214715\n",
      "************************************************************\n",
      "EM Step: 10 Epoch: 3, Focus Loss: 0.5397042095661163\n",
      "EM Step: 10 Epoch: 3, Classification Loss: 0.010456619516480714\n",
      "************************************************************\n",
      "EM Step: 10 Epoch: 4, Focus Loss: 0.5426146283745765\n",
      "EM Step: 10 Epoch: 4, Classification Loss: 0.009396071510855109\n",
      "************************************************************\n",
      "EM Step: 10 Epoch: 5, Focus Loss: 0.5474903918802738\n",
      "EM Step: 10 Epoch: 5, Classification Loss: 0.00889278429094702\n",
      "************************************************************\n",
      "EM Step: 10 Epoch: 6, Focus Loss: 0.5452304936945438\n",
      "EM Step: 10 Epoch: 6, Classification Loss: 0.00841348007088527\n",
      "************************************************************\n",
      "EM Step: 10 Epoch: 7, Focus Loss: 0.5430119343101978\n",
      "EM Step: 10 Epoch: 7, Classification Loss: 0.008222561492584646\n",
      "************************************************************\n",
      "EM Step: 10 Epoch: 8, Focus Loss: 0.5407157756388188\n",
      "EM Step: 10 Epoch: 8, Classification Loss: 0.00790132163092494\n",
      "************************************************************\n",
      "EM Step: 10 Epoch: 9, Focus Loss: 0.5377098895609379\n",
      "EM Step: 10 Epoch: 9, Classification Loss: 0.007933798641897739\n",
      "************************************************************\n",
      "EM Step: 10 Epoch: 10, Focus Loss: 0.535434590280056\n",
      "EM Step: 10 Epoch: 10, Classification Loss: 0.007741665339563042\n",
      "************************************************************\n",
      "EM Step: 11 Epoch: 1, Focus Loss: 0.4634416885674\n",
      "EM Step: 11 Epoch: 1, Classification Loss: 0.0042772451823111625\n",
      "************************************************************\n",
      "EM Step: 11 Epoch: 2, Focus Loss: 0.46481355354189874\n",
      "EM Step: 11 Epoch: 2, Classification Loss: 0.004105934948893264\n",
      "************************************************************\n",
      "EM Step: 11 Epoch: 3, Focus Loss: 0.46815849393606185\n",
      "EM Step: 11 Epoch: 3, Classification Loss: 0.004498835827689618\n",
      "************************************************************\n",
      "EM Step: 11 Epoch: 4, Focus Loss: 0.472580074518919\n",
      "EM Step: 11 Epoch: 4, Classification Loss: 0.003807108974433504\n",
      "************************************************************\n",
      "EM Step: 11 Epoch: 5, Focus Loss: 0.4733485169708729\n",
      "EM Step: 11 Epoch: 5, Classification Loss: 0.0034010415663942696\n",
      "************************************************************\n",
      "EM Step: 11 Epoch: 6, Focus Loss: 0.4752079524099827\n",
      "EM Step: 11 Epoch: 6, Classification Loss: 0.003411196731030941\n",
      "************************************************************\n",
      "EM Step: 11 Epoch: 7, Focus Loss: 0.4770039662718773\n",
      "EM Step: 11 Epoch: 7, Classification Loss: 0.0034400818549329415\n",
      "************************************************************\n",
      "EM Step: 11 Epoch: 8, Focus Loss: 0.4796279765665531\n",
      "EM Step: 11 Epoch: 8, Classification Loss: 0.0032499915250809863\n",
      "************************************************************\n",
      "EM Step: 11 Epoch: 9, Focus Loss: 0.48257155269384383\n",
      "EM Step: 11 Epoch: 9, Classification Loss: 0.003135279429261573\n",
      "************************************************************\n",
      "EM Step: 11 Epoch: 10, Focus Loss: 0.4803991116583347\n",
      "EM Step: 11 Epoch: 10, Classification Loss: 0.00321455460798461\n",
      "************************************************************\n",
      "EM Step: 12 Epoch: 1, Focus Loss: 0.44905946776270866\n",
      "EM Step: 12 Epoch: 1, Classification Loss: 0.0024273631192045287\n",
      "************************************************************\n",
      "EM Step: 12 Epoch: 2, Focus Loss: 0.4485232442617416\n",
      "EM Step: 12 Epoch: 2, Classification Loss: 0.002586491097463295\n",
      "************************************************************\n",
      "EM Step: 12 Epoch: 3, Focus Loss: 0.4492406964302063\n",
      "EM Step: 12 Epoch: 3, Classification Loss: 0.002412483948864974\n",
      "************************************************************\n",
      "EM Step: 12 Epoch: 4, Focus Loss: 0.4505918577313423\n",
      "EM Step: 12 Epoch: 4, Classification Loss: 0.0024693806190043688\n",
      "************************************************************\n",
      "EM Step: 12 Epoch: 5, Focus Loss: 0.4525248117744923\n",
      "EM Step: 12 Epoch: 5, Classification Loss: 0.00227708172169514\n",
      "************************************************************\n",
      "EM Step: 12 Epoch: 6, Focus Loss: 0.45375716835260393\n",
      "EM Step: 12 Epoch: 6, Classification Loss: 0.0022333614324452356\n",
      "************************************************************\n",
      "EM Step: 12 Epoch: 7, Focus Loss: 0.4552288487553596\n",
      "EM Step: 12 Epoch: 7, Classification Loss: 0.0020566165359923614\n",
      "************************************************************\n",
      "EM Step: 12 Epoch: 8, Focus Loss: 0.4572574317455292\n",
      "EM Step: 12 Epoch: 8, Classification Loss: 0.002220452972687781\n",
      "************************************************************\n",
      "EM Step: 12 Epoch: 9, Focus Loss: 0.45804613679647443\n",
      "EM Step: 12 Epoch: 9, Classification Loss: 0.0020168367933365515\n",
      "************************************************************\n",
      "EM Step: 12 Epoch: 10, Focus Loss: 0.4579346731305122\n",
      "EM Step: 12 Epoch: 10, Classification Loss: 0.002064441614493262\n",
      "************************************************************\n",
      "EM Step: 13 Epoch: 1, Focus Loss: 0.41515230387449265\n",
      "EM Step: 13 Epoch: 1, Classification Loss: 0.001403037190902978\n",
      "************************************************************\n",
      "EM Step: 13 Epoch: 2, Focus Loss: 0.4149200595915318\n",
      "EM Step: 13 Epoch: 2, Classification Loss: 0.0014539695272105745\n",
      "************************************************************\n",
      "EM Step: 13 Epoch: 3, Focus Loss: 0.41605438068509104\n",
      "EM Step: 13 Epoch: 3, Classification Loss: 0.0014122034874162637\n",
      "************************************************************\n",
      "EM Step: 13 Epoch: 4, Focus Loss: 0.41810361817479136\n",
      "EM Step: 13 Epoch: 4, Classification Loss: 0.001421403235872276\n",
      "************************************************************\n",
      "EM Step: 13 Epoch: 5, Focus Loss: 0.4212698496878147\n",
      "EM Step: 13 Epoch: 5, Classification Loss: 0.0013259865168947726\n",
      "************************************************************\n",
      "EM Step: 13 Epoch: 6, Focus Loss: 0.42263158708810805\n",
      "EM Step: 13 Epoch: 6, Classification Loss: 0.0013825742207700387\n",
      "************************************************************\n",
      "EM Step: 13 Epoch: 7, Focus Loss: 0.4237404428422451\n",
      "EM Step: 13 Epoch: 7, Classification Loss: 0.0013188517725211569\n",
      "************************************************************\n",
      "EM Step: 13 Epoch: 8, Focus Loss: 0.42338535413146017\n",
      "EM Step: 13 Epoch: 8, Classification Loss: 0.0013338100456167013\n",
      "************************************************************\n",
      "EM Step: 13 Epoch: 9, Focus Loss: 0.4248766124248505\n",
      "EM Step: 13 Epoch: 9, Classification Loss: 0.0013169386307708918\n",
      "************************************************************\n",
      "EM Step: 13 Epoch: 10, Focus Loss: 0.42316346019506457\n",
      "EM Step: 13 Epoch: 10, Classification Loss: 0.0011777601946960203\n",
      "************************************************************\n",
      "EM Step: 14 Epoch: 1, Focus Loss: 0.4020109333097935\n",
      "EM Step: 14 Epoch: 1, Classification Loss: 0.0008668021328048781\n",
      "************************************************************\n",
      "EM Step: 14 Epoch: 2, Focus Loss: 0.40198410749435426\n",
      "EM Step: 14 Epoch: 2, Classification Loss: 0.0008065043555689044\n",
      "************************************************************\n",
      "EM Step: 14 Epoch: 3, Focus Loss: 0.40257883369922637\n",
      "EM Step: 14 Epoch: 3, Classification Loss: 0.0007795836339937523\n",
      "************************************************************\n",
      "EM Step: 14 Epoch: 4, Focus Loss: 0.4033179357647896\n",
      "EM Step: 14 Epoch: 4, Classification Loss: 0.00075874584945268\n",
      "************************************************************\n",
      "EM Step: 14 Epoch: 5, Focus Loss: 0.4046477429568768\n",
      "EM Step: 14 Epoch: 5, Classification Loss: 0.0007409656092931983\n",
      "************************************************************\n",
      "EM Step: 14 Epoch: 6, Focus Loss: 0.4052462913095951\n",
      "EM Step: 14 Epoch: 6, Classification Loss: 0.0007259745259943883\n",
      "************************************************************\n",
      "EM Step: 14 Epoch: 7, Focus Loss: 0.4062995232641697\n",
      "EM Step: 14 Epoch: 7, Classification Loss: 0.0007121443704818375\n",
      "************************************************************\n",
      "EM Step: 14 Epoch: 8, Focus Loss: 0.40712708979845047\n",
      "EM Step: 14 Epoch: 8, Classification Loss: 0.0007004693405178841\n",
      "************************************************************\n",
      "EM Step: 14 Epoch: 9, Focus Loss: 0.40782302170991896\n",
      "EM Step: 14 Epoch: 9, Classification Loss: 0.0006893161946209148\n",
      "************************************************************\n",
      "EM Step: 14 Epoch: 10, Focus Loss: 0.4110474489629269\n",
      "EM Step: 14 Epoch: 10, Classification Loss: 0.000679571769433096\n",
      "************************************************************\n",
      "EM Step: 15 Epoch: 1, Focus Loss: 0.3962625600397587\n",
      "EM Step: 15 Epoch: 1, Classification Loss: 0.0005395076332206372\n",
      "************************************************************\n",
      "EM Step: 15 Epoch: 2, Focus Loss: 0.39609846025705336\n",
      "EM Step: 15 Epoch: 2, Classification Loss: 0.0005175406215130352\n",
      "************************************************************\n",
      "EM Step: 15 Epoch: 3, Focus Loss: 0.3965368166565895\n",
      "EM Step: 15 Epoch: 3, Classification Loss: 0.0005038271650846582\n",
      "************************************************************\n",
      "EM Step: 15 Epoch: 4, Focus Loss: 0.3979212962090969\n",
      "EM Step: 15 Epoch: 4, Classification Loss: 0.0004918572005408351\n",
      "************************************************************\n",
      "EM Step: 15 Epoch: 5, Focus Loss: 0.4000009410083294\n",
      "EM Step: 15 Epoch: 5, Classification Loss: 0.0004815146567125339\n",
      "************************************************************\n",
      "EM Step: 15 Epoch: 6, Focus Loss: 0.401135528832674\n",
      "EM Step: 15 Epoch: 6, Classification Loss: 0.00047222327120834964\n",
      "************************************************************\n",
      "EM Step: 15 Epoch: 7, Focus Loss: 0.402617072314024\n",
      "EM Step: 15 Epoch: 7, Classification Loss: 0.0004637421123334207\n",
      "************************************************************\n",
      "EM Step: 15 Epoch: 8, Focus Loss: 0.4034679099917412\n",
      "EM Step: 15 Epoch: 8, Classification Loss: 0.00045590570589411074\n",
      "************************************************************\n",
      "EM Step: 15 Epoch: 9, Focus Loss: 0.4029930420219898\n",
      "EM Step: 15 Epoch: 9, Classification Loss: 0.00044869046832900493\n",
      "************************************************************\n",
      "EM Step: 15 Epoch: 10, Focus Loss: 0.40130607038736343\n",
      "EM Step: 15 Epoch: 10, Classification Loss: 0.00044191560009494425\n",
      "************************************************************\n",
      "EM Step: 16 Epoch: 1, Focus Loss: 0.39380018338561057\n",
      "EM Step: 16 Epoch: 1, Classification Loss: 0.000378087152421358\n",
      "************************************************************\n",
      "EM Step: 16 Epoch: 2, Focus Loss: 0.393635817617178\n",
      "EM Step: 16 Epoch: 2, Classification Loss: 0.00036522520786093083\n",
      "************************************************************\n",
      "EM Step: 16 Epoch: 3, Focus Loss: 0.39373810961842537\n",
      "EM Step: 16 Epoch: 3, Classification Loss: 0.00035630613128887487\n",
      "************************************************************\n",
      "EM Step: 16 Epoch: 4, Focus Loss: 0.3943468421697617\n",
      "EM Step: 16 Epoch: 4, Classification Loss: 0.0003486442405119305\n",
      "************************************************************\n",
      "EM Step: 16 Epoch: 5, Focus Loss: 0.39556972235441207\n",
      "EM Step: 16 Epoch: 5, Classification Loss: 0.0003418217056605499\n",
      "************************************************************\n",
      "EM Step: 16 Epoch: 6, Focus Loss: 0.3963637053966522\n",
      "EM Step: 16 Epoch: 6, Classification Loss: 0.00033562253520358354\n",
      "************************************************************\n",
      "EM Step: 16 Epoch: 7, Focus Loss: 0.39752193689346316\n",
      "EM Step: 16 Epoch: 7, Classification Loss: 0.00032984520912577865\n",
      "************************************************************\n",
      "EM Step: 16 Epoch: 8, Focus Loss: 0.397419548779726\n",
      "EM Step: 16 Epoch: 8, Classification Loss: 0.00032450431172037497\n",
      "************************************************************\n",
      "EM Step: 16 Epoch: 9, Focus Loss: 0.39700656309723853\n",
      "EM Step: 16 Epoch: 9, Classification Loss: 0.00031953201796568463\n",
      "************************************************************\n",
      "EM Step: 16 Epoch: 10, Focus Loss: 0.39694067761301993\n",
      "EM Step: 16 Epoch: 10, Classification Loss: 0.00031474621609959284\n",
      "************************************************************\n",
      "EM Step: 17 Epoch: 1, Focus Loss: 0.3831372819840908\n",
      "EM Step: 17 Epoch: 1, Classification Loss: 0.0002535419767809799\n",
      "************************************************************\n",
      "EM Step: 17 Epoch: 2, Focus Loss: 0.38305569589138033\n",
      "EM Step: 17 Epoch: 2, Classification Loss: 0.00024805449465929995\n",
      "************************************************************\n",
      "EM Step: 17 Epoch: 3, Focus Loss: 0.3832647643983364\n",
      "EM Step: 17 Epoch: 3, Classification Loss: 0.00024349100422114134\n",
      "************************************************************\n",
      "EM Step: 17 Epoch: 4, Focus Loss: 0.3838920556008816\n",
      "EM Step: 17 Epoch: 4, Classification Loss: 0.0002392666046944214\n",
      "************************************************************\n",
      "EM Step: 17 Epoch: 5, Focus Loss: 0.3844939790666103\n",
      "EM Step: 17 Epoch: 5, Classification Loss: 0.0002354158837988507\n",
      "************************************************************\n",
      "EM Step: 17 Epoch: 6, Focus Loss: 0.3856159634888172\n",
      "EM Step: 17 Epoch: 6, Classification Loss: 0.00023169501218944788\n",
      "************************************************************\n",
      "EM Step: 17 Epoch: 7, Focus Loss: 0.3866156689822674\n",
      "EM Step: 17 Epoch: 7, Classification Loss: 0.0002282671062857844\n",
      "************************************************************\n",
      "EM Step: 17 Epoch: 8, Focus Loss: 0.3874540992081165\n",
      "EM Step: 17 Epoch: 8, Classification Loss: 0.00022501341882161797\n",
      "************************************************************\n",
      "EM Step: 17 Epoch: 9, Focus Loss: 0.387474724650383\n",
      "EM Step: 17 Epoch: 9, Classification Loss: 0.00022190135823620948\n",
      "************************************************************\n",
      "EM Step: 17 Epoch: 10, Focus Loss: 0.38830849900841713\n",
      "EM Step: 17 Epoch: 10, Classification Loss: 0.00021889419913350138\n",
      "************************************************************\n",
      "EM Step: 18 Epoch: 1, Focus Loss: 0.37883325666189194\n",
      "EM Step: 18 Epoch: 1, Classification Loss: 0.00020720047723443713\n",
      "************************************************************\n",
      "EM Step: 18 Epoch: 2, Focus Loss: 0.3787569463253021\n",
      "EM Step: 18 Epoch: 2, Classification Loss: 0.00020304628524172585\n",
      "************************************************************\n",
      "EM Step: 18 Epoch: 3, Focus Loss: 0.3789320714771748\n",
      "EM Step: 18 Epoch: 3, Classification Loss: 0.00019943199586123229\n",
      "************************************************************\n",
      "EM Step: 18 Epoch: 4, Focus Loss: 0.3797330066561699\n",
      "EM Step: 18 Epoch: 4, Classification Loss: 0.00019617919442680432\n",
      "************************************************************\n",
      "EM Step: 18 Epoch: 5, Focus Loss: 0.3810531049966812\n",
      "EM Step: 18 Epoch: 5, Classification Loss: 0.00019307847505842802\n",
      "************************************************************\n",
      "EM Step: 18 Epoch: 6, Focus Loss: 0.3821419417858124\n",
      "EM Step: 18 Epoch: 6, Classification Loss: 0.00019017669692402705\n",
      "************************************************************\n",
      "EM Step: 18 Epoch: 7, Focus Loss: 0.38359090238809584\n",
      "EM Step: 18 Epoch: 7, Classification Loss: 0.00018748290876828831\n",
      "************************************************************\n",
      "EM Step: 18 Epoch: 8, Focus Loss: 0.385404884070158\n",
      "EM Step: 18 Epoch: 8, Classification Loss: 0.00018487832130631432\n",
      "************************************************************\n",
      "EM Step: 18 Epoch: 9, Focus Loss: 0.38641980811953547\n",
      "EM Step: 18 Epoch: 9, Classification Loss: 0.00018243394497403643\n",
      "************************************************************\n",
      "EM Step: 18 Epoch: 10, Focus Loss: 0.3864250905811787\n",
      "EM Step: 18 Epoch: 10, Classification Loss: 0.00018001960961555596\n",
      "************************************************************\n",
      "EM Step: 19 Epoch: 1, Focus Loss: 0.35558815449476244\n",
      "EM Step: 19 Epoch: 1, Classification Loss: 0.00015785768227942752\n",
      "************************************************************\n",
      "EM Step: 19 Epoch: 2, Focus Loss: 0.3555594407021999\n",
      "EM Step: 19 Epoch: 2, Classification Loss: 0.0001546070841868641\n",
      "************************************************************\n",
      "EM Step: 19 Epoch: 3, Focus Loss: 0.3559052780270576\n",
      "EM Step: 19 Epoch: 3, Classification Loss: 0.00015191180573310704\n",
      "************************************************************\n",
      "EM Step: 19 Epoch: 4, Focus Loss: 0.35727031230926515\n",
      "EM Step: 19 Epoch: 4, Classification Loss: 0.00014947114723327105\n",
      "************************************************************\n",
      "EM Step: 19 Epoch: 5, Focus Loss: 0.3590423919260502\n",
      "EM Step: 19 Epoch: 5, Classification Loss: 0.00014723911917826626\n",
      "************************************************************\n",
      "EM Step: 19 Epoch: 6, Focus Loss: 0.36049824878573417\n",
      "EM Step: 19 Epoch: 6, Classification Loss: 0.0001451661131795845\n",
      "************************************************************\n",
      "EM Step: 19 Epoch: 7, Focus Loss: 0.36143375039100645\n",
      "EM Step: 19 Epoch: 7, Classification Loss: 0.00014321091784950113\n",
      "************************************************************\n",
      "EM Step: 19 Epoch: 8, Focus Loss: 0.363309258967638\n",
      "EM Step: 19 Epoch: 8, Classification Loss: 0.00014136562786006834\n",
      "************************************************************\n",
      "EM Step: 19 Epoch: 9, Focus Loss: 0.3636415548622608\n",
      "EM Step: 19 Epoch: 9, Classification Loss: 0.00013961503573227673\n",
      "************************************************************\n",
      "EM Step: 19 Epoch: 10, Focus Loss: 0.36227487325668334\n",
      "EM Step: 19 Epoch: 10, Classification Loss: 0.00013794124624837422\n",
      "************************************************************\n",
      "EM Step: 20 Epoch: 1, Focus Loss: 0.3566659212112427\n",
      "EM Step: 20 Epoch: 1, Classification Loss: 0.0001577107424964197\n",
      "************************************************************\n",
      "EM Step: 20 Epoch: 2, Focus Loss: 0.3565781839191914\n",
      "EM Step: 20 Epoch: 2, Classification Loss: 0.00015467516241187695\n",
      "************************************************************\n",
      "EM Step: 20 Epoch: 3, Focus Loss: 0.3567046202719212\n",
      "EM Step: 20 Epoch: 3, Classification Loss: 0.00015226401847030503\n",
      "************************************************************\n",
      "EM Step: 20 Epoch: 4, Focus Loss: 0.3572557710111141\n",
      "EM Step: 20 Epoch: 4, Classification Loss: 0.00014996307909314054\n",
      "************************************************************\n",
      "EM Step: 20 Epoch: 5, Focus Loss: 0.3583707593381405\n",
      "EM Step: 20 Epoch: 5, Classification Loss: 0.0001479048096371116\n",
      "************************************************************\n",
      "EM Step: 20 Epoch: 6, Focus Loss: 0.3595254525542259\n",
      "EM Step: 20 Epoch: 6, Classification Loss: 0.00014597453500755364\n",
      "************************************************************\n",
      "EM Step: 20 Epoch: 7, Focus Loss: 0.36018102094531057\n",
      "EM Step: 20 Epoch: 7, Classification Loss: 0.00014416109734156636\n",
      "************************************************************\n",
      "EM Step: 20 Epoch: 8, Focus Loss: 0.3600021906197071\n",
      "EM Step: 20 Epoch: 8, Classification Loss: 0.0001424495469109388\n",
      "************************************************************\n",
      "EM Step: 20 Epoch: 9, Focus Loss: 0.3602102905511856\n",
      "EM Step: 20 Epoch: 9, Classification Loss: 0.00014081268873269436\n",
      "************************************************************\n",
      "EM Step: 20 Epoch: 10, Focus Loss: 0.36095734164118765\n",
      "EM Step: 20 Epoch: 10, Classification Loss: 0.00013925478397140977\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(12)\n",
    "focus = Focus()\n",
    "focus = focus.to(device)\n",
    "\n",
    "\n",
    "torch.manual_seed(12)\n",
    "classification = Classification()\n",
    "classification = classification.to(device)\n",
    "\n",
    "\n",
    "Criterion = nn.CrossEntropyLoss(reduction=\"none\") #nn.BCELoss(reduction=\"none\")\n",
    "\n",
    "\n",
    "em_steps = 20\n",
    "\n",
    "\n",
    "for i in range(em_steps):\n",
    "    # calculate p_z\n",
    "    gamma_ = []\n",
    "    focus_optimizer = optim.SGD(focus.parameters(), lr=0.03,momentum=0.9)\n",
    "    classification_optimizer = optim.SGD(classification.parameters(),lr=0.03,momentum=0.9)\n",
    "    for j,data in enumerate(train_loader):\n",
    "        images,labels,foreground_index = data\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        p_z = expectation_step(focus,classification,images,labels)\n",
    "        gamma_.append(p_z)\n",
    "    for epoch in range(10):\n",
    "        focus_epoch_loss = []\n",
    "        classification_epoch_loss = []\n",
    "        for j,data in enumerate(train_loader):\n",
    "            images,labels,foreground_index = data\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            focus,classification,focus_optimizer,classification_optimizer=maximization_step(gamma_[j],\n",
    "                                                                                            focus,classification,\n",
    "                                                                                            images,labels,\n",
    "                                                                                            focus_optimizer,\n",
    "                                                                                            classification_optimizer,\n",
    "                                                                                            Criterion)\n",
    "            with torch.no_grad():\n",
    "                batch = images.size(0)\n",
    "                patches = images.size(1)\n",
    "                focus_outputs = focus(images)\n",
    "                images = images.reshape(batch*patches,3,32,32)\n",
    "                classification_outputs = classification(images) # classification returns output after sigmoid/softmax]\n",
    "                classification_outputs = classification_outputs.reshape(batch,patches,3)\n",
    "                #print(focus_outputs,classification_outputs)\n",
    "                loss_focus = calculate_loss_focus(gamma_[j],focus_outputs)\n",
    "                loss_classification = calculate_loss_classification(gamma_[j],classification_outputs,\n",
    "                                                        labels,Criterion,patches)\n",
    "                focus_epoch_loss.append(loss_focus.item())\n",
    "                classification_epoch_loss.append(loss_classification.item())\n",
    "        print(\"*\"*60)\n",
    "        print(\"EM Step: \"+str(i+1)+\" Epoch: \" + str(epoch+1)+\", Focus Loss: \"+str(np.mean(focus_epoch_loss)))\n",
    "        print(\"EM Step: \"+str(i+1)+\" Epoch: \" + str(epoch+1)+\", Classification Loss: \"+str(np.mean(classification_epoch_loss))) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8da1c1",
   "metadata": {
    "papermill": {
     "duration": 0.154917,
     "end_time": "2022-03-21T04:45:18.680986",
     "exception": false,
     "start_time": "2022-03-21T04:45:18.526069",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "02d71c48",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-21T04:45:19.007964Z",
     "iopub.status.busy": "2022-03-21T04:45:19.007273Z",
     "iopub.status.idle": "2022-03-21T04:45:19.486261Z",
     "shell.execute_reply": "2022-03-21T04:45:19.486670Z",
     "shell.execute_reply.started": "2022-03-20T09:05:57.376590Z"
    },
    "papermill": {
     "duration": 0.644966,
     "end_time": "2022-03-21T04:45:19.486847",
     "exception": false,
     "start_time": "2022-03-21T04:45:18.841881",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Focus True 66.64\n",
      "Accuracy 100.0\n"
     ]
    }
   ],
   "source": [
    "# method 1\n",
    "predicted_indexes = []\n",
    "foreground_index_list = []\n",
    "prediction_list = []\n",
    "labels_list = []\n",
    "with torch.no_grad():\n",
    "    for j,data in enumerate(train_loader):\n",
    "        images,labels,foreground_index = data\n",
    "        images = images.to(device)\n",
    "        foreground_index_list.append(foreground_index.numpy())\n",
    "        labels_list.append(labels.numpy())\n",
    "        batch = images.size(0)\n",
    "        scores = focus(images)\n",
    "        indexes = torch.argmax(F.softmax(scores,dim=1),dim=1).cpu().numpy()\n",
    "        predicted_indexes.append(indexes)\n",
    "        outputs = F.softmax(classification(images[np.arange(batch),indexes,:]),dim=1)\n",
    "        prediction = torch.argmax(outputs,dim=1)\n",
    "        prediction_list.append(prediction.cpu().numpy())\n",
    "    \n",
    "predicted_indexes = np.concatenate(predicted_indexes,axis=0)\n",
    "foreground_index_list = np.concatenate(foreground_index_list,axis=0)\n",
    "prediction_list = np.concatenate(prediction_list,axis=0)\n",
    "labels_list = np.concatenate(labels_list,axis=0)\n",
    "    \n",
    "\n",
    "print(\"Focus True\",(np.sum(predicted_indexes == foreground_index_list,axis=0).item()/\n",
    "                    len(foreground_index_list))*100)\n",
    "accuracy = (np.sum(prediction_list == labels_list,axis=0)/len(labels_list) )*100\n",
    "print(\"Accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0abafd34",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-21T04:45:19.816836Z",
     "iopub.status.busy": "2022-03-21T04:45:19.814114Z",
     "iopub.status.idle": "2022-03-21T04:45:20.416225Z",
     "shell.execute_reply": "2022-03-21T04:45:20.417373Z",
     "shell.execute_reply.started": "2022-03-20T09:06:00.879083Z"
    },
    "papermill": {
     "duration": 0.772918,
     "end_time": "2022-03-21T04:45:20.417761",
     "exception": false,
     "start_time": "2022-03-21T04:45:19.644843",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Focus True 66.64\n",
      "Accuracy 100.0\n"
     ]
    }
   ],
   "source": [
    "# method 2\n",
    "predicted_indexes = []\n",
    "foreground_index_list = []\n",
    "prediction_list = []\n",
    "labels_list = []\n",
    "with torch.no_grad():\n",
    "    for j,data in enumerate(train_loader):\n",
    "        images,labels,foreground_index = data\n",
    "        images = images.to(device)\n",
    "        batch = images.size(0)\n",
    "        patches = images.size(1)\n",
    "        foreground_index_list.append(foreground_index.numpy())\n",
    "        labels_list.append(labels.numpy())\n",
    "        batch = images.size(0)\n",
    "        focus_outputs = F.softmax(focus(images),dim=1)\n",
    "        indexes = torch.argmax(focus_outputs,dim=1).cpu().numpy()\n",
    "        predicted_indexes.append(indexes)\n",
    "        images = images.reshape(batch*patches,3,32,32)\n",
    "        classification_outputs = F.softmax(classification(images),dim=1)\n",
    "        classification_outputs = classification_outputs.reshape(batch,patches,3)\n",
    "\n",
    "        #print(classification_outputs.shape,focus_outputs.shape)\n",
    "\n",
    "        prediction = torch.argmax(torch.sum(focus_outputs[:,:,None]*classification_outputs,dim=1),dim=1)\n",
    "        prediction_list.append(prediction.cpu().numpy())\n",
    "    \n",
    "predicted_indexes = np.concatenate(predicted_indexes,axis=0)\n",
    "foreground_index_list = np.concatenate(foreground_index_list,axis=0)\n",
    "prediction_list = np.concatenate(prediction_list,axis=0)\n",
    "labels_list = np.concatenate(labels_list,axis=0)\n",
    "    \n",
    "\n",
    "print(\"Focus True\",(np.sum(predicted_indexes == foreground_index_list,axis=0).item()/\n",
    "                    len(foreground_index_list))*100)\n",
    "accuracy = (np.sum(prediction_list == labels_list,axis=0)/len(labels_list) )*100\n",
    "print(\"Accuracy\", accuracy)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# method 2\n",
    "# focus_output = F.softmax(focus(data),dim=1)\n",
    "# indexes = torch.argmax(F.softmax(focus(data),dim=1),dim=1)[:,0].numpy()\n",
    "# classification_output = F.softmax(classification(data),dim=2)\n",
    "# print(\"Focus True\",(np.sum(indexes == fore_idx,axis=0).item()/len(fore_idx))*100)\n",
    "# prediction = torch.argmax(torch.sum(focus_output*classification_output,dim=1),dim=1)\n",
    "# accuracy = (torch.sum(prediction == labels,dim=0)/len(labels) )*100\n",
    "# print(\"Accuracy\", accuracy.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "99fbe38c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-21T04:45:20.736988Z",
     "iopub.status.busy": "2022-03-21T04:45:20.736244Z",
     "iopub.status.idle": "2022-03-21T04:45:20.739019Z",
     "shell.execute_reply": "2022-03-21T04:45:20.739582Z",
     "shell.execute_reply.started": "2022-03-20T09:06:07.629136Z"
    },
    "papermill": {
     "duration": 0.164292,
     "end_time": "2022-03-21T04:45:20.739769",
     "exception": false,
     "start_time": "2022-03-21T04:45:20.575477",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.34318849444389343, 0.3125825822353363, 0.4032024145126343, 0.30373021960258484, 0.3537137508392334, 0.30752986669540405, 0.35030555725097656, 0.3919786810874939, 0.32504159212112427, 0.3308740258216858, 0.34273236989974976, 0.3474247455596924, 0.3343151807785034, 0.36108681559562683, 0.35865113139152527, 0.373769611120224, 0.3124193847179413, 0.43670853972435, 0.35805782675743103, 0.28654205799102783, 0.3804490566253662, 0.3710477948188782, 0.3579195737838745, 0.3451807498931885, 0.3848350942134857, 0.4021489918231964, 0.3522492051124573, 0.3246283531188965, 0.36605700850486755, 0.3725142478942871, 0.35845381021499634, 0.34693443775177, 0.42243844270706177, 0.37030115723609924, 0.4112245738506317, 0.34831202030181885, 0.4026280641555786, 0.3990907073020935, 0.37447139620780945, 0.4135541319847107]\n"
     ]
    }
   ],
   "source": [
    "print(focus_epoch_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff117408",
   "metadata": {
    "papermill": {
     "duration": 0.155988,
     "end_time": "2022-03-21T04:45:21.054321",
     "exception": false,
     "start_time": "2022-03-21T04:45:20.898333",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "> **Test data Evaluation**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ac3adc0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-21T04:45:21.376847Z",
     "iopub.status.busy": "2022-03-21T04:45:21.376291Z",
     "iopub.status.idle": "2022-03-21T04:45:21.862373Z",
     "shell.execute_reply": "2022-03-21T04:45:21.861910Z",
     "shell.execute_reply.started": "2022-03-20T09:06:13.618167Z"
    },
    "papermill": {
     "duration": 0.652601,
     "end_time": "2022-03-21T04:45:21.862520",
     "exception": false,
     "start_time": "2022-03-21T04:45:21.209919",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Focus True 60.209999999999994\n",
      "Accuracy 71.64\n"
     ]
    }
   ],
   "source": [
    "# method 1\n",
    "predicted_indexes = []\n",
    "foreground_index_list = []\n",
    "prediction_list = []\n",
    "labels_list = []\n",
    "with torch.no_grad():\n",
    "    for j,data in enumerate(test_loader):\n",
    "        images,labels,foreground_index = data\n",
    "        images = images.to(device)\n",
    "        foreground_index_list.append(foreground_index.numpy())\n",
    "        labels_list.append(labels.numpy())\n",
    "        batch = images.size(0)\n",
    "        scores = focus(images)\n",
    "        indexes = torch.argmax(F.softmax(scores,dim=1),dim=1).cpu().numpy()\n",
    "        predicted_indexes.append(indexes)\n",
    "        outputs = F.softmax(classification(images[np.arange(batch),indexes,:]),dim=1)\n",
    "        prediction = torch.argmax(outputs,dim=1)\n",
    "        prediction_list.append(prediction.cpu().numpy())\n",
    "    \n",
    "predicted_indexes = np.concatenate(predicted_indexes,axis=0)\n",
    "foreground_index_list = np.concatenate(foreground_index_list,axis=0)\n",
    "prediction_list = np.concatenate(prediction_list,axis=0)\n",
    "labels_list = np.concatenate(labels_list,axis=0)\n",
    "    \n",
    "\n",
    "print(\"Focus True\",(np.sum(predicted_indexes == foreground_index_list,axis=0).item()/\n",
    "                    len(foreground_index_list))*100)\n",
    "accuracy = (np.sum(prediction_list == labels_list,axis=0)/len(labels_list) )*100\n",
    "print(\"Accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fe9d8b44",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-21T04:45:22.189442Z",
     "iopub.status.busy": "2022-03-21T04:45:22.186258Z",
     "iopub.status.idle": "2022-03-21T04:45:22.713143Z",
     "shell.execute_reply": "2022-03-21T04:45:22.713949Z",
     "shell.execute_reply.started": "2022-03-20T09:06:18.105584Z"
    },
    "papermill": {
     "duration": 0.693146,
     "end_time": "2022-03-21T04:45:22.714149",
     "exception": false,
     "start_time": "2022-03-21T04:45:22.021003",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Focus True 60.209999999999994\n",
      "Accuracy 72.54\n"
     ]
    }
   ],
   "source": [
    "# method 2\n",
    "predicted_indexes = []\n",
    "foreground_index_list = []\n",
    "prediction_list = []\n",
    "labels_list = []\n",
    "with torch.no_grad():\n",
    "    for j,data in enumerate(test_loader):\n",
    "        images,labels,foreground_index = data\n",
    "        images = images.to(device)\n",
    "        batch = images.size(0)\n",
    "        patches = images.size(1)\n",
    "        foreground_index_list.append(foreground_index.numpy())\n",
    "        labels_list.append(labels.numpy())\n",
    "        batch = images.size(0)\n",
    "        focus_outputs = F.softmax(focus(images),dim=1)\n",
    "        indexes = torch.argmax(focus_outputs,dim=1).cpu().numpy()\n",
    "        predicted_indexes.append(indexes)\n",
    "        images = images.reshape(batch*patches,3,32,32)\n",
    "        classification_outputs = F.softmax(classification(images),dim=1)\n",
    "        classification_outputs = classification_outputs.reshape(batch,patches,3)\n",
    "\n",
    "        #print(classification_outputs.shape,focus_outputs.shape)\n",
    "\n",
    "        prediction = torch.argmax(torch.sum(focus_outputs[:,:,None]*classification_outputs,dim=1),dim=1)\n",
    "        prediction_list.append(prediction.cpu().numpy())\n",
    "    \n",
    "predicted_indexes = np.concatenate(predicted_indexes,axis=0)\n",
    "foreground_index_list = np.concatenate(foreground_index_list,axis=0)\n",
    "prediction_list = np.concatenate(prediction_list,axis=0)\n",
    "labels_list = np.concatenate(labels_list,axis=0)\n",
    "    \n",
    "\n",
    "print(\"Focus True\",(np.sum(predicted_indexes == foreground_index_list,axis=0).item()/\n",
    "                    len(foreground_index_list))*100)\n",
    "accuracy = (np.sum(prediction_list == labels_list,axis=0)/len(labels_list) )*100\n",
    "print(\"Accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660655b2",
   "metadata": {
    "papermill": {
     "duration": 0.158367,
     "end_time": "2022-03-21T04:45:23.031272",
     "exception": false,
     "start_time": "2022-03-21T04:45:22.872905",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e266b448",
   "metadata": {
    "papermill": {
     "duration": 0.157085,
     "end_time": "2022-03-21T04:45:23.345880",
     "exception": false,
     "start_time": "2022-03-21T04:45:23.188795",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00625163",
   "metadata": {
    "papermill": {
     "duration": 0.158448,
     "end_time": "2022-03-21T04:45:23.662402",
     "exception": false,
     "start_time": "2022-03-21T04:45:23.503954",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 467.214024,
   "end_time": "2022-03-21T04:45:25.233266",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-03-21T04:37:38.019242",
   "version": "2.3.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "2aa2183a2e784bfe9e89ef34b4585ec1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "40e6676f7eb047658675888f2f451d2a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_f172b84c1fc145bebd2900ea07713319",
       "placeholder": "​",
       "style": "IPY_MODEL_2aa2183a2e784bfe9e89ef34b4585ec1",
       "value": ""
      }
     },
     "4d5b6ff6e04a4bb880faa4fad591a478": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_9a0d986426ce46d4a3bb849e0e450a0b",
       "max": 170498071.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_cb93dead9b2c4eb68293a594f56162ec",
       "value": 170498071.0
      }
     },
     "53cd77c73ea5464cbf41a0813c360a80": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "877104c962454b9d9ef8ac300d12380a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_40e6676f7eb047658675888f2f451d2a",
        "IPY_MODEL_4d5b6ff6e04a4bb880faa4fad591a478",
        "IPY_MODEL_f6df41fcfa7745c2bdd8c2bff69f4e87"
       ],
       "layout": "IPY_MODEL_8dabf2d7556e4ccc88f91eb30df17b6c"
      }
     },
     "8dabf2d7556e4ccc88f91eb30df17b6c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9a0d986426ce46d4a3bb849e0e450a0b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "cb93dead9b2c4eb68293a594f56162ec": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "dd3f01b93b21429aa2ea374050afabe7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "f172b84c1fc145bebd2900ea07713319": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f6df41fcfa7745c2bdd8c2bff69f4e87": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_53cd77c73ea5464cbf41a0813c360a80",
       "placeholder": "​",
       "style": "IPY_MODEL_dd3f01b93b21429aa2ea374050afabe7",
       "value": " 170499072/? [00:05&lt;00:00, 33235261.42it/s]"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

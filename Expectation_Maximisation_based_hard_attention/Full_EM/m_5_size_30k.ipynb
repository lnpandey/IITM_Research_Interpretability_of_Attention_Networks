{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0dec017b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-21T04:57:57.476053Z",
     "iopub.status.busy": "2022-03-21T04:57:57.474576Z",
     "iopub.status.idle": "2022-03-21T04:57:59.236135Z",
     "shell.execute_reply": "2022-03-21T04:57:59.236589Z",
     "shell.execute_reply.started": "2022-03-20T08:28:27.859219Z"
    },
    "papermill": {
     "duration": 1.787375,
     "end_time": "2022-03-21T04:57:59.236842",
     "exception": false,
     "start_time": "2022-03-21T04:57:57.449467",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "from tqdm import tqdm\n",
    "%matplotlib inline\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm as tqdm\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05633c9d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-21T04:57:59.293771Z",
     "iopub.status.busy": "2022-03-21T04:57:59.293200Z",
     "iopub.status.idle": "2022-03-21T04:57:59.296813Z",
     "shell.execute_reply": "2022-03-21T04:57:59.296390Z",
     "shell.execute_reply.started": "2022-03-20T08:28:37.120300Z"
    },
    "papermill": {
     "duration": 0.037431,
     "end_time": "2022-03-21T04:57:59.296930",
     "exception": false,
     "start_time": "2022-03-21T04:57:59.259499",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Focus(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Focus, self).__init__()\n",
    "\n",
    "    self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=0, bias=False)\n",
    "    self.pool = nn.MaxPool2d(2, 2)\n",
    "    self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=0, bias=False)\n",
    "    self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=0, bias=False)\n",
    "    self.fc1 = nn.Linear(1024, 512, bias=False)\n",
    "    self.fc2 = nn.Linear(512, 64, bias=False)\n",
    "    self.fc3 = nn.Linear(64, 10, bias=False)\n",
    "    self.fc4 = nn.Linear(10,1, bias=False)\n",
    "\n",
    "    torch.nn.init.xavier_normal_(self.conv1.weight)\n",
    "    torch.nn.init.xavier_normal_(self.conv2.weight)\n",
    "    torch.nn.init.xavier_normal_(self.conv3.weight)\n",
    "    torch.nn.init.xavier_normal_(self.fc1.weight)\n",
    "    torch.nn.init.xavier_normal_(self.fc2.weight)\n",
    "    torch.nn.init.xavier_normal_(self.fc3.weight)\n",
    "    torch.nn.init.xavier_normal_(self.fc4.weight)\n",
    "\n",
    "  def forward(self,z):  #y is avg image #z batch of list of 9 images\n",
    "    batch = z.size(0)\n",
    "    patches = z.size(1)\n",
    "    z = z.view(batch*patches,3,32,32)\n",
    "    alpha =  self.helper(z)\n",
    "    alpha = alpha.view(batch,patches,-1)\n",
    "    return alpha[:,:,0] # scores \n",
    "    \n",
    "  def helper(self, x):\n",
    "    x = self.pool(F.relu(self.conv1(x)))\n",
    "    x = self.pool(F.relu(self.conv2(x)))\n",
    "    # print(x.shape)\n",
    "    x = (F.relu(self.conv3(x)))\n",
    "    x =  x.view(x.size(0), -1)\n",
    "    # print(x.shape)\n",
    "    x = F.relu(self.fc1(x))\n",
    "    x = F.relu(self.fc2(x))\n",
    "    x = F.relu(self.fc3(x))\n",
    "    x = self.fc4(x)\n",
    "    return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d1a5553",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-21T04:57:59.348113Z",
     "iopub.status.busy": "2022-03-21T04:57:59.347400Z",
     "iopub.status.idle": "2022-03-21T04:57:59.349445Z",
     "shell.execute_reply": "2022-03-21T04:57:59.349804Z",
     "shell.execute_reply.started": "2022-03-20T08:28:40.017216Z"
    },
    "papermill": {
     "duration": 0.032964,
     "end_time": "2022-03-21T04:57:59.349927",
     "exception": false,
     "start_time": "2022-03-21T04:57:59.316963",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Classification(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Classification, self).__init__()\n",
    "    self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "    self.pool = nn.MaxPool2d(2, 2)\n",
    "    self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "    self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "    self.fc2 = nn.Linear(120, 84)\n",
    "    self.fc3 = nn.Linear(84, 10)\n",
    "    self.fc4 = nn.Linear(10,3)\n",
    "\n",
    "    torch.nn.init.xavier_normal_(self.conv1.weight)\n",
    "    torch.nn.init.zeros_(self.conv1.bias)\n",
    "    torch.nn.init.xavier_normal_(self.conv2.weight)\n",
    "    torch.nn.init.zeros_(self.conv2.bias)\n",
    "    torch.nn.init.xavier_normal_(self.fc1.weight)\n",
    "    torch.nn.init.zeros_(self.fc1.bias)\n",
    "    torch.nn.init.xavier_normal_(self.fc2.weight)\n",
    "    torch.nn.init.zeros_(self.fc2.bias)\n",
    "    torch.nn.init.xavier_normal_(self.fc3.weight)\n",
    "    torch.nn.init.zeros_(self.fc3.bias)\n",
    "    torch.nn.init.xavier_normal_(self.fc4.weight)\n",
    "    torch.nn.init.zeros_(self.fc4.bias)\n",
    "\n",
    "  def forward(self,z): \n",
    "    y1 = self.pool(F.relu(self.conv1(z)))\n",
    "    y1 = self.pool(F.relu(self.conv2(y1)))\n",
    "    y1 = y1.view(-1, 16 * 5 * 5)\n",
    "\n",
    "    y1 = F.relu(self.fc1(y1))\n",
    "    y1 = F.relu(self.fc2(y1))\n",
    "    y1 = F.relu(self.fc3(y1))\n",
    "    y1 = self.fc4(y1)\n",
    "    return y1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca57bd26",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-21T04:57:59.394864Z",
     "iopub.status.busy": "2022-03-21T04:57:59.394101Z",
     "iopub.status.idle": "2022-03-21T04:57:59.395994Z",
     "shell.execute_reply": "2022-03-21T04:57:59.396385Z",
     "shell.execute_reply.started": "2022-03-20T08:36:06.558404Z"
    },
    "papermill": {
     "duration": 0.02668,
     "end_time": "2022-03-21T04:57:59.396508",
     "exception": false,
     "start_time": "2022-03-21T04:57:59.369828",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_loss_focus(gamma,focus_output):\n",
    "    \n",
    "    m = torch.nn.LogSoftmax(dim=1)\n",
    "    log_outputs = m(focus_output)    \n",
    "    loss_ = gamma*log_outputs\n",
    "    loss_ = torch.sum(loss_,dim=1)\n",
    "    loss_ = -torch.mean(loss_,dim=0)  \n",
    "    return loss_ \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57d78d77",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-21T04:57:59.441750Z",
     "iopub.status.busy": "2022-03-21T04:57:59.438992Z",
     "iopub.status.idle": "2022-03-21T04:57:59.444719Z",
     "shell.execute_reply": "2022-03-21T04:57:59.444304Z",
     "shell.execute_reply.started": "2022-03-20T08:28:42.037522Z"
    },
    "papermill": {
     "duration": 0.028483,
     "end_time": "2022-03-21T04:57:59.444818",
     "exception": false,
     "start_time": "2022-03-21T04:57:59.416335",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_loss_classification(gamma,classification_output,label,criterion,n_patches):\n",
    "    #print(classification_output.shape)\n",
    "    \n",
    "    batch = label.size(0)\n",
    "    classes = classification_output.size(2)\n",
    "    label = label.repeat_interleave(n_patches)\n",
    "    classification_output = classification_output.reshape((batch*n_patches,classes))\n",
    "    loss_ = criterion(classification_output,label)\n",
    "    \n",
    "    loss_ = loss_.reshape((batch,n_patches))\n",
    "    \n",
    "    loss_ = gamma*loss_\n",
    "    loss_ = torch.sum(loss_,dim=1)\n",
    "    loss_ = torch.mean(loss_,dim=0)\n",
    "    \n",
    "    return loss_\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11819deb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-21T04:57:59.492941Z",
     "iopub.status.busy": "2022-03-21T04:57:59.491435Z",
     "iopub.status.idle": "2022-03-21T04:57:59.493666Z",
     "shell.execute_reply": "2022-03-21T04:57:59.494130Z",
     "shell.execute_reply.started": "2022-03-20T08:28:42.857391Z"
    },
    "papermill": {
     "duration": 0.029528,
     "end_time": "2022-03-21T04:57:59.494298",
     "exception": false,
     "start_time": "2022-03-21T04:57:59.464770",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def expectation_step(fc,cl,data,labels):\n",
    "    batch= data.size(0)\n",
    "    patches = data.size(1)\n",
    "    with torch.no_grad():\n",
    "        outputs_f = torch.softmax(fc(data),dim=1)\n",
    "        data = data.reshape(batch*patches,3,32,32)\n",
    "        outputs_g = cl(data)\n",
    "        \n",
    "    outputs_g = torch.softmax(outputs_g.reshape(batch,patches,3),dim=2)\n",
    "    \n",
    "        \n",
    "        \n",
    "    #print(\"Focus output\",outputs_f.shape,torch.sum(outputs_f,dim=1))\n",
    "    #print(\"Classification output\",outputs_g.shape,torch.sum(outputs_g,dim=1),torch.sum(outputs_g,dim=2))\n",
    "    outputs_g = outputs_g[np.arange(batch),:,labels]\n",
    "    p_x_y_z = outputs_f*outputs_g   #(1-outputs_g)    \n",
    "    \n",
    "    \n",
    "    normalized_p = p_x_y_z/torch.sum(p_x_y_z,dim=1,keepdims=True)\n",
    "#     print(outputs_f[0],outputs_g[0],normalized_p[0])\n",
    "    return normalized_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5a2f56c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-21T04:57:59.542465Z",
     "iopub.status.busy": "2022-03-21T04:57:59.541794Z",
     "iopub.status.idle": "2022-03-21T04:57:59.545336Z",
     "shell.execute_reply": "2022-03-21T04:57:59.544830Z",
     "shell.execute_reply.started": "2022-03-20T08:28:43.596869Z"
    },
    "papermill": {
     "duration": 0.029378,
     "end_time": "2022-03-21T04:57:59.545460",
     "exception": false,
     "start_time": "2022-03-21T04:57:59.516082",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def maximization_step(p_z,focus,classification,data,labels,focus_optimizer,classification_optimizer,Criterion):    \n",
    "    batch = data.size(0)\n",
    "    patches = data.size(1)\n",
    "    focus_optimizer.zero_grad()\n",
    "    classification_optimizer.zero_grad()\n",
    "    \n",
    "    focus_outputs = focus(data)\n",
    "    data = data.reshape(batch*patches,3,32,32)\n",
    "    classification_outputs = classification(data) \n",
    "    classification_outputs = classification_outputs.reshape(batch,patches,3)\n",
    "    \n",
    "    \n",
    "    #print(focus_outputs,classification_outputs)\n",
    "    \n",
    "    loss_focus = calculate_loss_focus(p_z,focus_outputs)\n",
    "    loss_classification = calculate_loss_classification(p_z,classification_outputs,\n",
    "                                                        labels,Criterion,patches)\n",
    "    \n",
    "    #print(\"Focus loss\",loss_focus.item())\n",
    "    #print(\"Classification loss\",loss_classification.item())\n",
    "    loss_focus.backward() \n",
    "    loss_classification.backward()\n",
    "    focus_optimizer.step()\n",
    "    classification_optimizer.step()\n",
    "    \n",
    "    return focus,classification,focus_optimizer,classification_optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd3452e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-21T04:57:59.591116Z",
     "iopub.status.busy": "2022-03-21T04:57:59.590576Z",
     "iopub.status.idle": "2022-03-21T04:57:59.592833Z",
     "shell.execute_reply": "2022-03-21T04:57:59.593201Z",
     "shell.execute_reply.started": "2022-03-20T08:28:44.173461Z"
    },
    "papermill": {
     "duration": 0.027896,
     "end_time": "2022-03-21T04:57:59.593344",
     "exception": false,
     "start_time": "2022-03-21T04:57:59.565448",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_mosaic_img(bg_idx,fg_idx,fg,m): \n",
    "    \"\"\"\n",
    "      bg_idx : list of indexes of background_data[] to be used as background images in mosaic\n",
    "      fg_idx : index of image to be used as foreground image from foreground data\n",
    "      fg : at what position/index foreground image has to be stored out of 0-8\n",
    "    \"\"\"\n",
    "    image_list=[]\n",
    "    j=0\n",
    "    for i in range(m):  # m value \n",
    "        if i != fg:\n",
    "            image_list.append(background_data[bg_idx[j]])\n",
    "            j+=1\n",
    "        else: \n",
    "            image_list.append(foreground_data[fg_idx])\n",
    "            label = foreground_label[fg_idx] - fg1  # minus fg1 because our fore ground classes are fg1,fg2,fg3 but we have to store it as 0,1,2\n",
    "    #image_list = np.concatenate(image_list ,axis=0)\n",
    "    image_list = torch.stack(image_list) \n",
    "    return image_list,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9cbe895",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-21T04:57:59.641732Z",
     "iopub.status.busy": "2022-03-21T04:57:59.640956Z",
     "iopub.status.idle": "2022-03-21T04:59:02.828024Z",
     "shell.execute_reply": "2022-03-21T04:59:02.827290Z",
     "shell.execute_reply.started": "2022-03-20T08:28:45.159285Z"
    },
    "papermill": {
     "duration": 63.214877,
     "end_time": "2022-03-21T04:59:02.828244",
     "exception": false,
     "start_time": "2022-03-21T04:57:59.613367",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f2fd0cb0e464eaf8173d0d96b40434f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/170498071 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:31<00:00, 161.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Foreground Background Data created\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [00:05<00:00, 7380.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mosaic Data Created\n"
     ]
    }
   ],
   "source": [
    "fg1, fg2, fg3 = 0,1,2\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=10, shuffle=False)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=10, shuffle=False)\n",
    "\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "foreground_classes = {'plane', 'car', 'bird'}\n",
    "\n",
    "background_classes = {'cat', 'deer', 'dog', 'frog', 'horse','ship', 'truck'}\n",
    "\n",
    "# print(type(foreground_classes))\n",
    "\n",
    "dataiter = iter(trainloader)\n",
    "background_data=[]\n",
    "background_label=[]\n",
    "foreground_data=[]\n",
    "foreground_label=[]\n",
    "batch_size=10\n",
    "\n",
    "for i in tqdm(range(5000)):   #5000*batch_size = 50000 data points\n",
    "    images, labels = dataiter.next()\n",
    "    for j in range(batch_size):\n",
    "        if(classes[labels[j]] in background_classes):\n",
    "            img = images[j].tolist()\n",
    "            background_data.append(img)\n",
    "            background_label.append(labels[j])\n",
    "        else:\n",
    "            img = images[j].tolist()\n",
    "            foreground_data.append(img)\n",
    "            foreground_label.append(labels[j])\n",
    "            \n",
    "foreground_data = torch.tensor(foreground_data)\n",
    "foreground_label = torch.tensor(foreground_label)\n",
    "background_data = torch.tensor(background_data)\n",
    "background_label = torch.tensor(background_label)\n",
    "print(\"Foreground Background Data created\")\n",
    "\n",
    "\n",
    "\n",
    "m = 5\n",
    "desired_num = 40000\n",
    "mosaic_data =[]      # list of mosaic images, each mosaic image is saved as list of 9 images\n",
    "fore_idx =[]                   # list of indexes at which foreground image is present in a mosaic image i.e from 0 to 9               \n",
    "mosaic_label=[]                # label of mosaic image = foreground class present in that mosaic\n",
    "list_set_labels = [] \n",
    "for i in tqdm(range(desired_num)):\n",
    "    set_idx = set()\n",
    "    np.random.seed(i)\n",
    "    bg_idx = np.random.randint(0,35000,m-1)\n",
    "    set_idx = set(background_label[bg_idx].tolist())\n",
    "    fg_idx = np.random.randint(0,15000)\n",
    "    set_idx.add(foreground_label[fg_idx].item())\n",
    "    fg = np.random.randint(0,m)\n",
    "    fore_idx.append(fg)\n",
    "    image_list,label = create_mosaic_img(bg_idx,fg_idx,fg,m)\n",
    "    mosaic_data.append(image_list)\n",
    "    mosaic_label.append(label)\n",
    "    list_set_labels.append(set_idx)\n",
    "print(\"Mosaic Data Created\")\n",
    "mosaic_data = torch.stack(mosaic_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f54ab4c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-21T04:59:03.093874Z",
     "iopub.status.busy": "2022-03-21T04:59:03.058111Z",
     "iopub.status.idle": "2022-03-21T04:59:03.219304Z",
     "shell.execute_reply": "2022-03-21T04:59:03.218840Z",
     "shell.execute_reply.started": "2022-03-20T08:29:53.921244Z"
    },
    "papermill": {
     "duration": 0.287487,
     "end_time": "2022-03-21T04:59:03.219437",
     "exception": false,
     "start_time": "2022-03-21T04:59:02.931950",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([40000, 5, 3, 32, 32]), (40000,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mosaic_data.shape,np.shape(mosaic_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0436846",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-21T04:59:03.431986Z",
     "iopub.status.busy": "2022-03-21T04:59:03.431118Z",
     "iopub.status.idle": "2022-03-21T04:59:03.433040Z",
     "shell.execute_reply": "2022-03-21T04:59:03.433507Z",
     "shell.execute_reply.started": "2022-03-20T08:29:54.013924Z"
    },
    "papermill": {
     "duration": 0.110027,
     "end_time": "2022-03-21T04:59:03.433656",
     "exception": false,
     "start_time": "2022-03-21T04:59:03.323629",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MosaicDataset(Dataset):\n",
    "  \"\"\"MosaicDataset dataset.\"\"\"\n",
    "\n",
    "  def __init__(self, mosaic_list_of_images, mosaic_label, fore_idx):\n",
    "    \"\"\"\n",
    "      Args:\n",
    "        csv_file (string): Path to the csv file with annotations.\n",
    "        root_dir (string): Directory with all the images.\n",
    "        transform (callable, optional): Optional transform to be applied\n",
    "            on a sample.\n",
    "    \"\"\"\n",
    "    self.mosaic = mosaic_list_of_images\n",
    "    self.label = mosaic_label\n",
    "    self.fore_idx = fore_idx\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.label)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return self.mosaic[idx] , self.label[idx], self.fore_idx[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04e96a86",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-21T04:59:03.645460Z",
     "iopub.status.busy": "2022-03-21T04:59:03.644626Z",
     "iopub.status.idle": "2022-03-21T04:59:03.646428Z",
     "shell.execute_reply": "2022-03-21T04:59:03.646857Z",
     "shell.execute_reply.started": "2022-03-20T08:29:56.548543Z"
    },
    "papermill": {
     "duration": 0.112026,
     "end_time": "2022-03-21T04:59:03.646988",
     "exception": false,
     "start_time": "2022-03-21T04:59:03.534962",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch = 250\n",
    "tr = 30000\n",
    "msd = MosaicDataset(mosaic_data[0:tr], mosaic_label[0:tr] , fore_idx[0:tr])\n",
    "train_loader = DataLoader( msd,batch_size= batch ,shuffle=False)\n",
    "\n",
    "batch = 250\n",
    "msd1 = MosaicDataset(mosaic_data[tr:], mosaic_label[tr:] , fore_idx[tr:])\n",
    "test_loader = DataLoader( msd1,batch_size= batch ,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd9bd87a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-21T04:59:03.872126Z",
     "iopub.status.busy": "2022-03-21T04:59:03.871284Z",
     "iopub.status.idle": "2022-03-21T04:59:03.873945Z",
     "shell.execute_reply": "2022-03-21T04:59:03.873473Z",
     "shell.execute_reply.started": "2022-03-20T08:30:03.599894Z"
    },
    "papermill": {
     "duration": 0.114337,
     "end_time": "2022-03-21T04:59:03.874054",
     "exception": false,
     "start_time": "2022-03-21T04:59:03.759717",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# torch.manual_seed(12)\n",
    "# focus = Focus()\n",
    "# focus = focus.to(device)\n",
    "# #print(focus.fc1.weight.data)\n",
    "# # focus.fc1.weight.data = torch.tensor([[0.,0.]])\n",
    "# torch.manual_seed(12)\n",
    "# classification = Classification()\n",
    "# classification = classification.to(device)\n",
    "# #print(classification.fc1.bias.data)\n",
    "# # classification.fc1.weight.data = torch.tensor([[0.1,0.1],[-0.1,-0.1],[0.,0.]])\n",
    "# # classification.fc1.bias.data = torch.tensor([0.,0.,0.])\n",
    "\n",
    "# Criterion = nn.CrossEntropyLoss(reduction=\"none\") #nn.BCELoss(reduction=\"none\")\n",
    "# focus_optimizer = optim.SGD(focus.parameters(), lr=0.07,momentum=0.9)\n",
    "# classification_optimizer = optim.SGD(classification.parameters(),lr=0.07,momentum=0.9)\n",
    "\n",
    "# # focus_optimizer = optim.Adam(focus.parameters(), lr=0.0001,weight_decay=0.00001)\n",
    "# # classification_optimizer = optim.Adam(classification.parameters(),lr=0.00005,weight_decay=0.001)\n",
    "\n",
    "# for i in range(100):\n",
    "#     focus_epoch_loss = []\n",
    "#     classification_epoch_loss = []\n",
    "#     for j,data in enumerate(train_loader):\n",
    "#         images,labels,foreground_index = data\n",
    "#         images = images.to(device)\n",
    "#         labels = labels.to(device)\n",
    "#         p_z = expectation_step(focus,classification,images,labels)\n",
    "#         focus,classification,focus_optimizer,classification_optimizer=maximization_step(p_z,focus,classification,\n",
    "#                                                                                         images,labels,\n",
    "#                                                                                         focus_optimizer,\n",
    "#                                                                                         classification_optimizer,\n",
    "#                                                                                         Criterion)\n",
    "#         with torch.no_grad():\n",
    "#             p_z = expectation_step(focus,classification,images,labels)\n",
    "#             batch = images.size(0)\n",
    "#             patches = images.size(1)\n",
    "#             focus_outputs = focus(images)\n",
    "#             images = images.reshape(batch*patches,3,32,32)\n",
    "#             classification_outputs = classification(images) # classification returns output after sigmoid/softmax]\n",
    "#             classification_outputs = classification_outputs.reshape(batch,patches,3)\n",
    "#             #print(focus_outputs,classification_outputs)\n",
    "#             loss_focus = calculate_loss_focus(p_z,focus_outputs)\n",
    "#             loss_classification = calculate_loss_classification(p_z,classification_outputs,\n",
    "#                                                         labels,Criterion,patches)\n",
    "#             focus_epoch_loss.append(loss_focus.item())\n",
    "#             classification_epoch_loss.append(loss_classification.item())\n",
    "#     print(\"*\"*60)\n",
    "#     print(\"Epoch: \" + str(i+1)+\", Focus Loss: \"+str(np.mean(focus_epoch_loss)))\n",
    "#     print(\"Epoch: \" + str(i+1)+\", Classification Loss: \"+str(np.mean(classification_epoch_loss))) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "53ac6640",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-21T04:59:04.098512Z",
     "iopub.status.busy": "2022-03-21T04:59:04.097860Z",
     "iopub.status.idle": "2022-03-21T05:18:08.902996Z",
     "shell.execute_reply": "2022-03-21T05:18:08.904079Z"
    },
    "papermill": {
     "duration": 1144.92341,
     "end_time": "2022-03-21T05:18:08.904531",
     "exception": false,
     "start_time": "2022-03-21T04:59:03.981121",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************************\n",
      "EM Step: 1 Epoch: 1, Focus Loss: 1.6089806427558264\n",
      "EM Step: 1 Epoch: 1, Classification Loss: 1.0859565685192745\n",
      "************************************************************\n",
      "EM Step: 1 Epoch: 2, Focus Loss: 1.6089772005875906\n",
      "EM Step: 1 Epoch: 2, Classification Loss: 1.069574592510859\n",
      "************************************************************\n",
      "EM Step: 1 Epoch: 3, Focus Loss: 1.6089749147494634\n",
      "EM Step: 1 Epoch: 3, Classification Loss: 1.0619431545337041\n",
      "************************************************************\n",
      "EM Step: 1 Epoch: 4, Focus Loss: 1.608973075946172\n",
      "EM Step: 1 Epoch: 4, Classification Loss: 1.056315416097641\n",
      "************************************************************\n",
      "EM Step: 1 Epoch: 5, Focus Loss: 1.6089714924494425\n",
      "EM Step: 1 Epoch: 5, Classification Loss: 1.0505760878324508\n",
      "************************************************************\n",
      "EM Step: 1 Epoch: 6, Focus Loss: 1.6089700678984324\n",
      "EM Step: 1 Epoch: 6, Classification Loss: 1.0451322863499324\n",
      "************************************************************\n",
      "EM Step: 1 Epoch: 7, Focus Loss: 1.6089687744776409\n",
      "EM Step: 1 Epoch: 7, Classification Loss: 1.0403753489255905\n",
      "************************************************************\n",
      "EM Step: 1 Epoch: 8, Focus Loss: 1.6089675664901733\n",
      "EM Step: 1 Epoch: 8, Classification Loss: 1.0357258091370265\n",
      "************************************************************\n",
      "EM Step: 1 Epoch: 9, Focus Loss: 1.6089664310216905\n",
      "EM Step: 1 Epoch: 9, Classification Loss: 1.0309871941804887\n",
      "************************************************************\n",
      "EM Step: 1 Epoch: 10, Focus Loss: 1.6089653700590134\n",
      "EM Step: 1 Epoch: 10, Classification Loss: 1.0263575563828151\n",
      "************************************************************\n",
      "EM Step: 2 Epoch: 1, Focus Loss: 1.6025930484135946\n",
      "EM Step: 2 Epoch: 1, Classification Loss: 0.9253067707022031\n",
      "************************************************************\n",
      "EM Step: 2 Epoch: 2, Focus Loss: 1.5972319334745406\n",
      "EM Step: 2 Epoch: 2, Classification Loss: 0.917750662068526\n",
      "************************************************************\n",
      "EM Step: 2 Epoch: 3, Focus Loss: 1.5945746382077535\n",
      "EM Step: 2 Epoch: 3, Classification Loss: 0.9131927743554116\n",
      "************************************************************\n",
      "EM Step: 2 Epoch: 4, Focus Loss: 1.5927841146787007\n",
      "EM Step: 2 Epoch: 4, Classification Loss: 0.9064396391312282\n",
      "************************************************************\n",
      "EM Step: 2 Epoch: 5, Focus Loss: 1.5916037917137147\n",
      "EM Step: 2 Epoch: 5, Classification Loss: 0.9002107342084249\n",
      "************************************************************\n",
      "EM Step: 2 Epoch: 6, Focus Loss: 1.5906014740467072\n",
      "EM Step: 2 Epoch: 6, Classification Loss: 0.8945282538731892\n",
      "************************************************************\n",
      "EM Step: 2 Epoch: 7, Focus Loss: 1.5898345092932382\n",
      "EM Step: 2 Epoch: 7, Classification Loss: 0.8885452315211296\n",
      "************************************************************\n",
      "EM Step: 2 Epoch: 8, Focus Loss: 1.5892007112503053\n",
      "EM Step: 2 Epoch: 8, Classification Loss: 0.8832871153950691\n",
      "************************************************************\n",
      "EM Step: 2 Epoch: 9, Focus Loss: 1.5886552919944128\n",
      "EM Step: 2 Epoch: 9, Classification Loss: 0.8786281630396843\n",
      "************************************************************\n",
      "EM Step: 2 Epoch: 10, Focus Loss: 1.5881903102000554\n",
      "EM Step: 2 Epoch: 10, Classification Loss: 0.8741038809219996\n",
      "************************************************************\n",
      "EM Step: 3 Epoch: 1, Focus Loss: 1.508633052309354\n",
      "EM Step: 3 Epoch: 1, Classification Loss: 0.7249439304073652\n",
      "************************************************************\n",
      "EM Step: 3 Epoch: 2, Focus Loss: 1.5004440317551295\n",
      "EM Step: 3 Epoch: 2, Classification Loss: 0.7161879365642866\n",
      "************************************************************\n",
      "EM Step: 3 Epoch: 3, Focus Loss: 1.4975244790315627\n",
      "EM Step: 3 Epoch: 3, Classification Loss: 0.7110090499122937\n",
      "************************************************************\n",
      "EM Step: 3 Epoch: 4, Focus Loss: 1.4943106551965077\n",
      "EM Step: 3 Epoch: 4, Classification Loss: 0.7080315391222636\n",
      "************************************************************\n",
      "EM Step: 3 Epoch: 5, Focus Loss: 1.492518382271131\n",
      "EM Step: 3 Epoch: 5, Classification Loss: 0.7055759424964587\n",
      "************************************************************\n",
      "EM Step: 3 Epoch: 6, Focus Loss: 1.49064206580321\n",
      "EM Step: 3 Epoch: 6, Classification Loss: 0.7018274153272311\n",
      "************************************************************\n",
      "EM Step: 3 Epoch: 7, Focus Loss: 1.4904225687185924\n",
      "EM Step: 3 Epoch: 7, Classification Loss: 0.7013160715500514\n",
      "************************************************************\n",
      "EM Step: 3 Epoch: 8, Focus Loss: 1.48868428170681\n",
      "EM Step: 3 Epoch: 8, Classification Loss: 0.7070022741953532\n",
      "************************************************************\n",
      "EM Step: 3 Epoch: 9, Focus Loss: 1.4868499130010604\n",
      "EM Step: 3 Epoch: 9, Classification Loss: 0.7066475600004196\n",
      "************************************************************\n",
      "EM Step: 3 Epoch: 10, Focus Loss: 1.4855090955893198\n",
      "EM Step: 3 Epoch: 10, Classification Loss: 0.701664145787557\n",
      "************************************************************\n",
      "EM Step: 4 Epoch: 1, Focus Loss: 1.3045590072870255\n",
      "EM Step: 4 Epoch: 1, Classification Loss: 0.5269321007033189\n",
      "************************************************************\n",
      "EM Step: 4 Epoch: 2, Focus Loss: 1.2989575644334157\n",
      "EM Step: 4 Epoch: 2, Classification Loss: 0.5201510546108087\n",
      "************************************************************\n",
      "EM Step: 4 Epoch: 3, Focus Loss: 1.2955425053834915\n",
      "EM Step: 4 Epoch: 3, Classification Loss: 0.5199170418083667\n",
      "************************************************************\n",
      "EM Step: 4 Epoch: 4, Focus Loss: 1.2940074096123377\n",
      "EM Step: 4 Epoch: 4, Classification Loss: 0.5207804595430692\n",
      "************************************************************\n",
      "EM Step: 4 Epoch: 5, Focus Loss: 1.291062162319819\n",
      "EM Step: 4 Epoch: 5, Classification Loss: 0.5202023262778918\n",
      "************************************************************\n",
      "EM Step: 4 Epoch: 6, Focus Loss: 1.2876771877209345\n",
      "EM Step: 4 Epoch: 6, Classification Loss: 0.5196036597092947\n",
      "************************************************************\n",
      "EM Step: 4 Epoch: 7, Focus Loss: 1.2856540441513062\n",
      "EM Step: 4 Epoch: 7, Classification Loss: 0.516939586152633\n",
      "************************************************************\n",
      "EM Step: 4 Epoch: 8, Focus Loss: 1.2855231682459514\n",
      "EM Step: 4 Epoch: 8, Classification Loss: 0.5078994490206241\n",
      "************************************************************\n",
      "EM Step: 4 Epoch: 9, Focus Loss: 1.2842650761206944\n",
      "EM Step: 4 Epoch: 9, Classification Loss: 0.5056268883248171\n",
      "************************************************************\n",
      "EM Step: 4 Epoch: 10, Focus Loss: 1.2829043845335641\n",
      "EM Step: 4 Epoch: 10, Classification Loss: 0.500424022724231\n",
      "************************************************************\n",
      "EM Step: 5 Epoch: 1, Focus Loss: 1.0230706055959067\n",
      "EM Step: 5 Epoch: 1, Classification Loss: 0.3076368215183417\n",
      "************************************************************\n",
      "EM Step: 5 Epoch: 2, Focus Loss: 1.0215397745370864\n",
      "EM Step: 5 Epoch: 2, Classification Loss: 0.30663967281579974\n",
      "************************************************************\n",
      "EM Step: 5 Epoch: 3, Focus Loss: 1.0162846167882285\n",
      "EM Step: 5 Epoch: 3, Classification Loss: 0.3154446452856064\n",
      "************************************************************\n",
      "EM Step: 5 Epoch: 4, Focus Loss: 1.014267093439897\n",
      "EM Step: 5 Epoch: 4, Classification Loss: 0.3113502783079942\n",
      "************************************************************\n",
      "EM Step: 5 Epoch: 5, Focus Loss: 1.0101196204622587\n",
      "EM Step: 5 Epoch: 5, Classification Loss: 0.3041965586443742\n",
      "************************************************************\n",
      "EM Step: 5 Epoch: 6, Focus Loss: 1.0087910383939742\n",
      "EM Step: 5 Epoch: 6, Classification Loss: 0.30164203643798826\n",
      "************************************************************\n",
      "EM Step: 5 Epoch: 7, Focus Loss: 1.0105929250518482\n",
      "EM Step: 5 Epoch: 7, Classification Loss: 0.2988218707342943\n",
      "************************************************************\n",
      "EM Step: 5 Epoch: 8, Focus Loss: 1.0080209786693255\n",
      "EM Step: 5 Epoch: 8, Classification Loss: 0.29836829329530395\n",
      "************************************************************\n",
      "EM Step: 5 Epoch: 9, Focus Loss: 1.0064733689030012\n",
      "EM Step: 5 Epoch: 9, Classification Loss: 0.2960172618428866\n",
      "************************************************************\n",
      "EM Step: 5 Epoch: 10, Focus Loss: 1.003511281311512\n",
      "EM Step: 5 Epoch: 10, Classification Loss: 0.2959801346063614\n",
      "************************************************************\n",
      "EM Step: 6 Epoch: 1, Focus Loss: 0.7293360168735187\n",
      "EM Step: 6 Epoch: 1, Classification Loss: 0.15793877666195233\n",
      "************************************************************\n",
      "EM Step: 6 Epoch: 2, Focus Loss: 0.7247013886769612\n",
      "EM Step: 6 Epoch: 2, Classification Loss: 0.15736320906629164\n",
      "************************************************************\n",
      "EM Step: 6 Epoch: 3, Focus Loss: 0.7246311475833257\n",
      "EM Step: 6 Epoch: 3, Classification Loss: 0.1494385439902544\n",
      "************************************************************\n",
      "EM Step: 6 Epoch: 4, Focus Loss: 0.7258597180247307\n",
      "EM Step: 6 Epoch: 4, Classification Loss: 0.1515294059490164\n",
      "************************************************************\n",
      "EM Step: 6 Epoch: 5, Focus Loss: 0.7249291270971299\n",
      "EM Step: 6 Epoch: 5, Classification Loss: 0.1677661884576082\n",
      "************************************************************\n",
      "EM Step: 6 Epoch: 6, Focus Loss: 0.7244912440578143\n",
      "EM Step: 6 Epoch: 6, Classification Loss: 0.16608033341666062\n",
      "************************************************************\n",
      "EM Step: 6 Epoch: 7, Focus Loss: 0.7169047519564629\n",
      "EM Step: 6 Epoch: 7, Classification Loss: 0.1552335737273097\n",
      "************************************************************\n",
      "EM Step: 6 Epoch: 8, Focus Loss: 0.7131769557793936\n",
      "EM Step: 6 Epoch: 8, Classification Loss: 0.1463110211615761\n",
      "************************************************************\n",
      "EM Step: 6 Epoch: 9, Focus Loss: 0.7119790559013685\n",
      "EM Step: 6 Epoch: 9, Classification Loss: 0.14516388171662886\n",
      "************************************************************\n",
      "EM Step: 6 Epoch: 10, Focus Loss: 0.7115827262401581\n",
      "EM Step: 6 Epoch: 10, Classification Loss: 0.1434034068758289\n",
      "************************************************************\n",
      "EM Step: 7 Epoch: 1, Focus Loss: 0.556725197037061\n",
      "EM Step: 7 Epoch: 1, Classification Loss: 0.08258729899923006\n",
      "************************************************************\n",
      "EM Step: 7 Epoch: 2, Focus Loss: 0.5554394269982974\n",
      "EM Step: 7 Epoch: 2, Classification Loss: 0.0859568023433288\n",
      "************************************************************\n",
      "EM Step: 7 Epoch: 3, Focus Loss: 0.553768643985192\n",
      "EM Step: 7 Epoch: 3, Classification Loss: 0.09629046407838662\n",
      "************************************************************\n",
      "EM Step: 7 Epoch: 4, Focus Loss: 0.5527392829457919\n",
      "EM Step: 7 Epoch: 4, Classification Loss: 0.10963360872119665\n",
      "************************************************************\n",
      "EM Step: 7 Epoch: 5, Focus Loss: 0.5525763834516207\n",
      "EM Step: 7 Epoch: 5, Classification Loss: 0.09719438360383113\n",
      "************************************************************\n",
      "EM Step: 7 Epoch: 6, Focus Loss: 0.5544148795306683\n",
      "EM Step: 7 Epoch: 6, Classification Loss: 0.0873422279333075\n",
      "************************************************************\n",
      "EM Step: 7 Epoch: 7, Focus Loss: 0.5549212050934632\n",
      "EM Step: 7 Epoch: 7, Classification Loss: 0.0822720275570949\n",
      "************************************************************\n",
      "EM Step: 7 Epoch: 8, Focus Loss: 0.5541148267686367\n",
      "EM Step: 7 Epoch: 8, Classification Loss: 0.07631873389085134\n",
      "************************************************************\n",
      "EM Step: 7 Epoch: 9, Focus Loss: 0.5516220214466254\n",
      "EM Step: 7 Epoch: 9, Classification Loss: 0.07084013152246674\n",
      "************************************************************\n",
      "EM Step: 7 Epoch: 10, Focus Loss: 0.550246673822403\n",
      "EM Step: 7 Epoch: 10, Classification Loss: 0.06784577229991555\n",
      "************************************************************\n",
      "EM Step: 8 Epoch: 1, Focus Loss: 0.5129638371368249\n",
      "EM Step: 8 Epoch: 1, Classification Loss: 0.04202943506340186\n",
      "************************************************************\n",
      "EM Step: 8 Epoch: 2, Focus Loss: 0.5106712833046914\n",
      "EM Step: 8 Epoch: 2, Classification Loss: 0.04187690378166735\n",
      "************************************************************\n",
      "EM Step: 8 Epoch: 3, Focus Loss: 0.510192813227574\n",
      "EM Step: 8 Epoch: 3, Classification Loss: 0.03892216496169567\n",
      "************************************************************\n",
      "EM Step: 8 Epoch: 4, Focus Loss: 0.5115073057512443\n",
      "EM Step: 8 Epoch: 4, Classification Loss: 0.03806468768355747\n",
      "************************************************************\n",
      "EM Step: 8 Epoch: 5, Focus Loss: 0.5116615250706673\n",
      "EM Step: 8 Epoch: 5, Classification Loss: 0.03653850859651963\n",
      "************************************************************\n",
      "EM Step: 8 Epoch: 6, Focus Loss: 0.5106881409883499\n",
      "EM Step: 8 Epoch: 6, Classification Loss: 0.037180967954918744\n",
      "************************************************************\n",
      "EM Step: 8 Epoch: 7, Focus Loss: 0.5109608131150405\n",
      "EM Step: 8 Epoch: 7, Classification Loss: 0.09741484999346237\n",
      "************************************************************\n",
      "EM Step: 8 Epoch: 8, Focus Loss: 0.5107834567626317\n",
      "EM Step: 8 Epoch: 8, Classification Loss: 0.12021274886404475\n",
      "************************************************************\n",
      "EM Step: 8 Epoch: 9, Focus Loss: 0.5090501102308432\n",
      "EM Step: 8 Epoch: 9, Classification Loss: 0.0626093356559674\n",
      "************************************************************\n",
      "EM Step: 8 Epoch: 10, Focus Loss: 0.5064590292672316\n",
      "EM Step: 8 Epoch: 10, Classification Loss: 0.046565865321705736\n",
      "************************************************************\n",
      "EM Step: 9 Epoch: 1, Focus Loss: 0.4382026635110378\n",
      "EM Step: 9 Epoch: 1, Classification Loss: 0.023767127632163466\n",
      "************************************************************\n",
      "EM Step: 9 Epoch: 2, Focus Loss: 0.43814287930727003\n",
      "EM Step: 9 Epoch: 2, Classification Loss: 0.02170793099794537\n",
      "************************************************************\n",
      "EM Step: 9 Epoch: 3, Focus Loss: 0.4378376324971517\n",
      "EM Step: 9 Epoch: 3, Classification Loss: 0.020051196007989346\n",
      "************************************************************\n",
      "EM Step: 9 Epoch: 4, Focus Loss: 0.4372296045223872\n",
      "EM Step: 9 Epoch: 4, Classification Loss: 0.01933830021880567\n",
      "************************************************************\n",
      "EM Step: 9 Epoch: 5, Focus Loss: 0.4375337292750677\n",
      "EM Step: 9 Epoch: 5, Classification Loss: 0.018595297704450786\n",
      "************************************************************\n",
      "EM Step: 9 Epoch: 6, Focus Loss: 0.43811127319931986\n",
      "EM Step: 9 Epoch: 6, Classification Loss: 0.018296755943447353\n",
      "************************************************************\n",
      "EM Step: 9 Epoch: 7, Focus Loss: 0.43819659277796746\n",
      "EM Step: 9 Epoch: 7, Classification Loss: 0.017786029384781916\n",
      "************************************************************\n",
      "EM Step: 9 Epoch: 8, Focus Loss: 0.4382487329343955\n",
      "EM Step: 9 Epoch: 8, Classification Loss: 0.01761585995554924\n",
      "************************************************************\n",
      "EM Step: 9 Epoch: 9, Focus Loss: 0.437410514553388\n",
      "EM Step: 9 Epoch: 9, Classification Loss: 0.017313935320513944\n",
      "************************************************************\n",
      "EM Step: 9 Epoch: 10, Focus Loss: 0.4355808692673842\n",
      "EM Step: 9 Epoch: 10, Classification Loss: 0.017210306879132987\n",
      "************************************************************\n",
      "EM Step: 10 Epoch: 1, Focus Loss: 0.3955586689213912\n",
      "EM Step: 10 Epoch: 1, Classification Loss: 0.0104505733664458\n",
      "************************************************************\n",
      "EM Step: 10 Epoch: 2, Focus Loss: 0.39533799986044565\n",
      "EM Step: 10 Epoch: 2, Classification Loss: 0.009868809833036115\n",
      "************************************************************\n",
      "EM Step: 10 Epoch: 3, Focus Loss: 0.3953150947888692\n",
      "EM Step: 10 Epoch: 3, Classification Loss: 0.009293970310439666\n",
      "************************************************************\n",
      "EM Step: 10 Epoch: 4, Focus Loss: 0.39549676030874253\n",
      "EM Step: 10 Epoch: 4, Classification Loss: 0.008802436016655217\n",
      "************************************************************\n",
      "EM Step: 10 Epoch: 5, Focus Loss: 0.3949243520696958\n",
      "EM Step: 10 Epoch: 5, Classification Loss: 0.00874308409790198\n",
      "************************************************************\n",
      "EM Step: 10 Epoch: 6, Focus Loss: 0.3943104883035024\n",
      "EM Step: 10 Epoch: 6, Classification Loss: 0.0088510047372741\n",
      "************************************************************\n",
      "EM Step: 10 Epoch: 7, Focus Loss: 0.3947775279482206\n",
      "EM Step: 10 Epoch: 7, Classification Loss: 0.008383249716522793\n",
      "************************************************************\n",
      "EM Step: 10 Epoch: 8, Focus Loss: 0.39470921059449515\n",
      "EM Step: 10 Epoch: 8, Classification Loss: 0.008189099150088926\n",
      "************************************************************\n",
      "EM Step: 10 Epoch: 9, Focus Loss: 0.3951333743830522\n",
      "EM Step: 10 Epoch: 9, Classification Loss: 0.008052211724376927\n",
      "************************************************************\n",
      "EM Step: 10 Epoch: 10, Focus Loss: 0.3956659863392512\n",
      "EM Step: 10 Epoch: 10, Classification Loss: 0.007776171310494343\n",
      "************************************************************\n",
      "EM Step: 11 Epoch: 1, Focus Loss: 0.37385318453113237\n",
      "EM Step: 11 Epoch: 1, Classification Loss: 0.005647478244888286\n",
      "************************************************************\n",
      "EM Step: 11 Epoch: 2, Focus Loss: 0.3742294669151306\n",
      "EM Step: 11 Epoch: 2, Classification Loss: 0.005247186085519691\n",
      "************************************************************\n",
      "EM Step: 11 Epoch: 3, Focus Loss: 0.37475328221917154\n",
      "EM Step: 11 Epoch: 3, Classification Loss: 0.005078820564085618\n",
      "************************************************************\n",
      "EM Step: 11 Epoch: 4, Focus Loss: 0.3749797649681568\n",
      "EM Step: 11 Epoch: 4, Classification Loss: 0.004954254468126844\n",
      "************************************************************\n",
      "EM Step: 11 Epoch: 5, Focus Loss: 0.3750642287234465\n",
      "EM Step: 11 Epoch: 5, Classification Loss: 0.004812829231377691\n",
      "************************************************************\n",
      "EM Step: 11 Epoch: 6, Focus Loss: 0.3756070420145988\n",
      "EM Step: 11 Epoch: 6, Classification Loss: 0.004748694266891107\n",
      "************************************************************\n",
      "EM Step: 11 Epoch: 7, Focus Loss: 0.3758230624099573\n",
      "EM Step: 11 Epoch: 7, Classification Loss: 0.004631923653262978\n",
      "************************************************************\n",
      "EM Step: 11 Epoch: 8, Focus Loss: 0.3752322656412919\n",
      "EM Step: 11 Epoch: 8, Classification Loss: 0.0045594979485031216\n",
      "************************************************************\n",
      "EM Step: 11 Epoch: 9, Focus Loss: 0.3752877096335093\n",
      "EM Step: 11 Epoch: 9, Classification Loss: 0.004514150084772458\n",
      "************************************************************\n",
      "EM Step: 11 Epoch: 10, Focus Loss: 0.3751285625000795\n",
      "EM Step: 11 Epoch: 10, Classification Loss: 0.004507595636338616\n",
      "************************************************************\n",
      "EM Step: 12 Epoch: 1, Focus Loss: 0.3397543010612329\n",
      "EM Step: 12 Epoch: 1, Classification Loss: 0.003025670725037344\n",
      "************************************************************\n",
      "EM Step: 12 Epoch: 2, Focus Loss: 0.3421442545950413\n",
      "EM Step: 12 Epoch: 2, Classification Loss: 0.0030491623852867632\n",
      "************************************************************\n",
      "EM Step: 12 Epoch: 3, Focus Loss: 0.3429017998278141\n",
      "EM Step: 12 Epoch: 3, Classification Loss: 0.0028004526286774007\n",
      "************************************************************\n",
      "EM Step: 12 Epoch: 4, Focus Loss: 0.3425488568842411\n",
      "EM Step: 12 Epoch: 4, Classification Loss: 0.002694650777266361\n",
      "************************************************************\n",
      "EM Step: 12 Epoch: 5, Focus Loss: 0.3420228933294614\n",
      "EM Step: 12 Epoch: 5, Classification Loss: 0.0026006698103932043\n",
      "************************************************************\n",
      "EM Step: 12 Epoch: 6, Focus Loss: 0.34197577411929764\n",
      "EM Step: 12 Epoch: 6, Classification Loss: 0.002533292667552208\n",
      "************************************************************\n",
      "EM Step: 12 Epoch: 7, Focus Loss: 0.34151418805122374\n",
      "EM Step: 12 Epoch: 7, Classification Loss: 0.0024861447386986886\n",
      "************************************************************\n",
      "EM Step: 12 Epoch: 8, Focus Loss: 0.341364952425162\n",
      "EM Step: 12 Epoch: 8, Classification Loss: 0.0024132086992419014\n",
      "************************************************************\n",
      "EM Step: 12 Epoch: 9, Focus Loss: 0.34153021971384684\n",
      "EM Step: 12 Epoch: 9, Classification Loss: 0.0023987662182965628\n",
      "************************************************************\n",
      "EM Step: 12 Epoch: 10, Focus Loss: 0.34164260427157084\n",
      "EM Step: 12 Epoch: 10, Classification Loss: 0.0023565036108872542\n",
      "************************************************************\n",
      "EM Step: 13 Epoch: 1, Focus Loss: 0.3237725555896759\n",
      "EM Step: 13 Epoch: 1, Classification Loss: 0.002023921371437609\n",
      "************************************************************\n",
      "EM Step: 13 Epoch: 2, Focus Loss: 0.32497794181108475\n",
      "EM Step: 13 Epoch: 2, Classification Loss: 0.0019078172161243856\n",
      "************************************************************\n",
      "EM Step: 13 Epoch: 3, Focus Loss: 0.3256344405313333\n",
      "EM Step: 13 Epoch: 3, Classification Loss: 0.001872696391365025\n",
      "************************************************************\n",
      "EM Step: 13 Epoch: 4, Focus Loss: 0.32531568333506583\n",
      "EM Step: 13 Epoch: 4, Classification Loss: 0.0018014225575219219\n",
      "************************************************************\n",
      "EM Step: 13 Epoch: 5, Focus Loss: 0.32536354983846344\n",
      "EM Step: 13 Epoch: 5, Classification Loss: 0.0019121761550195515\n",
      "************************************************************\n",
      "EM Step: 13 Epoch: 6, Focus Loss: 0.325432813167572\n",
      "EM Step: 13 Epoch: 6, Classification Loss: 0.002363619324751198\n",
      "************************************************************\n",
      "EM Step: 13 Epoch: 7, Focus Loss: 0.32520259941617646\n",
      "EM Step: 13 Epoch: 7, Classification Loss: 0.0018259375176664131\n",
      "************************************************************\n",
      "EM Step: 13 Epoch: 8, Focus Loss: 0.3249221791823705\n",
      "EM Step: 13 Epoch: 8, Classification Loss: 0.0018716539159261932\n",
      "************************************************************\n",
      "EM Step: 13 Epoch: 9, Focus Loss: 0.3247667262951533\n",
      "EM Step: 13 Epoch: 9, Classification Loss: 0.0017967119657744965\n",
      "************************************************************\n",
      "EM Step: 13 Epoch: 10, Focus Loss: 0.32469609479109446\n",
      "EM Step: 13 Epoch: 10, Classification Loss: 0.0016942967738335331\n",
      "************************************************************\n",
      "EM Step: 14 Epoch: 1, Focus Loss: 0.30370743200182915\n",
      "EM Step: 14 Epoch: 1, Classification Loss: 0.001337418208034554\n",
      "************************************************************\n",
      "EM Step: 14 Epoch: 2, Focus Loss: 0.3048690324028333\n",
      "EM Step: 14 Epoch: 2, Classification Loss: 0.0012743391904223244\n",
      "************************************************************\n",
      "EM Step: 14 Epoch: 3, Focus Loss: 0.3062509834766388\n",
      "EM Step: 14 Epoch: 3, Classification Loss: 0.0012720984612921407\n",
      "************************************************************\n",
      "EM Step: 14 Epoch: 4, Focus Loss: 0.30655929309626423\n",
      "EM Step: 14 Epoch: 4, Classification Loss: 0.0012539102914161048\n",
      "************************************************************\n",
      "EM Step: 14 Epoch: 5, Focus Loss: 0.30592951315144695\n",
      "EM Step: 14 Epoch: 5, Classification Loss: 0.0012292234229486592\n",
      "************************************************************\n",
      "EM Step: 14 Epoch: 6, Focus Loss: 0.3055733638505141\n",
      "EM Step: 14 Epoch: 6, Classification Loss: 0.001199732708967834\n",
      "************************************************************\n",
      "EM Step: 14 Epoch: 7, Focus Loss: 0.30545817961295446\n",
      "EM Step: 14 Epoch: 7, Classification Loss: 0.0011863955480900283\n",
      "************************************************************\n",
      "EM Step: 14 Epoch: 8, Focus Loss: 0.3054333376387755\n",
      "EM Step: 14 Epoch: 8, Classification Loss: 0.0011677617148961872\n",
      "************************************************************\n",
      "EM Step: 14 Epoch: 9, Focus Loss: 0.30532702604929607\n",
      "EM Step: 14 Epoch: 9, Classification Loss: 0.001156861465521312\n",
      "************************************************************\n",
      "EM Step: 14 Epoch: 10, Focus Loss: 0.30553248959283036\n",
      "EM Step: 14 Epoch: 10, Classification Loss: 0.001182285155421899\n",
      "************************************************************\n",
      "EM Step: 15 Epoch: 1, Focus Loss: 0.2992140846947829\n",
      "EM Step: 15 Epoch: 1, Classification Loss: 0.0009396506939083338\n",
      "************************************************************\n",
      "EM Step: 15 Epoch: 2, Focus Loss: 0.29966183242698513\n",
      "EM Step: 15 Epoch: 2, Classification Loss: 0.0008995476178824902\n",
      "************************************************************\n",
      "EM Step: 15 Epoch: 3, Focus Loss: 0.3004246100783348\n",
      "EM Step: 15 Epoch: 3, Classification Loss: 0.00087026821905359\n",
      "************************************************************\n",
      "EM Step: 15 Epoch: 4, Focus Loss: 0.3008570502201716\n",
      "EM Step: 15 Epoch: 4, Classification Loss: 0.0008512883662964063\n",
      "************************************************************\n",
      "EM Step: 15 Epoch: 5, Focus Loss: 0.3006428341070811\n",
      "EM Step: 15 Epoch: 5, Classification Loss: 0.0008372275735988902\n",
      "************************************************************\n",
      "EM Step: 15 Epoch: 6, Focus Loss: 0.30062641873955726\n",
      "EM Step: 15 Epoch: 6, Classification Loss: 0.0008265863553485058\n",
      "************************************************************\n",
      "EM Step: 15 Epoch: 7, Focus Loss: 0.300932910417517\n",
      "EM Step: 15 Epoch: 7, Classification Loss: 0.0008174733731721063\n",
      "************************************************************\n",
      "EM Step: 15 Epoch: 8, Focus Loss: 0.3012196863691012\n",
      "EM Step: 15 Epoch: 8, Classification Loss: 0.0008090366922260728\n",
      "************************************************************\n",
      "EM Step: 15 Epoch: 9, Focus Loss: 0.3013369558999936\n",
      "EM Step: 15 Epoch: 9, Classification Loss: 0.0008005509909708053\n",
      "************************************************************\n",
      "EM Step: 15 Epoch: 10, Focus Loss: 0.3009544017414252\n",
      "EM Step: 15 Epoch: 10, Classification Loss: 0.0007947151447297074\n",
      "************************************************************\n",
      "EM Step: 16 Epoch: 1, Focus Loss: 0.2937202574064334\n",
      "EM Step: 16 Epoch: 1, Classification Loss: 0.000690187229095803\n",
      "************************************************************\n",
      "EM Step: 16 Epoch: 2, Focus Loss: 0.2945089613397916\n",
      "EM Step: 16 Epoch: 2, Classification Loss: 0.0006719847722706617\n",
      "************************************************************\n",
      "EM Step: 16 Epoch: 3, Focus Loss: 0.2959352790067593\n",
      "EM Step: 16 Epoch: 3, Classification Loss: 0.0006557259856587431\n",
      "************************************************************\n",
      "EM Step: 16 Epoch: 4, Focus Loss: 0.29670931547880175\n",
      "EM Step: 16 Epoch: 4, Classification Loss: 0.0006492751125430611\n",
      "************************************************************\n",
      "EM Step: 16 Epoch: 5, Focus Loss: 0.2971988490472237\n",
      "EM Step: 16 Epoch: 5, Classification Loss: 0.000638851221447112\n",
      "************************************************************\n",
      "EM Step: 16 Epoch: 6, Focus Loss: 0.29645547854403653\n",
      "EM Step: 16 Epoch: 6, Classification Loss: 0.0006316912520560436\n",
      "************************************************************\n",
      "EM Step: 16 Epoch: 7, Focus Loss: 0.2955871435503165\n",
      "EM Step: 16 Epoch: 7, Classification Loss: 0.0006268668056388075\n",
      "************************************************************\n",
      "EM Step: 16 Epoch: 8, Focus Loss: 0.29543743170797826\n",
      "EM Step: 16 Epoch: 8, Classification Loss: 0.000620006243965084\n",
      "************************************************************\n",
      "EM Step: 16 Epoch: 9, Focus Loss: 0.2950174484401941\n",
      "EM Step: 16 Epoch: 9, Classification Loss: 0.00061651569934232\n",
      "************************************************************\n",
      "EM Step: 16 Epoch: 10, Focus Loss: 0.2950788427144289\n",
      "EM Step: 16 Epoch: 10, Classification Loss: 0.0006088332866056589\n",
      "************************************************************\n",
      "EM Step: 17 Epoch: 1, Focus Loss: 0.28467997337381046\n",
      "EM Step: 17 Epoch: 1, Classification Loss: 0.0005506928774896854\n",
      "************************************************************\n",
      "EM Step: 17 Epoch: 2, Focus Loss: 0.286028449734052\n",
      "EM Step: 17 Epoch: 2, Classification Loss: 0.0005392280701926211\n",
      "************************************************************\n",
      "EM Step: 17 Epoch: 3, Focus Loss: 0.28731628383199376\n",
      "EM Step: 17 Epoch: 3, Classification Loss: 0.0006839123635775953\n",
      "************************************************************\n",
      "EM Step: 17 Epoch: 4, Focus Loss: 0.28790036365389826\n",
      "EM Step: 17 Epoch: 4, Classification Loss: 0.0005879644902355115\n",
      "************************************************************\n",
      "EM Step: 17 Epoch: 5, Focus Loss: 0.2886388706664244\n",
      "EM Step: 17 Epoch: 5, Classification Loss: 0.0005306176428954738\n",
      "************************************************************\n",
      "EM Step: 17 Epoch: 6, Focus Loss: 0.28772008580466113\n",
      "EM Step: 17 Epoch: 6, Classification Loss: 0.0005123618376577118\n",
      "************************************************************\n",
      "EM Step: 17 Epoch: 7, Focus Loss: 0.2875195283442736\n",
      "EM Step: 17 Epoch: 7, Classification Loss: 0.0005048693435431536\n",
      "************************************************************\n",
      "EM Step: 17 Epoch: 8, Focus Loss: 0.2871242872128884\n",
      "EM Step: 17 Epoch: 8, Classification Loss: 0.0004968950563731293\n",
      "************************************************************\n",
      "EM Step: 17 Epoch: 9, Focus Loss: 0.28695305101573465\n",
      "EM Step: 17 Epoch: 9, Classification Loss: 0.0004944806525600143\n",
      "************************************************************\n",
      "EM Step: 17 Epoch: 10, Focus Loss: 0.2866009743263324\n",
      "EM Step: 17 Epoch: 10, Classification Loss: 0.0004881213767172691\n",
      "************************************************************\n",
      "EM Step: 18 Epoch: 1, Focus Loss: 0.27156375360985596\n",
      "EM Step: 18 Epoch: 1, Classification Loss: 0.00046857571530078225\n",
      "************************************************************\n",
      "EM Step: 18 Epoch: 2, Focus Loss: 0.2730916136254867\n",
      "EM Step: 18 Epoch: 2, Classification Loss: 0.0004553068338888503\n",
      "************************************************************\n",
      "EM Step: 18 Epoch: 3, Focus Loss: 0.2749234203249216\n",
      "EM Step: 18 Epoch: 3, Classification Loss: 0.00048706952463059376\n",
      "************************************************************\n",
      "EM Step: 18 Epoch: 4, Focus Loss: 0.2753296807408333\n",
      "EM Step: 18 Epoch: 4, Classification Loss: 0.00047512054370599797\n",
      "************************************************************\n",
      "EM Step: 18 Epoch: 5, Focus Loss: 0.2749996246149143\n",
      "EM Step: 18 Epoch: 5, Classification Loss: 0.00045544420863734557\n",
      "************************************************************\n",
      "EM Step: 18 Epoch: 6, Focus Loss: 0.27455613849063715\n",
      "EM Step: 18 Epoch: 6, Classification Loss: 0.00043448544080699016\n",
      "************************************************************\n",
      "EM Step: 18 Epoch: 7, Focus Loss: 0.27454519756138324\n",
      "EM Step: 18 Epoch: 7, Classification Loss: 0.0004286930079009229\n",
      "************************************************************\n",
      "EM Step: 18 Epoch: 8, Focus Loss: 0.2744838832567135\n",
      "EM Step: 18 Epoch: 8, Classification Loss: 0.0004378160053723453\n",
      "************************************************************\n",
      "EM Step: 18 Epoch: 9, Focus Loss: 0.2742388918995857\n",
      "EM Step: 18 Epoch: 9, Classification Loss: 0.00042729043452709447\n",
      "************************************************************\n",
      "EM Step: 18 Epoch: 10, Focus Loss: 0.27420999817550185\n",
      "EM Step: 18 Epoch: 10, Classification Loss: 0.00041710827693653606\n",
      "************************************************************\n",
      "EM Step: 19 Epoch: 1, Focus Loss: 0.26275584859152634\n",
      "EM Step: 19 Epoch: 1, Classification Loss: 0.00039455267748659634\n",
      "************************************************************\n",
      "EM Step: 19 Epoch: 2, Focus Loss: 0.2634471099823713\n",
      "EM Step: 19 Epoch: 2, Classification Loss: 0.00039805236313744294\n",
      "************************************************************\n",
      "EM Step: 19 Epoch: 3, Focus Loss: 0.2646698617686828\n",
      "EM Step: 19 Epoch: 3, Classification Loss: 0.00038507402568939143\n",
      "************************************************************\n",
      "EM Step: 19 Epoch: 4, Focus Loss: 0.26595604258279004\n",
      "EM Step: 19 Epoch: 4, Classification Loss: 0.00037868815931384837\n",
      "************************************************************\n",
      "EM Step: 19 Epoch: 5, Focus Loss: 0.26661154913405577\n",
      "EM Step: 19 Epoch: 5, Classification Loss: 0.00037738697804646414\n",
      "************************************************************\n",
      "EM Step: 19 Epoch: 6, Focus Loss: 0.26646580584347246\n",
      "EM Step: 19 Epoch: 6, Classification Loss: 0.0003716807328601135\n",
      "************************************************************\n",
      "EM Step: 19 Epoch: 7, Focus Loss: 0.2660960322866837\n",
      "EM Step: 19 Epoch: 7, Classification Loss: 0.0003685760629499176\n",
      "************************************************************\n",
      "EM Step: 19 Epoch: 8, Focus Loss: 0.2658395626892646\n",
      "EM Step: 19 Epoch: 8, Classification Loss: 0.00036396801830657446\n",
      "************************************************************\n",
      "EM Step: 19 Epoch: 9, Focus Loss: 0.265498781700929\n",
      "EM Step: 19 Epoch: 9, Classification Loss: 0.0003757500223097547\n",
      "************************************************************\n",
      "EM Step: 19 Epoch: 10, Focus Loss: 0.26578992120921613\n",
      "EM Step: 19 Epoch: 10, Classification Loss: 0.0003684949197122478\n",
      "************************************************************\n",
      "EM Step: 20 Epoch: 1, Focus Loss: 0.24865013609329858\n",
      "EM Step: 20 Epoch: 1, Classification Loss: 0.0002990971341811625\n",
      "************************************************************\n",
      "EM Step: 20 Epoch: 2, Focus Loss: 0.24989203860362372\n",
      "EM Step: 20 Epoch: 2, Classification Loss: 0.00034981263737184537\n",
      "************************************************************\n",
      "EM Step: 20 Epoch: 3, Focus Loss: 0.2510743216921886\n",
      "EM Step: 20 Epoch: 3, Classification Loss: 0.0003222583194049851\n",
      "************************************************************\n",
      "EM Step: 20 Epoch: 4, Focus Loss: 0.25185479745268824\n",
      "EM Step: 20 Epoch: 4, Classification Loss: 0.0003081224775693651\n",
      "************************************************************\n",
      "EM Step: 20 Epoch: 5, Focus Loss: 0.2522781637807687\n",
      "EM Step: 20 Epoch: 5, Classification Loss: 0.00033186960148062403\n",
      "************************************************************\n",
      "EM Step: 20 Epoch: 6, Focus Loss: 0.2521742547551791\n",
      "EM Step: 20 Epoch: 6, Classification Loss: 0.48607020874890927\n",
      "************************************************************\n",
      "EM Step: 20 Epoch: 7, Focus Loss: 0.251542990654707\n",
      "EM Step: 20 Epoch: 7, Classification Loss: 0.3614908975859483\n",
      "************************************************************\n",
      "EM Step: 20 Epoch: 8, Focus Loss: 0.2512002026041349\n",
      "EM Step: 20 Epoch: 8, Classification Loss: 0.2550826511035363\n",
      "************************************************************\n",
      "EM Step: 20 Epoch: 9, Focus Loss: 0.2512986432760954\n",
      "EM Step: 20 Epoch: 9, Classification Loss: 0.1929825263718764\n",
      "************************************************************\n",
      "EM Step: 20 Epoch: 10, Focus Loss: 0.25092231581608454\n",
      "EM Step: 20 Epoch: 10, Classification Loss: 0.16446144400785367\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(12)\n",
    "focus = Focus()\n",
    "focus = focus.to(device)\n",
    "\n",
    "\n",
    "torch.manual_seed(12)\n",
    "classification = Classification()\n",
    "classification = classification.to(device)\n",
    "\n",
    "\n",
    "Criterion = nn.CrossEntropyLoss(reduction=\"none\") #nn.BCELoss(reduction=\"none\")\n",
    "\n",
    "\n",
    "em_steps = 20\n",
    "\n",
    "\n",
    "for i in range(em_steps):\n",
    "    # calculate p_z\n",
    "    gamma_ = []\n",
    "    focus_optimizer = optim.SGD(focus.parameters(), lr=0.03,momentum=0.9)\n",
    "    classification_optimizer = optim.SGD(classification.parameters(),lr=0.03,momentum=0.9)\n",
    "    for j,data in enumerate(train_loader):\n",
    "        images,labels,foreground_index = data\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        p_z = expectation_step(focus,classification,images,labels)\n",
    "        gamma_.append(p_z)\n",
    "    for epoch in range(10):\n",
    "        focus_epoch_loss = []\n",
    "        classification_epoch_loss = []\n",
    "        for j,data in enumerate(train_loader):\n",
    "            images,labels,foreground_index = data\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            focus,classification,focus_optimizer,classification_optimizer=maximization_step(gamma_[j],\n",
    "                                                                                            focus,classification,\n",
    "                                                                                            images,labels,\n",
    "                                                                                            focus_optimizer,\n",
    "                                                                                            classification_optimizer,\n",
    "                                                                                            Criterion)\n",
    "            with torch.no_grad():\n",
    "                batch = images.size(0)\n",
    "                patches = images.size(1)\n",
    "                focus_outputs = focus(images)\n",
    "                images = images.reshape(batch*patches,3,32,32)\n",
    "                classification_outputs = classification(images) # classification returns output after sigmoid/softmax]\n",
    "                classification_outputs = classification_outputs.reshape(batch,patches,3)\n",
    "                #print(focus_outputs,classification_outputs)\n",
    "                loss_focus = calculate_loss_focus(gamma_[j],focus_outputs)\n",
    "                loss_classification = calculate_loss_classification(gamma_[j],classification_outputs,\n",
    "                                                        labels,Criterion,patches)\n",
    "                focus_epoch_loss.append(loss_focus.item())\n",
    "                classification_epoch_loss.append(loss_classification.item())\n",
    "        print(\"*\"*60)\n",
    "        print(\"EM Step: \"+str(i+1)+\" Epoch: \" + str(epoch+1)+\", Focus Loss: \"+str(np.mean(focus_epoch_loss)))\n",
    "        print(\"EM Step: \"+str(i+1)+\" Epoch: \" + str(epoch+1)+\", Classification Loss: \"+str(np.mean(classification_epoch_loss))) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7db55c",
   "metadata": {
    "papermill": {
     "duration": 0.210998,
     "end_time": "2022-03-21T05:18:09.391567",
     "exception": false,
     "start_time": "2022-03-21T05:18:09.180569",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "43b9b17e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-21T05:18:09.738341Z",
     "iopub.status.busy": "2022-03-21T05:18:09.737458Z",
     "iopub.status.idle": "2022-03-21T05:18:11.257268Z",
     "shell.execute_reply": "2022-03-21T05:18:11.254685Z",
     "shell.execute_reply.started": "2022-03-20T09:05:57.376590Z"
    },
    "papermill": {
     "duration": 1.705011,
     "end_time": "2022-03-21T05:18:11.258361",
     "exception": false,
     "start_time": "2022-03-21T05:18:09.553350",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Focus True 80.23666666666666\n",
      "Accuracy 94.86\n"
     ]
    }
   ],
   "source": [
    "# method 1\n",
    "predicted_indexes = []\n",
    "foreground_index_list = []\n",
    "prediction_list = []\n",
    "labels_list = []\n",
    "with torch.no_grad():\n",
    "    for j,data in enumerate(train_loader):\n",
    "        images,labels,foreground_index = data\n",
    "        images = images.to(device)\n",
    "        foreground_index_list.append(foreground_index.numpy())\n",
    "        labels_list.append(labels.numpy())\n",
    "        batch = images.size(0)\n",
    "        scores = focus(images)\n",
    "        indexes = torch.argmax(F.softmax(scores,dim=1),dim=1).cpu().numpy()\n",
    "        predicted_indexes.append(indexes)\n",
    "        outputs = F.softmax(classification(images[np.arange(batch),indexes,:]),dim=1)\n",
    "        prediction = torch.argmax(outputs,dim=1)\n",
    "        prediction_list.append(prediction.cpu().numpy())\n",
    "    \n",
    "predicted_indexes = np.concatenate(predicted_indexes,axis=0)\n",
    "foreground_index_list = np.concatenate(foreground_index_list,axis=0)\n",
    "prediction_list = np.concatenate(prediction_list,axis=0)\n",
    "labels_list = np.concatenate(labels_list,axis=0)\n",
    "    \n",
    "\n",
    "print(\"Focus True\",(np.sum(predicted_indexes == foreground_index_list,axis=0).item()/\n",
    "                    len(foreground_index_list))*100)\n",
    "accuracy = (np.sum(prediction_list == labels_list,axis=0)/len(labels_list) )*100\n",
    "print(\"Accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82325c7d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-21T05:18:11.818166Z",
     "iopub.status.busy": "2022-03-21T05:18:11.817357Z",
     "iopub.status.idle": "2022-03-21T05:18:13.398991Z",
     "shell.execute_reply": "2022-03-21T05:18:13.399670Z",
     "shell.execute_reply.started": "2022-03-20T09:06:00.879083Z"
    },
    "papermill": {
     "duration": 1.867989,
     "end_time": "2022-03-21T05:18:13.399825",
     "exception": false,
     "start_time": "2022-03-21T05:18:11.531836",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Focus True 80.23666666666666\n",
      "Accuracy 95.61666666666667\n"
     ]
    }
   ],
   "source": [
    "# method 2\n",
    "predicted_indexes = []\n",
    "foreground_index_list = []\n",
    "prediction_list = []\n",
    "labels_list = []\n",
    "with torch.no_grad():\n",
    "    for j,data in enumerate(train_loader):\n",
    "        images,labels,foreground_index = data\n",
    "        images = images.to(device)\n",
    "        batch = images.size(0)\n",
    "        patches = images.size(1)\n",
    "        foreground_index_list.append(foreground_index.numpy())\n",
    "        labels_list.append(labels.numpy())\n",
    "        batch = images.size(0)\n",
    "        focus_outputs = F.softmax(focus(images),dim=1)\n",
    "        indexes = torch.argmax(focus_outputs,dim=1).cpu().numpy()\n",
    "        predicted_indexes.append(indexes)\n",
    "        images = images.reshape(batch*patches,3,32,32)\n",
    "        classification_outputs = F.softmax(classification(images),dim=1)\n",
    "        classification_outputs = classification_outputs.reshape(batch,patches,3)\n",
    "\n",
    "        #print(classification_outputs.shape,focus_outputs.shape)\n",
    "\n",
    "        prediction = torch.argmax(torch.sum(focus_outputs[:,:,None]*classification_outputs,dim=1),dim=1)\n",
    "        prediction_list.append(prediction.cpu().numpy())\n",
    "    \n",
    "predicted_indexes = np.concatenate(predicted_indexes,axis=0)\n",
    "foreground_index_list = np.concatenate(foreground_index_list,axis=0)\n",
    "prediction_list = np.concatenate(prediction_list,axis=0)\n",
    "labels_list = np.concatenate(labels_list,axis=0)\n",
    "    \n",
    "\n",
    "print(\"Focus True\",(np.sum(predicted_indexes == foreground_index_list,axis=0).item()/\n",
    "                    len(foreground_index_list))*100)\n",
    "accuracy = (np.sum(prediction_list == labels_list,axis=0)/len(labels_list) )*100\n",
    "print(\"Accuracy\", accuracy)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# method 2\n",
    "# focus_output = F.softmax(focus(data),dim=1)\n",
    "# indexes = torch.argmax(F.softmax(focus(data),dim=1),dim=1)[:,0].numpy()\n",
    "# classification_output = F.softmax(classification(data),dim=2)\n",
    "# print(\"Focus True\",(np.sum(indexes == fore_idx,axis=0).item()/len(fore_idx))*100)\n",
    "# prediction = torch.argmax(torch.sum(focus_output*classification_output,dim=1),dim=1)\n",
    "# accuracy = (torch.sum(prediction == labels,dim=0)/len(labels) )*100\n",
    "# print(\"Accuracy\", accuracy.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "98c37ca6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-21T05:18:13.729508Z",
     "iopub.status.busy": "2022-03-21T05:18:13.728872Z",
     "iopub.status.idle": "2022-03-21T05:18:13.731750Z",
     "shell.execute_reply": "2022-03-21T05:18:13.732158Z",
     "shell.execute_reply.started": "2022-03-20T09:06:07.629136Z"
    },
    "papermill": {
     "duration": 0.171465,
     "end_time": "2022-03-21T05:18:13.732304",
     "exception": false,
     "start_time": "2022-03-21T05:18:13.560839",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.23160898685455322, 0.20825183391571045, 0.26961591839790344, 0.22652606666088104, 0.2822815179824829, 0.26133278012275696, 0.24374254047870636, 0.27502650022506714, 0.24863536655902863, 0.2332628220319748, 0.2713073790073395, 0.2523052990436554, 0.26460257172584534, 0.24730750918388367, 0.25115418434143066, 0.26181459426879883, 0.22098501026630402, 0.2951747179031372, 0.25180527567863464, 0.2349870353937149, 0.27131712436676025, 0.295121431350708, 0.22330091893672943, 0.20936112105846405, 0.25340595841407776, 0.2682759463787079, 0.26291772723197937, 0.22228607535362244, 0.24109819531440735, 0.24506129324436188, 0.2206636369228363, 0.20295432209968567, 0.2812653183937073, 0.24659934639930725, 0.270048588514328, 0.17192624509334564, 0.25391682982444763, 0.26894107460975647, 0.2152675986289978, 0.2658972442150116, 0.23209483921527863, 0.2885556221008301, 0.2475430965423584, 0.2690689265727997, 0.23085753619670868, 0.25203171372413635, 0.2617640793323517, 0.25335970520973206, 0.22597043216228485, 0.23154692351818085, 0.24281808733940125, 0.22916270792484283, 0.26288652420043945, 0.23800276219844818, 0.2758331894874573, 0.24318508803844452, 0.24265174567699432, 0.25445201992988586, 0.23897872865200043, 0.2852981388568878, 0.24310356378555298, 0.24612772464752197, 0.20538155734539032, 0.223897323012352, 0.23874260485172272, 0.23200421035289764, 0.2878226339817047, 0.2598132789134979, 0.25124484300613403, 0.2619841396808624, 0.25614479184150696, 0.2346782088279724, 0.2775278389453888, 0.2623983919620514, 0.24466712772846222, 0.2673550546169281, 0.26340869069099426, 0.22489292919635773, 0.2403532862663269, 0.21319320797920227, 0.1932867467403412, 0.2193518429994583, 0.2358785718679428, 0.20353448390960693, 0.21142835915088654, 0.21693962812423706, 0.24900315701961517, 0.2816654145717621, 0.26275715231895447, 0.23873800039291382, 0.22990980744361877, 0.29955896735191345, 0.25222015380859375, 0.25259482860565186, 0.26923805475234985, 0.2488582581281662, 0.2660169005393982, 0.2658608853816986, 0.26431602239608765, 0.20958761870861053, 0.26079145073890686, 0.2466612011194229, 0.24776311218738556, 0.2702595591545105, 0.2830215096473694, 0.2599430978298187, 0.24380378425121307, 0.2856907546520233, 0.2335890680551529, 0.2320721596479416, 0.30137819051742554, 0.24387100338935852, 0.2596425712108612, 0.31985679268836975, 0.29771098494529724, 0.3061107397079468, 0.305870920419693, 0.2592596411705017, 0.2615593373775482, 0.25891998410224915]\n"
     ]
    }
   ],
   "source": [
    "print(focus_epoch_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8eafc31",
   "metadata": {
    "papermill": {
     "duration": 0.163437,
     "end_time": "2022-03-21T05:18:14.056701",
     "exception": false,
     "start_time": "2022-03-21T05:18:13.893264",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "> **Test data Evaluation**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3261cb91",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-21T05:18:14.399334Z",
     "iopub.status.busy": "2022-03-21T05:18:14.398515Z",
     "iopub.status.idle": "2022-03-21T05:18:14.882272Z",
     "shell.execute_reply": "2022-03-21T05:18:14.882693Z",
     "shell.execute_reply.started": "2022-03-20T09:06:13.618167Z"
    },
    "papermill": {
     "duration": 0.660228,
     "end_time": "2022-03-21T05:18:14.882838",
     "exception": false,
     "start_time": "2022-03-21T05:18:14.222610",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Focus True 76.14\n",
      "Accuracy 84.33\n"
     ]
    }
   ],
   "source": [
    "# method 1\n",
    "predicted_indexes = []\n",
    "foreground_index_list = []\n",
    "prediction_list = []\n",
    "labels_list = []\n",
    "with torch.no_grad():\n",
    "    for j,data in enumerate(test_loader):\n",
    "        images,labels,foreground_index = data\n",
    "        images = images.to(device)\n",
    "        foreground_index_list.append(foreground_index.numpy())\n",
    "        labels_list.append(labels.numpy())\n",
    "        batch = images.size(0)\n",
    "        scores = focus(images)\n",
    "        indexes = torch.argmax(F.softmax(scores,dim=1),dim=1).cpu().numpy()\n",
    "        predicted_indexes.append(indexes)\n",
    "        outputs = F.softmax(classification(images[np.arange(batch),indexes,:]),dim=1)\n",
    "        prediction = torch.argmax(outputs,dim=1)\n",
    "        prediction_list.append(prediction.cpu().numpy())\n",
    "    \n",
    "predicted_indexes = np.concatenate(predicted_indexes,axis=0)\n",
    "foreground_index_list = np.concatenate(foreground_index_list,axis=0)\n",
    "prediction_list = np.concatenate(prediction_list,axis=0)\n",
    "labels_list = np.concatenate(labels_list,axis=0)\n",
    "    \n",
    "\n",
    "print(\"Focus True\",(np.sum(predicted_indexes == foreground_index_list,axis=0).item()/\n",
    "                    len(foreground_index_list))*100)\n",
    "accuracy = (np.sum(prediction_list == labels_list,axis=0)/len(labels_list) )*100\n",
    "print(\"Accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d5885a08",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-21T05:18:15.229921Z",
     "iopub.status.busy": "2022-03-21T05:18:15.227552Z",
     "iopub.status.idle": "2022-03-21T05:18:15.767110Z",
     "shell.execute_reply": "2022-03-21T05:18:15.767566Z",
     "shell.execute_reply.started": "2022-03-20T09:06:18.105584Z"
    },
    "papermill": {
     "duration": 0.723254,
     "end_time": "2022-03-21T05:18:15.767714",
     "exception": false,
     "start_time": "2022-03-21T05:18:15.044460",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Focus True 76.14\n",
      "Accuracy 85.39\n"
     ]
    }
   ],
   "source": [
    "# method 2\n",
    "predicted_indexes = []\n",
    "foreground_index_list = []\n",
    "prediction_list = []\n",
    "labels_list = []\n",
    "with torch.no_grad():\n",
    "    for j,data in enumerate(test_loader):\n",
    "        images,labels,foreground_index = data\n",
    "        images = images.to(device)\n",
    "        batch = images.size(0)\n",
    "        patches = images.size(1)\n",
    "        foreground_index_list.append(foreground_index.numpy())\n",
    "        labels_list.append(labels.numpy())\n",
    "        batch = images.size(0)\n",
    "        focus_outputs = F.softmax(focus(images),dim=1)\n",
    "        indexes = torch.argmax(focus_outputs,dim=1).cpu().numpy()\n",
    "        predicted_indexes.append(indexes)\n",
    "        images = images.reshape(batch*patches,3,32,32)\n",
    "        classification_outputs = F.softmax(classification(images),dim=1)\n",
    "        classification_outputs = classification_outputs.reshape(batch,patches,3)\n",
    "\n",
    "        #print(classification_outputs.shape,focus_outputs.shape)\n",
    "\n",
    "        prediction = torch.argmax(torch.sum(focus_outputs[:,:,None]*classification_outputs,dim=1),dim=1)\n",
    "        prediction_list.append(prediction.cpu().numpy())\n",
    "    \n",
    "predicted_indexes = np.concatenate(predicted_indexes,axis=0)\n",
    "foreground_index_list = np.concatenate(foreground_index_list,axis=0)\n",
    "prediction_list = np.concatenate(prediction_list,axis=0)\n",
    "labels_list = np.concatenate(labels_list,axis=0)\n",
    "    \n",
    "\n",
    "print(\"Focus True\",(np.sum(predicted_indexes == foreground_index_list,axis=0).item()/\n",
    "                    len(foreground_index_list))*100)\n",
    "accuracy = (np.sum(prediction_list == labels_list,axis=0)/len(labels_list) )*100\n",
    "print(\"Accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb1685e",
   "metadata": {
    "papermill": {
     "duration": 0.161155,
     "end_time": "2022-03-21T05:18:16.094138",
     "exception": false,
     "start_time": "2022-03-21T05:18:15.932983",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7325575",
   "metadata": {
    "papermill": {
     "duration": 0.161996,
     "end_time": "2022-03-21T05:18:16.419172",
     "exception": false,
     "start_time": "2022-03-21T05:18:16.257176",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe4823b",
   "metadata": {
    "papermill": {
     "duration": 0.170855,
     "end_time": "2022-03-21T05:18:16.752029",
     "exception": false,
     "start_time": "2022-03-21T05:18:16.581174",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1229.945751,
   "end_time": "2022-03-21T05:18:19.285967",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-03-21T04:57:49.340216",
   "version": "2.3.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "068b0e7e1db4471fa6b18b68f8485f9f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_45c886d8950b4ce1b5c1685f41c1033f",
       "placeholder": "​",
       "style": "IPY_MODEL_19036c36269c4de785429aea1eba1843",
       "value": " 170499072/? [00:03&lt;00:00, 54597177.70it/s]"
      }
     },
     "19036c36269c4de785429aea1eba1843": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "45c886d8950b4ce1b5c1685f41c1033f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4994d645f12c440d8d50e3d894ad3fb1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "5ac16b06cd4b466d9d57ac578e9d7798": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5f2fd0cb0e464eaf8173d0d96b40434f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_87bfd95a89554dffb672254477ae0ed3",
        "IPY_MODEL_b7e0548bbd684d26abfc4ead7c330ad6",
        "IPY_MODEL_068b0e7e1db4471fa6b18b68f8485f9f"
       ],
       "layout": "IPY_MODEL_c01d99759f3d4651b2c6db998d057f8d"
      }
     },
     "717354002fe74a0f9b2db75f11a5325f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "87bfd95a89554dffb672254477ae0ed3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_5ac16b06cd4b466d9d57ac578e9d7798",
       "placeholder": "​",
       "style": "IPY_MODEL_717354002fe74a0f9b2db75f11a5325f",
       "value": ""
      }
     },
     "895834d8156e45fb814ee87f5e5f9614": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b7e0548bbd684d26abfc4ead7c330ad6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_895834d8156e45fb814ee87f5e5f9614",
       "max": 170498071.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_4994d645f12c440d8d50e3d894ad3fb1",
       "value": 170498071.0
      }
     },
     "c01d99759f3d4651b2c6db998d057f8d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

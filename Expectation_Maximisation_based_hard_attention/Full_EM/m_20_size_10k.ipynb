{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3929fba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-21T04:04:03.761716Z",
     "iopub.status.busy": "2022-03-21T04:04:03.760200Z",
     "iopub.status.idle": "2022-03-21T04:04:05.487236Z",
     "shell.execute_reply": "2022-03-21T04:04:05.487977Z",
     "shell.execute_reply.started": "2022-03-20T08:28:27.859219Z"
    },
    "papermill": {
     "duration": 1.753796,
     "end_time": "2022-03-21T04:04:05.488521",
     "exception": false,
     "start_time": "2022-03-21T04:04:03.734725",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "from tqdm import tqdm\n",
    "%matplotlib inline\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm as tqdm\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fac2a423",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-21T04:04:05.535489Z",
     "iopub.status.busy": "2022-03-21T04:04:05.534663Z",
     "iopub.status.idle": "2022-03-21T04:04:05.548517Z",
     "shell.execute_reply": "2022-03-21T04:04:05.548083Z",
     "shell.execute_reply.started": "2022-03-20T08:28:37.120300Z"
    },
    "papermill": {
     "duration": 0.037554,
     "end_time": "2022-03-21T04:04:05.548638",
     "exception": false,
     "start_time": "2022-03-21T04:04:05.511084",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Focus(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Focus, self).__init__()\n",
    "\n",
    "    self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=0, bias=False)\n",
    "    self.pool = nn.MaxPool2d(2, 2)\n",
    "    self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=0, bias=False)\n",
    "    self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=0, bias=False)\n",
    "    self.fc1 = nn.Linear(1024, 512, bias=False)\n",
    "    self.fc2 = nn.Linear(512, 64, bias=False)\n",
    "    self.fc3 = nn.Linear(64, 10, bias=False)\n",
    "    self.fc4 = nn.Linear(10,1, bias=False)\n",
    "\n",
    "    torch.nn.init.xavier_normal_(self.conv1.weight)\n",
    "    torch.nn.init.xavier_normal_(self.conv2.weight)\n",
    "    torch.nn.init.xavier_normal_(self.conv3.weight)\n",
    "    torch.nn.init.xavier_normal_(self.fc1.weight)\n",
    "    torch.nn.init.xavier_normal_(self.fc2.weight)\n",
    "    torch.nn.init.xavier_normal_(self.fc3.weight)\n",
    "    torch.nn.init.xavier_normal_(self.fc4.weight)\n",
    "\n",
    "  def forward(self,z):  #y is avg image #z batch of list of 9 images\n",
    "    batch = z.size(0)\n",
    "    patches = z.size(1)\n",
    "    z = z.view(batch*patches,3,32,32)\n",
    "    alpha =  self.helper(z)\n",
    "    alpha = alpha.view(batch,patches,-1)\n",
    "    return alpha[:,:,0] # scores \n",
    "    \n",
    "  def helper(self, x):\n",
    "    x = self.pool(F.relu(self.conv1(x)))\n",
    "    x = self.pool(F.relu(self.conv2(x)))\n",
    "    # print(x.shape)\n",
    "    x = (F.relu(self.conv3(x)))\n",
    "    x =  x.view(x.size(0), -1)\n",
    "    # print(x.shape)\n",
    "    x = F.relu(self.fc1(x))\n",
    "    x = F.relu(self.fc2(x))\n",
    "    x = F.relu(self.fc3(x))\n",
    "    x = self.fc4(x)\n",
    "    return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b9f5a55",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-21T04:04:05.604112Z",
     "iopub.status.busy": "2022-03-21T04:04:05.603202Z",
     "iopub.status.idle": "2022-03-21T04:04:05.613608Z",
     "shell.execute_reply": "2022-03-21T04:04:05.615189Z",
     "shell.execute_reply.started": "2022-03-20T08:28:40.017216Z"
    },
    "papermill": {
     "duration": 0.046338,
     "end_time": "2022-03-21T04:04:05.615519",
     "exception": false,
     "start_time": "2022-03-21T04:04:05.569181",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Classification(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Classification, self).__init__()\n",
    "    self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "    self.pool = nn.MaxPool2d(2, 2)\n",
    "    self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "    self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "    self.fc2 = nn.Linear(120, 84)\n",
    "    self.fc3 = nn.Linear(84, 10)\n",
    "    self.fc4 = nn.Linear(10,3)\n",
    "\n",
    "    torch.nn.init.xavier_normal_(self.conv1.weight)\n",
    "    torch.nn.init.zeros_(self.conv1.bias)\n",
    "    torch.nn.init.xavier_normal_(self.conv2.weight)\n",
    "    torch.nn.init.zeros_(self.conv2.bias)\n",
    "    torch.nn.init.xavier_normal_(self.fc1.weight)\n",
    "    torch.nn.init.zeros_(self.fc1.bias)\n",
    "    torch.nn.init.xavier_normal_(self.fc2.weight)\n",
    "    torch.nn.init.zeros_(self.fc2.bias)\n",
    "    torch.nn.init.xavier_normal_(self.fc3.weight)\n",
    "    torch.nn.init.zeros_(self.fc3.bias)\n",
    "    torch.nn.init.xavier_normal_(self.fc4.weight)\n",
    "    torch.nn.init.zeros_(self.fc4.bias)\n",
    "\n",
    "  def forward(self,z): \n",
    "    y1 = self.pool(F.relu(self.conv1(z)))\n",
    "    y1 = self.pool(F.relu(self.conv2(y1)))\n",
    "    y1 = y1.view(-1, 16 * 5 * 5)\n",
    "\n",
    "    y1 = F.relu(self.fc1(y1))\n",
    "    y1 = F.relu(self.fc2(y1))\n",
    "    y1 = F.relu(self.fc3(y1))\n",
    "    y1 = self.fc4(y1)\n",
    "    return y1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bd882ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-21T04:04:05.692617Z",
     "iopub.status.busy": "2022-03-21T04:04:05.691753Z",
     "iopub.status.idle": "2022-03-21T04:04:05.693429Z",
     "shell.execute_reply": "2022-03-21T04:04:05.693912Z",
     "shell.execute_reply.started": "2022-03-20T08:36:06.558404Z"
    },
    "papermill": {
     "duration": 0.041458,
     "end_time": "2022-03-21T04:04:05.694049",
     "exception": false,
     "start_time": "2022-03-21T04:04:05.652591",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_loss_focus(gamma,focus_output):\n",
    "    \n",
    "    m = torch.nn.LogSoftmax(dim=1)\n",
    "    log_outputs = m(focus_output)    \n",
    "    loss_ = gamma*log_outputs\n",
    "    loss_ = torch.sum(loss_,dim=1)\n",
    "    loss_ = -torch.mean(loss_,dim=0)  \n",
    "    return loss_ \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70fc1e5a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-21T04:04:05.741741Z",
     "iopub.status.busy": "2022-03-21T04:04:05.740986Z",
     "iopub.status.idle": "2022-03-21T04:04:05.743356Z",
     "shell.execute_reply": "2022-03-21T04:04:05.742965Z",
     "shell.execute_reply.started": "2022-03-20T08:28:42.037522Z"
    },
    "papermill": {
     "duration": 0.029051,
     "end_time": "2022-03-21T04:04:05.743482",
     "exception": false,
     "start_time": "2022-03-21T04:04:05.714431",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_loss_classification(gamma,classification_output,label,criterion,n_patches):\n",
    "    #print(classification_output.shape)\n",
    "    \n",
    "    batch = label.size(0)\n",
    "    classes = classification_output.size(2)\n",
    "    label = label.repeat_interleave(n_patches)\n",
    "    classification_output = classification_output.reshape((batch*n_patches,classes))\n",
    "    loss_ = criterion(classification_output,label)\n",
    "    \n",
    "    loss_ = loss_.reshape((batch,n_patches))\n",
    "    \n",
    "    loss_ = gamma*loss_\n",
    "    loss_ = torch.sum(loss_,dim=1)\n",
    "    loss_ = torch.mean(loss_,dim=0)\n",
    "    \n",
    "    return loss_\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96014377",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-21T04:04:05.790703Z",
     "iopub.status.busy": "2022-03-21T04:04:05.790075Z",
     "iopub.status.idle": "2022-03-21T04:04:05.793289Z",
     "shell.execute_reply": "2022-03-21T04:04:05.793770Z",
     "shell.execute_reply.started": "2022-03-20T08:28:42.857391Z"
    },
    "papermill": {
     "duration": 0.029874,
     "end_time": "2022-03-21T04:04:05.793893",
     "exception": false,
     "start_time": "2022-03-21T04:04:05.764019",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def expectation_step(fc,cl,data,labels):\n",
    "    batch= data.size(0)\n",
    "    patches = data.size(1)\n",
    "    with torch.no_grad():\n",
    "        outputs_f = torch.softmax(fc(data),dim=1)\n",
    "        data = data.reshape(batch*patches,3,32,32)\n",
    "        outputs_g = cl(data)\n",
    "        \n",
    "    outputs_g = torch.softmax(outputs_g.reshape(batch,patches,3),dim=2)\n",
    "    \n",
    "        \n",
    "        \n",
    "    #print(\"Focus output\",outputs_f.shape,torch.sum(outputs_f,dim=1))\n",
    "    #print(\"Classification output\",outputs_g.shape,torch.sum(outputs_g,dim=1),torch.sum(outputs_g,dim=2))\n",
    "    outputs_g = outputs_g[np.arange(batch),:,labels]\n",
    "    p_x_y_z = outputs_f*outputs_g   #(1-outputs_g)    \n",
    "    \n",
    "    \n",
    "    normalized_p = p_x_y_z/torch.sum(p_x_y_z,dim=1,keepdims=True)\n",
    "#     print(outputs_f[0],outputs_g[0],normalized_p[0])\n",
    "    return normalized_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "714bd912",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-21T04:04:05.842920Z",
     "iopub.status.busy": "2022-03-21T04:04:05.838294Z",
     "iopub.status.idle": "2022-03-21T04:04:05.844902Z",
     "shell.execute_reply": "2022-03-21T04:04:05.845262Z",
     "shell.execute_reply.started": "2022-03-20T08:28:43.596869Z"
    },
    "papermill": {
     "duration": 0.029484,
     "end_time": "2022-03-21T04:04:05.845381",
     "exception": false,
     "start_time": "2022-03-21T04:04:05.815897",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def maximization_step(p_z,focus,classification,data,labels,focus_optimizer,classification_optimizer,Criterion):    \n",
    "    batch = data.size(0)\n",
    "    patches = data.size(1)\n",
    "    focus_optimizer.zero_grad()\n",
    "    classification_optimizer.zero_grad()\n",
    "    \n",
    "    focus_outputs = focus(data)\n",
    "    data = data.reshape(batch*patches,3,32,32)\n",
    "    classification_outputs = classification(data) \n",
    "    classification_outputs = classification_outputs.reshape(batch,patches,3)\n",
    "    \n",
    "    \n",
    "    #print(focus_outputs,classification_outputs)\n",
    "    \n",
    "    loss_focus = calculate_loss_focus(p_z,focus_outputs)\n",
    "    loss_classification = calculate_loss_classification(p_z,classification_outputs,\n",
    "                                                        labels,Criterion,patches)\n",
    "    \n",
    "    #print(\"Focus loss\",loss_focus.item())\n",
    "    #print(\"Classification loss\",loss_classification.item())\n",
    "    loss_focus.backward() \n",
    "    loss_classification.backward()\n",
    "    focus_optimizer.step()\n",
    "    classification_optimizer.step()\n",
    "    \n",
    "    return focus,classification,focus_optimizer,classification_optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c688f34",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-21T04:04:05.892454Z",
     "iopub.status.busy": "2022-03-21T04:04:05.891579Z",
     "iopub.status.idle": "2022-03-21T04:04:05.893394Z",
     "shell.execute_reply": "2022-03-21T04:04:05.893853Z",
     "shell.execute_reply.started": "2022-03-20T08:28:44.173461Z"
    },
    "papermill": {
     "duration": 0.027932,
     "end_time": "2022-03-21T04:04:05.893969",
     "exception": false,
     "start_time": "2022-03-21T04:04:05.866037",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_mosaic_img(bg_idx,fg_idx,fg,m): \n",
    "    \"\"\"\n",
    "      bg_idx : list of indexes of background_data[] to be used as background images in mosaic\n",
    "      fg_idx : index of image to be used as foreground image from foreground data\n",
    "      fg : at what position/index foreground image has to be stored out of 0-8\n",
    "    \"\"\"\n",
    "    image_list=[]\n",
    "    j=0\n",
    "    for i in range(m):  # m value \n",
    "        if i != fg:\n",
    "            image_list.append(background_data[bg_idx[j]])\n",
    "            j+=1\n",
    "        else: \n",
    "            image_list.append(foreground_data[fg_idx])\n",
    "            label = foreground_label[fg_idx] - fg1  # minus fg1 because our fore ground classes are fg1,fg2,fg3 but we have to store it as 0,1,2\n",
    "    #image_list = np.concatenate(image_list ,axis=0)\n",
    "    image_list = torch.stack(image_list) \n",
    "    return image_list,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a38fc6e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-21T04:04:05.949299Z",
     "iopub.status.busy": "2022-03-21T04:04:05.948528Z",
     "iopub.status.idle": "2022-03-21T04:05:22.356044Z",
     "shell.execute_reply": "2022-03-21T04:05:22.355311Z",
     "shell.execute_reply.started": "2022-03-20T08:28:45.159285Z"
    },
    "papermill": {
     "duration": 76.442093,
     "end_time": "2022-03-21T04:05:22.356237",
     "exception": false,
     "start_time": "2022-03-21T04:04:05.914144",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6ef1ea2023f473e8f4b42bd7e19a6b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/170498071 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:32<00:00, 155.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Foreground Background Data created\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [00:06<00:00, 3304.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mosaic Data Created\n"
     ]
    }
   ],
   "source": [
    "fg1, fg2, fg3 = 0,1,2\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=10, shuffle=False)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=10, shuffle=False)\n",
    "\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "foreground_classes = {'plane', 'car', 'bird'}\n",
    "\n",
    "background_classes = {'cat', 'deer', 'dog', 'frog', 'horse','ship', 'truck'}\n",
    "\n",
    "# print(type(foreground_classes))\n",
    "\n",
    "dataiter = iter(trainloader)\n",
    "background_data=[]\n",
    "background_label=[]\n",
    "foreground_data=[]\n",
    "foreground_label=[]\n",
    "batch_size=10\n",
    "\n",
    "for i in tqdm(range(5000)):   #5000*batch_size = 50000 data points\n",
    "    images, labels = dataiter.next()\n",
    "    for j in range(batch_size):\n",
    "        if(classes[labels[j]] in background_classes):\n",
    "            img = images[j].tolist()\n",
    "            background_data.append(img)\n",
    "            background_label.append(labels[j])\n",
    "        else:\n",
    "            img = images[j].tolist()\n",
    "            foreground_data.append(img)\n",
    "            foreground_label.append(labels[j])\n",
    "            \n",
    "foreground_data = torch.tensor(foreground_data)\n",
    "foreground_label = torch.tensor(foreground_label)\n",
    "background_data = torch.tensor(background_data)\n",
    "background_label = torch.tensor(background_label)\n",
    "print(\"Foreground Background Data created\")\n",
    "\n",
    "\n",
    "\n",
    "m = 20\n",
    "desired_num = 20000\n",
    "mosaic_data =[]      # list of mosaic images, each mosaic image is saved as list of 9 images\n",
    "fore_idx =[]                   # list of indexes at which foreground image is present in a mosaic image i.e from 0 to 9               \n",
    "mosaic_label=[]                # label of mosaic image = foreground class present in that mosaic\n",
    "list_set_labels = [] \n",
    "for i in tqdm(range(desired_num)):\n",
    "    set_idx = set()\n",
    "    np.random.seed(i)\n",
    "    bg_idx = np.random.randint(0,35000,m-1)\n",
    "    set_idx = set(background_label[bg_idx].tolist())\n",
    "    fg_idx = np.random.randint(0,15000)\n",
    "    set_idx.add(foreground_label[fg_idx].item())\n",
    "    fg = np.random.randint(0,m)\n",
    "    fore_idx.append(fg)\n",
    "    image_list,label = create_mosaic_img(bg_idx,fg_idx,fg,m)\n",
    "    mosaic_data.append(image_list)\n",
    "    mosaic_label.append(label)\n",
    "    list_set_labels.append(set_idx)\n",
    "print(\"Mosaic Data Created\")\n",
    "mosaic_data = torch.stack(mosaic_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5e07fca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-21T04:05:22.588710Z",
     "iopub.status.busy": "2022-03-21T04:05:22.572252Z",
     "iopub.status.idle": "2022-03-21T04:05:22.658817Z",
     "shell.execute_reply": "2022-03-21T04:05:22.659191Z",
     "shell.execute_reply.started": "2022-03-20T08:29:53.921244Z"
    },
    "papermill": {
     "duration": 0.197183,
     "end_time": "2022-03-21T04:05:22.659326",
     "exception": false,
     "start_time": "2022-03-21T04:05:22.462143",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([20000, 20, 3, 32, 32]), (20000,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mosaic_data.shape,np.shape(mosaic_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5a06ab1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-21T04:05:22.885432Z",
     "iopub.status.busy": "2022-03-21T04:05:22.884701Z",
     "iopub.status.idle": "2022-03-21T04:05:22.886618Z",
     "shell.execute_reply": "2022-03-21T04:05:22.887066Z",
     "shell.execute_reply.started": "2022-03-20T08:29:54.013924Z"
    },
    "papermill": {
     "duration": 0.121829,
     "end_time": "2022-03-21T04:05:22.887193",
     "exception": false,
     "start_time": "2022-03-21T04:05:22.765364",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MosaicDataset(Dataset):\n",
    "  \"\"\"MosaicDataset dataset.\"\"\"\n",
    "\n",
    "  def __init__(self, mosaic_list_of_images, mosaic_label, fore_idx):\n",
    "    \"\"\"\n",
    "      Args:\n",
    "        csv_file (string): Path to the csv file with annotations.\n",
    "        root_dir (string): Directory with all the images.\n",
    "        transform (callable, optional): Optional transform to be applied\n",
    "            on a sample.\n",
    "    \"\"\"\n",
    "    self.mosaic = mosaic_list_of_images\n",
    "    self.label = mosaic_label\n",
    "    self.fore_idx = fore_idx\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.label)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return self.mosaic[idx] , self.label[idx], self.fore_idx[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e7a9960",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-21T04:05:23.107368Z",
     "iopub.status.busy": "2022-03-21T04:05:23.106634Z",
     "iopub.status.idle": "2022-03-21T04:05:23.108637Z",
     "shell.execute_reply": "2022-03-21T04:05:23.109016Z",
     "shell.execute_reply.started": "2022-03-20T08:29:56.548543Z"
    },
    "papermill": {
     "duration": 0.115967,
     "end_time": "2022-03-21T04:05:23.109144",
     "exception": false,
     "start_time": "2022-03-21T04:05:22.993177",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch = 250\n",
    "msd = MosaicDataset(mosaic_data[0:10000], mosaic_label[0:10000] , fore_idx[0:10000])\n",
    "train_loader = DataLoader( msd,batch_size= batch ,shuffle=False)\n",
    "\n",
    "batch = 250\n",
    "msd1 = MosaicDataset(mosaic_data[10000:], mosaic_label[10000:] , fore_idx[10000:])\n",
    "test_loader = DataLoader( msd1,batch_size= batch ,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ded52c97",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-21T04:05:23.328174Z",
     "iopub.status.busy": "2022-03-21T04:05:23.327343Z",
     "iopub.status.idle": "2022-03-21T04:05:23.329801Z",
     "shell.execute_reply": "2022-03-21T04:05:23.329359Z",
     "shell.execute_reply.started": "2022-03-20T08:30:03.599894Z"
    },
    "papermill": {
     "duration": 0.113755,
     "end_time": "2022-03-21T04:05:23.329907",
     "exception": false,
     "start_time": "2022-03-21T04:05:23.216152",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# torch.manual_seed(12)\n",
    "# focus = Focus()\n",
    "# focus = focus.to(device)\n",
    "# #print(focus.fc1.weight.data)\n",
    "# # focus.fc1.weight.data = torch.tensor([[0.,0.]])\n",
    "# torch.manual_seed(12)\n",
    "# classification = Classification()\n",
    "# classification = classification.to(device)\n",
    "# #print(classification.fc1.bias.data)\n",
    "# # classification.fc1.weight.data = torch.tensor([[0.1,0.1],[-0.1,-0.1],[0.,0.]])\n",
    "# # classification.fc1.bias.data = torch.tensor([0.,0.,0.])\n",
    "\n",
    "# Criterion = nn.CrossEntropyLoss(reduction=\"none\") #nn.BCELoss(reduction=\"none\")\n",
    "# focus_optimizer = optim.SGD(focus.parameters(), lr=0.07,momentum=0.9)\n",
    "# classification_optimizer = optim.SGD(classification.parameters(),lr=0.07,momentum=0.9)\n",
    "\n",
    "# # focus_optimizer = optim.Adam(focus.parameters(), lr=0.0001,weight_decay=0.00001)\n",
    "# # classification_optimizer = optim.Adam(classification.parameters(),lr=0.00005,weight_decay=0.001)\n",
    "\n",
    "# for i in range(100):\n",
    "#     focus_epoch_loss = []\n",
    "#     classification_epoch_loss = []\n",
    "#     for j,data in enumerate(train_loader):\n",
    "#         images,labels,foreground_index = data\n",
    "#         images = images.to(device)\n",
    "#         labels = labels.to(device)\n",
    "#         p_z = expectation_step(focus,classification,images,labels)\n",
    "#         focus,classification,focus_optimizer,classification_optimizer=maximization_step(p_z,focus,classification,\n",
    "#                                                                                         images,labels,\n",
    "#                                                                                         focus_optimizer,\n",
    "#                                                                                         classification_optimizer,\n",
    "#                                                                                         Criterion)\n",
    "#         with torch.no_grad():\n",
    "#             p_z = expectation_step(focus,classification,images,labels)\n",
    "#             batch = images.size(0)\n",
    "#             patches = images.size(1)\n",
    "#             focus_outputs = focus(images)\n",
    "#             images = images.reshape(batch*patches,3,32,32)\n",
    "#             classification_outputs = classification(images) # classification returns output after sigmoid/softmax]\n",
    "#             classification_outputs = classification_outputs.reshape(batch,patches,3)\n",
    "#             #print(focus_outputs,classification_outputs)\n",
    "#             loss_focus = calculate_loss_focus(p_z,focus_outputs)\n",
    "#             loss_classification = calculate_loss_classification(p_z,classification_outputs,\n",
    "#                                                         labels,Criterion,patches)\n",
    "#             focus_epoch_loss.append(loss_focus.item())\n",
    "#             classification_epoch_loss.append(loss_classification.item())\n",
    "#     print(\"*\"*60)\n",
    "#     print(\"Epoch: \" + str(i+1)+\", Focus Loss: \"+str(np.mean(focus_epoch_loss)))\n",
    "#     print(\"Epoch: \" + str(i+1)+\", Classification Loss: \"+str(np.mean(classification_epoch_loss))) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef6ae279",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-21T04:05:23.555777Z",
     "iopub.status.busy": "2022-03-21T04:05:23.555105Z",
     "iopub.status.idle": "2022-03-21T04:33:12.720781Z",
     "shell.execute_reply": "2022-03-21T04:33:12.720157Z"
    },
    "papermill": {
     "duration": 1669.285722,
     "end_time": "2022-03-21T04:33:12.720954",
     "exception": false,
     "start_time": "2022-03-21T04:05:23.435232",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************************\n",
      "EM Step: 1 Epoch: 1, Focus Loss: 2.9951875686645506\n",
      "EM Step: 1 Epoch: 1, Classification Loss: 1.0977875977754592\n",
      "************************************************************\n",
      "EM Step: 1 Epoch: 2, Focus Loss: 2.9951866269111633\n",
      "EM Step: 1 Epoch: 2, Classification Loss: 1.097240599989891\n",
      "************************************************************\n",
      "EM Step: 1 Epoch: 3, Focus Loss: 2.995185774564743\n",
      "EM Step: 1 Epoch: 3, Classification Loss: 1.096875175833702\n",
      "************************************************************\n",
      "EM Step: 1 Epoch: 4, Focus Loss: 2.995185041427612\n",
      "EM Step: 1 Epoch: 4, Classification Loss: 1.0964944332838058\n",
      "************************************************************\n",
      "EM Step: 1 Epoch: 5, Focus Loss: 2.995184379816055\n",
      "EM Step: 1 Epoch: 5, Classification Loss: 1.0961597561836243\n",
      "************************************************************\n",
      "EM Step: 1 Epoch: 6, Focus Loss: 2.9951838195323943\n",
      "EM Step: 1 Epoch: 6, Classification Loss: 1.0958278477191925\n",
      "************************************************************\n",
      "EM Step: 1 Epoch: 7, Focus Loss: 2.995183265209198\n",
      "EM Step: 1 Epoch: 7, Classification Loss: 1.0954965561628343\n",
      "************************************************************\n",
      "EM Step: 1 Epoch: 8, Focus Loss: 2.99518278837204\n",
      "EM Step: 1 Epoch: 8, Classification Loss: 1.095118224620819\n",
      "************************************************************\n",
      "EM Step: 1 Epoch: 9, Focus Loss: 2.995182305574417\n",
      "EM Step: 1 Epoch: 9, Classification Loss: 1.094743075966835\n",
      "************************************************************\n",
      "EM Step: 1 Epoch: 10, Focus Loss: 2.995181918144226\n",
      "EM Step: 1 Epoch: 10, Classification Loss: 1.0943775355815888\n",
      "************************************************************\n",
      "EM Step: 2 Epoch: 1, Focus Loss: 2.995153933763504\n",
      "EM Step: 2 Epoch: 1, Classification Loss: 1.0847039639949798\n",
      "************************************************************\n",
      "EM Step: 2 Epoch: 2, Focus Loss: 2.9951257705688477\n",
      "EM Step: 2 Epoch: 2, Classification Loss: 1.0846498787403107\n",
      "************************************************************\n",
      "EM Step: 2 Epoch: 3, Focus Loss: 2.99511062502861\n",
      "EM Step: 2 Epoch: 3, Classification Loss: 1.0835375607013702\n",
      "************************************************************\n",
      "EM Step: 2 Epoch: 4, Focus Loss: 2.995099240541458\n",
      "EM Step: 2 Epoch: 4, Classification Loss: 1.082868766784668\n",
      "************************************************************\n",
      "EM Step: 2 Epoch: 5, Focus Loss: 2.9950897693634033\n",
      "EM Step: 2 Epoch: 5, Classification Loss: 1.0822764098644257\n",
      "************************************************************\n",
      "EM Step: 2 Epoch: 6, Focus Loss: 2.995081216096878\n",
      "EM Step: 2 Epoch: 6, Classification Loss: 1.0815733611583709\n",
      "************************************************************\n",
      "EM Step: 2 Epoch: 7, Focus Loss: 2.995073413848877\n",
      "EM Step: 2 Epoch: 7, Classification Loss: 1.0809627234935761\n",
      "************************************************************\n",
      "EM Step: 2 Epoch: 8, Focus Loss: 2.995066386461258\n",
      "EM Step: 2 Epoch: 8, Classification Loss: 1.0803843915462494\n",
      "************************************************************\n",
      "EM Step: 2 Epoch: 9, Focus Loss: 2.9950598955154417\n",
      "EM Step: 2 Epoch: 9, Classification Loss: 1.0797555238008498\n",
      "************************************************************\n",
      "EM Step: 2 Epoch: 10, Focus Loss: 2.995053732395172\n",
      "EM Step: 2 Epoch: 10, Classification Loss: 1.0792069792747498\n",
      "************************************************************\n",
      "EM Step: 3 Epoch: 1, Focus Loss: 2.9944571554660797\n",
      "EM Step: 3 Epoch: 1, Classification Loss: 1.0586353421211243\n",
      "************************************************************\n",
      "EM Step: 3 Epoch: 2, Focus Loss: 2.9942897617816926\n",
      "EM Step: 3 Epoch: 2, Classification Loss: 1.0592717468738555\n",
      "************************************************************\n",
      "EM Step: 3 Epoch: 3, Focus Loss: 2.994201201200485\n",
      "EM Step: 3 Epoch: 3, Classification Loss: 1.0578221708536149\n",
      "************************************************************\n",
      "EM Step: 3 Epoch: 4, Focus Loss: 2.9941306531429293\n",
      "EM Step: 3 Epoch: 4, Classification Loss: 1.0567856013774872\n",
      "************************************************************\n",
      "EM Step: 3 Epoch: 5, Focus Loss: 2.9940716087818147\n",
      "EM Step: 3 Epoch: 5, Classification Loss: 1.0561697691679002\n",
      "************************************************************\n",
      "EM Step: 3 Epoch: 6, Focus Loss: 2.994016832113266\n",
      "EM Step: 3 Epoch: 6, Classification Loss: 1.0554641723632812\n",
      "************************************************************\n",
      "EM Step: 3 Epoch: 7, Focus Loss: 2.993965208530426\n",
      "EM Step: 3 Epoch: 7, Classification Loss: 1.0547748923301696\n",
      "************************************************************\n",
      "EM Step: 3 Epoch: 8, Focus Loss: 2.9939140677452087\n",
      "EM Step: 3 Epoch: 8, Classification Loss: 1.054225680232048\n",
      "************************************************************\n",
      "EM Step: 3 Epoch: 9, Focus Loss: 2.9938653647899627\n",
      "EM Step: 3 Epoch: 9, Classification Loss: 1.0533938139677048\n",
      "************************************************************\n",
      "EM Step: 3 Epoch: 10, Focus Loss: 2.99381805062294\n",
      "EM Step: 3 Epoch: 10, Classification Loss: 1.0515158027410507\n",
      "************************************************************\n",
      "EM Step: 4 Epoch: 1, Focus Loss: 2.989362132549286\n",
      "EM Step: 4 Epoch: 1, Classification Loss: 1.0147772401571273\n",
      "************************************************************\n",
      "EM Step: 4 Epoch: 2, Focus Loss: 2.9886078953742983\n",
      "EM Step: 4 Epoch: 2, Classification Loss: 1.014162391424179\n",
      "************************************************************\n",
      "EM Step: 4 Epoch: 3, Focus Loss: 2.9881659150123596\n",
      "EM Step: 4 Epoch: 3, Classification Loss: 1.0125213146209717\n",
      "************************************************************\n",
      "EM Step: 4 Epoch: 4, Focus Loss: 2.9878647089004517\n",
      "EM Step: 4 Epoch: 4, Classification Loss: 1.0117741346359252\n",
      "************************************************************\n",
      "EM Step: 4 Epoch: 5, Focus Loss: 2.9875941276550293\n",
      "EM Step: 4 Epoch: 5, Classification Loss: 1.0100322023034096\n",
      "************************************************************\n",
      "EM Step: 4 Epoch: 6, Focus Loss: 2.9873358428478243\n",
      "EM Step: 4 Epoch: 6, Classification Loss: 1.0070583447813988\n",
      "************************************************************\n",
      "EM Step: 4 Epoch: 7, Focus Loss: 2.9871181309223176\n",
      "EM Step: 4 Epoch: 7, Classification Loss: 1.0046830579638482\n",
      "************************************************************\n",
      "EM Step: 4 Epoch: 8, Focus Loss: 2.986913967132568\n",
      "EM Step: 4 Epoch: 8, Classification Loss: 1.002844288945198\n",
      "************************************************************\n",
      "EM Step: 4 Epoch: 9, Focus Loss: 2.986706531047821\n",
      "EM Step: 4 Epoch: 9, Classification Loss: 1.0031314805150031\n",
      "************************************************************\n",
      "EM Step: 4 Epoch: 10, Focus Loss: 2.9865076422691343\n",
      "EM Step: 4 Epoch: 10, Classification Loss: 1.0048262611031533\n",
      "************************************************************\n",
      "EM Step: 5 Epoch: 1, Focus Loss: 2.9711149632930756\n",
      "EM Step: 5 Epoch: 1, Classification Loss: 0.9429113700985908\n",
      "************************************************************\n",
      "EM Step: 5 Epoch: 2, Focus Loss: 2.969089549779892\n",
      "EM Step: 5 Epoch: 2, Classification Loss: 0.9380425021052361\n",
      "************************************************************\n",
      "EM Step: 5 Epoch: 3, Focus Loss: 2.9681123077869414\n",
      "EM Step: 5 Epoch: 3, Classification Loss: 0.9341987609863281\n",
      "************************************************************\n",
      "EM Step: 5 Epoch: 4, Focus Loss: 2.96733335852623\n",
      "EM Step: 5 Epoch: 4, Classification Loss: 0.9308191001415252\n",
      "************************************************************\n",
      "EM Step: 5 Epoch: 5, Focus Loss: 2.9667777538299562\n",
      "EM Step: 5 Epoch: 5, Classification Loss: 0.9287444561719894\n",
      "************************************************************\n",
      "EM Step: 5 Epoch: 6, Focus Loss: 2.9661624312400816\n",
      "EM Step: 5 Epoch: 6, Classification Loss: 0.928968234360218\n",
      "************************************************************\n",
      "EM Step: 5 Epoch: 7, Focus Loss: 2.965632677078247\n",
      "EM Step: 5 Epoch: 7, Classification Loss: 0.9293344557285309\n",
      "************************************************************\n",
      "EM Step: 5 Epoch: 8, Focus Loss: 2.96517094373703\n",
      "EM Step: 5 Epoch: 8, Classification Loss: 0.9270211920142174\n",
      "************************************************************\n",
      "EM Step: 5 Epoch: 9, Focus Loss: 2.964572960138321\n",
      "EM Step: 5 Epoch: 9, Classification Loss: 0.9280685886740685\n",
      "************************************************************\n",
      "EM Step: 5 Epoch: 10, Focus Loss: 2.9641004502773285\n",
      "EM Step: 5 Epoch: 10, Classification Loss: 0.9294658824801445\n",
      "************************************************************\n",
      "EM Step: 6 Epoch: 1, Focus Loss: 2.9012036859989165\n",
      "EM Step: 6 Epoch: 1, Classification Loss: 0.8335247129201889\n",
      "************************************************************\n",
      "EM Step: 6 Epoch: 2, Focus Loss: 2.8971176147460938\n",
      "EM Step: 6 Epoch: 2, Classification Loss: 0.8274402782320976\n",
      "************************************************************\n",
      "EM Step: 6 Epoch: 3, Focus Loss: 2.894910377264023\n",
      "EM Step: 6 Epoch: 3, Classification Loss: 0.8254406005144119\n",
      "************************************************************\n",
      "EM Step: 6 Epoch: 4, Focus Loss: 2.8901472687721252\n",
      "EM Step: 6 Epoch: 4, Classification Loss: 0.8241394698619843\n",
      "************************************************************\n",
      "EM Step: 6 Epoch: 5, Focus Loss: 2.888540357351303\n",
      "EM Step: 6 Epoch: 5, Classification Loss: 0.8269009724259376\n",
      "************************************************************\n",
      "EM Step: 6 Epoch: 6, Focus Loss: 2.8881639778614043\n",
      "EM Step: 6 Epoch: 6, Classification Loss: 0.8276991426944733\n",
      "************************************************************\n",
      "EM Step: 6 Epoch: 7, Focus Loss: 2.888931769132614\n",
      "EM Step: 6 Epoch: 7, Classification Loss: 0.8290332108736038\n",
      "************************************************************\n",
      "EM Step: 6 Epoch: 8, Focus Loss: 2.886397135257721\n",
      "EM Step: 6 Epoch: 8, Classification Loss: 0.8323273599147797\n",
      "************************************************************\n",
      "EM Step: 6 Epoch: 9, Focus Loss: 2.884870767593384\n",
      "EM Step: 6 Epoch: 9, Classification Loss: 0.8322404742240905\n",
      "************************************************************\n",
      "EM Step: 6 Epoch: 10, Focus Loss: 2.8847665071487425\n",
      "EM Step: 6 Epoch: 10, Classification Loss: 0.8257336050271988\n",
      "************************************************************\n",
      "EM Step: 7 Epoch: 1, Focus Loss: 2.744201362133026\n",
      "EM Step: 7 Epoch: 1, Classification Loss: 0.6724275916814804\n",
      "************************************************************\n",
      "EM Step: 7 Epoch: 2, Focus Loss: 2.739125061035156\n",
      "EM Step: 7 Epoch: 2, Classification Loss: 0.6674795418977737\n",
      "************************************************************\n",
      "EM Step: 7 Epoch: 3, Focus Loss: 2.7393101036548613\n",
      "EM Step: 7 Epoch: 3, Classification Loss: 0.6726358905434608\n",
      "************************************************************\n",
      "EM Step: 7 Epoch: 4, Focus Loss: 2.733535611629486\n",
      "EM Step: 7 Epoch: 4, Classification Loss: 0.6718375936150551\n",
      "************************************************************\n",
      "EM Step: 7 Epoch: 5, Focus Loss: 2.7316325902938843\n",
      "EM Step: 7 Epoch: 5, Classification Loss: 0.670386104285717\n",
      "************************************************************\n",
      "EM Step: 7 Epoch: 6, Focus Loss: 2.7349873423576354\n",
      "EM Step: 7 Epoch: 6, Classification Loss: 0.6706276297569275\n",
      "************************************************************\n",
      "EM Step: 7 Epoch: 7, Focus Loss: 2.7358372271060944\n",
      "EM Step: 7 Epoch: 7, Classification Loss: 0.6687829673290253\n",
      "************************************************************\n",
      "EM Step: 7 Epoch: 8, Focus Loss: 2.7334238052368165\n",
      "EM Step: 7 Epoch: 8, Classification Loss: 0.6674084305763245\n",
      "************************************************************\n",
      "EM Step: 7 Epoch: 9, Focus Loss: 2.7291555225849153\n",
      "EM Step: 7 Epoch: 9, Classification Loss: 0.6680604085326195\n",
      "************************************************************\n",
      "EM Step: 7 Epoch: 10, Focus Loss: 2.72717467546463\n",
      "EM Step: 7 Epoch: 10, Classification Loss: 0.6677072927355766\n",
      "************************************************************\n",
      "EM Step: 8 Epoch: 1, Focus Loss: 2.5771047711372375\n",
      "EM Step: 8 Epoch: 1, Classification Loss: 0.5354310899972916\n",
      "************************************************************\n",
      "EM Step: 8 Epoch: 2, Focus Loss: 2.5700628995895385\n",
      "EM Step: 8 Epoch: 2, Classification Loss: 0.5309897184371948\n",
      "************************************************************\n",
      "EM Step: 8 Epoch: 3, Focus Loss: 2.580786281824112\n",
      "EM Step: 8 Epoch: 3, Classification Loss: 0.5333615466952324\n",
      "************************************************************\n",
      "EM Step: 8 Epoch: 4, Focus Loss: 2.5779126226902007\n",
      "EM Step: 8 Epoch: 4, Classification Loss: 0.5419953897595405\n",
      "************************************************************\n",
      "EM Step: 8 Epoch: 5, Focus Loss: 2.5740902423858643\n",
      "EM Step: 8 Epoch: 5, Classification Loss: 0.5548568323254586\n",
      "************************************************************\n",
      "EM Step: 8 Epoch: 6, Focus Loss: 2.571702653169632\n",
      "EM Step: 8 Epoch: 6, Classification Loss: 0.5649899855256081\n",
      "************************************************************\n",
      "EM Step: 8 Epoch: 7, Focus Loss: 2.569589000940323\n",
      "EM Step: 8 Epoch: 7, Classification Loss: 0.5547229170799255\n",
      "************************************************************\n",
      "EM Step: 8 Epoch: 8, Focus Loss: 2.5652889013290405\n",
      "EM Step: 8 Epoch: 8, Classification Loss: 0.5539416566491127\n",
      "************************************************************\n",
      "EM Step: 8 Epoch: 9, Focus Loss: 2.5607234656810762\n",
      "EM Step: 8 Epoch: 9, Classification Loss: 0.550553098320961\n",
      "************************************************************\n",
      "EM Step: 8 Epoch: 10, Focus Loss: 2.5587152540683746\n",
      "EM Step: 8 Epoch: 10, Classification Loss: 0.5591432452201843\n",
      "************************************************************\n",
      "EM Step: 9 Epoch: 1, Focus Loss: 2.3024980306625364\n",
      "EM Step: 9 Epoch: 1, Classification Loss: 0.4096864081919193\n",
      "************************************************************\n",
      "EM Step: 9 Epoch: 2, Focus Loss: 2.3027089297771455\n",
      "EM Step: 9 Epoch: 2, Classification Loss: 0.405831515789032\n",
      "************************************************************\n",
      "EM Step: 9 Epoch: 3, Focus Loss: 2.30994907617569\n",
      "EM Step: 9 Epoch: 3, Classification Loss: 0.4062662199139595\n",
      "************************************************************\n",
      "EM Step: 9 Epoch: 4, Focus Loss: 2.307932609319687\n",
      "EM Step: 9 Epoch: 4, Classification Loss: 0.4159612126648426\n",
      "************************************************************\n",
      "EM Step: 9 Epoch: 5, Focus Loss: 2.3069263994693756\n",
      "EM Step: 9 Epoch: 5, Classification Loss: 0.4278110861778259\n",
      "************************************************************\n",
      "EM Step: 9 Epoch: 6, Focus Loss: 2.306929385662079\n",
      "EM Step: 9 Epoch: 6, Classification Loss: 0.428659250587225\n",
      "************************************************************\n",
      "EM Step: 9 Epoch: 7, Focus Loss: 2.305344784259796\n",
      "EM Step: 9 Epoch: 7, Classification Loss: 0.4316365994513035\n",
      "************************************************************\n",
      "EM Step: 9 Epoch: 8, Focus Loss: 2.302768164873123\n",
      "EM Step: 9 Epoch: 8, Classification Loss: 0.43882409259676936\n",
      "************************************************************\n",
      "EM Step: 9 Epoch: 9, Focus Loss: 2.3022135615348818\n",
      "EM Step: 9 Epoch: 9, Classification Loss: 0.42732231244444846\n",
      "************************************************************\n",
      "EM Step: 9 Epoch: 10, Focus Loss: 2.3005600929260255\n",
      "EM Step: 9 Epoch: 10, Classification Loss: 0.4211647056043148\n",
      "************************************************************\n",
      "EM Step: 10 Epoch: 1, Focus Loss: 1.9308667540550233\n",
      "EM Step: 10 Epoch: 1, Classification Loss: 0.28297974839806556\n",
      "************************************************************\n",
      "EM Step: 10 Epoch: 2, Focus Loss: 1.9318574011325835\n",
      "EM Step: 10 Epoch: 2, Classification Loss: 0.2828990384936333\n",
      "************************************************************\n",
      "EM Step: 10 Epoch: 3, Focus Loss: 1.9421320289373398\n",
      "EM Step: 10 Epoch: 3, Classification Loss: 0.2848175935447216\n",
      "************************************************************\n",
      "EM Step: 10 Epoch: 4, Focus Loss: 1.9432429313659667\n",
      "EM Step: 10 Epoch: 4, Classification Loss: 0.28896970823407175\n",
      "************************************************************\n",
      "EM Step: 10 Epoch: 5, Focus Loss: 1.937948739528656\n",
      "EM Step: 10 Epoch: 5, Classification Loss: 0.2864538684487343\n",
      "************************************************************\n",
      "EM Step: 10 Epoch: 6, Focus Loss: 1.9297556281089783\n",
      "EM Step: 10 Epoch: 6, Classification Loss: 0.28493662402033804\n",
      "************************************************************\n",
      "EM Step: 10 Epoch: 7, Focus Loss: 1.9273698836565019\n",
      "EM Step: 10 Epoch: 7, Classification Loss: 0.29466345757246015\n",
      "************************************************************\n",
      "EM Step: 10 Epoch: 8, Focus Loss: 1.9292079746723174\n",
      "EM Step: 10 Epoch: 8, Classification Loss: 0.3061758831143379\n",
      "************************************************************\n",
      "EM Step: 10 Epoch: 9, Focus Loss: 1.9253836959600448\n",
      "EM Step: 10 Epoch: 9, Classification Loss: 0.31057300716638564\n",
      "************************************************************\n",
      "EM Step: 10 Epoch: 10, Focus Loss: 1.9205817967653274\n",
      "EM Step: 10 Epoch: 10, Classification Loss: 0.3141322910785675\n",
      "************************************************************\n",
      "EM Step: 11 Epoch: 1, Focus Loss: 1.6010133653879166\n",
      "EM Step: 11 Epoch: 1, Classification Loss: 0.22436099648475646\n",
      "************************************************************\n",
      "EM Step: 11 Epoch: 2, Focus Loss: 1.6043691605329513\n",
      "EM Step: 11 Epoch: 2, Classification Loss: 0.25221906155347823\n",
      "************************************************************\n",
      "EM Step: 11 Epoch: 3, Focus Loss: 1.611246833205223\n",
      "EM Step: 11 Epoch: 3, Classification Loss: 0.23731458857655524\n",
      "************************************************************\n",
      "EM Step: 11 Epoch: 4, Focus Loss: 1.61199993789196\n",
      "EM Step: 11 Epoch: 4, Classification Loss: 0.22959320656955243\n",
      "************************************************************\n",
      "EM Step: 11 Epoch: 5, Focus Loss: 1.6096508860588075\n",
      "EM Step: 11 Epoch: 5, Classification Loss: 0.21994810365140438\n",
      "************************************************************\n",
      "EM Step: 11 Epoch: 6, Focus Loss: 1.6059059739112853\n",
      "EM Step: 11 Epoch: 6, Classification Loss: 0.20944341830909252\n",
      "************************************************************\n",
      "EM Step: 11 Epoch: 7, Focus Loss: 1.6017887502908708\n",
      "EM Step: 11 Epoch: 7, Classification Loss: 0.2053020115941763\n",
      "************************************************************\n",
      "EM Step: 11 Epoch: 8, Focus Loss: 1.6004777610301972\n",
      "EM Step: 11 Epoch: 8, Classification Loss: 0.2035657547414303\n",
      "************************************************************\n",
      "EM Step: 11 Epoch: 9, Focus Loss: 1.605348601937294\n",
      "EM Step: 11 Epoch: 9, Classification Loss: 0.19494624324142934\n",
      "************************************************************\n",
      "EM Step: 11 Epoch: 10, Focus Loss: 1.6079782843589783\n",
      "EM Step: 11 Epoch: 10, Classification Loss: 0.1909149255603552\n",
      "************************************************************\n",
      "EM Step: 12 Epoch: 1, Focus Loss: 1.3185466527938843\n",
      "EM Step: 12 Epoch: 1, Classification Loss: 0.12333017066121102\n",
      "************************************************************\n",
      "EM Step: 12 Epoch: 2, Focus Loss: 1.3137045174837112\n",
      "EM Step: 12 Epoch: 2, Classification Loss: 0.12742882482707502\n",
      "************************************************************\n",
      "EM Step: 12 Epoch: 3, Focus Loss: 1.3197373509407044\n",
      "EM Step: 12 Epoch: 3, Classification Loss: 0.12649802789092063\n",
      "************************************************************\n",
      "EM Step: 12 Epoch: 4, Focus Loss: 1.3301570177078248\n",
      "EM Step: 12 Epoch: 4, Classification Loss: 0.126036280952394\n",
      "************************************************************\n",
      "EM Step: 12 Epoch: 5, Focus Loss: 1.3332606673240661\n",
      "EM Step: 12 Epoch: 5, Classification Loss: 0.12768990490585566\n",
      "************************************************************\n",
      "EM Step: 12 Epoch: 6, Focus Loss: 1.325103759765625\n",
      "EM Step: 12 Epoch: 6, Classification Loss: 0.1322260182350874\n",
      "************************************************************\n",
      "EM Step: 12 Epoch: 7, Focus Loss: 1.3246672332286835\n",
      "EM Step: 12 Epoch: 7, Classification Loss: 0.13053095918148755\n",
      "************************************************************\n",
      "EM Step: 12 Epoch: 8, Focus Loss: 1.3268749207258224\n",
      "EM Step: 12 Epoch: 8, Classification Loss: 0.12668084539473057\n",
      "************************************************************\n",
      "EM Step: 12 Epoch: 9, Focus Loss: 1.3291322946548463\n",
      "EM Step: 12 Epoch: 9, Classification Loss: 0.12468385603278875\n",
      "************************************************************\n",
      "EM Step: 12 Epoch: 10, Focus Loss: 1.323396596312523\n",
      "EM Step: 12 Epoch: 10, Classification Loss: 0.12533442601561545\n",
      "************************************************************\n",
      "EM Step: 13 Epoch: 1, Focus Loss: 1.093198874592781\n",
      "EM Step: 13 Epoch: 1, Classification Loss: 0.0882227834314108\n",
      "************************************************************\n",
      "EM Step: 13 Epoch: 2, Focus Loss: 1.0992320656776429\n",
      "EM Step: 13 Epoch: 2, Classification Loss: 0.09078618604689837\n",
      "************************************************************\n",
      "EM Step: 13 Epoch: 3, Focus Loss: 1.1155590072274209\n",
      "EM Step: 13 Epoch: 3, Classification Loss: 0.09102643709629774\n",
      "************************************************************\n",
      "EM Step: 13 Epoch: 4, Focus Loss: 1.1206113174557686\n",
      "EM Step: 13 Epoch: 4, Classification Loss: 0.09156243707984686\n",
      "************************************************************\n",
      "EM Step: 13 Epoch: 5, Focus Loss: 1.1229424715042113\n",
      "EM Step: 13 Epoch: 5, Classification Loss: 0.10547039415687323\n",
      "************************************************************\n",
      "EM Step: 13 Epoch: 6, Focus Loss: 1.1241531386971473\n",
      "EM Step: 13 Epoch: 6, Classification Loss: 0.1112309791147709\n",
      "************************************************************\n",
      "EM Step: 13 Epoch: 7, Focus Loss: 1.1258288785815238\n",
      "EM Step: 13 Epoch: 7, Classification Loss: 0.11011081803590059\n",
      "************************************************************\n",
      "EM Step: 13 Epoch: 8, Focus Loss: 1.111719538271427\n",
      "EM Step: 13 Epoch: 8, Classification Loss: 0.10542571414262056\n",
      "************************************************************\n",
      "EM Step: 13 Epoch: 9, Focus Loss: 1.0995566323399544\n",
      "EM Step: 13 Epoch: 9, Classification Loss: 0.09844914861023427\n",
      "************************************************************\n",
      "EM Step: 13 Epoch: 10, Focus Loss: 1.0944022342562676\n",
      "EM Step: 13 Epoch: 10, Classification Loss: 0.09316814728081227\n",
      "************************************************************\n",
      "EM Step: 14 Epoch: 1, Focus Loss: 0.868662816286087\n",
      "EM Step: 14 Epoch: 1, Classification Loss: 0.0568731376901269\n",
      "************************************************************\n",
      "EM Step: 14 Epoch: 2, Focus Loss: 0.8736867159605026\n",
      "EM Step: 14 Epoch: 2, Classification Loss: 0.05984201794490218\n",
      "************************************************************\n",
      "EM Step: 14 Epoch: 3, Focus Loss: 0.8816917136311531\n",
      "EM Step: 14 Epoch: 3, Classification Loss: 0.06360830422490835\n",
      "************************************************************\n",
      "EM Step: 14 Epoch: 4, Focus Loss: 0.8940617516636848\n",
      "EM Step: 14 Epoch: 4, Classification Loss: 0.0695020298473537\n",
      "************************************************************\n",
      "EM Step: 14 Epoch: 5, Focus Loss: 0.8938098326325417\n",
      "EM Step: 14 Epoch: 5, Classification Loss: 0.06614016555249691\n",
      "************************************************************\n",
      "EM Step: 14 Epoch: 6, Focus Loss: 0.8952362477779389\n",
      "EM Step: 14 Epoch: 6, Classification Loss: 0.07539593111723661\n",
      "************************************************************\n",
      "EM Step: 14 Epoch: 7, Focus Loss: 0.8923514664173127\n",
      "EM Step: 14 Epoch: 7, Classification Loss: 0.07755353953689337\n",
      "************************************************************\n",
      "EM Step: 14 Epoch: 8, Focus Loss: 0.8913765951991082\n",
      "EM Step: 14 Epoch: 8, Classification Loss: 0.0832004651427269\n",
      "************************************************************\n",
      "EM Step: 14 Epoch: 9, Focus Loss: 0.88533695936203\n",
      "EM Step: 14 Epoch: 9, Classification Loss: 0.06896563246846199\n",
      "************************************************************\n",
      "EM Step: 14 Epoch: 10, Focus Loss: 0.8819413900375366\n",
      "EM Step: 14 Epoch: 10, Classification Loss: 0.06869109366089106\n",
      "************************************************************\n",
      "EM Step: 15 Epoch: 1, Focus Loss: 0.7401422321796417\n",
      "EM Step: 15 Epoch: 1, Classification Loss: 0.039324960811063646\n",
      "************************************************************\n",
      "EM Step: 15 Epoch: 2, Focus Loss: 0.7411393836140633\n",
      "EM Step: 15 Epoch: 2, Classification Loss: 0.05746996877714992\n",
      "************************************************************\n",
      "EM Step: 15 Epoch: 3, Focus Loss: 0.7478544324636459\n",
      "EM Step: 15 Epoch: 3, Classification Loss: 0.06802147980779409\n",
      "************************************************************\n",
      "EM Step: 15 Epoch: 4, Focus Loss: 0.7474558293819428\n",
      "EM Step: 15 Epoch: 4, Classification Loss: 0.06135482136160135\n",
      "************************************************************\n",
      "EM Step: 15 Epoch: 5, Focus Loss: 0.7511645779013634\n",
      "EM Step: 15 Epoch: 5, Classification Loss: 0.05222454536706209\n",
      "************************************************************\n",
      "EM Step: 15 Epoch: 6, Focus Loss: 0.7502666518092156\n",
      "EM Step: 15 Epoch: 6, Classification Loss: 0.05450113611295819\n",
      "************************************************************\n",
      "EM Step: 15 Epoch: 7, Focus Loss: 0.7475021615624428\n",
      "EM Step: 15 Epoch: 7, Classification Loss: 0.04816310163587332\n",
      "************************************************************\n",
      "EM Step: 15 Epoch: 8, Focus Loss: 0.7419585540890694\n",
      "EM Step: 15 Epoch: 8, Classification Loss: 0.04592648381367326\n",
      "************************************************************\n",
      "EM Step: 15 Epoch: 9, Focus Loss: 0.7400152385234833\n",
      "EM Step: 15 Epoch: 9, Classification Loss: 0.040152547089383005\n",
      "************************************************************\n",
      "EM Step: 15 Epoch: 10, Focus Loss: 0.7377951607108116\n",
      "EM Step: 15 Epoch: 10, Classification Loss: 0.038097055675461886\n",
      "************************************************************\n",
      "EM Step: 16 Epoch: 1, Focus Loss: 0.6925960421562195\n",
      "EM Step: 16 Epoch: 1, Classification Loss: 0.024596909899264573\n",
      "************************************************************\n",
      "EM Step: 16 Epoch: 2, Focus Loss: 0.6872423082590103\n",
      "EM Step: 16 Epoch: 2, Classification Loss: 0.028256334364414215\n",
      "************************************************************\n",
      "EM Step: 16 Epoch: 3, Focus Loss: 0.6862399607896805\n",
      "EM Step: 16 Epoch: 3, Classification Loss: 0.02560624382458627\n",
      "************************************************************\n",
      "EM Step: 16 Epoch: 4, Focus Loss: 0.689705128967762\n",
      "EM Step: 16 Epoch: 4, Classification Loss: 0.023594242543913423\n",
      "************************************************************\n",
      "EM Step: 16 Epoch: 5, Focus Loss: 0.6947400361299515\n",
      "EM Step: 16 Epoch: 5, Classification Loss: 0.023589839064516126\n",
      "************************************************************\n",
      "EM Step: 16 Epoch: 6, Focus Loss: 0.6952174127101898\n",
      "EM Step: 16 Epoch: 6, Classification Loss: 0.02236991000827402\n",
      "************************************************************\n",
      "EM Step: 16 Epoch: 7, Focus Loss: 0.6970541074872016\n",
      "EM Step: 16 Epoch: 7, Classification Loss: 0.021477229124866427\n",
      "************************************************************\n",
      "EM Step: 16 Epoch: 8, Focus Loss: 0.6961826592683792\n",
      "EM Step: 16 Epoch: 8, Classification Loss: 0.02102588696870953\n",
      "************************************************************\n",
      "EM Step: 16 Epoch: 9, Focus Loss: 0.6915533393621445\n",
      "EM Step: 16 Epoch: 9, Classification Loss: 0.021186068980023266\n",
      "************************************************************\n",
      "EM Step: 16 Epoch: 10, Focus Loss: 0.6938360214233399\n",
      "EM Step: 16 Epoch: 10, Classification Loss: 0.021043132245540618\n",
      "************************************************************\n",
      "EM Step: 17 Epoch: 1, Focus Loss: 0.5926151275634766\n",
      "EM Step: 17 Epoch: 1, Classification Loss: 0.015272215229924769\n",
      "************************************************************\n",
      "EM Step: 17 Epoch: 2, Focus Loss: 0.5907238699495793\n",
      "EM Step: 17 Epoch: 2, Classification Loss: 0.01660622252384201\n",
      "************************************************************\n",
      "EM Step: 17 Epoch: 3, Focus Loss: 0.5951952457427978\n",
      "EM Step: 17 Epoch: 3, Classification Loss: 0.015628173691220583\n",
      "************************************************************\n",
      "EM Step: 17 Epoch: 4, Focus Loss: 0.5938425861299038\n",
      "EM Step: 17 Epoch: 4, Classification Loss: 0.01463546467712149\n",
      "************************************************************\n",
      "EM Step: 17 Epoch: 5, Focus Loss: 0.5935022182762623\n",
      "EM Step: 17 Epoch: 5, Classification Loss: 0.014804384822491556\n",
      "************************************************************\n",
      "EM Step: 17 Epoch: 6, Focus Loss: 0.5925374895334243\n",
      "EM Step: 17 Epoch: 6, Classification Loss: 0.014901586249470711\n",
      "************************************************************\n",
      "EM Step: 17 Epoch: 7, Focus Loss: 0.5906026057898999\n",
      "EM Step: 17 Epoch: 7, Classification Loss: 0.014198033371940255\n",
      "************************************************************\n",
      "EM Step: 17 Epoch: 8, Focus Loss: 0.5904666088521481\n",
      "EM Step: 17 Epoch: 8, Classification Loss: 0.014175209251698106\n",
      "************************************************************\n",
      "EM Step: 17 Epoch: 9, Focus Loss: 0.5950895138084888\n",
      "EM Step: 17 Epoch: 9, Classification Loss: 0.013725714001338929\n",
      "************************************************************\n",
      "EM Step: 17 Epoch: 10, Focus Loss: 0.5935661442577839\n",
      "EM Step: 17 Epoch: 10, Classification Loss: 0.013883453409653157\n",
      "************************************************************\n",
      "EM Step: 18 Epoch: 1, Focus Loss: 0.5464873254299164\n",
      "EM Step: 18 Epoch: 1, Classification Loss: 0.012582620838657022\n",
      "************************************************************\n",
      "EM Step: 18 Epoch: 2, Focus Loss: 0.5438319839537143\n",
      "EM Step: 18 Epoch: 2, Classification Loss: 0.014105185749940574\n",
      "************************************************************\n",
      "EM Step: 18 Epoch: 3, Focus Loss: 0.5437212944030761\n",
      "EM Step: 18 Epoch: 3, Classification Loss: 0.013650123553816229\n",
      "************************************************************\n",
      "EM Step: 18 Epoch: 4, Focus Loss: 0.5441261105239391\n",
      "EM Step: 18 Epoch: 4, Classification Loss: 0.012579797755461187\n",
      "************************************************************\n",
      "EM Step: 18 Epoch: 5, Focus Loss: 0.5454114012420177\n",
      "EM Step: 18 Epoch: 5, Classification Loss: 0.011666033405344934\n",
      "************************************************************\n",
      "EM Step: 18 Epoch: 6, Focus Loss: 0.5502329044044018\n",
      "EM Step: 18 Epoch: 6, Classification Loss: 0.01179932978702709\n",
      "************************************************************\n",
      "EM Step: 18 Epoch: 7, Focus Loss: 0.5517363786697388\n",
      "EM Step: 18 Epoch: 7, Classification Loss: 0.011362951400224119\n",
      "************************************************************\n",
      "EM Step: 18 Epoch: 8, Focus Loss: 0.5524440862238407\n",
      "EM Step: 18 Epoch: 8, Classification Loss: 0.011246315063908696\n",
      "************************************************************\n",
      "EM Step: 18 Epoch: 9, Focus Loss: 0.5506204351782799\n",
      "EM Step: 18 Epoch: 9, Classification Loss: 0.011109629611019045\n",
      "************************************************************\n",
      "EM Step: 18 Epoch: 10, Focus Loss: 0.5483831860125065\n",
      "EM Step: 18 Epoch: 10, Classification Loss: 0.011032795999199152\n",
      "************************************************************\n",
      "EM Step: 19 Epoch: 1, Focus Loss: 0.455891315639019\n",
      "EM Step: 19 Epoch: 1, Classification Loss: 0.009513600246282294\n",
      "************************************************************\n",
      "EM Step: 19 Epoch: 2, Focus Loss: 0.4582978345453739\n",
      "EM Step: 19 Epoch: 2, Classification Loss: 0.009920363302808256\n",
      "************************************************************\n",
      "EM Step: 19 Epoch: 3, Focus Loss: 0.4657746009528637\n",
      "EM Step: 19 Epoch: 3, Classification Loss: 0.010753167857183143\n",
      "************************************************************\n",
      "EM Step: 19 Epoch: 4, Focus Loss: 0.4709771327674389\n",
      "EM Step: 19 Epoch: 4, Classification Loss: 0.010838527453597636\n",
      "************************************************************\n",
      "EM Step: 19 Epoch: 5, Focus Loss: 0.4668236330151558\n",
      "EM Step: 19 Epoch: 5, Classification Loss: 0.010560790385352448\n",
      "************************************************************\n",
      "EM Step: 19 Epoch: 6, Focus Loss: 0.46652494817972184\n",
      "EM Step: 19 Epoch: 6, Classification Loss: 0.009554834733717144\n",
      "************************************************************\n",
      "EM Step: 19 Epoch: 7, Focus Loss: 0.4681706115603447\n",
      "EM Step: 19 Epoch: 7, Classification Loss: 0.010054257471347227\n",
      "************************************************************\n",
      "EM Step: 19 Epoch: 8, Focus Loss: 0.4675611600279808\n",
      "EM Step: 19 Epoch: 8, Classification Loss: 0.011033373000100256\n",
      "************************************************************\n",
      "EM Step: 19 Epoch: 9, Focus Loss: 0.46512804478406905\n",
      "EM Step: 19 Epoch: 9, Classification Loss: 0.011225710849976167\n",
      "************************************************************\n",
      "EM Step: 19 Epoch: 10, Focus Loss: 0.46375901699066163\n",
      "EM Step: 19 Epoch: 10, Classification Loss: 0.010707335465122014\n",
      "************************************************************\n",
      "EM Step: 20 Epoch: 1, Focus Loss: 0.4574498899281025\n",
      "EM Step: 20 Epoch: 1, Classification Loss: 0.008203848276752978\n",
      "************************************************************\n",
      "EM Step: 20 Epoch: 2, Focus Loss: 0.4570735529065132\n",
      "EM Step: 20 Epoch: 2, Classification Loss: 0.009102283255197107\n",
      "************************************************************\n",
      "EM Step: 20 Epoch: 3, Focus Loss: 0.45910277366638186\n",
      "EM Step: 20 Epoch: 3, Classification Loss: 0.008397266134852543\n",
      "************************************************************\n",
      "EM Step: 20 Epoch: 4, Focus Loss: 0.45720576494932175\n",
      "EM Step: 20 Epoch: 4, Classification Loss: 0.008536375331459567\n",
      "************************************************************\n",
      "EM Step: 20 Epoch: 5, Focus Loss: 0.4563616104424\n",
      "EM Step: 20 Epoch: 5, Classification Loss: 0.008231636544223875\n",
      "************************************************************\n",
      "EM Step: 20 Epoch: 6, Focus Loss: 0.4569693088531494\n",
      "EM Step: 20 Epoch: 6, Classification Loss: 0.00791589574655518\n",
      "************************************************************\n",
      "EM Step: 20 Epoch: 7, Focus Loss: 0.45709014162421224\n",
      "EM Step: 20 Epoch: 7, Classification Loss: 0.008039480837760493\n",
      "************************************************************\n",
      "EM Step: 20 Epoch: 8, Focus Loss: 0.45707737654447556\n",
      "EM Step: 20 Epoch: 8, Classification Loss: 0.007761972321895882\n",
      "************************************************************\n",
      "EM Step: 20 Epoch: 9, Focus Loss: 0.457109035551548\n",
      "EM Step: 20 Epoch: 9, Classification Loss: 0.00766087073716335\n",
      "************************************************************\n",
      "EM Step: 20 Epoch: 10, Focus Loss: 0.45734666734933854\n",
      "EM Step: 20 Epoch: 10, Classification Loss: 0.007651461916975677\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(12)\n",
    "focus = Focus()\n",
    "focus = focus.to(device)\n",
    "\n",
    "\n",
    "torch.manual_seed(12)\n",
    "classification = Classification()\n",
    "classification = classification.to(device)\n",
    "\n",
    "\n",
    "Criterion = nn.CrossEntropyLoss(reduction=\"none\") #nn.BCELoss(reduction=\"none\")\n",
    "\n",
    "\n",
    "em_steps = 20\n",
    "\n",
    "\n",
    "for i in range(em_steps):\n",
    "    # calculate p_z\n",
    "    gamma_ = []\n",
    "    focus_optimizer = optim.SGD(focus.parameters(), lr=0.07,momentum=0.9)\n",
    "    classification_optimizer = optim.SGD(classification.parameters(),lr=0.07,momentum=0.9)\n",
    "    for j,data in enumerate(train_loader):\n",
    "        images,labels,foreground_index = data\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        p_z = expectation_step(focus,classification,images,labels)\n",
    "        gamma_.append(p_z)\n",
    "    for epoch in range(10):\n",
    "        focus_epoch_loss = []\n",
    "        classification_epoch_loss = []\n",
    "        for j,data in enumerate(train_loader):\n",
    "            images,labels,foreground_index = data\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            focus,classification,focus_optimizer,classification_optimizer=maximization_step(gamma_[j],\n",
    "                                                                                            focus,classification,\n",
    "                                                                                            images,labels,\n",
    "                                                                                            focus_optimizer,\n",
    "                                                                                            classification_optimizer,\n",
    "                                                                                            Criterion)\n",
    "            with torch.no_grad():\n",
    "                batch = images.size(0)\n",
    "                patches = images.size(1)\n",
    "                focus_outputs = focus(images)\n",
    "                images = images.reshape(batch*patches,3,32,32)\n",
    "                classification_outputs = classification(images) # classification returns output after sigmoid/softmax]\n",
    "                classification_outputs = classification_outputs.reshape(batch,patches,3)\n",
    "                #print(focus_outputs,classification_outputs)\n",
    "                loss_focus = calculate_loss_focus(gamma_[j],focus_outputs)\n",
    "                loss_classification = calculate_loss_classification(gamma_[j],classification_outputs,\n",
    "                                                        labels,Criterion,patches)\n",
    "                focus_epoch_loss.append(loss_focus.item())\n",
    "                classification_epoch_loss.append(loss_classification.item())\n",
    "        print(\"*\"*60)\n",
    "        print(\"EM Step: \"+str(i+1)+\" Epoch: \" + str(epoch+1)+\", Focus Loss: \"+str(np.mean(focus_epoch_loss)))\n",
    "        print(\"EM Step: \"+str(i+1)+\" Epoch: \" + str(epoch+1)+\", Classification Loss: \"+str(np.mean(classification_epoch_loss))) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd28cca",
   "metadata": {
    "papermill": {
     "duration": 0.166012,
     "end_time": "2022-03-21T04:33:13.058963",
     "exception": false,
     "start_time": "2022-03-21T04:33:12.892951",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "44d53ec5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-21T04:33:13.400994Z",
     "iopub.status.busy": "2022-03-21T04:33:13.400141Z",
     "iopub.status.idle": "2022-03-21T04:33:15.589478Z",
     "shell.execute_reply": "2022-03-21T04:33:15.588815Z",
     "shell.execute_reply.started": "2022-03-20T09:05:57.376590Z"
    },
    "papermill": {
     "duration": 2.364601,
     "end_time": "2022-03-21T04:33:15.589650",
     "exception": false,
     "start_time": "2022-03-21T04:33:13.225049",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Focus True 49.78\n",
      "Accuracy 99.03\n"
     ]
    }
   ],
   "source": [
    "# method 1\n",
    "predicted_indexes = []\n",
    "foreground_index_list = []\n",
    "prediction_list = []\n",
    "labels_list = []\n",
    "with torch.no_grad():\n",
    "    for j,data in enumerate(train_loader):\n",
    "        images,labels,foreground_index = data\n",
    "        images = images.to(device)\n",
    "        foreground_index_list.append(foreground_index.numpy())\n",
    "        labels_list.append(labels.numpy())\n",
    "        batch = images.size(0)\n",
    "        scores = focus(images)\n",
    "        indexes = torch.argmax(F.softmax(scores,dim=1),dim=1).cpu().numpy()\n",
    "        predicted_indexes.append(indexes)\n",
    "        outputs = F.softmax(classification(images[np.arange(batch),indexes,:]),dim=1)\n",
    "        prediction = torch.argmax(outputs,dim=1)\n",
    "        prediction_list.append(prediction.cpu().numpy())\n",
    "    \n",
    "predicted_indexes = np.concatenate(predicted_indexes,axis=0)\n",
    "foreground_index_list = np.concatenate(foreground_index_list,axis=0)\n",
    "prediction_list = np.concatenate(prediction_list,axis=0)\n",
    "labels_list = np.concatenate(labels_list,axis=0)\n",
    "    \n",
    "\n",
    "print(\"Focus True\",(np.sum(predicted_indexes == foreground_index_list,axis=0).item()/\n",
    "                    len(foreground_index_list))*100)\n",
    "accuracy = (np.sum(prediction_list == labels_list,axis=0)/len(labels_list) )*100\n",
    "print(\"Accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2c261cb9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-21T04:33:16.224419Z",
     "iopub.status.busy": "2022-03-21T04:33:16.218290Z",
     "iopub.status.idle": "2022-03-21T04:33:18.685291Z",
     "shell.execute_reply": "2022-03-21T04:33:18.684750Z",
     "shell.execute_reply.started": "2022-03-20T09:06:00.879083Z"
    },
    "papermill": {
     "duration": 2.774505,
     "end_time": "2022-03-21T04:33:18.685412",
     "exception": false,
     "start_time": "2022-03-21T04:33:15.910907",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Focus True 49.78\n",
      "Accuracy 99.25\n"
     ]
    }
   ],
   "source": [
    "# method 2\n",
    "predicted_indexes = []\n",
    "foreground_index_list = []\n",
    "prediction_list = []\n",
    "labels_list = []\n",
    "with torch.no_grad():\n",
    "    for j,data in enumerate(train_loader):\n",
    "        images,labels,foreground_index = data\n",
    "        images = images.to(device)\n",
    "        batch = images.size(0)\n",
    "        patches = images.size(1)\n",
    "        foreground_index_list.append(foreground_index.numpy())\n",
    "        labels_list.append(labels.numpy())\n",
    "        batch = images.size(0)\n",
    "        focus_outputs = F.softmax(focus(images),dim=1)\n",
    "        indexes = torch.argmax(focus_outputs,dim=1).cpu().numpy()\n",
    "        predicted_indexes.append(indexes)\n",
    "        images = images.reshape(batch*patches,3,32,32)\n",
    "        classification_outputs = F.softmax(classification(images),dim=1)\n",
    "        classification_outputs = classification_outputs.reshape(batch,patches,3)\n",
    "\n",
    "        #print(classification_outputs.shape,focus_outputs.shape)\n",
    "\n",
    "        prediction = torch.argmax(torch.sum(focus_outputs[:,:,None]*classification_outputs,dim=1),dim=1)\n",
    "        prediction_list.append(prediction.cpu().numpy())\n",
    "    \n",
    "predicted_indexes = np.concatenate(predicted_indexes,axis=0)\n",
    "foreground_index_list = np.concatenate(foreground_index_list,axis=0)\n",
    "prediction_list = np.concatenate(prediction_list,axis=0)\n",
    "labels_list = np.concatenate(labels_list,axis=0)\n",
    "    \n",
    "\n",
    "print(\"Focus True\",(np.sum(predicted_indexes == foreground_index_list,axis=0).item()/\n",
    "                    len(foreground_index_list))*100)\n",
    "accuracy = (np.sum(prediction_list == labels_list,axis=0)/len(labels_list) )*100\n",
    "print(\"Accuracy\", accuracy)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# method 2\n",
    "# focus_output = F.softmax(focus(data),dim=1)\n",
    "# indexes = torch.argmax(F.softmax(focus(data),dim=1),dim=1)[:,0].numpy()\n",
    "# classification_output = F.softmax(classification(data),dim=2)\n",
    "# print(\"Focus True\",(np.sum(indexes == fore_idx,axis=0).item()/len(fore_idx))*100)\n",
    "# prediction = torch.argmax(torch.sum(focus_output*classification_output,dim=1),dim=1)\n",
    "# accuracy = (torch.sum(prediction == labels,dim=0)/len(labels) )*100\n",
    "# print(\"Accuracy\", accuracy.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fdb865a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-21T04:33:19.029089Z",
     "iopub.status.busy": "2022-03-21T04:33:19.028503Z",
     "iopub.status.idle": "2022-03-21T04:33:19.031971Z",
     "shell.execute_reply": "2022-03-21T04:33:19.031459Z",
     "shell.execute_reply.started": "2022-03-20T09:06:07.629136Z"
    },
    "papermill": {
     "duration": 0.177737,
     "end_time": "2022-03-21T04:33:19.032080",
     "exception": false,
     "start_time": "2022-03-21T04:33:18.854343",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.49005255103111267, 0.4344673156738281, 0.43672195076942444, 0.45292016863822937, 0.4777691066265106, 0.425418496131897, 0.3968008756637573, 0.4426251947879791, 0.4040413498878479, 0.4525132179260254, 0.44591185450553894, 0.41897985339164734, 0.48306459188461304, 0.479790061712265, 0.43480074405670166, 0.398268461227417, 0.43900367617607117, 0.4704599380493164, 0.44323042035102844, 0.4540591537952423, 0.4061683416366577, 0.5380236506462097, 0.356527715921402, 0.43257296085357666, 0.4331657290458679, 0.47869181632995605, 0.4694206118583679, 0.46496328711509705, 0.47125503420829773, 0.4375845193862915, 0.43301859498023987, 0.44219934940338135, 0.46042507886886597, 0.5145013332366943, 0.5153147578239441, 0.4046565890312195, 0.5075059533119202, 0.5282624959945679, 0.6193235516548157, 0.49938634037971497]\n"
     ]
    }
   ],
   "source": [
    "print(focus_epoch_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8048a51",
   "metadata": {
    "papermill": {
     "duration": 0.166978,
     "end_time": "2022-03-21T04:33:19.366935",
     "exception": false,
     "start_time": "2022-03-21T04:33:19.199957",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "> **Test data Evaluation**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4d80848e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-21T04:33:19.712507Z",
     "iopub.status.busy": "2022-03-21T04:33:19.704282Z",
     "iopub.status.idle": "2022-03-21T04:33:21.901824Z",
     "shell.execute_reply": "2022-03-21T04:33:21.902793Z",
     "shell.execute_reply.started": "2022-03-20T09:06:13.618167Z"
    },
    "papermill": {
     "duration": 2.368635,
     "end_time": "2022-03-21T04:33:21.903008",
     "exception": false,
     "start_time": "2022-03-21T04:33:19.534373",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Focus True 36.39\n",
      "Accuracy 56.37\n"
     ]
    }
   ],
   "source": [
    "# method 1\n",
    "predicted_indexes = []\n",
    "foreground_index_list = []\n",
    "prediction_list = []\n",
    "labels_list = []\n",
    "with torch.no_grad():\n",
    "    for j,data in enumerate(test_loader):\n",
    "        images,labels,foreground_index = data\n",
    "        images = images.to(device)\n",
    "        foreground_index_list.append(foreground_index.numpy())\n",
    "        labels_list.append(labels.numpy())\n",
    "        batch = images.size(0)\n",
    "        scores = focus(images)\n",
    "        indexes = torch.argmax(F.softmax(scores,dim=1),dim=1).cpu().numpy()\n",
    "        predicted_indexes.append(indexes)\n",
    "        outputs = F.softmax(classification(images[np.arange(batch),indexes,:]),dim=1)\n",
    "        prediction = torch.argmax(outputs,dim=1)\n",
    "        prediction_list.append(prediction.cpu().numpy())\n",
    "    \n",
    "predicted_indexes = np.concatenate(predicted_indexes,axis=0)\n",
    "foreground_index_list = np.concatenate(foreground_index_list,axis=0)\n",
    "prediction_list = np.concatenate(prediction_list,axis=0)\n",
    "labels_list = np.concatenate(labels_list,axis=0)\n",
    "    \n",
    "\n",
    "print(\"Focus True\",(np.sum(predicted_indexes == foreground_index_list,axis=0).item()/\n",
    "                    len(foreground_index_list))*100)\n",
    "accuracy = (np.sum(prediction_list == labels_list,axis=0)/len(labels_list) )*100\n",
    "print(\"Accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f5046f08",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-21T04:33:22.260634Z",
     "iopub.status.busy": "2022-03-21T04:33:22.259775Z",
     "iopub.status.idle": "2022-03-21T04:33:24.685558Z",
     "shell.execute_reply": "2022-03-21T04:33:24.684957Z",
     "shell.execute_reply.started": "2022-03-20T09:06:18.105584Z"
    },
    "papermill": {
     "duration": 2.604987,
     "end_time": "2022-03-21T04:33:24.685733",
     "exception": false,
     "start_time": "2022-03-21T04:33:22.080746",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Focus True 36.39\n",
      "Accuracy 56.620000000000005\n"
     ]
    }
   ],
   "source": [
    "# method 2\n",
    "predicted_indexes = []\n",
    "foreground_index_list = []\n",
    "prediction_list = []\n",
    "labels_list = []\n",
    "with torch.no_grad():\n",
    "    for j,data in enumerate(test_loader):\n",
    "        images,labels,foreground_index = data\n",
    "        images = images.to(device)\n",
    "        batch = images.size(0)\n",
    "        patches = images.size(1)\n",
    "        foreground_index_list.append(foreground_index.numpy())\n",
    "        labels_list.append(labels.numpy())\n",
    "        batch = images.size(0)\n",
    "        focus_outputs = F.softmax(focus(images),dim=1)\n",
    "        indexes = torch.argmax(focus_outputs,dim=1).cpu().numpy()\n",
    "        predicted_indexes.append(indexes)\n",
    "        images = images.reshape(batch*patches,3,32,32)\n",
    "        classification_outputs = F.softmax(classification(images),dim=1)\n",
    "        classification_outputs = classification_outputs.reshape(batch,patches,3)\n",
    "\n",
    "        #print(classification_outputs.shape,focus_outputs.shape)\n",
    "\n",
    "        prediction = torch.argmax(torch.sum(focus_outputs[:,:,None]*classification_outputs,dim=1),dim=1)\n",
    "        prediction_list.append(prediction.cpu().numpy())\n",
    "    \n",
    "predicted_indexes = np.concatenate(predicted_indexes,axis=0)\n",
    "foreground_index_list = np.concatenate(foreground_index_list,axis=0)\n",
    "prediction_list = np.concatenate(prediction_list,axis=0)\n",
    "labels_list = np.concatenate(labels_list,axis=0)\n",
    "    \n",
    "\n",
    "print(\"Focus True\",(np.sum(predicted_indexes == foreground_index_list,axis=0).item()/\n",
    "                    len(foreground_index_list))*100)\n",
    "accuracy = (np.sum(prediction_list == labels_list,axis=0)/len(labels_list) )*100\n",
    "print(\"Accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11296546",
   "metadata": {
    "papermill": {
     "duration": 0.168869,
     "end_time": "2022-03-21T04:33:25.023257",
     "exception": false,
     "start_time": "2022-03-21T04:33:24.854388",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a97568c",
   "metadata": {
    "papermill": {
     "duration": 0.166404,
     "end_time": "2022-03-21T04:33:25.356079",
     "exception": false,
     "start_time": "2022-03-21T04:33:25.189675",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804e7090",
   "metadata": {
    "papermill": {
     "duration": 0.172108,
     "end_time": "2022-03-21T04:33:25.701414",
     "exception": false,
     "start_time": "2022-03-21T04:33:25.529306",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1772.067526,
   "end_time": "2022-03-21T04:33:27.684093",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-03-21T04:03:55.616567",
   "version": "2.3.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "674b5225ad7f4557bee5b779fd051e7c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_f001e4864e7c413fb44bc2ae5f60f2d1",
       "placeholder": "​",
       "style": "IPY_MODEL_8b8a6d2eaacf4abba48bece67e3abcc1",
       "value": " 170499072/? [00:12&lt;00:00, 16629113.84it/s]"
      }
     },
     "702c598578a648529dd8ffeaab9ba9c7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "776b4c2dfe754b589b12b45682460648": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_9d128d537a774bd7859de738bda52b04",
       "max": 170498071.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_9d95d3f07fd746d68e0585605730e72a",
       "value": 170498071.0
      }
     },
     "8b8a6d2eaacf4abba48bece67e3abcc1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "9d128d537a774bd7859de738bda52b04": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9d95d3f07fd746d68e0585605730e72a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "b5509c1fad9c4669b7baee40b894b960": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b6ef1ea2023f473e8f4b42bd7e19a6b7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_c5747dc85d3c40b48d172ecc7581a549",
        "IPY_MODEL_776b4c2dfe754b589b12b45682460648",
        "IPY_MODEL_674b5225ad7f4557bee5b779fd051e7c"
       ],
       "layout": "IPY_MODEL_b5509c1fad9c4669b7baee40b894b960"
      }
     },
     "c5747dc85d3c40b48d172ecc7581a549": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_fa853fa25a09474b95f404ca550ad53b",
       "placeholder": "​",
       "style": "IPY_MODEL_702c598578a648529dd8ffeaab9ba9c7",
       "value": ""
      }
     },
     "f001e4864e7c413fb44bc2ae5f60f2d1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "fa853fa25a09474b95f404ca550ad53b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

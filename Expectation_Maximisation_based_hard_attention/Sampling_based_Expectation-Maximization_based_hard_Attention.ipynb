{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51664248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "%matplotlib inline\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm as tqdm\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91400c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Focus(nn.Module):\n",
    "    def __init__(self,input_dims):\n",
    "        super(Focus,self).__init__()\n",
    "        self.input_dims = input_dims\n",
    "        self.fc1 = nn.Linear(input_dims,1,bias=False)\n",
    "    def forward(self,x):\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00fa5275",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classification(nn.Module):\n",
    "    def __init__(self,input_dims,output_dims):\n",
    "        super(Classification,self).__init__()\n",
    "        self.input_dims = input_dims\n",
    "        self.output_dims = output_dims\n",
    "        self.fc1 = nn.Linear(input_dims,output_dims)\n",
    "    def forward(self,x):\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        if self.output_dims > 1:\n",
    "            x = F.softmax(x,dim=1)\n",
    "        else:\n",
    "            x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e873cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expectation_step(fc,cl,data):\n",
    "    with torch.no_grad():\n",
    "        outputs_f = F.softmax(fc(data),dim=1)\n",
    "        outputs_g = cl(data)\n",
    "    \n",
    "    p_x_y_z = outputs_f*(1-outputs_g)\n",
    "    #print(p_x_y_z)\n",
    "    #print(torch.sum(p_x_y_z,dim=1,keepdims=True))\n",
    "    \n",
    "    normalized_p = p_x_y_z/torch.sum(p_x_y_z,dim=1,keepdims=True)\n",
    "    #print(normalized_p)\n",
    "    return normalized_p[:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8cbcd09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def maximization_step(p_z,focus,classification,data,labels,focus_optimizer,classification_optimizer,Criterion):\n",
    "   \n",
    "    torch.manual_seed(1234)\n",
    "    Z = torch.multinomial(p_z,10,replacement=True)\n",
    "    Z = Z.reshape((2*10))  # number of data points*number of samples\n",
    "    data_repeat = data.repeat_interleave(torch.tensor([10]),dim=0)\n",
    "    \n",
    "    \n",
    "    # classification module data\n",
    "    X_Z = data_repeat[np.arange(20),Z] # batch size*number_of_z\n",
    "    Y_Z = labels.repeat_interleave(torch.tensor([10]))\n",
    "    #print(data_repeat.dtype,X_Z.dtype)\n",
    "    \n",
    "    \n",
    "    # focus module data\n",
    "    Y_fc = torch.zeros((10*2,3))\n",
    "    Y_fc[np.arange(10*2),Z] = 1\n",
    "    X_fc = data_repeat.reshape((20*3,1))\n",
    "    Y_fc = Y_fc.reshape(20*3)\n",
    "    \n",
    "    \n",
    "    focus_optimizer.zero_grad()\n",
    "    classification_optimizer.zero_grad()\n",
    "    \n",
    "    focus_outputs = torch.sigmoid(focus(X_fc))\n",
    "    classification_outputs = classification(X_Z) # classification returns output after sigmoid/softmax\n",
    "    \n",
    "    \n",
    "    #print(focus_outputs,classification_outputs)\n",
    "    \n",
    "    \n",
    "    loss_focus = Criterion(focus_outputs[:,0],Y_fc)\n",
    "    loss_classification = Criterion(classification_outputs[:,0],Y_Z) \n",
    "    \n",
    "    print(\"Focus loss\",loss_focus.item())\n",
    "    print(\"Classification loss\",loss_classification.item())\n",
    "    \n",
    "    loss_focus.backward() \n",
    "    loss_classification.backward()\n",
    "    \n",
    "    focus_optimizer.step()\n",
    "    classification_optimizer.step()\n",
    "    \n",
    "    return focus,classification,focus_optimizer,classification_optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e032d3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3, 1]), torch.Size([2]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.tensor([[[3.],[3.],[-1.]],[[3.],[3.],[+1.]]])\n",
    "labels = torch.tensor([0.,1.])\n",
    "data.shape,labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4427b8f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Focus loss 0.6931472420692444\n",
      "Classification loss 0.4235605299472809\n",
      "Focus loss 0.6239665150642395\n",
      "Classification loss 0.42342910170555115\n",
      "Focus loss 0.5641141533851624\n",
      "Classification loss 0.28625819087028503\n",
      "Focus loss 0.513045072555542\n",
      "Classification loss 0.2988192141056061\n",
      "Focus loss 0.47748252749443054\n",
      "Classification loss 0.29813408851623535\n",
      "Focus loss 0.4509064257144928\n",
      "Classification loss 0.29745179414749146\n",
      "Focus loss 0.4306432604789734\n",
      "Classification loss 0.2967722713947296\n",
      "Focus loss 0.4149051606655121\n",
      "Classification loss 0.2960955500602722\n",
      "Focus loss 0.40247687697410583\n",
      "Classification loss 0.29542168974876404\n",
      "Focus loss 0.3925171196460724\n",
      "Classification loss 0.29475054144859314\n",
      "Focus loss 0.38443121314048767\n",
      "Classification loss 0.29408207535743713\n",
      "Focus loss 0.3777911961078644\n",
      "Classification loss 0.29341641068458557\n",
      "Focus loss 0.3722832500934601\n",
      "Classification loss 0.2927533984184265\n",
      "Focus loss 0.36767318844795227\n",
      "Classification loss 0.2920931279659271\n",
      "Focus loss 0.36378395557403564\n",
      "Classification loss 0.2914355397224426\n",
      "Focus loss 0.3604794144630432\n",
      "Classification loss 0.29078057408332825\n",
      "Focus loss 0.35765376687049866\n",
      "Classification loss 0.29012829065322876\n",
      "Focus loss 0.35522380471229553\n",
      "Classification loss 0.289478600025177\n",
      "Focus loss 0.3531232476234436\n",
      "Classification loss 0.2888316512107849\n",
      "Focus loss 0.3512989580631256\n",
      "Classification loss 0.2881871163845062\n",
      "Focus loss 0.3497079312801361\n",
      "Classification loss 0.2875453233718872\n",
      "Focus loss 0.3483148217201233\n",
      "Classification loss 0.28690606355667114\n",
      "Focus loss 0.34709078073501587\n",
      "Classification loss 0.28626930713653564\n",
      "Focus loss 0.34601178765296936\n",
      "Classification loss 0.28563517332077026\n",
      "Focus loss 0.3450576663017273\n",
      "Classification loss 0.2850034236907959\n",
      "Focus loss 0.3442119359970093\n",
      "Classification loss 0.2843744158744812\n",
      "Focus loss 0.3434601128101349\n",
      "Classification loss 0.28374776244163513\n",
      "Focus loss 0.34279024600982666\n",
      "Classification loss 0.28312361240386963\n",
      "Focus loss 0.34219223260879517\n",
      "Classification loss 0.2825019955635071\n",
      "Focus loss 0.3416571319103241\n",
      "Classification loss 0.2818828225135803\n",
      "Focus loss 0.34117746353149414\n",
      "Classification loss 0.2812659740447998\n",
      "Focus loss 0.34074676036834717\n",
      "Classification loss 0.2806517481803894\n",
      "Focus loss 0.3403593599796295\n",
      "Classification loss 0.28003981709480286\n",
      "Focus loss 0.34001046419143677\n",
      "Classification loss 0.2794302701950073\n",
      "Focus loss 0.339695543050766\n",
      "Classification loss 0.27882325649261475\n",
      "Focus loss 0.3394111692905426\n",
      "Classification loss 0.27821850776672363\n",
      "Focus loss 0.339153915643692\n",
      "Classification loss 0.2776161730289459\n",
      "Focus loss 0.3389210104942322\n",
      "Classification loss 0.2770161032676697\n",
      "Focus loss 0.3387097716331482\n",
      "Classification loss 0.276418536901474\n",
      "Focus loss 0.3385182321071625\n",
      "Classification loss 0.275823175907135\n",
      "Focus loss 0.33834409713745117\n",
      "Classification loss 0.2752301096916199\n",
      "Focus loss 0.33818575739860535\n",
      "Classification loss 0.27463942766189575\n",
      "Focus loss 0.3380416929721832\n",
      "Classification loss 0.27405092120170593\n",
      "Focus loss 0.3379104435443878\n",
      "Classification loss 0.2734648287296295\n",
      "Focus loss 0.33779075741767883\n",
      "Classification loss 0.272880882024765\n",
      "Focus loss 0.3376816213130951\n",
      "Classification loss 0.27229928970336914\n",
      "Focus loss 0.33758196234703064\n",
      "Classification loss 0.2717198133468628\n",
      "Focus loss 0.337490975856781\n",
      "Classification loss 0.2711426019668579\n",
      "Focus loss 0.3374077081680298\n",
      "Classification loss 0.2705675959587097\n",
      "Focus loss 0.337331622838974\n",
      "Classification loss 0.269994854927063\n",
      "Focus loss 0.33726194500923157\n",
      "Classification loss 0.2694242596626282\n",
      "Focus loss 0.3371981680393219\n",
      "Classification loss 0.26885586977005005\n",
      "Focus loss 0.33713966608047485\n",
      "Classification loss 0.2682896554470062\n",
      "Focus loss 0.33708611130714417\n",
      "Classification loss 0.26772555708885193\n",
      "Focus loss 0.33703699707984924\n",
      "Classification loss 0.26716357469558716\n",
      "Focus loss 0.3369918465614319\n",
      "Classification loss 0.2666037678718567\n",
      "Focus loss 0.3369504511356354\n",
      "Classification loss 0.26604607701301575\n",
      "Focus loss 0.33691248297691345\n",
      "Classification loss 0.26549050211906433\n",
      "Focus loss 0.3368775546550751\n",
      "Classification loss 0.26493701338768005\n",
      "Focus loss 0.3368454575538635\n",
      "Classification loss 0.2643855810165405\n",
      "Focus loss 0.3368159830570221\n",
      "Classification loss 0.2638362944126129\n",
      "Focus loss 0.33678877353668213\n",
      "Classification loss 0.26328906416893005\n",
      "Focus loss 0.3367639183998108\n",
      "Classification loss 0.2627437710762024\n",
      "Focus loss 0.3367408812046051\n",
      "Classification loss 0.26220065355300903\n",
      "Focus loss 0.33671972155570984\n",
      "Classification loss 0.2616594135761261\n",
      "Focus loss 0.33670032024383545\n",
      "Classification loss 0.2611202597618103\n",
      "Focus loss 0.33668234944343567\n",
      "Classification loss 0.26058319211006165\n",
      "Focus loss 0.30586880445480347\n",
      "Classification loss 0.2722315490245819\n",
      "Focus loss 0.3056632876396179\n",
      "Classification loss 0.27166393399238586\n",
      "Focus loss 0.30547377467155457\n",
      "Classification loss 0.2710985839366913\n",
      "Focus loss 0.3052988648414612\n",
      "Classification loss 0.2705352306365967\n",
      "Focus loss 0.30513718724250793\n",
      "Classification loss 0.2699739634990692\n",
      "Focus loss 0.3049878776073456\n",
      "Classification loss 0.26941460371017456\n",
      "Focus loss 0.3048497438430786\n",
      "Classification loss 0.26885727047920227\n",
      "Focus loss 0.3047219216823578\n",
      "Classification loss 0.26830199360847473\n",
      "Focus loss 0.3046034276485443\n",
      "Classification loss 0.26774874329566956\n",
      "Focus loss 0.30449363589286804\n",
      "Classification loss 0.2671973705291748\n",
      "Focus loss 0.30439186096191406\n",
      "Classification loss 0.2666481137275696\n",
      "Focus loss 0.30429741740226746\n",
      "Classification loss 0.26610079407691956\n",
      "Focus loss 0.30420970916748047\n",
      "Classification loss 0.26555535197257996\n",
      "Focus loss 0.3041283190250397\n",
      "Classification loss 0.2650119662284851\n",
      "Focus loss 0.30405253171920776\n",
      "Classification loss 0.26447048783302307\n",
      "Focus loss 0.3039821684360504\n",
      "Classification loss 0.2639308273792267\n",
      "Focus loss 0.3039167523384094\n",
      "Classification loss 0.26339319348335266\n",
      "Focus loss 0.3038558065891266\n",
      "Classification loss 0.26285746693611145\n",
      "Focus loss 0.3037990629673004\n",
      "Classification loss 0.26232361793518066\n",
      "Focus loss 0.3037462532520294\n",
      "Classification loss 0.2617916762828827\n",
      "Focus loss 0.30369704961776733\n",
      "Classification loss 0.2612617015838623\n",
      "Focus loss 0.30365118384361267\n",
      "Classification loss 0.26073339581489563\n",
      "Focus loss 0.30360835790634155\n",
      "Classification loss 0.2602071464061737\n",
      "Focus loss 0.3035684823989868\n",
      "Classification loss 0.25968268513679504\n",
      "Focus loss 0.3035312592983246\n",
      "Classification loss 0.2591600716114044\n",
      "Focus loss 0.30349647998809814\n",
      "Classification loss 0.2586393356323242\n",
      "Focus loss 0.30346402525901794\n",
      "Classification loss 0.2581203281879425\n",
      "Focus loss 0.3034338057041168\n",
      "Classification loss 0.25760334730148315\n",
      "Focus loss 0.30340543389320374\n",
      "Classification loss 0.25708797574043274\n",
      "Focus loss 0.3033790588378906\n",
      "Classification loss 0.2565745413303375\n",
      "Focus loss 0.30335432291030884\n",
      "Classification loss 0.256062775850296\n",
      "Focus loss 0.30333125591278076\n",
      "Classification loss 0.25555285811424255\n",
      "Focus loss 0.3033096194267273\n",
      "Classification loss 0.25504475831985474\n"
     ]
    }
   ],
   "source": [
    "focus = Focus(1)\n",
    "focus.fc1.weight.data = torch.tensor([[0.]])\n",
    "classification = Classification(1,1)\n",
    "classification.fc1.weight.data = torch.tensor([[1.]])\n",
    "classification.fc1.bias.data = torch.tensor([0.])\n",
    "\n",
    "Criterion = nn.BCELoss()\n",
    "focus_optimizer = optim.SGD(focus.parameters(), lr=0.1)\n",
    "classification_optimizer = optim.SGD(classification.parameters(),lr=0.01)\n",
    "\n",
    "for i in range(100):\n",
    "    p_z = expectation_step(focus,classification,data)\n",
    "    #print(p_z.shape)\n",
    "    focus,classification,focus_optimizer,classification_optimizer=maximization_step(p_z\n",
    "                                                                                ,focus,classification,data,\n",
    "                                                                                labels,focus_optimizer,\n",
    "                                                                                classification_optimizer,\n",
    "                                                                                Criterion)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7688a5ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-1.0120]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for params in focus.parameters():\n",
    "    print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "43a91ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[1.2384]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0073], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for params in classification.parameters():\n",
    "    print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "15a421eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0169],\n",
       "         [0.0169],\n",
       "         [0.9663]],\n",
       "\n",
       "        [[0.1045],\n",
       "         [0.1045],\n",
       "         [0.7910]]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(focus(data),dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f20f7cc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-3.0359],\n",
       "         [-3.0359],\n",
       "         [ 1.0120]],\n",
       "\n",
       "        [[-3.0359],\n",
       "         [-3.0359],\n",
       "         [-1.0120]]], grad_fn=<UnsafeViewBackward>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "focus(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e07ca3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198dfab7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
